[
 {
  "repo": "gocodeup/codeup-setup-script",
  "language": "Shell",
  "readme_contents": "# Codeup Setup Script\n\nSetup script for Codeup students' laptops to install the all the tools we will\nneed for the java course. We will install the following:\n\n- [xcode](https://developer.apple.com/xcode/features/): command line tools for\n  macs\n- [brew](http://brew.sh/): package manager for macs\n- [java](https://en.wikipedia.org/wiki/Java_(programming_language))\n- [tomcat](http://tomcat.apache.org/): the java webserver\n- [maven](https://maven.apache.org/): a java dependency and build management tool\n- [mysql](https://www.mysql.com/): the database we'll use for the class\n- [node js](https://nodejs.org/en/): a JavaScript runtime outside of the browser\n- [npm](https://www.npmjs.com/): a package manager for JavaScript\n\nIn addition, we will:\n\n- setup ssh keys for the student's laptop and guide them through the process of\n  linking their ssh key to their Github account.\n- Setup a global gitignore file and set the default commit editor to `nano`\n  (only if these are not already set)\n\n## For Students\n\nCopy and paste the following in your terminal:\n\n```\nbash -c \"$(curl -sS https://raw.githubusercontent.com/gocodeup/codeup-setup-script/master/install.sh)\"\n```\n\n## Note for Instructors\n\nIf students already have and `id_rsa` ssh key generated the script will *not*\ntry to generate a new ones, and you will need to walk them through the process\nof adding their existing key to Github.\n\nThe following should do the trick if they already have a ssh key pair, but it's\nnot wired up to Github.\n\n```bash\npbcopy < ~/.ssh/id_rsa.pub\nopen https://github.com/settings/ssh\n```\n"
 },
 {
  "repo": "gocodeup/movies-application",
  "language": "JavaScript",
  "readme_contents": "# Movies Application\n\nFor this project, we will be building a single page movie application (SPA). It\nwill allow users to add, edit, and delete movies, as well as rate them. We will\nbe using [`json-server`](https://github.com/typicode/json-server) to mock a\ndatabase and our backend, so that we can just worry about the front end and AJAX\nrequests.\n\nThe webpack dev server is configured to watch for changes both in the javascript\nsource, as well as the `public` directory. Whenever changes are detected, the\npage will be reloaded. It will also proxy any requests that start with `/api` to\nlocalhost:3000, which is where json-server is configured run.\n\n`json-server` is configured to have a delay of 1.2 seconds, so you can see what\nyour application might actually look like, instead of serving instantaneous\nreponses. You can modify this behavior by changing (or removing) the number\nafter the `-d` flag inside of the npm `dev` script.\n\n## Setup\n\n1. Fork this repository to your own github account or your github organization.\n\n1. Clone your fork locally into your computer.\n\n1. Run `npm install`\n\n1. Run `npm run build`\n\n1. Run `npm run dev` and visit\n   [http://localhost:1313/](http://localhost:1313/) in your browser. Open up\n   the console and inspect the output.\n\n## Development\n\nIn general, you should have the development web server (`npm run dev`) running\nwhile you are working on the project. You should view your project through\nhttp://localhost:1313, **not** from IntelliJ's web server, or by dragging the\nfile(s) into chrome. As you make changes to your source files, all you need to\ndo is save the file, and the website will be live reloaded.\n\nTake a look at the `src/index.js` file to get started. You will notice it has\nexamples of importing and requiring separate javascript files. Regardless of\nwhich you choose to use, you should pick one and use it throughout your code\nbase, don't mix and match the two.\n\nThe file `src/api.js` also contains an example api request. You can\nreference this to get started building out the parts of your application that\ninteract with the api.\n\nThe `db.json` file contains your \"database\". You can edit this file directly to\nmake changes to your data, and this file will be updated if you make api\nrequests that modify the data.\n\nThe server will serve files from the `public` directory, meaning any files\noutside of `public` will not be visible. This means if you have any frontend\nassets (e.g. bootstrap, or images) they will need to be in the `public`\ndirectory.\n\n## Specification\n\nYour application should:\n\nOn page load:\n\n- Display a \"loading...\" message\n- Make an ajax request to get a listing of all the movies\n- When the initial ajax request comes back, remove the \"loading...\" message\n  and replace it with HTML generated from the json response your code\n  receives\n\nAllow users to add new movies\n\n- Create a form for adding a new movie that has fields for the movie's title\n  and rating\n- When the form is submitted, the page should **not** reload / refresh,\n  instead, your javascript should make a POST request to `/api/movies` with the\n  information the user put into the form\n\nAllow users to edit existing movies\n\n- Give users the option to edit an existing movie\n- A form should be pre-populated with the selected movie's details\n- Like creating a movie, this should not involve any page reloads, instead\n  your javascript code should make an ajax request when the form is\n  submitted.\n\nDelete movies\n\n- Each movie should have a \"delete\" button\n- When this button is clicked, your javascript should send a `DELETE` request\n\n### Bonuses\n\n- Add a `disabled` attribute to buttons while their corresponding ajax request\n  is still pending.\n- Show a loading animation instead of just text that says \"loading...\"\n- Use modals for the creating and editing movie forms\n- Add a `genre` property to every movie\n- Allow users to sort the movies by rating, title, or genre (if you have it)\n- Allow users to search through the movies by rating, title, or genre (if you\n  have it)\n\n## Helpful Hints\n\n- The id property of every movie should not be edited by hand. The purpose of\n  this property is to uniquely identify that particular movie. That is, if we\n  want to delete or modify an existing movie, we can specify what movie we want\n  to change by referencing it's id. When a new movie is created (i.e.  when you\n  send a `POST` request to `/api/movies` with a title and a rating), the server\n  will respond with the movie object that was created, including a generated id.\n- Take a look at the other branches in this repository, as they have\n  configuration/setup for common scenarios, such as including bootstrap in your\n  application.\n"
 },
 {
  "repo": "torvalds/linux",
  "language": "C",
  "readme_contents": "Linux kernel\n============\n\nThere are several guides for kernel developers and users. These guides can\nbe rendered in a number of formats, like HTML and PDF. Please read\nDocumentation/admin-guide/README.rst first.\n\nIn order to build the documentation, use ``make htmldocs`` or\n``make pdfdocs``.  The formatted documentation can also be read online at:\n\n    https://www.kernel.org/doc/html/latest/\n\nThere are various text files in the Documentation/ subdirectory,\nseveral of them using the Restructured Text markup notation.\n\nPlease read the Documentation/process/changes.rst file, as it contains the\nrequirements for building and running the kernel, and information about\nthe problems which may result by upgrading your kernel.\n"
 },
 {
  "repo": "beetbox/beets",
  "language": "Python",
  "readme_contents": ".. image:: https://img.shields.io/pypi/v/beets.svg\n    :target: https://pypi.python.org/pypi/beets\n\n.. image:: https://img.shields.io/codecov/c/github/beetbox/beets.svg\n    :target: https://codecov.io/github/beetbox/beets\n\n.. image:: https://github.com/beetbox/beets/workflows/ci/badge.svg?branch=master\n    :target: https://github.com/beetbox/beets/actions\n\n.. image:: https://repology.org/badge/tiny-repos/beets.svg\n    :target: https://repology.org/project/beets/versions\n\n\nbeets\n=====\n\nBeets is the media library management system for obsessive music geeks.\n\nThe purpose of beets is to get your music collection right once and for all.\nIt catalogs your collection, automatically improving its metadata as it goes.\nIt then provides a bouquet of tools for manipulating and accessing your music.\n\nHere's an example of beets' brainy tag corrector doing its thing::\n\n  $ beet import ~/music/ladytron\n  Tagging:\n      Ladytron - Witching Hour\n  (Similarity: 98.4%)\n   * Last One Standing      -> The Last One Standing\n   * Beauty                 -> Beauty*2\n   * White Light Generation -> Whitelightgenerator\n   * All the Way            -> All the Way...\n\nBecause beets is designed as a library, it can do almost anything you can\nimagine for your music collection. Via `plugins`_, beets becomes a panacea:\n\n- Fetch or calculate all the metadata you could possibly need: `album art`_,\n  `lyrics`_, `genres`_, `tempos`_, `ReplayGain`_ levels, or `acoustic\n  fingerprints`_.\n- Get metadata from `MusicBrainz`_, `Discogs`_, and `Beatport`_. Or guess\n  metadata using songs' filenames or their acoustic fingerprints.\n- `Transcode audio`_ to any format you like.\n- Check your library for `duplicate tracks and albums`_ or for `albums that\n  are missing tracks`_.\n- Clean up crufty tags left behind by other, less-awesome tools.\n- Embed and extract album art from files' metadata.\n- Browse your music library graphically through a Web browser and play it in any\n  browser that supports `HTML5 Audio`_.\n- Analyze music files' metadata from the command line.\n- Listen to your library with a music player that speaks the `MPD`_ protocol\n  and works with a staggering variety of interfaces.\n\nIf beets doesn't do what you want yet, `writing your own plugin`_ is\nshockingly simple if you know a little Python.\n\n.. _plugins: https://beets.readthedocs.org/page/plugins/\n.. _MPD: https://www.musicpd.org/\n.. _MusicBrainz music collection: https://musicbrainz.org/doc/Collections/\n.. _writing your own plugin:\n    https://beets.readthedocs.org/page/dev/plugins.html\n.. _HTML5 Audio:\n    http://www.w3.org/TR/html-markup/audio.html\n.. _albums that are missing tracks:\n    https://beets.readthedocs.org/page/plugins/missing.html\n.. _duplicate tracks and albums:\n    https://beets.readthedocs.org/page/plugins/duplicates.html\n.. _Transcode audio:\n    https://beets.readthedocs.org/page/plugins/convert.html\n.. _Discogs: https://www.discogs.com/\n.. _acoustic fingerprints:\n    https://beets.readthedocs.org/page/plugins/chroma.html\n.. _ReplayGain: https://beets.readthedocs.org/page/plugins/replaygain.html\n.. _tempos: https://beets.readthedocs.org/page/plugins/acousticbrainz.html\n.. _genres: https://beets.readthedocs.org/page/plugins/lastgenre.html\n.. _album art: https://beets.readthedocs.org/page/plugins/fetchart.html\n.. _lyrics: https://beets.readthedocs.org/page/plugins/lyrics.html\n.. _MusicBrainz: https://musicbrainz.org/\n.. _Beatport: https://www.beatport.com\n\nInstall\n-------\n\nYou can install beets by typing ``pip install beets``.\nBeets has also been packaged in the `software repositories`_ of several distributions.\nCheck out the `Getting Started`_ guide for more information.\n\n.. _Getting Started: https://beets.readthedocs.org/page/guides/main.html\n.. _software repositories: https://repology.org/project/beets/versions\n\nContribute\n----------\n\nThank you for considering contributing to ``beets``! Whether you're a programmer or not, you should be able to find all the info you need at `CONTRIBUTING.rst`_.\n\n.. _CONTRIBUTING.rst: https://github.com/beetbox/beets/blob/master/CONTRIBUTING.rst\n\nRead More\n---------\n\nLearn more about beets at `its Web site`_. Follow `@b33ts`_ on Twitter for\nnews and updates.\n\n.. _its Web site: https://beets.io/\n.. _@b33ts: https://twitter.com/b33ts/\n\nContact\n-------\n* Encountered a bug you'd like to report or have an idea for a new feature? Check out our `issue tracker`_! If your issue or feature hasn't already been reported, please `open a new ticket`_ and we'll be in touch with you shortly. If you'd like to vote on a feature/bug, simply give a :+1: on issues you'd like to see prioritized over others.\n* Need help/support, would like to start a discussion, or would just like to introduce yourself to the team? Check out our `forums`_!\n\n.. _issue tracker: https://github.com/beetbox/beets/issues\n.. _open a new ticket: https://github.com/beetbox/beets/issues/new/choose\n.. _forums: https://discourse.beets.io/\n\nAuthors\n-------\n\nBeets is by `Adrian Sampson`_ with a supporting cast of thousands.\n\n.. _Adrian Sampson: https://www.cs.cornell.edu/~asampson/\n"
 },
 {
  "repo": "scottschiller/SoundManager2",
  "language": "JavaScript",
  "readme_contents": "# SoundManager 2: JavaScript Sound for the Web \ud83d\udd0a\r\n\r\nBy wrapping and extending HTML5 and Flash Audio APIs, SoundManager 2 brings reliable cross-platform audio to JavaScript.\r\n\r\n## HTML5 `Audio()` Support\r\n\r\n* 100% Flash-free MP3 + MP4/AAC (and OGG, FLAC, etc.) where supported\r\n* Compatible with Apple iPad (iOS 3.2), iPhone/iOS 4 and newer\r\n* Fallback to Flash for MP3/MP4 support, if needed\r\n* SM2 API is transparent; HTML5/flash switching handled internally\r\n* HTML5 API support approximates Flash 8 API features\r\n\r\n## Basic API Features\r\n\r\n* Load, stop, play, pause, mute, seek, pan (Flash-only) and volume control of sounds from JavaScript\r\n* Events: `onload`, `whileloading`, `whileplaying`, `onfinish` and more\r\n\r\n## Flash-based Features (Legacy Support)\r\n\r\n* (Flash 8+): ID3V1 and ID3V2 tag support for MP3s (title, artist, genre etc.)\r\n* RTMP / Flash Media Server streaming support\r\n* MPEG-4 (AAC, HE-AAC, H.264) audio support\r\n* \"MultiShot\" play (layered / chorusing effects)\r\n* Waveform/frequency spectrum data\r\n* Peak (L/R channel volume) data\r\n* Audio buffering state / event handling\r\n\r\n## General Tech Stuff\r\n\r\n* Full API Documentation with examples and notes\r\n* `console.log()`-style debug output and troubleshooting tools\r\n* GitHub Issues for discussion/support\r\n\r\n## As Heard On The Internets\r\n\r\nSome places that do or have used SM2 include SoundCloud, Tidal, Beats, Songza, freesound.org, last.fm, 8tracks, Discogs, and The Hype Machine among others - but most importantly, http://nyan.cat. ;)\r\n\r\n## Project Home, Documentation, Live Demos etc.\r\n\r\nhttp://www.schillmania.com/projects/soundmanager2/\r\n\r\n## Compiling JS builds (-nodebug, -jsmin) and Flash components, AS2/AS3 to SWF\r\n\r\n_(Note: This process is pretty outdated and relies on ancient binaries for the Flash bits. Here be dragons.)_\r\n\r\nAn Ant build file defines the tasks for compiling JS and SWF components, useful if you make changes to the SM2 source and want to recompile.\r\nGoogle's Closure Compiler is used for the JS. AS2 compilation is done by MTASC, and AS3 is handled by Adobe's Open Source Flex SDK (mxmlc) compiler.\r\nRefer to `build.xml` for compiler downloads and path definitions.\r\n\r\n## Versioning / Development Notes\r\n\r\nReleases are versioned by date, e.g., `V2.97a.20170601` and are tagged as such.\r\nThe latest official release is always on trunk/master.\r\nPost-release development builds may be available on the appropriate +DEV branch, eg., `V2.97a.20170601+DEV`\r\n\r\n## Forks and Pull Requests\r\n\r\nFirstly, thank you for wanting to contribute! Bug fixes and tweaks are welcomed, particularly if they follow the general coding style of the project.\r\nIf making a pull request, use the project's current +DEV development branch as the merge target instead of \"master\", if possible; please and thank-you.\r\n\r\n## Random Trivia: SoundManager / SoundManager 2 History\r\n\r\nThe original \"SoundManager\" implementation was created in 2001 and used JavaScript and Flash 6 (or thereabouts), and was hacked together to get JS-driven sound on a personal portfolio site. It was later used for the \"DHTML Arkanoid\" project in 2002.\r\n\r\nThe original inspiration came from the sonify.org \"Flashsound\" project; they had tutorials and examples on getting Flash to play sounds when an HTML element was hovered on. This was very up my alley at the time. It all started with a cheezy kung-fu demo.\r\n\r\nhttp://sonify.org/flashsound/kungfu/\r\n\r\nFlash's `ExternalInterface` API was not introduced until Flash 8, but a limited JS <-> Flash API existed via LiveConnect et al which still let the basics work. The original SoundManager used Flash's `TCallLabel()` methods, exposed to JS, to perform specific actions within \"Movie Clips\" (essentially, objects).\r\n\r\nMovie Clips contained Frames (in the animation sense) which could be given IDs (labels), and could also accept name/value parameters via `SetVariable()`. Thus, it was possible to create a Movie Clip for each sound, which would have a labeled frame for each sound action desired (stop, seek, pause, volume, and pan), e.g., `flashMovie.SetVariable('/MySound:volume, 50);`\r\n\r\nhttp://web.archive.org/web/20020404030916/http://sonify.org:80/flashsound/timeline/actionscript.html\r\n\r\nWhen a sound was created, playing it and setting parameters became a matter of having JS tell Flash to go to a specific frame within a Movie Clip to perform the desired \"action\" e.g., `flashMovie.TCallLabel('/soundID', 'play');` and then an additional call to set a variable if needed to apply effects like volume, pan and so on.\r\n\r\nInternet Explorer on the Mac did not support the JS/Flash API via LiveConnect etc., but Netscape on MacOS was OK.\r\n\r\nThe original SoundManager project still lives at http://schillmania.com/projects/soundmanager/ and was deprecated in 2007.\r\n\r\nSoundManager 2 became a reality when Flash 8 was released, as it introduced `ExternalInterface` which was a more complete JS <-> Flash API that allowed Flash methods to be exposed to JS, and could also accept full parameters. ExternalInterface is quite an interesting little hack, as the Flash movie injects some JS into the browser to make it work. Under the hood, it uses XML as a transport layer for calls. (Recall that in the 2000s, XML was hugely popular - the JSON of its day.)\r\n\r\nMore here on how SM2 / Flash / EI interaction worked.\r\n\r\nhttp://www.schillmania.com/content/entries/2010/how-soundmanager2-works/\r\n\r\nSoundManager 2 was released in 2006 and had a much more feature-rich and better-structured API, particularly at the time, thanks to learnings and feedback from the original SoundManager project. SM2 grew to be relatively popular among sites that used sound, whether as effects or a core part of the site experience. (Most sites used either SM2, or the jQuery-library-friendly jPlayer project.) \r\n\r\n## Why version 2.97?\r\n\r\nSoundManager 2 has been at \"version\" 2.97 for a long time, because 2.97 was arguably the best llama-ass-whipping version of WinAmp. (WinAmp 3 was not as good, and WinAmp 5 was \"the best of 2 and 3 combined.\") This MP3 player was my favourite Windows app during the 90's, and is missed as there's nothing quite like it on OS X where I spend most of my time these days.\r\n"
 },
 {
  "repo": "CreateJS/SoundJS",
  "language": "JavaScript",
  "readme_contents": "# SoundJS\r\n\r\nSoundJS is a library to make working with audio on the web easier. It provides a consistent API for playing audio in\r\ndifferent browsers, including using a target plugin model to provide an easy way to provide additional audio plugins\r\nlike a Flash fallback (included, but must be used separately from the combined/minified version).\r\n\r\nA mechanism has been provided for easily tying in audio preloading to [PreloadJS](http://createjs.com/preloadjs/).\r\n\r\n\r\n## Example\r\n\r\n```javascript\r\ncreatejs.Sound.on(\"fileload\", handleLoadComplete);\r\ncreatejs.Sound.alternateExtensions = [\"mp3\"];\r\ncreatejs.Sound.registerSound({src:\"path/to/sound.ogg\", id:\"sound\"});\r\nfunction handleLoadComplete(event) {\r\n\tcreatejs.Sound.play(\"sound\");\r\n}\r\n```\r\n\r\n## License\r\nBuilt by gskinner.com, and released for free under the MIT license, which means you can use it for almost any purpose\r\n(including commercial projects). We appreciate credit where possible, but it is not a requirement.\r\n\r\n\r\n## Support and Resources\r\n* Find examples and more information at the [SoundJS web site](http://soundjs.com/)\r\n* Read the [documentation](http://createjs.com/docs/soundjs/)\r\n* Discuss, share projects, and interact with other users on [reddit](http://www.reddit.com/r/createjs/).\r\n* Ask technical questions on [Stack Overflow](http://stackoverflow.com/questions/tagged/soundjs).\r\n* File verified bugs or formal feature requests using Issues on [GitHub](https://github.com/CreateJS/SoundJS/issues).\r\n* Have a look at the included [examples](https://github.com/CreateJS/SoundJS/tree/master/examples) and\r\n[API documentation](http://createjs.com/docs/soundjs/) for more in-depth information.\r\n\r\n\r\n## Classes\r\n\r\n### [Sound](http://createjs.com/Docs/SoundJS/classes/Sound.html)\r\nThe core API for playing sounds. Call createjs.Sound.play(sound, ...options), and a sound instance is created that can be\r\nused to control the audio, and dispatches events when it is complete, loops, or is interrupted.\r\n\r\n### [SoundInstance](http://createjs.com/Docs/SoundJS/classes/AbstractSoundInstance.html)\r\nA controllable sound object that wraps the actual plugin implementation, providing a consistent API for audio playback,\r\nno matter what happens in the background. Sound instances can be paused, muted, and stopped; and the volume, pan (where\r\navailable), and position changed using the simple API.\r\n\r\n### [WebAudioPlugin](http://createjs.com/Docs/SoundJS/classes/WebAudioPlugin.html)\r\nThe default, built-in plugin, which uses Web Audio APIs to playback sounds. Note that WebAudio will fail to load when\r\nrun locally, and the HTML audio plugin will be used instead.\r\n\r\n### [HTMLAudioPlugin](http://createjs.com/Docs/SoundJS/classes/HTMLAudioPlugin.html)\r\nThe fallback built-in plugin, which manages audio playback via the HTML5 <audio> tag. This will be used in instances\r\nwhere the WebAudio plugin is not available.\r\n\r\n### [CordovaAudioPlugin](http://createjs.com/docs/soundjs/classes/CordovaAudioPlugin.html)\r\nAn additional plugin which will playback audio in a Cordova app and tools that utilize Cordova such as PhoneGap or Ionic.\r\nYou must manually register this plugin. Currently available on github since SoundJS-0.6.1.\r\n\r\n### [FlashAudioPlugin](http://createjs.com/Docs/SoundJS/classes/FlashAudioPlugin.html)\r\nAn additional plugin which uses a flash shim (and SWFObject) to playback audio using Flash. You must manually set up and\r\nregister this plugin.\r\n\r\n## [Documentation and examples](http://createjs.com/docs/soundjs/)\r\nHave a look at the included examples and API documentation for more in-depth information.\r\n"
 },
 {
  "repo": "musescore/MuseScore",
  "language": "C++",
  "readme_contents": "![MuseScore](mscore/data/musescore_logo_full.png)  \nMusic notation and composition software\n\n[![Travis CI](https://secure.travis-ci.org/musescore/MuseScore.svg?branch=master)](https://travis-ci.org/musescore/MuseScore)\n[![Appveyor](https://ci.appveyor.com/api/projects/status/bp3ww6v985i64ece/branch/master?svg=true)](https://ci.appveyor.com/project/MuseScore/musescore/branch/master)\n[![License: GPL v2](https://img.shields.io/badge/License-GPL%20v2-blue.svg)](https://www.gnu.org/licenses/old-licenses/gpl-2.0.html)\n\nMuseScore is an open source and free music notation software. For support, contribution, and bug reports visit MuseScore.org. Fork and make pull requests!\n\n## Features\n\n- WYSIWYG design, notes are entered on a \"virtual notepaper\"\n- TrueType font(s) for printing & display allows for high quality scaling to all sizes\n- Easy & fast note entry\n- Many editing functions\n- MusicXML import/export\n- MIDI (SMF) import/export\n- MuseData import\n- MIDI input for note entry\n- Integrated sequencer and software synthesizer to play the score\n- Print or create pdf files\n\n## More info\n- [MuseScore Homepage](https://musescore.org)\n- [MuseScore Git workflow instructions](https://musescore.org/en/developers-handbook/git-workflow)\n- [How to compile MuseScore?](https://musescore.org/en/developers-handbook/compilation)\n\n## License\nMuseScore is licensed under GPL version 2.0. See [LICENSE.GPL](https://github.com/musescore/MuseScore/blob/master/LICENSE.GPL) in the same directory.\n\n## Packages\n- **aeolus:** Clone of [Aeolus](http://kokkinizita.linuxaudio.org/linuxaudio/aeolus/). Disabled by default in the stable releases. See http://dev-list.musescore.org/Aeolus-Organ-Synth-td7578364.html. Kept as an example of how to integrate with a complex synthesizer.\n\n- **assets:** Graphical assets, use them if you need a MuseScore icon. For logo, color, etc., see https://musescore.org/en/about/logos-and-graphics.\n\n- **awl:** Audio Widget Library, from the MusE project.\n\n- **build:** Utility files for build.\n\n- **bww2mxml:** Command line tool to convert BWW files to MusicXML. BWW parser is used by MuseScore to import BWW files.\n\n- **demos:** A few MuseScore files to demonstrate what can be done.\n\n- **fluid:** Clone of [FluidSynth@sourceforge](https://sourceforge.net/projects/fluidsynth), ported to C++ and customized. Code now at [fluidsynth@github](https://github.com/FluidSynth/fluidsynth).\n\n- **fonts:** Contains fontforge source (sfd) + ttf/otf fonts. MuseScore includes the \"Emmentaler\" font from the Lilypond project.\n\n- **libmscore:** Data model of MuseScore.\n\n- **mscore:** Main code for the MuseScore UI.\n\n- **msynth:** Abstract interface to Fluid + Aeolus.\n\n- **mtest:** Unit testing using QTest.\n\n- **omr:** Optical music recognition.\n\n- **share:** Files moved to /usr/share/... on install.\n\n- **test:** Old tests. Should move to mtest.\n\n- **vtest:** Visual tests. Compare reference images with current implementation.\n\n- **thirdparty:** Contains projects which are included for convenience, usually to integrate them into the build system to make them available for all supported platforms.\n\n    - **thirdparty/rtf2html:**\n    Used for capella import. Clone from [rtf2html@sourceforge](https://sourceforge.net/projects/rtf2html), code now at [rtf2html@github](https://github.com/lvu/rtf2html).\n\n    - **thirdparty/dtl:**\n    Used for the score comparison tool. [Diff Template Library](https://github.com/cubicdaiya/dtl).\n\n    - **thirdparty/ofqf:**\n    OSC server interface. Based on [OSC for Qt4](http://www.arnoldarts.de/projects/ofqf/), code now at [ofq@github](https://github.com/kampfschlaefer/ofq).\n\n    - **thirdparty/singleapp:**\n    Clone from [Qt Single Application](https://github.com/qtproject/qt-solutions/tree/master/qtsingleapplication).\n\n    - **thirdparty/portmidi:**\n    Clone from [PortMidi](https://sourceforge.net/projects/portmedia/).\n\n    - **thirdparty/beatroot:**\n    It's a core part of [BeatRoot Vamp Plugin](https://code.soundsoftware.ac.uk/projects/beatroot-vamp/repository) by Simon Dixon and Chris Cannam, used in MIDI import for beat detection.\n\n    - **thirdparty/qt-google-analytics:**\n    Clone from [qt-google-analytics](https://github.com/HSAnet/qt-google-analytics).\n\n    - **thirdparty/libcrashreporter-qt:**\n    Clone from [libcrashreporter-qt](https://github.com/dmitrio95/libcrashreporter-qt).\n\n\n## Building\n**Read the developer handbook for a [complete build walkthrough](https://musescore.org/en/developers-handbook/compilation) and a list of dependencies.**\n\n### Getting sources\nIf using git to download repo of entire code history, type:\n\n    git clone https://github.com/musescore/MuseScore.git\n    cd MuseScore\n\nOtherwise, you can just download the latest source release tarball from the [Releases page](https://github.com/musescore/MuseScore/releases), and then from your download directory type:\n\n    tar xzf MuseScore-x.x.x.tar.gz\n    cd MuseScore-x.x.x\n\n### Release Build\nTo compile MuseScore, type:\n\n    make release\n\nIf something goes wrong, then remove the whole build subdirectory with `make clean` and start new with `make release`.\n\n### Running\nTo start MuseScore, type:\n\n    ./build.release/mscore/mscore\n\nThe Start Center window will appear on every invocation until you disable that setting via the \"Preferences\" dialog.\n\n### Installing\nTo install to default prefix using root user, type:\n\n    sudo make install\n\n### Debug Build\nA debug version can be built by doing `make debug` instead of `make release`.\n\nTo run the debug version, type:\n\n    ./build.debug/mscore/mscore\n\n### Testing\nSee [mtest/README.md](/mtest/README.md) or [the developer handbook](https://musescore.org/handbook/developers-handbook/finding-your-way-around/automated-tests) for instructions on how to run the test suite.\n\nThe new [script testing facility](https://musescore.org/node/278278) is also available to create your own automated tests. Please try it out!\n\n### Code Formatting\n\nRun `./hooks/install.sh` to install a pre-commit hook that will format your staged files. Requires that you install `uncrustify`.\n\nIf you have problems, please report them. To uninstall, run `./hooks/uninstall.sh`.\n"
 },
 {
  "repo": "tomahawk-player/tomahawk",
  "language": "C++",
  "readme_contents": "# This project is essentially abandoned\nThere is no one working on it.\nThere isn't much sense in adding any new issues in the issue tracker unless you want to fix them yourself.\n\n# WHAT TOMAHAWK IS\n\nTomahawk is a free multi-source and cross-platform music player. An application that can play not only your local files, but also stream from services like Spotify, Beats, SoundCloud, Google Music, YouTube and many others. You can even connect with your friends' Tomahawks, share your musical gems or listen along with them. Let the music play!\n\n![Tomahawk Screenshot](/data/screenshots/tomahawk-screenshot.png?raw=true)\n\n## HOW TOMAHAWK WORKS\n\nTomahawk is basically a **player for music metadata**. At its core it decouples the metadata about a song from the source and reassembles it for each user based on their individual music accessibility and rights. In short, given the name of a song and artist, Tomahawk will find the right source, for the right user at the right time.  This fundamentally different approach to music enables a range of new music consumption and sharing experiences previously not possible.\n\n## MUSIC SOURCES\n\n* Local music library (MP3, Ogg, FLAC and many other formats)\n* Networked music libraries (other connected computers)\n\n### Subscription Music Services\n\n* Spotify\n* Beats Music\n* Google Play Music (on-demand streaming and music locker)\n* TIDAL\n* Rdio (Android only)\n* Deezer (Android only)\n\n### Free Streaming/Music Promotion Platforms\n\n* Soundcloud\n* Bandcamp\n* Last.fm\n* Jamendo\n* Official.fm\n* YouTube\n\n### Network/Cloud storage\n\n* Ampache\n* Owncloud\n* Subsonic\n* Beets\n\nThird party-developed resolvers have also been written for services like YouTube, Qobuz and others. We've also heard of digital music distributors writing their own for their internal CMSes to help them navigate and preview their content. That's cool.\n\nPackaged binary resolvers (.axes) are available: [here](http://teom.org/axes).\n\nSource code (and examples) can be found in our [Resolver repository](https://github.com/tomahawk-player/tomahawk-resolvers).\n\n## DOWNLOAD TOMAHAWK\n\nYou can download one of our nightly or stable builds:\n\n| *BUILD* | MAC / OSX | WINDOWS | LINUX |\n|:-------:|:---------:|:-------:|:-----:|\n|**NIGHTLY** | [**latest**](http://download.tomahawk-player.org/nightly/mac/Tomahawk-latest.dmg) | [**latest**](http://download.tomahawk-player.org/nightly/windows/tomahawk-latest.exe) | [**latest**](https://launchpad.net/~tomahawk/+archive/ubuntu/nightly) (Ubuntu) |\n|**STABLE** | [**0.8.4**](http://download.tomahawk-player.org/Tomahawk-0.8.4.dmg) | [**0.8.4**](http://download.tomahawk-player.org/tomahawk-0.8.4.exe) | [**0.8.4**](http://www.tomahawk-player.org/#page-about) (various distros) |\n\n## BUILD TOMAHAWK\n\n... or you can compile it yourself:\n\n    $ mkdir build && cd build\n    $ cmake ..\n    $ make\n\n### Detailed Build Instructions\n\n| Linux: | [Arch](https://github.com/tomahawk-player/tomahawk/wiki/ArchLinux---Build-Instructions) **-** [Debian](https://github.com/tomahawk-player/tomahawk/wiki/Debian-Build-Instructions) **-** [Fedora](https://github.com/tomahawk-player/tomahawk/wiki/Fedora-Build-Instructions) **-** [Ubuntu](https://github.com/tomahawk-player/tomahawk/wiki/Ubuntu---Build-Instructions) |\n|------:|:------|\n| **Windows**: | [**Windows**](https://github.com/tomahawk-player/tomahawk/wiki/Windows-Build-Instructions) |\n| **Mac**: | [**OS X**](https://github.com/tomahawk-player/tomahawk/wiki/OS-X---Build-Instructions) |\n\n### Dependencies\n\nRequired dependencies:\n\n* [CMake 3](http://www.cmake.org/)\n* [Qt >= 5.4.0](http://qt-project.org/)\n* [VLC 2.1.0](https://videolan.org/vlc/)\n* [SQLite 3.6.22](http://www.sqlite.org/)\n* [TagLib 1.8](https://taglib.github.io/)\n* [Boost 1.3](http://www.boost.org/)\n* [Lucene++ 3.0.6](https://github.com/luceneplusplus/LucenePlusPlus/)\n* [Attica 5.6.0](http://ftp.kde.org/stable/attica/)\n* [QuaZip 0.4.3](http://quazip.sourceforge.net/)\n* [liblastfm 1.0.9](https://github.com/lastfm/liblastfm/)\n* [QtKeychain 0.1](https://github.com/frankosterfeld/qtkeychain/)\n* [Sparsehash](https://code.google.com/p/sparsehash/)\n* [GnuTLS](http://gnutls.org/)\n\nIf you are using Qt>5.6 you need to build and install QtWebKit\n\n* [QtWebKit](https://github.com/qt/qtwebkit)\n\nThe following dependencies are optional (but *recommended*):\n\n* [Jreen 1.1.1](http://qutim.org/jreen/)\n* [Snorenotify 0.5.2](https://github.com/Snorenotify/Snorenotify/)\n\nThird party libraries that we ship with our source:\n\n* [MiniUPnP 1.6](http://miniupnp.free.fr/)\n* [Qocoa](https://github.com/mikemcquaid/Qocoa/)\n* [libqnetwm](https://code.google.com/p/libqnetwm/)\n* [libqxt](http://libqxt.org/) (QxtWeb module)\n* [SPMediaKeyTap](https://github.com/nevyn/SPMediaKeyTap/)\n* [kdSingleApplicationGuard](http://www.kdab.com/)\n\n## SUPPORT TOMAHAWK\n\n* [Bug / Issue Tracker](https://bugs.tomahawk-player.org/secure/Dashboard.jspa)\n* [Translations](https://www.transifex.com/projects/p/tomahawk/)\n* [Donations](https://flattr.com/thing/169312/Tomahawk)\n\n## GET HELP\n\n* [Support & Feedback](https://tomahawk.uservoice.com)\n* Chat with us in IRC: **#tomahawk** on Freenode, and [Scrollback.io](https://scrollback.io/tomahawk)\n* [Twitter](https://twitter.com/tomahawk)\n* [Facebook](https://facebook.com/tomahawkplayer)\n* [Developer API Documentation](http://dev.tomahawk-player.org/api/classes.html)\n\n## SCREENSHOTS\n\nBROWSE FRIENDS' MUSIC & LISTEN ALONG\n\n![Browse](https://dchtm6r471mui.cloudfront.net/hackpad.com_ZRZMJDdxrVe_p.242147_1410998050088_listen-along.jpg)\n\nINBOX - RECEIVED & FORWARDING\n\n![Inbox](https://dchtm6r471mui.cloudfront.net/hackpad.com_ZRZMJDdxrVe_p.242147_1410997751044_inbox.jpg)\n\nCHARTS - BILLBOARD'S TASTEMAKER ALBUMS\n\n![Charts](https://dchtm6r471mui.cloudfront.net/hackpad.com_ZRZMJDdxrVe_p.242147_1410997901969_charts.jpg)\n\nFRIEND FEED\n\n![Feed](https://dchtm6r471mui.cloudfront.net/hackpad.com_ZRZMJDdxrVe_p.242147_1410971283885_heroshot.png)\n\nDYNAMIC (AUTO-UPDATING) PLAYLIST\n\n![Xspf](https://dchtm6r471mui.cloudfront.net/hackpad.com_ZRZMJDdxrVe_p.242147_1410998362549_dynamic-playlist-1.jpg)\n\nPLUG-INS / RESOLVER SETTINGS\n\n![Settings](https://dchtm6r471mui.cloudfront.net/hackpad.com_ZRZMJDdxrVe_p.242147_1410998587408_prefs.jpg)\n\n##Enjoy!\n"
 },
 {
  "repo": "cashmusic/platform",
  "language": "PHP",
  "readme_contents": "# CASH Music Platform\n\nThe CASH Music platform gives everyone access to tools that let them manage, \npromote, and sell their music online \u2014 all owned and controlled themselves.\n\nThe platform can be used as a PHP library, integrated into popular CMS systems, \nor standalone with the included admin app. This repo contains the core framework, \ninstallers, an admin webapp, APIs, demos, and a full suite of tests.\n\n[![Build Status](https://secure.travis-ci.org/cashmusic/platform.svg)](http://travis-ci.org/cashmusic/platform)\n\n  \n## Get up and running\n\nAll you need to get started is [VirtualBox](https://www.virtualbox.org/wiki/Downloads), \n[Vagrant 1.4+](http://www.vagrantup.com/downloads.html), and this repo. Just fork, install\nVirtualBox and Vagrant, then open a terminal window and in the repo directory type:\n\n```bash\nvagrant up\n```  \n\nVagrant will fire up a VM, set up Apache, install the platform, and start serving a \nspecial dev website with tools, docs, and a live instance of the platform \u2014 all mapped \nright to localhost:8888.\n\n![Dev site included in repo](https://b6febe3773eb5c5bc449-6d885a724441c07ff9b675222419a9d2.ssl.cf2.rackcdn.com/special/docs/dev_screenshot.jpg)\n\n\n## Requirements\n\nOne of our goals is for this to run in as many places as possible, so we've worked \nhard to keep the requirements minimal:\n\n * PHP 5.4+\n * PDO (a default) and MySQL OR SQLite \n * mod_rewrite (for admin app)\n * fopen wrappers OR cURL \n\n## More\n\nFor more about installation, working with the platform, check out [the wiki](https://github.com/cashmusic/platform/wiki).\n\n## Submitting a pull request\n\nWe the 'master' branch release-ready at all times, so we ask all contributors to [TEST](https://github.com/cashmusic/platform/blob/master/tests/README.md) your code before submitting a pull request. Please \ncreate a descriptively named branch off your repo and give as many details in your pull request as possible.\n\nWe view pull requests as conversations. Submit a pull request early if you're working on something and\nhave questions. We'll work with you to get it where it needs to be for a merge.\n\n## Copyright & License\n\nThe CASH Music platform is (c) 2010-2014 CASH Music, licensed under an \n[AGPL license](http://www.gnu.org/licenses/agpl-3.0.html) (Some components, like\nthe core framework, are licensed LGPL. See LICENSE docs for more.)\n"
 },
 {
  "repo": "mopidy/mopidy",
  "language": "Python",
  "readme_contents": "******\nMopidy\n******\n\n`Mopidy`_ is an extensible music server written in Python.\n\nMopidy plays music from local disk, Spotify, SoundCloud, Google Play Music, and\nmore. You edit the playlist from any phone, tablet, or computer using a variety\nof MPD and web clients.\n\n**Stream music from the cloud**\n\nVanilla Mopidy only plays music from files and radio streams.  Through\n`extensions`_, Mopidy can play music from cloud services like Spotify,\nSoundCloud, and Google Play Music.\nWith Mopidy's extension support, backends for new music sources can be easily\nadded.\n\n**Mopidy is just a server**\n\nMopidy is a Python application that runs in a terminal or in the background on\nLinux computers or Macs that have network connectivity and audio output.\nOut of the box, Mopidy is an HTTP server. If you install the `Mopidy-MPD`_\nextension, it becomes an MPD server too. Many additional frontends for\ncontrolling Mopidy are available as extensions.\n\n**Pick your favorite client**\n\nYou and the people around you can all connect their favorite MPD or web client\nto the Mopidy server to search for music and manage the playlist together.\nWith a browser or MPD client, which is available for all popular operating\nsystems, you can control the music from any phone, tablet, or computer.\n\n**Mopidy on Raspberry Pi**\n\nThe `Raspberry Pi`_ is an popular device to run Mopidy on, either using\nRaspbian, Ubuntu, or Arch Linux.\nPimoroni recommends Mopidy for use with their `Pirate Audio`_ audio gear for\nRaspberry Pi.\nMopidy is also a significant building block in the `Pi Musicbox`_ integrated\naudio jukebox system for Raspberry Pi.\n\n**Mopidy is hackable**\n\nMopidy's extension support and Python, JSON-RPC, and JavaScript APIs make\nMopidy a perfect base for your projects.\nIn one hack, a Raspberry Pi was embedded in an old cassette player. The buttons\nand volume control are wired up with GPIO on the Raspberry Pi, and is used to\ncontrol playback through a custom Mopidy extension. The cassettes have NFC tags\nused to select playlists from Spotify.\n\n.. _Mopidy: https://mopidy.com/\n.. _extensions: https://mopidy.com/ext/\n.. _Mopidy-MPD: https://mopidy.com/ext/mpd/\n.. _Raspberry Pi: https://www.raspberrypi.org/\n.. _Pirate Audio: https://shop.pimoroni.com/collections/pirate-audio\n.. _Pi Musicbox: https://www.pimusicbox.com/\n\n\n**Getting started**\n\nTo get started with Mopidy, begin by reading the\n`installation docs <https://docs.mopidy.com/en/latest/installation/>`_.\n\n\n**Project resources**\n\n- `Documentation <https://docs.mopidy.com/>`_\n- `Discourse forum <https://discourse.mopidy.com/>`_\n- `Zulip chat <https://mopidy.zulipchat.com/>`_\n- `Source code <https://github.com/mopidy/mopidy>`_\n- `Issue tracker <https://github.com/mopidy/mopidy/issues>`_\n\n.. image:: https://img.shields.io/pypi/v/Mopidy.svg?style=flat\n    :target: https://pypi.python.org/pypi/Mopidy/\n    :alt: Latest PyPI version\n\n.. image:: https://img.shields.io/circleci/project/github/mopidy/mopidy/develop.svg\n    :target: https://circleci.com/gh/mopidy/mopidy\n    :alt: CircleCI build status\n\n.. image:: https://img.shields.io/readthedocs/mopidy.svg\n    :target: https://docs.mopidy.com/\n    :alt: Read the Docs build status\n\n.. image:: https://img.shields.io/codecov/c/github/mopidy/mopidy/develop.svg\n    :target: https://codecov.io/gh/mopidy/mopidy\n    :alt: Test coverage\n\n.. image:: https://img.shields.io/badge/chat-on%20zulip-brightgreen\n    :target: https://mopidy.zulipchat.com/\n    :alt: Chat on Zulip\n"
 },
 {
  "repo": "AudioKit/AudioKit",
  "language": "C",
  "readme_contents": "AudioKit V5.0 Beta / Developer's Release\n===\n\n[![Build Status](https://github.com/AudioKit/AudioKit/workflows/CI/badge.svg)](https://github.com/AudioKit/AudioKit/actions?query=workflow%3ACI)\n[![License](https://img.shields.io/cocoapods/l/AudioKit.svg?style=flat)](https://github.com/AudioKit/AudioKit/blob/v5-main/LICENSE)\n[![Platform](https://img.shields.io/cocoapods/p/AudioKit.svg?style=flat)](https://github.com/AudioKit/AudioKit/wiki)\n[![Reviewed by Hound](https://img.shields.io/badge/Reviewed_by-Hound-8E64B0.svg)](https://houndci.com)\n[![Twitter Follow](https://img.shields.io/twitter/follow/AudioKitPro.svg?style=social)](http://twitter.com/AudioKitPro)\n\nAudioKit is an audio synthesis, processing, and analysis platform for iOS, macOS (including Catalyst), and tvOS. \n\n## Important notes for AudioKit Version 4 Users\n\nIf you are using AudioKit in production, you may want to stick to our latest stable release of Version 4 because there are a number of things were are still working out. \nBut, since Version 5 is well on its way, we don't think new users should use Version 4 anymore. When AudioKit 5 is ready, we will make a 5.0 release, but even then\na Version 4 branch will be maintained because of its large user base, and also because there are things in AudioKit 4 that are not yet available in version 5.\n\nMost importantly, you must read the [Migration Guide](docs/MigrationGuide.md). \nYou will also want to check the progress of the [AudioKit 5 Release Push Project](https://github.com/AudioKit/AudioKit/projects/5).\n\n## Installation via Swift Package Manager\n\nTo add AudioKit to your Xcode project, select File -> Swift Packages -> Add Package Depedancy. Enter `https://github.com/AudioKit/AudioKit` for the URL. Check the use branch option and enter `v5-main` or `v5-develop`.\n\nInstalling AudioKit via Cocoapods was supported through AudioKit 4, and will be reintroduced when AudioKit 5 is officially released.\n\n## Documentation\n\nIn addition to the [Migration Guide](docs/MigrationGuide.md), our documentation is now automatically generated on the [Github wiki](https://github.com/AudioKit/AudioKit/wiki).\n\n## Examples\n\nThe [AudioKit Cookbook](https://github.com/AudioKit/Cookbook) contains many recipes for simple uses for AudioKit components. More examples are [here](docs/Examples.md).\n\n## Getting help\n\n1. Post your problem to [StackOverflow with the #AudioKit hashtag](https://stackoverflow.com/questions/tagged/audiokit).\n\n2. Once you are sure the problem is not in your implementation, but in AudioKit itself, you can open a [Github Issue](https://github.com/audiokit/AudioKit/issues).\n\n3. If you, your team or your company is using AudioKit, please consider [sponsoring Aure on Github Sponsors](http://github.com/sponsors/aure).\n\n## Contributing Code\n\nWhen you want to modify AudioKit, check out the [v5-develop](https://github.com/audiokit/AudioKit/tree/v5-develop) branch (as opposed to v5-main), \nmake your changes, and send us a [pull request](https://github.com/audiokit/AudioKit/pulls).\n\n\n## About Us\n\nAudioKit was created by \n[Aurelius Prochazka](https://github.com/aure) who is your life line if you need help!  \n[Matthew Fecher](https://github.com/analogcode), \n[Jeff Cooper](https://github.com/eljeff), and Aure create [AudioKitPro](http://audiokitpro.com/) apps together, and\n[Stephane Peter](https://github.com/megastep) is Aure's co-admin and manages AudioKit's releases. \n[Taylor Holliday](https://github.com/wtholliday) has been instrumental in AudioKit 5 improvements.\n\nBut, there are many other important people in our family:\n\n| Group                                                                    | Description                                                                      |\n| ------------------------------------------------------------------------ | -------------------------------------------------------------------------------- |\n| [Core Team](https://github.com/orgs/AudioKit/people)                     | The biggest contributors to AudioKit!                                            |\n| [Slack](https://audiokit.slack.com)                                      | Pro-level developer chat group, contact a core team member for an in invitation. |\n| [Contributors](https://github.com/AudioKit/AudioKit/graphs/contributors) | A list of all people who have submitted code to AudioKit.                        |\n\n<a href=\"https://github.com/AudioKit/AudioKit/graphs/contributors\"><img src=\"https://opencollective.com/AudioKit/contributors.svg?width=890&button=false\" /></a>\n\n\n\n"
 },
 {
  "repo": "Soundnode/soundnode-app",
  "language": "JavaScript",
  "readme_contents": "[![Join the chat at https://gitter.im/Soundnode/soundnode-app](https://badges.gitter.im/Soundnode/soundnode-app.svg)](https://gitter.im/Soundnode/soundnode-app?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nSoundnode App\n============\n\nSoundnode App is an Open-Source project to support Soundcloud for desktop Mac, Windows, and Linux. <br>\nIt's built with Electron, Node.js, Angular.js, and uses the Soundcloud API.\n\n> Be aware that Soundnode relies on Soundcloud API which only allows third party apps to play 15 thousand tracks daily. When the rate limit is reached all users are blocked from playing/streaming tracks. The stream will be re-enable one day after (at the same time) streams were blocked.\n\nFollow us on twitter for updates [@Soundnodeapp](https://www.twitter.com/soundnodeapp).\n\nFeatured on [Producthunt](https://www.producthunt.com/tech/soundnode-2), [TNW](http://thenextweb.com/apps/2016/01/25/soundnode-is-the-soundcloud-desktop-app-youve-been-waiting-for/#gref)\nand [Gizmodo](http://gizmodo.com/soundnode-turns-soundcloud-into-a-spotify-like-desktop-1754953529)\n\n![alt tag](https://raw.githubusercontent.com/Soundnode/soundnode-app/master/Soundnode-app.png)\n\n## Features\n\n- No need to install\n- Native media keyboard shortcuts\n- Search for new songs\n- Easy navigation\n- Listen to songs from your Stream, Likes, Tracks, Following or Playlists\n- Like songs and save to your liked playlist\n- Full playlist feature\n- Follow/Unfollow users\n\nAnd much more!\n\n## Configuration\n\nSince soundcloud applies a rate limit to third party apps, you need to configure your own API key to make soundnode work.\n\nUnfortunately soundcloud suspended new application creation, so to retrieve your api key, you have to dig into the soundcloud [website](https://soundcloud.com/).\n\n* Login to soundcloud.com on favorite browser\n* Look for an api call and write down the client_id parameter\n![dev tools](doc/img/dev_tools.png)\n* Edit your userConfig.json file (see here for location : https://github.com/eliecharra/soundnode-app/blob/master/app/public/js/common/configLocation.js#L34) and update clientId parameter with the previously retrieved one.\n\n## How to contribute\n\nFirst, building, testing, and reporting bugs is highly appreciated. Please include the console's output and steps to reproduce the problem in your bug report, if possible.\n\nIf you want to develop, you can look at the issues, especially the bugs, and then fix them.\nHere's a [list of issues](https://github.com/Soundnode/soundnode-app/issues?state=open).\n\nPlease follow the [contribution guidelines](https://github.com/Soundnode/soundnode-app/blob/master/CONTRIBUTING.md).\n\n## Development\n\nSee the [Development page](https://github.com/Soundnode/soundnode-app/wiki/Development) for a complete guide on how to build\nthe app locally on your computer.\n\nCheck out [Electron documentation](https://electron.atom.io/docs/)\n\n## Supported Platforms\n\n- Windows\n- Mac\n- Linux\n\n## Author\n\n- [Michael Lancaster](https://github.com/weblancaster)\n\n## Contributors\n\nThanks to all [contributors](https://github.com/Soundnode/soundnode-app/graphs/contributors) that are helping or helped making Soundnode better.\n\n## License\n\nGNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 [license](https://github.com/Soundnode/soundnode-app/blob/master/LICENSE.md).\n"
 },
 {
  "repo": "gillesdemey/Cumulus",
  "language": "JavaScript",
  "readme_contents": "# Cumulus\nA SoundCloud player that lives in your menubar.\n\n[![GitHub release](https://img.shields.io/badge/download-latest-blue.svg)](https://github.com/gillesdemey/Cumulus/releases/latest)\n\n<img height=\"600\" width=\"auto\" src=\"assets/cumulus_app.png\">\n\n# Installing\n\nDownload the [latest release for OSX](https://github.com/gillesdemey/Cumulus/releases/latest).\n\n*IntelliJ users be warned: This app hijacks the \u2318+Alt+L shortcurt used by IntelliJ to reformat code. See [#40](https://github.com/gillesdemey/Cumulus/issues/40#issuecomment-261022368) and [#77](https://github.com/gillesdemey/Cumulus/issues/77).*\n\n# Developing\n\n## Install dependencies\n`npm install`\n\n`npm install -g electron`\n\n## Compile the application\n`grunt` or `grunt build`\n\n## Run the application with the [Chrome DevTools](https://developer.chrome.com/devtools)\n`NODE_ENV=development electron .`\n\n### Or in Windows:\n- PowerShell: `$env:NODE_ENV=\"development\"; electron .`\n- CMD: `set \"NODE_ENV=development\" & electron .`\n"
 },
 {
  "repo": "metabrainz/picard",
  "language": "Python",
  "readme_contents": "MusicBrainz Picard\n==================\n![Github Actions Status](https://github.com/metabrainz/picard/workflows/Run%20tests/badge.svg)\n[![Codacy Grade](https://img.shields.io/codacy/grade/53a33607234a4c18a11a6207d1173c0c/master.svg?style=flat-square&label=Codacy)](https://www.codacy.com/app/MetaBrainz/picard)\n\n[MusicBrainz Picard](http://picard.musicbrainz.org) is a cross-platform (Linux/Mac OS X/Windows) application written in Python and is the official [MusicBrainz](http://musicbrainz.org) tagger.\n\nPicard supports the majority of audio file formats, is capable of using audio fingerprints ([AcoustIDs](http://musicbrainz.org/doc/AcoustID)), performing CD lookups and [disc ID](http://musicbrainz.org/doc/Disc_ID) submissions, and it has excellent Unicode support. Additionally, there are several plugins available that extend Picard's features.\n\nWhen tagging files, Picard uses an album-oriented approach. This approach allows it to utilize the MusicBrainz data as effectively as possible and correctly tag your music. For more information, [see the illustrated quick start guide to tagging](https://picard.musicbrainz.org/quick-start/).\n\nPicard is named after Captain Jean-Luc Picard from the TV series Star Trek: The Next Generation.\n\nBinary downloads are available [here](http://picard.musicbrainz.org/downloads/).\n\nSupport and issue reporting\n---------------------------\n\nPlease report all bugs and feature requests in the [MusicBrainz issue tracker](https://tickets.metabrainz.org/browse/PICARD). If you need support in using Picard please read the [documentation](https://picard-docs.musicbrainz.org/) first and have a look at the [MusicBrainz community forums](https://community.metabrainz.org/c/picard).\n\nInstalling\n---------------------------\n\n[INSTALL.md has instructions on building this codebase.](INSTALL.md)\n"
 },
 {
  "repo": "overtone/overtone",
  "language": "Clojure",
  "readme_contents": "                                                              888\n                                                              888\n             _ooooooooo._                                     888\n          ,o888PP\"\"\"\"PP88   .d88b.  888  888  .d88b.  888d888 888888 .d88b.  88888b.   .d88b.\n        d88P''          '  d88\"\"88b 888  888 d8P  Y8b 888P\"   888   d88\"\"88b 888 \"88b d8P  Y8b\n      ,88P                 88    88 Y88  88P 88888888 888     888   88    88 888  888 88888888\n     ,88                   Y88..88P  Y8bd8P  Y8b.     888     Y88b. Y88..88P 888  888 Y8b.\n    ,88'                    \"Y88P\"    Y88P    \"Y8888  888      \"Y888 \"Y88P\"  888  888  \"Y8888\n    d8P\n    d8b                        88[\n    `88                       J88\n     Y8b                     ,88'\n      Y8b.                  d88'\n       `Y8b._            _o88P\n         `Y888oo.____ooo888P'\n            '\"PP888888PP''\n\n\n\n# Collaborative Programmable Music.\n\nOvertone is an Open Source toolkit for designing synthesizers and\ncollaborating with music.  It provides:\n\n* A Clojure API to the SuperCollider synthesis engine\n* A growing library of musical functions (scales, chords, rhythms,\n  arpeggiators, etc.)\n* Metronome and timing system to support live-programming and sequencing\n* Plug and play MIDI device I/O\n* A full Open Sound Control (OSC) client and server implementation.\n* Pre-cache - a system for locally caching external assets such as .wav\n  files\n* An API for querying and fetching sounds from http://freesound.org\n* A global concurrent event stream\n\n## Quick Start\n\n### Installation\n\n```sh\n    # Install the clojure-cli tools\n    # https://clojure.org/guides/getting_started\n\n    # Create a deps.edn file with a minimum\n    # {:deps {overtone/overtone {:mvn/version \"0.10.6\"}}}\n    $ clojure\n    $ (use 'overtone.live)\n```\n\n```sh\n    # Or install leiningen\n    # https://github.com/technomancy/leiningen\n\n    $ lein new insane-noises\n\n    # add the following dependencies to insane-noises/project.clj\n    # [org.clojure/clojure \"1.9.0\"]\n    # [overtone \"0.10.6\"]\n\n    $ cd insane-noises\n    $ lein repl\n```\n\n### Making sounds\n\n\n```clj\n    ;; boot the server\n    user=> (use 'overtone.live)\n\n    ;; listen to the joys of a simple sine wave\n    user=> (demo (sin-osc))\n\n    ;; or something more interesting...\n    user=> (demo 7 (lpf (mix (saw [50 (line 100 1600 5) 101 100.5]))\n                   (lin-lin (lf-tri (line 2 20 5)) -1 1 400 4000)))\n```\n\n### Detailed Instructions\n\nFor a more detailed set of setup instructions (including details\nspecific to Windows and Linux) head over to the\n[Overtone wiki installation page](https://github.com/overtone/overtone/wiki/Installing-Overtone)\n\nWe maintain documentation for all aspects of the system in the\n[project wiki](https://github.com/overtone/overtone/wiki/Home), you'll\nfind tutorials and examples on topics such as synthesizing new sounds\nfrom scratch, live-coding and generating musical scores on the fly. If\nyou see anything missing, please feel free to add it yourself, or hit us\nup on the [mailing list](http://groups.google.com/group/overtone) and\nwe'll sort something out.\n\n## Cheat Sheet\n\nFor a quick glance at all the exciting functionality Overtone puts at\nyour musical fingertips check out the cheat sheet:\n\nhttps://github.com/overtone/overtone/raw/master/docs/cheatsheet/overtone-cheat-sheet.pdf\n\n## Overtone Powered Bands\n\nA list of bands using Overtone to generate sounds:\n\n* [Meta-eX](http://meta-ex.com)\n* [Repl Electric](http://repl-electric.com)\n\n## Community\n\n### Mailing List\n\nWe encourage you to join the\n[mailing list](http://groups.google.com/group/overtone) to see what\nother people are getting up to with Overtone. Use it to ask questions,\nshow off what you've made and even meet fellow Overtoners in your area\nso you can meet up for impromptu jam sessions. All we ask is that you be\nconsiderate, courteous and respectful and that you share as much of your\ncode as possible so we can all learn how to make crazy cool sounds\ntogether.\n\n### Twitter\n\nFollow `@overtone` on Twitter: http://twitter.com/overtone\n\n### Web\n\nOur main website is hosted on GitHub: http://overtone.github.io\n\n##  Videos\n\n### Introductions\n\nHead over to Vimeo for a fast-paced 4 minute introduction to live-coding\nwith Overtone to see what's possible\n\n  http://vimeo.com/22798433\n\nFor a nice overview of the basics of creating and playing with\nsynthesized instruments in Overtone checkout Philip Potter's 20 minute\nworkshop:\n\n  http://skillsmatter.com/podcast/scala/clojurex-unpanel-2894\n\nChris Ford also delivers a beautifully paced introduction to fundamental music\nconcepts from basic sine waves to Bach's Goldberg Variations with live examples throughout:\n\n  http://skillsmatter.com/podcast/home/functional-composition\n\nThere are also the following tutorials:\n\n* Setting up an Overtone Development Environment - Running on Edge\n  http://vimeo.com/25102399\n* How to Hack Overtone with Emacs http://vimeo.com/25190186\n\n### Presentations\n\n* Rich Hickey - Harmonikit: http://www.youtube.com/watch?v=bhkdyCPYgLs\n* Sam Aaron - Programming Music With Overtone: http://www.youtube.com/watch?v=imoWGsipe4k\n* Chris Ford - Functional Composition: http://www.youtube.com/watch?v=Mfsnlbd-4xQ\n* Meta-eX - Live Coding with Meta-eX: https://www.youtube.com/watch?v=zJqH5bNcIN0\n\n### Interviews\n\nOvertone has generated quite a bit of interest. Here's a list of\navailable interviews which go into further depth on the background and\nphilosophy of Overtone:\n\n* http://twit.tv/show/floss-weekly/197\n* http://mostlylazy.com/2011/11/18/episode-0-0-2-sam-aaron-and-overtone-at-clojure-conj-2011/\n* http://codebassradio.net/2011/11/29/runtime-expectations-episode-13-hot-clojure-conj/\n  (scroll down to the section with Sam Aaron, Ghadi Shayban, and Daniel Spiewak)\n* http://clojure.com/blog/2012/01/04/take5-sam-aaron.html\n\n### Performances\n\n* Repl Electric: https://vimeo.com/95988263\n* Piotr Jagielski\u200f: https://www.youtube.com/watch?v=r8YKC7Qugm8\n* Sam Aaron Live @ Arnolfini:  https://vimeo.com/46867490\n* Meta-eX Live @ Music Tech Fest: http://youtu.be/zJqH5bNcIN0?t=15m25s\n\n\n## Source Repository\n\nDownloads and the source repository can be found on GitHub:\n\n  http://github.com/overtone/overtone\n\nClone the repository on GitHub to get started developing, and if you are\nready to submit a patch then fork your own copy and do a pull request.\n\n##  clojure.tools.deps and Leiningen Support\n\nOvertone and its dependencies are on http://clojars.org, and the\ndependency for your `deps.edn` is:\n\n```Clojure\n    {overtone/overtone {:mvn/version \"0.10.6\"}}\n```\n\nor for your `project.clj` (Leiningen)\n\n```Clojure\n    [overtone \"0.10.6\"]\n```\n\n## Contributors\n\nSee: https://github.com/overtone/overtone/graphs/contributors\n"
 },
 {
  "repo": "sonic-pi-net/sonic-pi",
  "language": "Ruby",
  "readme_contents": "                                           \u2558\n                                    \u2500       \u255b\u2592\u255b\n                                     \u2590\u256b       \u2584\u2588\u251c\n                              \u2500\u255f\u255b      \u2588\u2584      \u256a\u2593\u2580\n                    \u2553\u2524\u2524\u2524\u2524\u2524\u2524\u2524\u2524\u2524  \u2569\u258c      \u2588\u2588      \u2580\u2593\u258c\n                     \u2590\u2592   \u256c\u2592     \u255f\u2593\u2558    \u2500\u2593\u2588      \u2593\u2593\u251c\n                     \u2592\u256b   \u2592\u256a      \u2593\u2588     \u2593\u2593\u2500     \u2593\u2593\u2584\n                    \u2552\u2592\u2500  \u2502\u2592       \u2593\u2588     \u2593\u2593     \u2500\u2593\u2593\u2500\n                    \u256c\u2592   \u2584\u2592 \u2552    \u256a\u2593\u2550    \u256c\u2593\u256c     \u258c\u2593\u2584\n                    \u2565\u2552   \u2566\u2565     \u2555\u2588\u2552    \u2559\u2593\u2590     \u2584\u2593\u256b\n                               \u2590\u2569     \u2592\u2592      \u2580\u2580\n                                    \u2552\u256a      \u2590\u2584\n\n                 _____             __        ____  __\n                / ___/____  ____  /_/____   / __ \\/_/\n                \\__ \\/ __ \\/ __ \\/ / ___/  / /_/ / /\n               ___/ / /_/ / / / / / /__   / ____/ /\n              /____/\\____/_/ /_/_/\\___/  /_/   /_/\n\n\n## Code. Music. Live.\n\nSonic Pi is a *new kind of musical instrument*. Instead of strumming\nstrings or whacking things with sticks - you write code - **live**.\n\nSonic Pi has been designed with the aim to find a harmonious balance\nbetween three core principles:\n\n* **Simple** enough for the 10 year old within you\n* **Joyful** enough for you to lose yourself through play\n* **Powerful** enough for your own expressions\n\nSonic Pi is a complete open source programming environment originally\ndesigned to explore and *teach programming concepts* within schools through\nthe process of creating *new sounds*.\n\nIn addition to being an engaging education resource it has evolved into\nan *extremely powerful* and performance-ready *live coding instrument* suitable\nfor professional artists and DJs.\n\nWhilst Education is a core focus it now sits at the intersection\nbetween three core domains:\n\n* **Art** - providing the means to express yourself and ask new questions of music and notation\n* **Technology** - exploring questions related to liveness, time and concurrency in programming languages\n* **Education**  - demonstrating that open play rather than rigid structures increases motivation and engagement in the classroom\n\nAgain, finding a balance working to the best benefits of all these\ndomains is the objective.\n\n\n* Info & Latest Releases: https://sonic-pi.net\n* Source: https://github.com/sonic-pi-net/sonic-pi\n* Computing Education Resources for Schools: https://www.raspberrypi.org/learning/sonic-pi-lessons/\n* Music Education Toolkit for Schools: https://sonic-pi.mehackit.org\n\n## Contributors\n\nSonic Pi has been developed with support from many individuals and organisations. Please see the [CONTRIBUTORS.md](https://github.com/samaaron/sonic-pi/blob/main/CONTRIBUTORS.md) file for a more complete list.\n\n## Translations\n\nSonic Pi's application and built in tutorial have been translated into several languages thanks to the awesome work of several [volunteer translators](CONTRIBUTORS.md#translation):\n\nStatus                                                                                                                                                       | Language            | Application        | Tutorial\n-----------------------------------------------------------------------------------------------------------------------------------------------------------: | :------------------ | :----------------: | :----------------:\n[![Translation Status of Sonic Pi: da](https://hosted.weblate.org/widgets/sonic-pi/da/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)           | Danish              | :white_check_mark: | :red_circle:\n[![Translation Status of Sonic Pi: nl](https://hosted.weblate.org/widgets/sonic-pi/nl/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)           | Dutch               | :white_check_mark: | :white_check_mark:\n[![Translation Status of Sonic Pi: fi](https://hosted.weblate.org/widgets/sonic-pi/fi/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)           | Finnish             | :white_check_mark: | :red_circle:\n[![Translation Status of Sonic Pi: fr](https://hosted.weblate.org/widgets/sonic-pi/fr/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)           | French              | :white_check_mark: | :white_check_mark:\n[![Translation Status of Sonic Pi: de](https://hosted.weblate.org/widgets/sonic-pi/de/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)           | German              | :white_check_mark: | :white_check_mark:\n[![Translation Status of Sonic Pi: hu](https://hosted.weblate.org/widgets/sonic-pi/hu/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)           | Hungarian           | :white_check_mark: | :red_circle:\n[![Translation Status of Sonic Pi: is](https://hosted.weblate.org/widgets/sonic-pi/is/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)           | Icelandic           | :white_check_mark: | :red_circle:\n[![Translation Status of Sonic Pi: it](https://hosted.weblate.org/widgets/sonic-pi/it/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)           | Italian             | :white_check_mark: | :white_check_mark:\n[![Translation Status of Sonic Pi: ja](https://hosted.weblate.org/widgets/sonic-pi/ja/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)           | Japanese            | :white_check_mark: | :white_check_mark:\n[![Translation Status of Sonic Pi: nb](https://hosted.weblate.org/widgets/sonic-pi/nb/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)           | Norwegian Bokm\u00e5l    | :white_check_mark: | :white_check_mark:\n[![Translation Status of Sonic Pi: pl](https://hosted.weblate.org/widgets/sonic-pi/pl/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)           | Polish              | :white_check_mark: | :white_check_mark:\n[![Translation Status of Sonic Pi: pt](https://hosted.weblate.org/widgets/sonic-pi/pt/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)           | Portuguese          | :red_circle:       | :white_check_mark:\n[![Translation Status of Sonic Pi: ro](https://hosted.weblate.org/widgets/sonic-pi/ro/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)           | Romanian            | :white_check_mark: | :white_check_mark:\n[![Translation Status of Sonic Pi: ru](https://hosted.weblate.org/widgets/sonic-pi/ru/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)           | Russian             | :white_check_mark: | :white_check_mark:\n[![Translation Status of Sonic Pi: zh_Hans](https://hosted.weblate.org/widgets/sonic-pi/zh_Hans/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/) | Simplified Chinese  | :white_check_mark: | :red_circle:\n[![Translation Status of Sonic Pi: es](https://hosted.weblate.org/widgets/sonic-pi/es/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)           | Spanish             | :white_check_mark: | :white_check_mark:\n[![Translation Status of Sonic Pi: si](https://hosted.weblate.org/widgets/sonic-pi/si/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)     | sinhalese| :white_check_mark: | :red_circle:\n\nWould you like to contribute a translation too? If so, please take a look at our [translation docs](https://github.com/samaaron/sonic-pi/blob/main/TRANSLATION.md) to get started.\n\n## Information for developers\n\nSonic Pi is under active development, and welcomes new contributors:\n\n* [How to contribute](HOW-TO-CONTRIBUTE.md)\n* [Change log](CHANGELOG.md)\n* [Community](COMMUNITY.md)\n* [Contributors](CONTRIBUTORS.md)\n* [Installation](INSTALL.md)\n* [License](LICENSE.md)\n* [Testing](TESTING.md)\n* [Translation](TRANSLATION.md)\n\n[![Weblate](https://hosted.weblate.org/widgets/sonic-pi/-/svg-badge.svg)](https://hosted.weblate.org/engage/sonic-pi/)\n<br/>\n[![Travis CI](https://travis-ci.org/samaaron/sonic-pi.svg?branch=main)](https://travis-ci.org/samaaron/sonic-pi)\n<br/>\n[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/samaaron/sonic-pi?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n"
 },
 {
  "repo": "nukeop/nuclear",
  "language": "JavaScript",
  "readme_contents": "# ![nuclear](https://i.imgur.com/oT1006i.png) \n[![Maintainability](https://api.codeclimate.com/v1/badges/a15c4888a63c900f6cc1/maintainability)](https://codeclimate.com/github/nukeop/nuclear/maintainability) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/30750586202742279fa8958a12e519ed)](https://www.codacy.com/app/nukeop/nuclear?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=nukeop/nuclear&amp;utm_campaign=Badge_Grade) [![nuclear](https://snapcraft.io//nuclear/badge.svg)](https://snapcraft.io/nuclear) ![Travis](https://api.travis-ci.org/nukeop/nuclear.svg?branch=master)\n\nDesktop music player focused on streaming from free sources\n\n![Showcase](https://i.imgur.com/G9BqIHl.png)\n\n# Links\n\n[Official website](https://nuclear.js.org)\n\n[Mastodon](https://mstdn.io/@nuclear)\n\n[Twitter](https://twitter.com/nuclear_player)\n\nSupport channel (Matrix): `#nuclear:matrix.org`\n\nDiscord channel: https://discord.gg/JqPjKxE\n\nReadme translations: \n* [Brazilian Portuguese](docs/README-ptbr.md)\n* [Swedish](docs/README-se.md)\n\n## What is this?\nnuclear is a free music streaming program that pulls content from free sources all over the internet.\n\nIf you know [mps-youtube](https://github.com/mps-youtube/mps-youtube), this is a similar music player but with a GUI.\nIt's also focusing more on audio. Imagine Spotify which you don't have to pay for and with a bigger library.\n\n## What if I am religiously opposed to Electron?\nSee [this](docs/electron.md).\n\n## Features\n\n- Searching for and playing music from YouTube (including integration with playlists), Jamendo, Audius and SoundCloud\n- Searching for albums (powered by Last.fm and Discogs), album view, automatic song lookup based on artist and track name (in progress, can be dodgy sometimes)\n- Song queue, which can be exported as a playlist\n- Loading saved playlists (stored in json files)\n- Scrobbling to last.fm (along with updating the 'now playing' status)\n- Newest releases with reviews - tracks and albums\n- Browsing by genre\n- Radio mode (automatically queue similar tracks)\n- Unlimited downloads (powered by youtube)\n- Realtime lyrics\n- Browsing by popularity\n- List of favorite tracks\n- Listening from local library\n- No accounts\n- No ads\n- No CoC\n- No CLA\n\n## Manual and docs\nhttps://nuclearmusic.rtfd.io/\n\n## Community-maintained packages\n\nHere's a list of packages for various managers, most of which are maintained by third parties. We would like to thank the maintainers for their work.\n\n| Package type   | Link                                                    | Maintainer                                    |\n|:--------------:|:-------------------------------------------------------:|:---------------------------------------------:|\n| AUR (Arch)     | https://aur.archlinux.org/packages/nuclear-player-bin/  | [mikelpint](https://github.com/mikelpint)     |\n| Choco (Win)    | https://chocolatey.org/packages/nuclear/                | [JourneyOver](https://github.com/JourneyOver) |\n| Homebrew (Mac) | https://formulae.brew.sh/cask/nuclear                   | Homebrew                                      |\n| Snap           | https://snapcraft.io/nuclear                            | [nukeop](https://github.com/nukeop)           |\n| Flatpak        | https://flathub.org/apps/details/org.js.nuclear.Nuclear | [advaithm](https://github.com/advaithm)       |\n\nbig thanks to [ayyeve](https://github.com/ayyEve) for letting me (advaithm) use her server as a compile machine.\n## Community translations\nNuclear has already been translated to several languages, and we're always looking for contributors who would like to add more. Below is a list of currently available languages, along with contributors who helped to translate Nuclear to that language.\n\n| Language             | Contributor                                                                                          |\n|:--------------------:|:----------------------------------------------------------------------------------------------------:|\n| English              | N/A                                                                                                  |\n| French               | [charjac](https://github.com/charjac), [Zalax](https://github.com/Zalaxx)                            |\n| Dutch                | [Vistaus](https://github.com/Vistaus)                                                                |\n| Danish               | [Hansen1992](https://github.com/Hansen1992)                                                          |\n| Spanish              | [mlucas94](https://github.com/mlucas94), [emlautarom1](https://github.com/emlautarom1)               |\n| Polish               | [kazimierczak-robert](https://github.com/kazimierczak-robert), [gradzka](https://github.com/gradzka) |\n| German               | [schippas](https://github.com/schippas)                                                              |\n| Russian              | [ramstore07](https://github.com/ramstore07), [dmtrshat](https://github.com/dmtrshat)                 |\n| Brazilian Portuguese | [JoaoPedroMoraes](https://github.com/JoaoPedroMoraes)                                                |\n| Turkish              | [3DShark](https://github.com/3DShark)                                                                |\n| Italian              | [gello94](https://github.com/gello94)                                                                |\n| Slovak               | [MartinT](https://github.com/MartinTuroci)                                                           |\n| Czech                | [PetrTodorov](https://github.com/PetrTodorov)                                                        |\n| Tagalog              | [giftofgrub](https://github.com/giftofgrub)                                                          |\n| Traditional Chinese  | [oxygen-TW](https://github.com/oxygen-TW)                                                            |\n| Swedish              | [PalleKarlsson](https://github.com/PalleKarlsson)                                                    |\n| Greek                | [Shuin-San](https://github.com/Shuin-San)                                                            |\n\n## Development process\n\nFirst of all, be sure to check out the [Contribution Wiki Page](https://github.com/nukeop/nuclear/wiki/Contributing).\n\nUse npm:\n```shell\n$ npm install # installs dependencies\n$ npm start\n```\n\nA new window should open that will load the web app and run Nuclear.\n\n---\nTo build for current operating system:\n```shell\n$ lerna bootstrap\n$ npm run build\n```\n\nInstead of `build` you can use `build:all` to build for all operating systems. The binaries will be in `packages/app/release`\n\n---\nIt's also possible to run the development environment using docker containers, but this should be considered experimental.\n\nYou will need docker and docker-compose. You need to allow the root user to connect to X11 display, and then you can run docker-compose:\n\n```shell\n$ xhost SI:localuser:root\n$ sudo docker-compose up dev\n```\nAs of now you can also build a flatpak version. You will need to install gobject-introspection, and flatpak-builder. After this you will need to install the runtimes and depedencies required by flatpak-builder for the compile process. You will need the 19.08 version of these flatpaks.\n```shell\n$ flatpak install flathub org.freedesktop.Platform\n$ flatpak install flathub org.freedesktop.Sdk\n$ flatpak install flathub io.atom.electron.BaseApp\n```\nNext, to build the project (use the `--verbose` flag to get more output):\n```shell\n$ flatpak-builder build-dir org.js.nuclear.Nuclear.json\n```\nTo run the built app: \n```shell\n$ flatpak-builder --run build-dir org.js.nuclear.Nuclear.json run.sh\n```\nYou can turn the app to a local repo. currently the file builds the latest release.\n\n## Screenshots\nThis will be updated as the program evolves.\n\n![Album Search](https://i.imgur.com/idFVnAF.png)\n\n![Album Display](https://i.imgur.com/Kvzo3q7.png)\n\n![Artist View](https://i.imgur.com/imBLYl3.png)\n\n![Dashboard Best New Music](https://i.imgur.com/bMDrR4M.png)\n\n![Dashboard Genres](https://i.imgur.com/g0aCmKx.png)\n\n![Playlist View](https://i.imgur.com/2VMXHDC.png)\n\n![Lyrics View](https://i.imgur.com/7e3DJKJ.png)\n\n![Equalizer View](https://i.imgur.com/WreRL0w.png)\n\n## License\n\nThis program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n"
 },
 {
  "repo": "twbs/bootstrap",
  "language": "JavaScript",
  "readme_contents": "<p align=\"center\">\n  <a href=\"https://v5.getbootstrap.com/\">\n    <img src=\"https://v5.getbootstrap.com/docs/5.0/assets/brand/bootstrap-logo-shadow.png\" alt=\"Bootstrap logo\" width=\"200\" height=\"165\">\n  </a>\n</p>\n\n<h3 align=\"center\">Bootstrap</h3>\n\n<p align=\"center\">\n  Sleek, intuitive, and powerful front-end framework for faster and easier web development.\n  <br>\n  <a href=\"https://v5.getbootstrap.com/docs/5.0/\"><strong>Explore Bootstrap docs \u00bb</strong></a>\n  <br>\n  <br>\n  <a href=\"https://github.com/twbs/bootstrap/issues/new?template=bug_report.md\">Report bug</a>\n  \u00b7\n  <a href=\"https://github.com/twbs/bootstrap/issues/new?template=feature_request.md\">Request feature</a>\n  \u00b7\n  <a href=\"https://themes.getbootstrap.com/\">Themes</a>\n  \u00b7\n  <a href=\"https://blog.getbootstrap.com/\">Blog</a>\n</p>\n\n\n## Bootstrap 4\n\nOur default branch is for development of our upcoming Bootstrap 5 release. Head to the [`v4-dev` branch](https://github.com/twbs/bootstrap/tree/v4-dev) to view the readme, documentation, and source code for Bootstrap 4.\n\n\n## Table of contents\n\n- [Quick start](#quick-start)\n- [Status](#status)\n- [What's included](#whats-included)\n- [Bugs and feature requests](#bugs-and-feature-requests)\n- [Documentation](#documentation)\n- [Contributing](#contributing)\n- [Community](#community)\n- [Versioning](#versioning)\n- [Creators](#creators)\n- [Thanks](#thanks)\n- [Copyright and license](#copyright-and-license)\n\n\n## Quick start\n\nSeveral quick start options are available:\n\n- [Download the latest release](https://github.com/twbs/bootstrap/archive/v5.0.0-alpha3.zip)\n- Clone the repo: `git clone https://github.com/twbs/bootstrap.git`\n- Install with [npm](https://www.npmjs.com/): `npm install bootstrap@next`\n- Install with [yarn](https://yarnpkg.com/): `yarn add bootstrap@next`\n- Install with [Composer](https://getcomposer.org/): `composer require twbs/bootstrap:5.0.0-alpha3`\n- Install with [NuGet](https://www.nuget.org/): CSS: `Install-Package bootstrap` Sass: `Install-Package bootstrap.sass`\n\nRead the [Getting started page](https://v5.getbootstrap.com/docs/5.0/getting-started/introduction/) for information on the framework contents, templates and examples, and more.\n\n\n## Status\n\n[![Slack](https://bootstrap-slack.herokuapp.com/badge.svg)](https://bootstrap-slack.herokuapp.com/)\n[![Build Status](https://github.com/twbs/bootstrap/workflows/JS%20Tests/badge.svg?branch=main)](https://github.com/twbs/bootstrap/actions?query=workflow%3AJS+Tests+branch%3Amain)\n[![npm version](https://img.shields.io/npm/v/bootstrap)](https://www.npmjs.com/package/bootstrap)\n[![Gem version](https://img.shields.io/gem/v/bootstrap)](https://rubygems.org/gems/bootstrap)\n[![Meteor Atmosphere](https://img.shields.io/badge/meteor-twbs%3Abootstrap-blue)](https://atmospherejs.com/twbs/bootstrap)\n[![Packagist Prerelease](https://img.shields.io/packagist/vpre/twbs/bootstrap)](https://packagist.org/packages/twbs/bootstrap)\n[![NuGet](https://img.shields.io/nuget/vpre/bootstrap)](https://www.nuget.org/packages/bootstrap/absoluteLatest)\n[![peerDependencies Status](https://img.shields.io/david/peer/twbs/bootstrap)](https://david-dm.org/twbs/bootstrap?type=peer)\n[![devDependency Status](https://img.shields.io/david/dev/twbs/bootstrap)](https://david-dm.org/twbs/bootstrap?type=dev)\n[![Coverage Status](https://img.shields.io/coveralls/github/twbs/bootstrap/main)](https://coveralls.io/github/twbs/bootstrap?branch=main)\n[![CSS gzip size](https://img.badgesize.io/twbs/bootstrap/main/dist/css/bootstrap.min.css?compression=gzip&label=CSS%20gzip%20size)](https://github.com/twbs/bootstrap/blob/main/dist/css/bootstrap.min.css)\n[![CSS Brotli size](https://img.badgesize.io/twbs/bootstrap/main/dist/css/bootstrap.min.css?compression=brotli&label=CSS%20Brotli%20size)](https://github.com/twbs/bootstrap/blob/main/dist/css/bootstrap.min.css)\n[![JS gzip size](https://img.badgesize.io/twbs/bootstrap/main/dist/js/bootstrap.min.js?compression=gzip&label=JS%20gzip%20size)](https://github.com/twbs/bootstrap/blob/main/dist/js/bootstrap.min.js)\n[![JS Brotli size](https://img.badgesize.io/twbs/bootstrap/main/dist/js/bootstrap.min.js?compression=brotli&label=JS%20Brotli%20size)](https://github.com/twbs/bootstrap/blob/main/dist/js/bootstrap.min.js)\n[![BrowserStack Status](https://www.browserstack.com/automate/badge.svg?badge_key=SkxZcStBeExEdVJqQ2hWYnlWckpkNmNEY213SFp6WHFETWk2bGFuY3pCbz0tLXhqbHJsVlZhQnRBdEpod3NLSDMzaHc9PQ==--3d0b75245708616eb93113221beece33e680b229)](https://www.browserstack.com/automate/public-build/SkxZcStBeExEdVJqQ2hWYnlWckpkNmNEY213SFp6WHFETWk2bGFuY3pCbz0tLXhqbHJsVlZhQnRBdEpod3NLSDMzaHc9PQ==--3d0b75245708616eb93113221beece33e680b229)\n[![Backers on Open Collective](https://img.shields.io/opencollective/backers/bootstrap)](#backers)\n[![Sponsors on Open Collective](https://img.shields.io/opencollective/sponsors/bootstrap)](#sponsors)\n\n\n## What's included\n\nWithin the download you'll find the following directories and files, logically grouping common assets and providing both compiled and minified variations. You'll see something like this:\n\n```text\nbootstrap/\n\u2514\u2500\u2500 dist/\n    \u251c\u2500\u2500 css/\n    \u2502   \u251c\u2500\u2500 bootstrap-grid.css\n    \u2502   \u251c\u2500\u2500 bootstrap-grid.css.map\n    \u2502   \u251c\u2500\u2500 bootstrap-grid.min.css\n    \u2502   \u251c\u2500\u2500 bootstrap-grid.min.css.map\n    \u2502   \u251c\u2500\u2500 bootstrap-reboot.css\n    \u2502   \u251c\u2500\u2500 bootstrap-reboot.css.map\n    \u2502   \u251c\u2500\u2500 bootstrap-reboot.min.css\n    \u2502   \u251c\u2500\u2500 bootstrap-reboot.min.css.map\n    \u2502   \u251c\u2500\u2500 bootstrap-utilities.css\n    \u2502   \u251c\u2500\u2500 bootstrap-utilities.css.map\n    \u2502   \u251c\u2500\u2500 bootstrap-utilities.min.css\n    \u2502   \u251c\u2500\u2500 bootstrap-utilities.min.css.map\n    \u2502   \u251c\u2500\u2500 bootstrap.css\n    \u2502   \u251c\u2500\u2500 bootstrap.css.map\n    \u2502   \u251c\u2500\u2500 bootstrap.min.css\n    \u2502   \u2514\u2500\u2500 bootstrap.min.css.map\n    \u2514\u2500\u2500 js/\n        \u251c\u2500\u2500 bootstrap.bundle.js\n        \u251c\u2500\u2500 bootstrap.bundle.js.map\n        \u251c\u2500\u2500 bootstrap.bundle.min.js\n        \u251c\u2500\u2500 bootstrap.bundle.min.js.map\n        \u251c\u2500\u2500 bootstrap.esm.js\n        \u251c\u2500\u2500 bootstrap.esm.js.map\n        \u251c\u2500\u2500 bootstrap.esm.min.js\n        \u251c\u2500\u2500 bootstrap.esm.min.js.map\n        \u251c\u2500\u2500 bootstrap.js\n        \u251c\u2500\u2500 bootstrap.js.map\n        \u251c\u2500\u2500 bootstrap.min.js\n        \u2514\u2500\u2500 bootstrap.min.js.map\n```\n\nWe provide compiled CSS and JS (`bootstrap.*`), as well as compiled and minified CSS and JS (`bootstrap.min.*`). [source maps](https://developers.google.com/web/tools/chrome-devtools/javascript/source-maps) (`bootstrap.*.map`) are available for use with certain browsers' developer tools. Bundled JS files (`bootstrap.bundle.js` and minified `bootstrap.bundle.min.js`) include [Popper](https://popper.js.org/).\n\n\n## Bugs and feature requests\n\nHave a bug or a feature request? Please first read the [issue guidelines](https://github.com/twbs/bootstrap/blob/main/.github/CONTRIBUTING.md#using-the-issue-tracker) and search for existing and closed issues. If your problem or idea is not addressed yet, [please open a new issue](https://github.com/twbs/bootstrap/issues/new).\n\n\n## Documentation\n\nBootstrap's documentation, included in this repo in the root directory, is built with [Hugo](https://gohugo.io/) and publicly hosted on GitHub Pages at <https://v5.getbootstrap.com/>. The docs may also be run locally.\n\nDocumentation search is powered by [Algolia's DocSearch](https://community.algolia.com/docsearch/). Working on our search? Be sure to set `debug: true` in `site/assets/js/src/search.js` file.\n\n### Running documentation locally\n\n1. Run `npm install` to install the Node.js dependencies, including Hugo (the site builder).\n2. Run `npm run test` (or a specific npm script) to rebuild distributed CSS and JavaScript files, as well as our docs assets.\n3. From the root `/bootstrap` directory, run `npm run docs-serve` in the command line.\n4. Open `http://localhost:9001/` in your browser, and voil\u00e0.\n\nLearn more about using Hugo by reading its [documentation](https://gohugo.io/documentation/).\n\n### Documentation for previous releases\n\nYou can find all our previous releases docs on <https://v5.getbootstrap.com/docs/versions/>.\n\n[Previous releases](https://github.com/twbs/bootstrap/releases) and their documentation are also available for download.\n\n\n## Contributing\n\nPlease read through our [contributing guidelines](https://github.com/twbs/bootstrap/blob/main/.github/CONTRIBUTING.md). Included are directions for opening issues, coding standards, and notes on development.\n\nMoreover, if your pull request contains JavaScript patches or features, you must include [relevant unit tests](https://github.com/twbs/bootstrap/tree/main/js/tests). All HTML and CSS should conform to the [Code Guide](https://github.com/mdo/code-guide), maintained by [Mark Otto](https://github.com/mdo).\n\nEditor preferences are available in the [editor config](https://github.com/twbs/bootstrap/blob/main/.editorconfig) for easy use in common text editors. Read more and download plugins at <https://editorconfig.org/>.\n\n\n## Community\n\nGet updates on Bootstrap's development and chat with the project maintainers and community members.\n\n- Follow [@getbootstrap on Twitter](https://twitter.com/getbootstrap).\n- Read and subscribe to [The Official Bootstrap Blog](https://blog.getbootstrap.com/).\n- Join [the official Slack room](https://bootstrap-slack.herokuapp.com/).\n- Chat with fellow Bootstrappers in IRC. On the `irc.freenode.net` server, in the `##bootstrap` channel.\n- Implementation help may be found at Stack Overflow (tagged [`bootstrap-5`](https://stackoverflow.com/questions/tagged/bootstrap-5)).\n- Developers should use the keyword `bootstrap` on packages which modify or add to the functionality of Bootstrap when distributing through [npm](https://www.npmjs.com/browse/keyword/bootstrap) or similar delivery mechanisms for maximum discoverability.\n\n\n## Versioning\n\nFor transparency into our release cycle and in striving to maintain backward compatibility, Bootstrap is maintained under [the Semantic Versioning guidelines](https://semver.org/). Sometimes we screw up, but we adhere to those rules whenever possible.\n\nSee [the Releases section of our GitHub project](https://github.com/twbs/bootstrap/releases) for changelogs for each release version of Bootstrap. Release announcement posts on [the official Bootstrap blog](https://blog.getbootstrap.com/) contain summaries of the most noteworthy changes made in each release.\n\n\n## Creators\n\n**Mark Otto**\n\n- <https://twitter.com/mdo>\n- <https://github.com/mdo>\n\n**Jacob Thornton**\n\n- <https://twitter.com/fat>\n- <https://github.com/fat>\n\n\n## Thanks\n\n<a href=\"https://www.browserstack.com/\">\n  <img src=\"https://live.browserstack.com/images/opensource/browserstack-logo.svg\" alt=\"BrowserStack Logo\" width=\"192\" height=\"42\">\n</a>\n\nThanks to [BrowserStack](https://www.browserstack.com/) for providing the infrastructure that allows us to test in real browsers!\n\n\n## Sponsors\n\nSupport this project by becoming a sponsor. Your logo will show up here with a link to your website. [[Become a sponsor](https://opencollective.com/bootstrap#sponsor)]\n\n[![OC sponsor 0](https://opencollective.com/bootstrap/sponsor/0/avatar.svg)](https://opencollective.com/bootstrap/sponsor/0/website)\n[![OC sponsor 1](https://opencollective.com/bootstrap/sponsor/1/avatar.svg)](https://opencollective.com/bootstrap/sponsor/1/website)\n[![OC sponsor 2](https://opencollective.com/bootstrap/sponsor/2/avatar.svg)](https://opencollective.com/bootstrap/sponsor/2/website)\n[![OC sponsor 3](https://opencollective.com/bootstrap/sponsor/3/avatar.svg)](https://opencollective.com/bootstrap/sponsor/3/website)\n[![OC sponsor 4](https://opencollective.com/bootstrap/sponsor/4/avatar.svg)](https://opencollective.com/bootstrap/sponsor/4/website)\n[![OC sponsor 5](https://opencollective.com/bootstrap/sponsor/5/avatar.svg)](https://opencollective.com/bootstrap/sponsor/5/website)\n[![OC sponsor 6](https://opencollective.com/bootstrap/sponsor/6/avatar.svg)](https://opencollective.com/bootstrap/sponsor/6/website)\n[![OC sponsor 7](https://opencollective.com/bootstrap/sponsor/7/avatar.svg)](https://opencollective.com/bootstrap/sponsor/7/website)\n[![OC sponsor 8](https://opencollective.com/bootstrap/sponsor/8/avatar.svg)](https://opencollective.com/bootstrap/sponsor/8/website)\n[![OC sponsor 9](https://opencollective.com/bootstrap/sponsor/9/avatar.svg)](https://opencollective.com/bootstrap/sponsor/9/website)\n\n\n## Backers\n\nThank you to all our backers! \ud83d\ude4f [[Become a backer](https://opencollective.com/bootstrap#backer)]\n\n[![Backers](https://opencollective.com/bootstrap/backers.svg?width=890)](https://opencollective.com/bootstrap#backers)\n\n\n## Copyright and license\n\nCode and documentation copyright 2011\u20132020 the [Bootstrap Authors](https://github.com/twbs/bootstrap/graphs/contributors) and [Twitter, Inc.](https://twitter.com) Code released under the [MIT License](https://github.com/twbs/bootstrap/blob/main/LICENSE). Docs released under [Creative Commons](https://creativecommons.org/licenses/by/3.0/).\n"
 },
 {
  "repo": "animate-css/animate.css",
  "language": "CSS",
  "readme_contents": "# Animate.css\n\n[![GitHub Version](https://img.shields.io/github/release/animate-css/animate.css.svg?style=for-the-badge)](https://github.com/animate-css/animate.css/releases) [![Github Star](https://img.shields.io/github/stars/animate-css/animate.css.svg?style=for-the-badge)](https://github.com/animate-css/animate.css/stargazers) [![Github Fork](https://img.shields.io/github/forks/animate-css/animate.css.svg?style=for-the-badge)](https://github.com/animate-css/animate.css/network/members) [![License](https://img.shields.io/github/license/animate-css/animate.css.svg?style=for-the-badge)](https://github.com/animate-css/animate.css/blob/main/LICENSE)\n\n> If you need the old docs - v3.x.x and under - you can find it [here](https://github.com/animate-css/animate.css/tree/a8d92e585b1b302f7749809c3308d5e381f9cb17).\n\n## _Just-add-water CSS animation_\n\n## Installation\n\nInstall with npm:\n\n```shell\nnpm install animate.css --save\n```\n\nInstall with yarn:\n\n```shell\nyarn add animate.css\n```\n\n## Getting started\n\nYou can find the Animate.css documentation on the [website](https://animate.style/).\n\n## Accessibility\n\nAnimate.css supports the [`prefers-reduced-motion` media query](https://webkit.org/blog/7551/responsive-design-for-motion/) so that users with motion sensitivity can opt out of animations. On supported platforms (currently all the majors browsers and OS), users can select \"reduce motion\" on their operating system preferences and it will turn off CSS transitions for them without any further work required.\n\n## Core team\n\n| ![Daniel Eden](https://avatars2.githubusercontent.com/u/439365?s=460&u=512b4cc5324938ae40bbb8f3b7769d335953cd3a&v=4) | ![Elton Mesquita](https://avatars2.githubusercontent.com/u/5007208?s=460&u=418401ee605824272e5dcb955fd64ea24546a857&v=4) | ![Waren Gonzaga](https://avatars1.githubusercontent.com/u/15052701?s=460&u=9e58364978379536d3f26c4ce5cae1a2a449a0e4&v=4) |\n| --- | --- | --- |\n| [Daniel Eden](https://github.com/daneden) | [Elton Mesquita](https://github.com/eltonmesquita) | [Waren Gonzaga](https://github.com/WarenGonzaga) |\n| Animate.css Creator | Maintainer | Core Contributor |\n\n## License\n\nAnimate.css is licensed under the MIT license. <https://opensource.org/licenses/MIT>\n\n## Code of Conduct\n\nThis project and everyone participating in it is governed by the [Contributor Covenant Code of Conduct](CODE_OF_CONDUCT.md). By participating, you are expected to uphold this code. Please report unacceptable behavior to [callmeelton@gmail.com](mailto:callmeelton@gmail.com).\n\n## Contributing\n\nPull requests are the way to go here. We only have two rules for submitting a pull request: match the naming convention (camelCase, categorised [fades, bounces, etc]) and let us see a demo of submitted animations in a [pen](https://codepen.io). That **last one is important**.\n"
 },
 {
  "repo": "designmodo/Flat-UI",
  "language": "JavaScript",
  "readme_contents": "\n# [Flat UI Free](https://designmodo.github.io/Flat-UI/)\n\nFlat UI is a beautiful theme for [Bootstrap](http://getbootstrap.com). We have redesigned many of its components to look flat in every pixel.\n\nTo get started, check out [getting started page](https://designmodo.github.io/Flat-UI/docs/getting-started.html) or follow instructions below.\n\n## Links:\n\n+ [Demo Page](https://designmodo.github.io/Flat-UI/)\n+ [Download PSD](https://designmodo.com/flat-free/) (for designers)\n+ [Flat UI Pro 1.4.0](https://designmodo.com/flat/) (from $39)\n\n## Quick start\n\nThree quick start options are available:\n\n- [Download the latest release](https://github.com/designmodo/Flat-UI/archive/2.3.0.zip).\n- Clone the repo: `git clone https://github.com/designmodo/Flat-UI.git`.\n- Install with [Bower](http://bower.io): `bower install flat-ui`.\n\nRun `npm install` and `bower install` to install dependencies.\n\nRead the [getting started page](https://designmodo.github.io/Flat-UI/docs/getting-started.html) for information on the framework contents.\n\nExamples: <https://github.com/designmodo/Flat-UI/tree/master/docs/examples>.\n\n\n### What's included\n\nWithin the download you'll find the following directories and files, logically grouping common assets and providing both compiled and minified variations. You'll see something like this:\n\n```\nflat-ui/\n\u2514\u2500\u2500 app/\n  \u251c\u2500\u2500 css/\n  \u251c\u2500\u2500 fonts/\n  \u251c\u2500\u2500 images/\n  \u251c\u2500\u2500 scripts/\n  \u2514\u2500\u2500 styles/\n      \u251c\u2500\u2500 mixins/\n      \u251c\u2500\u2500 modules/\n      \u251c\u2500\u2500 flat-ui.scss\n      \u251c\u2500\u2500 _mixins.scss\n      \u251c\u2500\u2500 _spaces.scss\n      \u2514\u2500\u2500 _variables.scss\n\u251c\u2500\u2500 dist/\n  \u251c\u2500\u2500 css/\n  \u251c\u2500\u2500 fonts/\n  \u251c\u2500\u2500 images/\n  \u251c\u2500\u2500 scripts/\n\u251c\u2500\u2500 docs/\n  \u251c\u2500\u2500 examples/\n  \u251c\u2500\u2500 components.html\n  \u251c\u2500\u2500 getting-started.html\n  \u251c\u2500\u2500 index.html\n  \u2514\u2500\u2500 template.html\n```\n\nWe provide compiled CSS and JS (`flat-ui.*`), as well as compiled and minified CSS and JS (`flat-ui.min.*`). Fonts with icons are included.\n\n## Documentation\n\nFlat UI's documentation, included in this repo in the docs directory and publicly hosted on GitHub Pages at <https://designmodo.github.io/Flat-UI/docs/components.html>.\n\n\n## SASS support\n\nIf you are interested in SASS source files - you can visit this project made by @wingrunr21:\n[Designmodo's Flat-UI ported to SASS with support for Flat-UI Pro](https://github.com/wingrunr21/flat-ui-sass)\n\n## Copyright and license\n\nFlat UI Free is licensed under a Creative Commons Attribution 3.0 Unported (CC BY 3.0)  (http://creativecommons.org/licenses/by/3.0/) and MIT License - http://opensource.org/licenses/mit-license.html.\n\nYou are allowed to use these elements anywhere you want, however we\u2019ll highly appreciate if you will link to our [website](https://designmodo.com).\n\n## Typeface\n\nFlat UI Free is made using the Lato typeface, which can be downloaded for free here: http://www.google.com/webfonts/specimen/Lato\n\n## Designmodo Products\n\n**Postcards - [https://designmodo.com/postcards/](https://designmodo.com/postcards/)**\n\nCreate beautiful responsive emails and newsletters with a simple drag & drop. It includes more than 100 modules to\nhelp you create custom emails faster than ever before.\n\n**Slides Framework - [https://designmodo.com/slides/](https://designmodo.com/slides/)**\n\nSlides is a framework based on its own CSS3, JavaScript, and HTML5 language with a unique code and structure that lets you create, with minimum effort, visually captivating websites with a lot of built-in options.\n\n**Startup Framework - [https://designmodo.com/startup/](https://designmodo.com/startup/)**\n\nStartup Framework is based on Bootstrap and help you to create beautiful and responsive websites. Build your website quickly with an intuitive and easy-to-use drag and drop interface.\n\n## Other Resources:\n\n**[Bootstrap Templates](https://designmodo.com/bootstrap-templates/)**\n\n**[Newsletter Templates](https://designmodo.com/email-newsletter-templates/)**\n\n**[Website Templates](https://designmodo.com/static-website-templates/)**\n\n\n## Useful Links\n\nMore products from Designmodo: <https://designmodo.com/products/>\n\nDesignmodo Market: <http://market.designmodo.com/>\n\nTutorials: <https://designmodo.com/tutorials/>\n\nFreebies: <https://designmodo.com/freebies/>\n\nAffiliate Program (earn money): <https://designmodo.com/affiliates/>\n\n**Social Media:**\n\nTwitter: <http://www.twitter.com/designmodo>\n\nFacebook: <http://www.facebook.com/designmodo>\n\nRSS: <http://feeds.feedburner.com/designmodo>\n\nGoogle+: <https://www.google.com/+Designmodo>\n"
 },
 {
  "repo": "h5bp/html5-boilerplate",
  "language": "JavaScript",
  "readme_contents": "# [HTML5 Boilerplate](https://html5boilerplate.com/)\n\n[![Build status](https://github.com/h5bp/html5-boilerplate/workflows/Build%20status/badge.svg)](https://github.com/h5bp/html5-boilerplate/actions?query=workflow%3A%22Build+status%22+branch%3Amaster)\n[![LICENSE](https://img.shields.io/badge/license-MIT-lightgrey.svg)](https://github.com/h5bp/html5-boilerplate/blob/master/LICENSE.txt)\n[![devDependency Status](https://david-dm.org/h5bp/html5-boilerplate/dev-status.svg)](https://david-dm.org/h5bp/html5-boilerplate#info=devDependencies)\n[![NPM Downloads](https://img.shields.io/npm/dt/html5-boilerplate.svg)](https://www.npmjs.com/package/html5-boilerplate)\n[![github-stars-image](https://img.shields.io/github/stars/h5bp/html5-boilerplate.svg?label=github%20stars)](https://github.com/h5bp/html5-boilerplate)\n\nHTML5 Boilerplate is a professional front-end template for building\nfast, robust, and adaptable web apps or sites.\n\nThis project is the product of over 10 years of iterative development and\ncommunity knowledge. It does not impose a specific development\nphilosophy or framework, so you're free to architect your code in the\nway that you want.\n\n* Homepage: [https://html5boilerplate.com/](https://html5boilerplate.com/)\n* Source: [https://github.com/h5bp/html5-boilerplate](https://github.com/h5bp/html5-boilerplate)\n* Twitter: [@h5bp](https://twitter.com/h5bp)\n\n## Quick start\n\nChoose one of the following options:\n\n* Download the latest stable release from\n  [html5boilerplate.com](https://html5boilerplate.com/). This zip file is a\n  snapshot of the `dist` folder. On Windows, Mac and from the file manager on\n  Linux unzipping this folder will output to a folder named something like\n  `html5-boilerplate_v7.3.0`. From the command line will need to create a\n  folder and unzip the contents into that folder.\n\n  ```bash\n  mkdir html5-boilerplate\n  unzip html5-boilerplate*.zip -d html5-boilerplate\n  ```\n\n* Clone the git repo \u2014 `git clone\n  https://github.com/h5bp/html5-boilerplate.git` - and checkout the\n  [tagged release](https://github.com/h5bp/html5-boilerplate/releases)\n  you'd like to use. The `dist` folder represents the latest version of the\n  project for end users.\n\n* Install with [npm](https://www.npmjs.com/): `npm install html5-boilerplate`\n  or [yarn](https://yarnpkg.com/): `yarn add html5-boilerplate`. The resulting\n  `node_modules/html5-boilerplate/dist` folder represents the latest version of\n  the project for end users. Depending on what you want to use and how you want\n  to use it, you may have to copy and paste the contents of that folder into\n  your project directory.\n\n* Using our new [create-html5-boilerplate](https://github.com/h5bp/create-html5-boilerplate)\n  project, instantly fetch the latest npm published package (or any version\n  available on npm) with `npx`, `npm init` or `yarn create` without having to\n  install any dependencies. Running the following `npx` command installs the\n  latest version into a folder called `new-site`\n\n  ```\n  npx create-html5-boilerplate new-site\n  cd new-site\n  npm install\n  npm start\n  ```\n\n## Features\n\n* A finely-tuned starter template. Reap the benefits of 10 years of analysis,\n  research and experimentation by over 200 contributors.\n* Designed with progressive enhancement in mind.\n* Includes:\n  * [`Normalize.css`](https://necolas.github.com/normalize.css/)\n    for CSS normalizations and common bug fixes\n  * A custom build of [`Modernizr`](https://modernizr.com/) for feature\n    detection\n  * [`Apache Server Configs`](https://github.com/h5bp/server-configs-apache)\n    that improve the web site's performance and security\n* Placeholder Open Graph elements and attributes.\n* An example package.json file with [Parcel](https://parceljs.org/) commands\n  built in to jumpstart application development\n* Placeholder CSS Media Queries.\n* Useful CSS helper classes.\n* Default print styles, performance optimized.\n* An optimized version of the Google Universal Analytics snippet.\n* Protection against any stray `console` statements causing JavaScript\n  errors in older browsers.\n* \"Delete-key friendly.\" Easy to strip out parts you don't need.\n* Extensive documentation.\n\n## Browser support\n\n* Chrome *(latest 2)*\n* Edge *(latest 2)*\n* Firefox *(latest 2)*\n* Internet Explorer 11\n* Opera *(latest 2)*\n* Safari *(latest 2)*\n\n*This doesn't mean that HTML5 Boilerplate cannot be used in older browsers,\njust that we'll ensure compatibility with the ones mentioned above.*\n\nIf you need legacy browser support you can use [HTML5 Boilerplate v6](https://github.com/h5bp/html5-boilerplate/releases/tag/6.1.0) (IE9/IE10)\nor [HTML5 Boilerplate v5](https://github.com/h5bp/html5-boilerplate/releases/tag/5.3.0)\n(IE 8). They are no longer actively developed.\n\n## Documentation\n\nTake a look at the [documentation table of contents](dist/doc/TOC.md). This\ndocumentation is bundled with the project which makes it available for offline\nreading and provides a useful starting point for any documentation you want to\nwrite about your project.\n\n## Contributing\n\nHundreds of developers have helped to make the HTML5 Boilerplate. Anyone is\nwelcome to [contribute](.github/CONTRIBUTING.md), however, if you decide to get\ninvolved, please take a moment to review the [guidelines](.github/CONTRIBUTING.md):\n\n* [Bug reports](.github/CONTRIBUTING.md#bugs)\n* [Feature requests](.github/CONTRIBUTING.md#features)\n* [Pull requests](.github/CONTRIBUTING.md#pull-requests)\n\n## License\n\nThe code is available under the [MIT license](LICENSE.txt).\n"
 },
 {
  "repo": "foundation/foundation-sites",
  "language": "HTML",
  "readme_contents": "<p align=\"center\">\n  <a href=\"http://get.foundation/\">\n    <img src=\"https://user-images.githubusercontent.com/9939075/38782856-2a64a43e-40fa-11e8-89cd-e873af03b3c4.png\" alt=\"Foundation for Sites 6\" width=\"448px\" style=\"max-width:100%;\"/>\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://get.foundation/sites/docs/installation.html\"><b>Install</b></a>\n  | <a href=\"https://get.foundation/sites/docs\">Documentation</a>\n  | <a href=\"https://github.com/foundation/foundation-sites/releases\">Releases</a>\n  | <a href=\"CONTRIBUTING.md\">Contributing</a>\n</p>\n\n---\n\n[![Build Status](https://github.com/foundation/foundation-sites/workflows/CI/badge.svg)](https://github.com/foundation/foundation-sites/actions?workflow=CI)\n[![dependencies Status](https://david-dm.org/foundation/foundation-sites/status.svg)](https://david-dm.org/foundation/foundation-sites)\n[![devDependencies Status](https://david-dm.org/foundation/foundation-sites/dev-status.svg)](https://david-dm.org/foundation/foundation-sites?type=dev)\n[![npm version](https://badge.fury.io/js/foundation-sites.svg)](https://badge.fury.io/js/foundation-sites)\n[![Bower version](https://badge.fury.io/bo/foundation-sites.svg)](https://badge.fury.io/bo/foundation-sites)\n[![Gem Version](https://badge.fury.io/rb/foundation-rails.svg)](https://badge.fury.io/rb/foundation-rails)\n[![jsDelivr Hits](https://data.jsdelivr.com/v1/package/npm/foundation-sites/badge?style=rounded)](https://www.jsdelivr.com/package/npm/foundation-sites)\n[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/zurb/foundation-sites?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n[![Netlify Status](https://api.netlify.com/api/v1/badges/da72b0f9-3d51-4d50-951e-6bbf5fe88601/deploy-status)](https://app.netlify.com/sites/foundation-sites/deploys)\n[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=foundation_foundation-sites&metric=alert_status)](https://sonarcloud.io/dashboard?id=foundation_foundation-sites)\n[![Known Vulnerabilities](https://snyk.io/test/github/foundation/foundation-sites/badge.svg)](https://snyk.io/test/github/foundation/foundation-sites)\n[![BrowserStack Status](https://automate.browserstack.com/badge.svg?badge_key=ZlJCVGIxaEgvaFc4TWhBZ0hFWGtIMjBRZEw0UnFrUys3ZGdrbmZ6TW5lZz0tLU9wZUdFV2lmNVd1dU9XbWxuQ05BOGc9PQ==--915d78e23eeed2ae37ce7a670bc370011a9e4fd9)](https://automate.browserstack.com/public-build/ZlJCVGIxaEgvaFc4TWhBZ0hFWGtIMjBRZEw0UnFrUys3ZGdrbmZ6TW5lZz0tLU9wZUdFV2lmNVd1dU9XbWxuQ05BOGc9PQ==--915d78e23eeed2ae37ce7a670bc370011a9e4fd9)\n\n\nFoundation is the most advanced responsive front-end framework in the world. Quickly go from prototype to production, building sites or apps that work on any kind of device with Foundation. Includes a fully customizable, responsive grid, a large library of Sass mixins, commonly used JavaScript plugins, and full accessibility support.\n\n---\n\n## Run locally\n\n### Documentation\n\nTo run the documentation locally on your machine, you need [Node.js](https://nodejs.org/en/) installed on your computer. (Your Node.js version must be **6.4.0** or higher). Run these commands to set up the documentation:\n\n```bash\n# Install\ngit clone https://github.com/foundation/foundation-sites\ncd foundation-sites\nyarn\n\n# Start the documentation\nyarn start\n```\n\n### Testing\n\nFoundation has three kinds of tests: JavaScript, Sass, and visual regression. Refer to our [testing guide](https://github.com/foundation/foundation-sites/wiki/Testing-Guide) for more details.\n\nRun tests with:\n```bash\n# Sass unit tests\nyarn test:sass\n# JavaScript unit tests\nyarn test:javascript:units\n# Visual tests\nyarn test:visual\n```\n\n## Contributing\n\nCheck out [CONTRIBUTING.md](CONTRIBUTING.md) to see how to report an issue or submit a bug fix or a new feature, and our [contributing guide](https://get.foundation/get-involved/contribute.html) to learn how you can contribute more globally to Foundation. You can also browse the [Help Wanted](https://github.com/foundation/foundation-sites/labels/help%20wanted) tag in our issue tracker to find things to do.\n\n## Training\n\nWant the guided tour to Foundation from the team that built it? The ZURB team offers comprehensive training courses for developers of all skill levels. If you're new to Foundation, check out the [Introduction to Foundation Course](http://zurb.com/university/foundation-intro?utm_source=Github%20Repo&utm_medium=website&utm_campaign=readme&utm_content=readme%20training%20link) to kickstart your skills, amplify your productivity, and get a comprehensive overview of everything Foundation has to offer. More advanced users should check out the [Advanced Foundation Course](http://zurb.com/university/advanced-foundation-training?utm_source=Github%20Repo&utm_medium=website&utm_campaign=readme&utm_content=readme%20training%20link) to learn the Advanced skills that ZURB uses to deliver quality client work in short timeframes.\n\nCopyright \u00a9 2020 Foundation Community\n"
 },
 {
  "repo": "Modernizr/Modernizr",
  "language": "JavaScript",
  "readme_contents": "# Modernizr \n[![npm version](https://badge.fury.io/js/modernizr.svg)](https://badge.fury.io/js/modernizr)\n[![Build Status](https://api.travis-ci.org/Modernizr/Modernizr.svg?branch=master)](https://travis-ci.org/Modernizr/Modernizr) \n[![Build Status](https://ci.appveyor.com/api/projects/status/github/Modernizr/modernizr?branch=master&svg=true)](https://ci.appveyor.com/project/rejas/modernizr) \n[![codecov](https://codecov.io/gh/Modernizr/Modernizr/branch/master/graph/badge.svg)](https://codecov.io/gh/Modernizr/Modernizr)\n[![Inline docs](https://inch-ci.org/github/Modernizr/Modernizr.svg?branch=master)](https://inch-ci.org/github/Modernizr/Modernizr)\n\n\n- Read this file in Portuguese-BR [here](/README.pt_br.md)\n\n##### Modernizr is a JavaScript library that detects HTML5 and CSS3 features in the user\u2019s browser.\n\n- [Website](https://modernizr.com)\n- [Documentation](https://modernizr.com/docs/)\n- [Integration tests](https://modernizr.github.io/Modernizr/test/integration.html)\n- [Unit tests](https://modernizr.github.io/Modernizr/test/unit.html)\n\nModernizr tests which native CSS3 and HTML5 features are available in the current UA and makes the results available to you in two ways: as properties on a global `Modernizr` object, and as classes on the `<html>` element. This information allows you to progressively enhance your pages with a granular level of control over the experience.\n\n## Breaking changes with v4\n\n- Dropped Node 8 Support, please upgrade to Node v10\n\n- These tests got removed:\n\n    - `touchevents`: [discussion](https://github.com/Modernizr/Modernizr/pull/2432) \n    - `unicode`: [discussion](https://github.com/Modernizr/Modernizr/issues/2468) \n    - `templatestrings`: duplicate of the es6 detect `stringtemplate`\n    - `contains`: duplicate of the es6 detect `es6string`\n    - `datalistelem`: A dupe of Modernizr.input.list\n\n## New Asynchronous Event Listeners\n\nOften times people want to know when an asynchronous test is done so they can allow their application to react to it.\nIn the past, you've had to rely on watching properties or `<html>` classes. Only events on **asynchronous** tests are\nsupported. Synchronous tests should be handled synchronously to improve speed and to maintain consistency.\n\nThe new API looks like this:\n\n```js\n// Listen to a test, give it a callback\nModernizr.on('testname', function( result ) {\n  if (result) {\n    console.log('The test passed!');\n  }\n  else {\n    console.log('The test failed!');\n  }\n});\n```\n\nWe guarantee that we'll only invoke your function once (per time that you call `on`). We are currently not exposing\na method for exposing the `trigger` functionality. Instead, if you'd like to have control over async tests, use the\n`src/addTest` feature, and any test that you set will automatically expose and trigger the `on` functionality.\n\n## Getting Started\n\n- Clone or download the repository\n- Install project dependencies with `npm install`\n\n## Building Modernizr \n\n### From javascript\n\nModernizr can be used programmatically via npm:\n\n```js\nvar modernizr = require(\"modernizr\");\n```\n\nA `build` method is exposed for generating custom Modernizr builds. Example:\n\n```javascript\nvar modernizr = require(\"modernizr\");\n\nmodernizr.build({}, function (result) {\n  console.log(result); // the build\n});\n```\n\nThe first parameter takes a JSON object of options and feature-detects to include. See [`lib/config-all.json`](lib/config-all.json) for all available options.\n\nThe second parameter is a function invoked on task completion.\n\n### From the command-line\n\nWe also provide a command line interface for building modernizr. \nTo see all available options run:\n\n```shell\n./bin/modernizr\n```\n\nOr to generate everything in 'config-all.json' run this with npm:\n\n```shell\nnpm start\n//outputs to ./dist/modernizr-build.js\n```\n\n## Testing Modernizr\n\nTo execute the tests using mocha-headless-chrome on the console run:\n\n```shell\nnpm test\n```\n\nYou can also run tests in the browser of your choice with this command:\n\n```shell\nnpm run serve-gh-pages\n```\n\nand navigating to these two URLs:\n\n```shell\nhttp://localhost:8080/test/unit.html\nhttp://localhost:8080/test/integration.html\n```\n\n## Code of Conduct\n\nThis project adheres to the [Open Code of Conduct](https://github.com/Modernizr/Modernizr/blob/master/.github/CODE_OF_CONDUCT.md). \nBy participating, you are expected to honor this code.\n\n\n## License\n\n[MIT License](https://opensource.org/licenses/MIT)"
 },
 {
  "repo": "twbs/ratchet",
  "language": "CSS",
  "readme_contents": "# [Ratchet](http://goratchet.com)\n[![GitHub Release](https://img.shields.io/github/release/twbs/ratchet.svg)](https://github.com/twbs/ratchet/releases)\n[![Build Status](https://img.shields.io/travis/twbs/ratchet/master.svg)](https://travis-ci.org/twbs/ratchet)\n[![devDependency Status](https://img.shields.io/david/dev/twbs/ratchet.svg)](https://david-dm.org/twbs/ratchet#info=devDependencies)\n\nBuild mobile apps with simple HTML, CSS, and JS components.\n\n## Table of contents\n\n* [Getting started](#getting-started)\n* [Documentation](#documentation)\n* [Support](#support)\n* [Contributing](#contributing)\n* [Troubleshooting](#troubleshooting)\n* [Versioning](#versioning)\n* [Maintainers](#maintainers)\n* [License](#license)\n\n## Getting started\n\n* Clone the repo with `git clone https://github.com/twbs/ratchet.git` or just [download](http://github.com/twbs/ratchet/archive/v2.0.2.zip) the bundled CSS and JS\n* [Read the docs](http://goratchet.com) to learn about the components and how to get a prototype on your phone\n* [Check out examples](http://goratchet.com/examples/)\n\nTake note that our master branch is our active, unstable development branch and that if you're looking to download a stable copy of the repo, check the [tagged downloads](https://github.com/twbs/ratchet/tags).\n\n### What's included\n\nWithin the download you'll find the following directories and files, logically grouping common assets and providing both compiled and minified variations. You'll see something like this:\n\n```\nratchet/\n\u251c\u2500\u2500 css/\n\u2502   \u251c\u2500\u2500 ratchet.css\n\u2502   \u251c\u2500\u2500 ratchet.min.css\n\u2502   \u251c\u2500\u2500 ratchet-theme-android.css\n\u2502   \u251c\u2500\u2500 ratchet-theme-android.min.css\n\u2502   \u251c\u2500\u2500 ratchet-theme-ios.css\n\u2502   \u2514\u2500\u2500 ratchet-theme-ios.min.css\n\u251c\u2500\u2500 js/\n\u2502   \u251c\u2500\u2500 ratchet.js\n\u2502   \u2514\u2500\u2500 ratchet.min.js\n\u2514\u2500\u2500 fonts/\n    \u251c\u2500\u2500 ratchicons.eot\n    \u251c\u2500\u2500 ratchicons.svg\n    \u251c\u2500\u2500 ratchicons.ttf\n    \u2514\u2500\u2500 ratchicons.woff\n```\n\nWe provide compiled CSS and JS (`ratchet.*`), as well as compiled and minified CSS and JS (`ratchet.min.*`). The Ratchicon fonts are included, as are the optional Android and iOS platform themes.\n\n## Documentation\n\nRatchet's documentation is built with [Jekyll](http://jekyllrb.com) and publicly hosted on GitHub Pages at <http://goratchet.com>. The docs may also be run locally.\n\n### Running documentation locally\n\n1. If necessary, [install Jekyll](http://jekyllrb.com/docs/installation) (requires v3.0.x).\n  * **Windows users:** Read [this unofficial guide](http://jekyll-windows.juthilo.com/) to get Jekyll up and running without problems.\n2. Install the Ruby-based syntax highlighter, [Rouge](https://github.com/jneen/rouge), with `gem install rouge`.\n3. From the root `/ratchet` directory, run `jekyll serve` in the command line.\n4. Open <http://localhost:4000> in your browser, and boom!\n\nLearn more about using Jekyll by reading its [documentation](http://jekyllrb.com/docs/home/).\n\n### Documentation for previous releases\n\nDocumentation for v1.0.2 has been made available for the time being at <http://goratchet.com/1.0.2/> while folks transition to Ratchet 2.\n\n[Previous releases](https://github.com/twbs/ratchet/releases) and their documentation are also available for download.\n\n## Support\n\nQuestions or discussions about Ratchet should happen in the [Google group](https://groups.google.com/forum/#!forum/goratchet) or hit us up on Twitter [@GoRatchet](https://twitter.com/goratchet).\n\n## Contributing\n\nPlease file a GitHub issue to [report a bug](https://github.com/twbs/ratchet/issues). When reporting a bug, be sure to follow the [contributor guidelines](https://github.com/twbs/ratchet/blob/master/CONTRIBUTING.md).\n\n## Troubleshooting\n\nA small list of \"gotchas\" is provided below for designers and developers starting to work with Ratchet.\n\n* Ratchet is designed to respond to touch events from a mobile device. In order to use mouse click events (for desktop browsing and testing), you have a few options:\n  * Enable touch event emulation in Chrome (found in the overrides tab in the web inspector preferences)\n  * Use a JavaScript library like fingerblast.js to emulate touch events (ideally only loaded from desktop devices)\n* Script tags containing JavaScript will not be executed on pages that are loaded with push.js. If you would like to attach event handlers to elements on other pages, document-level event delegation is a common solution.\n* Ratchet uses XHR requests to fetch additional pages inside the application. Due to security concerns, modern browsers prevent XHR requests when opening files locally (aka using the `file://` protocol); consequently, Ratchet does not work when opened directly as a file.\n  * A common solution to this is to simply serve the files from a local server. One convenient way to achieve this is to run `python -m SimpleHTTPServer <port>` to serve up the files in the current directory to `http://localhost:<port>`\n\n## Versioning\n\nFor transparency into our release cycle and in striving to maintain backward compatibility, Ratchet is maintained under the Semantic Versioning guidelines. Sometimes we screw up, but we'll adhere to these rules whenever possible.\n\nReleases will be numbered with the following format:\n\n`<major>.<minor>.<patch>`\n\nAnd constructed with the following guidelines:\n\n* Breaking backward compatibility **bumps the major** while resetting minor and patch\n* New additions without breaking backward compatibility **bumps the minor** while resetting the patch\n* Bug fixes and misc changes **bumps only the patch**\n\nFor more information on SemVer, please visit <http://semver.org/>.\n\n## Maintainers\n\nConnor Sears\n\n* <https://twitter.com/connors>\n* <https://github.com/connors>\n\n\nCreated by Connor Sears, Dave Gamache, and Jacob Thornton.\n\n\n## License\n\nRatchet is licensed under the [MIT License](http://opensource.org/licenses/MIT).\n"
 },
 {
  "repo": "atlemo/SubtlePatterns",
  "language": "HTML",
  "readme_contents": "Subtle Patterns\n===============\n\nView all the patterns from [Subtle Patterns](https://www.toptal.com/designers/subtlepatterns/) - including the famous Photoshop .pat file.\n\nIf you need Base64 versions of the patterns, you can use this generator:\nhttp://www.greywyvern.com/code/php/binary2base64.\n\nThe original authors of the patterns are credited on Subtle Patterns. Subtle Patterns is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. The patterns here can be used free of charge, but please read this before using them: [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/).\n\n[Subtle Patterns](https://www.toptal.com/designers/subtlepatterns/) \u00a9 2017 [Toptal](https://www.toptal.com).\n"
 },
 {
  "repo": "fivethirtyeight/data",
  "language": "Jupyter Notebook",
  "readme_contents": "See https://data.fivethirtyeight.com/ for a list of the data and code we've published.\n\nUnless otherwise noted, our data sets are available under the [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/), and the code is available under the [MIT License](https://opensource.org/licenses/MIT). If you find this information useful, please [let us know](mailto:data@fivethirtyeight.com).\n"
 },
 {
  "repo": "datadesk/notebooks",
  "language": "Python",
  "readme_contents": "# datadesk/notebooks\n\nAll 35 of our computational notebooks. Also available as [a CSV file](notebooks.csv). Elsewhere you can find our open-source [software packages](https://github.com/datadesk/packages) and [tutorials](https://github.com/datadesk/tutorials).\n\n| date | slug | description |\n|:--|:--|:--|\n|  2019-12-12 | [star-wars-analysis](https://github.com/datadesk/star-wars-analysis) | Star Wars dialogue analysis |\n|  2019-11-07 | [wine-country-fires](https://github.com/datadesk/wine-country-fires/) | Every fire in California's wine country since 1950 |\n|  2019-11-01 | [deadspin-scraper](https://github.com/datadesk/deadspin-scraper) | Scrape posts from Deadspin |\n|  2019-10-08 | [ripa-analysis](https://github.com/datadesk/ripa-analysis) | Racial disparities in LAPD search rates |\n|  2019-09-19 | [women-homicides-analysis](https://github.com/datadesk/women-homicides-analysis) | The rising number of women being killed |\n|  2019-09-09 | [homeless-sleeping-restrictions](https://github.com/datadesk/homeless-sleeping-restrictions) | Mapping a proposed ban on street sleeping |\n|  2019-06-13 | [native-american-census-analysis](https://github.com/datadesk/native-american-census-analysis) | Native American census analysis |\n|  2019-05-29 | [la-weedmaps-analysis](https://github.com/datadesk/la-weedmaps-analysis) | Mapping LA's unauthorized pot dispensaries |\n|  2019-04-29 | [census-hard-to-map-analysis](https://github.com/datadesk/census-hard-to-map-analysis) | Mapping hard-to-count Census tracts |\n|  2019-04-26 | [hsr-document-analysis](https://github.com/datadesk/hsr-document-analysis) | How many pages can one agency publish? |\n|  2019-04-16 | [lawlers-law](https://github.com/ryanvmenezes/lawlers-law) | Does the first team to score 100 points usually win? |\n|  2019-03-29 | [swana-census-analysis](https://github.com/datadesk/swana-census-analysis) | Are Arabs and Iranians white? |\n|  2019-03-22 | [fire-aircraft-delay](https://github.com/kyleykim/R_Scripts/tree/master/la-me-ln-california-fire-aircraft-delay) | When wildfires in California have started |\n|  2019-03-12 | [hhh-unequal](https://github.com/kyleykim/R_Scripts/tree/master/la-me-ln-hhh-unequal) | Some neighborhoods are behind on homeless housing |\n|  2019-02-27 | [highschool-homicide-analysis](https://github.com/datadesk/highschool-homicide-analysis) | Homicides near high schools |\n|  2019-02-11 | [deleon-district-election-results-analysis](https://github.com/datadesk/deleon-district-election-results-analysis) | How former state Sen. Kevin de Le\u00f3n fared |\n|  2018-12-27 | [nyrb-covers-analysis](https://github.com/datadesk/nyrb-covers-analysis) | Color and content of NYRB Classics book covers |\n|  2018-12-18 | [california-fire-zone-analysis](https://github.com/datadesk/california-fire-zone-analysis) | California buildings in fire hazard zones |\n|  2018-11-18 | [helicopter-accident-analysis](https://github.com/datadesk/helicopter-accident-analysis) | Helicopter accident rates |\n|  2018-10-20 | [hollister-ranch-analysis](https://github.com/datadesk/hollister-ranch-analysis) | Agricultural property tax breaks in Hollister Ranch |\n|  2018-06-27 | [la-settlements-analysis](https://github.com/datadesk/la-settlements-analysis) | Legal payouts made by L.A. city |\n|  2018-04-28 | [california-k12-notebooks](https://github.com/datadesk/california-k12-notebooks) | Download and process California K12 data |\n|  2018-03-16 | [street-racing-analysis](https://github.com/datadesk/street-racing-analysis) | Street racing fatalities in L.A. County |\n|  2018-02-04 | [homeless-arrests-analysis](https://github.com/datadesk/homeless-arrests-analysis) | Arrests of the homeless by the LAPD |\n|  2017-12-14 | [ferc-enforcement-analysis](https://github.com/datadesk/ferc-enforcement-analysis) | Civil penalties issued by FERC |\n|  2017-11-08 | [houston-flood-zone-analysis](https://github.com/datadesk/houston-flood-zone-analysis) | Houston homes after Hurricane Harvey |\n|  2017-08-13 | [california-kindergarten-vaccination-analysis](https://github.com/datadesk/california-kindergarten-vaccination-analysis) | California kindergartens with the lowest vaccination rates |\n|  2017-05-25 | [california-h2a-visas-analysis](https://github.com/datadesk/california-h2a-visas-analysis) | Temporary visas granted to foreign agricultural workers |\n|  2017-04-20 | [construction-jobs-analysis](https://github.com/datadesk/construction-jobs-analysis) | The demographics and pay of construction workers |\n|  2017-03-17 | [california-crop-production-wages-analysis](https://github.com/datadesk/california-crop-production-wages-analysis) | Crop worker pay in California |\n|  2017-02-05 | [california-electricity-capacity-analysis](https://github.com/datadesk/california-electricity-capacity-analysis) | California's costly power glut |\n|  2016-10-31 | [california-ccscore-analysis](https://github.com/datadesk/california-ccscore-analysis) | Water usage after the state eased drought restrictions |\n|  2016-06-15 | [la-vacant-building-complaints-analysis](https://github.com/datadesk/la-vacant-building-complaints-analysis) | Vacant building complaints filed with L.A. city |\n|  2016-04-14 | [kobe-every-shot-ever](https://github.com/datadesk/kobe-every-shot-ever) | Every shot in Kobe Bryant's NBA career |\n|  2015-10-15 | [lapd-crime-classification-analysis](https://github.com/datadesk/lapd-crime-classification-analysis) | Serious assaults misclassified by LAPD |"
 },
 {
  "repo": "newsapps/beeswithmachineguns",
  "language": "Python",
  "readme_contents": "h4. Bees with Machine Guns!\n\nA utility for arming (creating) many bees (micro EC2 instances) to attack (load test) targets (web applications).\n\nAlso, retribution for \"this shameful act\":http://kottke.org/10/10/tiny-catapult-for-throwing-pies-at-bees against a proud hive.\n\nh2. Dependencies\n\n* Python 2.6 - 3.6\n* boto\n* paramiko\n\nh2. Installation for users\n\n<pre>\npip install https://github.com/newsapps/beeswithmachineguns/archive/master.zip\n</pre>\n\nh2. Installation for developers (w/ virtualenv + virtualenvwrapper)\n\n<pre>\ngit clone git://github.com/newsapps/beeswithmachineguns.git\ncd beeswithmachineguns\nmkvirtualenv --no-site-packages beesenv\neasy_install pip\npip install -r requirements.txt\n</pre>\n\nh2. Configuring AWS credentials\n\nBees uses boto to communicate with EC2 and thus supports all the same methods of storing credentials that it does.  These include declaring environment variables, machine-global configuration files, and per-user configuration files. You can read more about these options on \"boto's configuration page\":http://code.google.com/p/boto/wiki/BotoConfig.\n\nAt minimum, create a .boto file in your home directory with the following contents:\n\n<pre>\n[Credentials]\naws_access_key_id = <your access key>\naws_secret_access_key = <your secret key>\n</pre>\n\nThe credentials used must have sufficient access to EC2.\n\nMake sure the .boto file is only accessible by the current account:\n\n<pre>\nchmod 600 .boto\n</pre>\n\nh2. Usage\n\nA typical bees session looks something like this:\n\n<pre>\nbees up -s 4 -g public -k frakkingtoasters\nbees attack -n 10000 -c 250 -u http://www.ournewwebbyhotness.com/\nbees down\n</pre>\n\nA bees session where this is being called from a python file, while specifying content type and a payload file.\nThis is a working example, all of these objects exist in the us-east-1 region.\n\n<pre>\nimport bees\nimport json\n\nsOptions = '{\"post_file\":\"data.json\",\"contenttype\":\"application/json\"}'\noptions = json.loads(sOptions)\n\nbees.up(1,'bees-sg','us-east-1b','ami-5d155d37','m3.medium','ubuntu','commerce-bees','subnet-b12880e8')\nbees.attack('<URL TO TEST>',2,2,**options)\nbees.down()\n</pre>\n\nIn this case the data.json is a simple json file, mind the path.\n\nThis spins up 4 servers in security group 'public' using the EC2 keypair 'frakkingtoasters', whose private key is expected to reside at ~/.ssh/frakkingtoasters.pem.\n\n*Note*: the default EC2 security group is called 'default' and by default it locks out SSH access. I recommend creating a 'public' security group for use with the bees and explicitly opening port 22 on that group.\n\nIt then uses those 4 servers to send 10,000 requests, 250 at a time, to attack OurNewWebbyHotness.com.\n\nLastly, it spins down the 4 servers.  *Please remember to do this*--we aren't responsible for your EC2 bills.\n\nIf you wanted 3 agents requesting url A and one requesting url B, your attack would look as follows (empty url -> use previous):\n\n<pre>\nbees attack -n 10000 -c 250 -u 'http://url.a,,,http://url.b'\n</pre>\n\nFor complete options type:\n\n<pre>\nbees -h\n</pre>\n\nh2. Introduction to additions:\n\nh4. Additions contributed Hurl integration and multi regional testing.\n\n\n  *hurl* is an http server load tester similar to ab/siege/weighttp/wrk with support for multithreading, parallelism, ssl, url ranges, and an api-server for querying the running performance statistics.  *hurl* is primarily useful for benchmarking http server applications.\nFor more information about *hurl* please visit https://github.com/VerizonDigital/hlx\n\n  Multi regional testing was added so user can call up multiple bees from different regions simultaneously. Users have the ability to \u201cup\u201d, \u201cattack\u201d, and  \u201cdown\u201d instances from single command. \u201cregions.json\u201d file is supplied which contains public ami images with hurl pre installed for all regions.\n\n\n*What kind of changes were made that's different from the old?*\n  Instead of writing bees information into a single ~/.bees file, each zone recognized in arguments creates a new unique bees file. Bees.py was modified to read these files. Up, attack, and down functions are run with threads.\n\nexample .bees files in user home directory\n\n<pre>\n$ ls ~/.bees* | xargs -0 basename\n.bees.ap-southeast-1b\n.bees.eu-west-1b\n.bees.us-west-2b\n</pre>\n\n\nh4. Motivation\n\nHaving the ability to generate a lot of HTTPS requests from many different regions around the world allows us to better test our platforms and services. This is also real helpful when there are tools that need to be tested for such things as location of requests.\n\n\nh4. Hurl Usage\n\n\nh4. bees up\n\n  Command line arguments are still the same however to add multiple zones with multiple amis, the values must be comma delimited. The ami and zones must also be in same order for it to work. So for example \u201c-i ami-zone1,ami-zone2,ami-zone3 -z zone1,zone2,zone3\u201d.\n\n\n<pre>\n  ./bees up -s 2 -k bees -g bees2 -l ubuntu -i ami-9342c0e0,ami-fd489d\n9e,ami-e8c93e88 -z eu-west-1b,ap-southeast-1b,us-west-2b\n</pre>\n\nh4. bees attack\n\n  In order to use the hurl platform, --hurl or -j must be supplied. Attacks will run concurrently and return a summarized output. The output is summarized per region. More information can be seen if user supplies the -o, --long_output options.\n\n<pre>\n./bees attack --hurl -u $testurl -S20 -M1000 -H \"Accept : text/html\"\n</pre>\n\nh4. bees down\n\n  Bringing down bees is the same and will bring down all bees for all regions\n\n<pre>\n./bees down\n</pre>\n\n\n*regions used*: eu-west-1b,ap-southeast-1b,us-west-2b\n\nSome options were added to work with hurl\n\nh4. Examples\n\n<pre>\n$ ./bees --help\nUsage: \nbees COMMAND [options]\n\nBees with Machine Guns\n\nA utility for arming (creating) many bees (small EC2 instances) to attack\n(load test) targets (web applications).\n\ncommands:\n  up      Start a batch of load testing servers.\n  attack  Begin the attack on a specific url.\n  down    Shutdown and deactivate the load testing servers.\n  report  Report the status of the load testing servers.\n    \n\nOptions:\n  -h, --help            show this help message and exit\n\n  up:\n    In order to spin up new servers you will need to specify at least the\n    -k command, which is the name of the EC2 keypair to use for creating\n    and connecting to the new servers. The bees will expect to find a .pem\n    file with this name in ~/.ssh/. Alternatively, bees can use SSH Agent\n    for the key.\n\n    -k KEY, --key=KEY   The ssh key pair name to use to connect to the new\n                        servers.\n    -s SERVERS, --servers=SERVERS\n                        The number of servers to start (default: 5).\n    -g GROUP, --group=GROUP\n                        The security group(s) to run the instances under\n                        (default: default).\n    -z ZONE, --zone=ZONE\n                        The availability zone to start the instances in\n                        (default: us-east-1d).\n    -i INSTANCE, --instance=INSTANCE\n                        The instance-id to use for each server from (default:\n                        ami-ff17fb96).\n    -t TYPE, --type=TYPE\n                        The instance-type to use for each server (default:\n                        t1.micro).\n    -l LOGIN, --login=LOGIN\n                        The ssh username name to use to connect to the new\n                        servers (default: newsapps).\n    -v SUBNET, --subnet=SUBNET\n                        The vpc subnet id in which the instances should be\n                        launched. (default: None).\n    -b BID, --bid=BID   The maximum bid price per spot instance (default:\n                        None).\n\n  attack:\n    Beginning an attack requires only that you specify the -u option with\n    the URL you wish to target.\n\n    -u URL, --url=URL   URL of the target to attack.\n    -K KEEP_ALIVE, --keepalive=KEEP_ALIVE\n                        Keep-Alive connection.\n    -p POST_FILE, --post-file=POST_FILE\n                        The POST file to deliver with the bee's payload.\n    -m MIME_TYPE, --mime-type=MIME_TYPE\n                        The MIME type to send with the request.\n    -n NUMBER, --number=NUMBER\n                        The number of total connections to make to the target\n                        (default: 1000).\n    -C COOKIES, --cookies=COOKIES\n                        Cookies to send during http requests. The cookies\n                        should be passed using standard cookie formatting,\n                        separated by semi-colons and assigned with equals\n                        signs.\n    -Z CIPHERS, --ciphers=CIPHERS\n                        Openssl SSL/TLS cipher name(s) to use for negotiation.  Passed\n                        directly to ab's -Z option.  ab-only.\n    -c CONCURRENT, --concurrent=CONCURRENT\n                        The number of concurrent connections to make to the\n                        target (default: 100).\n    -H HEADERS, --headers=HEADERS\n                        HTTP headers to send to the target to attack. Multiple\n                        headers should be separated by semi-colons, e.g\n                        header1:value1;header2:value2\n    -e FILENAME, --csv=FILENAME\n                        Store the distribution of results in a csv file for\n                        all completed bees (default: '').\n    -P CONTENTTYPE, --contenttype=CONTENTTYPE\n                        ContentType header to send to the target of the\n                        attack.\n    -S SECONDS, --seconds=SECONDS\n                        hurl only: The number of total seconds to attack the\n                        target (default: 60).\n    -X VERB, --verb=VERB\n                        hurl only: Request command -HTTP verb to use\n                        -GET/PUT/etc. Default GET\n    -M RATE, --rate=RATE\n                        hurl only: Max Request Rate.\n    -a THREADS, --threads=THREADS\n                        hurl only: Number of parallel threads. Default: 1\n    -f FETCHES, --fetches=FETCHES\n                        hurl only: Num fetches per instance.\n    -d TIMEOUT, --timeout=TIMEOUT\n                        hurl only: Timeout (seconds).\n    -E SEND_BUFFER, --send_buffer=SEND_BUFFER\n                        hurl only: Socket send buffer size.\n    -F RECV_BUFFER, --recv_buffer=RECV_BUFFER\n                        hurl only: Socket receive buffer size.\n    -T TPR, --tpr=TPR   The upper bounds for time per request. If this option\n                        is passed and the target is below the value a 1 will\n                        be returned with the report details (default: None).\n    -R RPS, --rps=RPS   The lower bounds for request per second. If this\n                        option is passed and the target is above the value a 1\n                        will be returned with the report details (default:\n                        None).\n    -A basic_auth, --basic_auth=basic_auth\n                        BASIC authentication credentials, format auth-\n                        username:password (default: None).\n    -j, --hurl          use hurl\n    -o, --long_output   display hurl output\n    -L, --responses_per\n                        hurl only: Display http(s) response codes per interval\n                        instead of request statistics\n</pre>\n\nA bringing up bees example\n\n<pre>\n$ ./bees up -s 2 -k bees -g bees2 -l ubuntu -i ami-9342c0e0,ami-fd489d\n9e,ami-e8c93e88 -z eu-west-1b,ap-southeast-1b,us-west-2b\nConnecting to the hive.\nGroupId found: bees2\nPlacement: eu-west-1b\nAttempting to call up 2 bees.\nConnecting to the hive.\nGroupId found: bees2\nPlacement: ap-southeast-1b\nAttempting to call up 2 bees.\nConnecting to the hive.\nGroupId found: bees2\nPlacement: us-west-2b\nAttempting to call up 2 bees.\nWaiting for bees to load their machine guns...\nWaiting for bees to load their machine guns...\n.\nWaiting for bees to load their machine guns...\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nBee i-5568c1d9 is ready for the attack.\nBee i-5668c1da is ready for the attack.\nBee i-2cf8aba2 is ready for the attack.\nThe swarm has assembled 2 bees.\nBee i-2bf8aba5 is ready for the attack.\nThe swarm has assembled 2 bees.\nBee i-d05a6c08 is ready for the attack.\nBee i-d15a6c09 is ready for the attack.\nThe swarm has assembled 2 bees.\n\n$ ./bees report\nRead 2 bees from the roster: eu-west-1b\nBee i-5568c1d9: running @ 54.194.192.20\nBee i-5668c1da: running @ 54.194.197.233\nRead 2 bees from the roster: ap-southeast-1b\nBee i-2cf8aba2: running @ 52.77.228.132\nBee i-2bf8aba5: running @ 52.221.250.224\nRead 2 bees from the roster: us-west-2b\nBee i-d05a6c08: running @ 54.187.100.142\nBee i-d15a6c09: running @ 54.201.177.125\n</pre>\n\n\nA bees attack example\n\n<pre>\n$ ./bees attack --hurl -u $testurl -S20 -M1000 -H \"Accept : text/html\"\neu-west-1b\nRead 2 bees from the roster: eu-west-1b\nConnecting to the hive.\nAssembling bees.\nap-southeast-1b\nRead 2 bees from the roster: ap-southeast-1b\nConnecting to the hive.\nAssembling bees.\nus-west-2b\nRead 2 bees from the roster: us-west-2b\nConnecting to the hive.\nAssembling bees.\nEach of 2 bees will fire 500 rounds, 50 at a time.\nStinging URL so it will be cached for the attack.\nOrganizing the swarm.\nBee 0 is joining the swarm.\nBee 1 is joining the swarm.\nEach of 2 bees will fire 500 rounds, 50 at a time.\nStinging URL so it will be cached for the attack.\nOrganizing the swarm.\nBee 0 is joining the swarm.\nBee 1 is joining the swarm.\nBee 1 is firing her machine gun. Bang bang!\nBee 0 is firing her machine gun. Bang bang!\nEach of 2 bees will fire 500 rounds, 50 at a time.\nStinging URL so it will be cached for the attack.\nOrganizing the swarm.\nBee 0 is joining the swarm.\nBee 1 is joining the swarm.\nBee 1 is firing her machine gun. Bang bang!\nBee 0 is firing her machine gun. Bang bang!\nBee 0 is firing her machine gun. Bang bang!\nBee 1 is firing her machine gun. Bang bang!\nOffensive complete.\n\nSummarized Results\n     Total bytes:               32806393\n     Seconds:                   20\n     Connect-ms-max:            8.751000\n     1st-resp-ms-max:           288.797000\n     1st-resp-ms-mean:          41.120797\n     Fetches/sec mean:          881.607553\n     connect-ms-min:            0.000000\n     Total fetches:             35274\n     bytes/sec mean:            819934.329261\n     end2end-ms-min mean:       7.538500\n     mean-bytes-per-conn:       930.044494\n     connect-ms-mean:           0.022659\n\nResponse Codes:\n     2xx:                       372\n     3xx:                       0\n     4xx:                       34802\n     5xx:                       0\n\nMission Assessment: Target crushed bee offensive.\nThe swarm is awaiting new orders.\nOffensive complete.\n\nSummarized Results\n     Total bytes:               7820249\n     Seconds:                   20\n     Connect-ms-max:            198.442000\n     1st-resp-ms-max:           799.969000\n     1st-resp-ms-mean:          183.104679\n     Fetches/sec mean:          265.758560\n     connect-ms-min:            0.000000\n     Total fetches:             10633\n     bytes/sec mean:            195457.360660\n     end2end-ms-min mean:       167.372500\n     mean-bytes-per-conn:       735.470800\n     connect-ms-mean:           1.685486\n\nResponse Codes:\n     2xx:                       423\n     3xx:                       0\n     4xx:                       10110\n     5xx:                       0\n\nMission Assessment: Target crushed bee offensive.\nThe swarm is awaiting new orders.\nOffensive complete.\n\nSummarized Results\n     Total bytes:               26038521\n     Seconds:                   20\n     Connect-ms-max:            15.233000\n     1st-resp-ms-max:           401.819000\n     1st-resp-ms-mean:          42.217669\n     Fetches/sec mean:          873.584785\n     connect-ms-min:            0.000000\n     Total fetches:             34953\n     bytes/sec mean:            650784.075365\n     end2end-ms-min mean:       11.345000\n     mean-bytes-per-conn:       744.958106\n     connect-ms-mean:           0.037327\n\nResponse Codes:\n     2xx:                       411\n     3xx:                       0\n     4xx:                       34442\n     5xx:                       0\n\nMission Assessment: Target crushed bee offensive.\nThe swarm is awaiting new orders.\n</pre>\n\nA bees attack example with --long_output\n\n<pre>\n$ ./bees attack --hurl -u $testurl -S20 -M1000 -H \"Accept : text/html\" --long_output\neu-west-1b\nRead 2 bees from the roster: eu-west-1b\nConnecting to the hive.\nAssembling bees.\nap-southeast-1b\nRead 2 bees from the roster: ap-southeast-1b\nConnecting to the hive.\nAssembling bees.\nus-west-2b\nRead 2 bees from the roster: us-west-2b\nConnecting to the hive.\nAssembling bees.\nEach of 2 bees will fire 500 rounds, 50 at a time.\nStinging URL so it will be cached for the attack.\nOrganizing the swarm.\nBee 0 is joining the swarm.\nBee 1 is joining the swarm.\nEach of 2 bees will fire 500 rounds, 50 at a time.\nStinging URL so it will be cached for the attack.\nOrganizing the swarm.\nBee 0 is joining the swarm.\nBee 1 is joining the swarm.\nEach of 2 bees will fire 500 rounds, 50 at a time.\nStinging URL so it will be cached for the attack.\nOrganizing the swarm.\nBee 0 is joining the swarm.\nBee 1 is joining the swarm.\nBee 0 is firing her machine gun. Bang bang!\nBee 1 is firing her machine gun. Bang bang!\nBee 1 is firing her machine gun. Bang bang!\nBee 0 is firing her machine gun. Bang bang!\nBee 0 is firing her machine gun. Bang bang!\nBee 1 is firing her machine gun. Bang bang!\nhurl http://can.192bf.transactcdn.com/00192BF/test.html/config.workflow:ContinueWarranty -p 50  -H \"Accept : text/html\" -H \"Content-Type : text/plain\" -o /tmp/tmp.aAojMFs3ob -l 20 -A 1000 -j\n\ni-fb457323\nec2-54-186-204-52.us-west-2.compute.amazonaws.com\nRunning 1 threads 50 parallel connections per thread with 1 reqests per connection\n+-----------/-----------+-----------+-----------+--------------+-----------+-------------+-----------+\n|    Cmpltd /     Total |    IdlKil |    Errors | kBytes Recvd |   Elapsed |       Req/s |      MB/s |\n+-----------/-----------+-----------+-----------+--------------+-----------+-------------+-----------+\n|         0 /         0 |         0 |         0 |         0.00 |     0.50s |       0.00s |     0.00s |\n|       505 /       505 |         0 |         0 |       313.02 |     1.00s |    1007.98s |     0.61s |\n|       970 /       970 |         0 |         0 |       633.81 |     1.50s |     930.00s |     0.63s |\n|      1442 /      1442 |         0 |         0 |       961.08 |     2.00s |     944.00s |     0.64s |\n|      1900 /      1900 |         0 |         0 |      1276.81 |     2.50s |     916.00s |     0.62s |\n|      2376 /      2376 |         0 |         0 |      1606.85 |     3.00s |     952.00s |     0.64s |\n|      2835 /      2835 |         0 |         0 |      1922.85 |     3.50s |     918.00s |     0.62s |\n|      3310 /      3310 |         0 |         0 |      2252.20 |     4.00s |     950.00s |     0.64s |\n|      3769 /      3769 |         0 |         0 |      2568.62 |     4.50s |     918.00s |     0.62s |\n|      4243 /      4243 |         0 |         0 |      2897.27 |     5.00s |     946.11s |     0.64s |\n|      4706 /      4706 |         0 |         0 |      3215.84 |     5.50s |     926.00s |     0.62s |\n|      5178 /      5178 |         0 |         0 |      3543.10 |     6.00s |     944.00s |     0.64s |\n|      5641 /      5641 |         0 |         0 |      3862.09 |     6.50s |     926.00s |     0.62s |\n|      6114 /      6114 |         0 |         0 |      4190.05 |     7.00s |     946.00s |     0.64s |\n|      6581 /      6581 |         0 |         0 |      4511.81 |     7.50s |     934.00s |     0.63s |\n|      7053 /      7053 |         0 |         0 |      4839.07 |     8.00s |     944.00s |     0.64s |\n|      7514 /      7514 |         0 |         0 |      5156.25 |     8.50s |     922.00s |     0.62s |\n|      7975 /      7975 |         0 |         0 |      5475.88 |     9.00s |     920.16s |     0.62s |\n|      8450 /      8450 |         0 |         0 |      5803.62 |     9.50s |     950.00s |     0.64s |\n|      8910 /      8910 |         0 |         0 |      6122.53 |    10.00s |     920.00s |     0.62s |\n|      9382 /      9382 |         0 |         0 |      6447.58 |    10.50s |     944.00s |     0.63s |\n|      9844 /      9844 |         0 |         0 |      6766.37 |    11.00s |     924.00s |     0.62s |\n|     10316 /     10316 |         0 |         0 |      7093.14 |    11.50s |     944.00s |     0.64s |\n|     10778 /     10778 |         0 |         0 |      7411.51 |    12.00s |     924.00s |     0.62s |\n|     11250 /     11250 |         0 |         0 |      7738.70 |    12.50s |     944.00s |     0.64s |\n|     11710 /     11710 |         0 |         0 |      8056.08 |    13.00s |     920.00s |     0.62s |\n|     12184 /     12184 |         0 |         0 |      8384.46 |    13.50s |     946.11s |     0.64s |\n|     12644 /     12644 |         0 |         0 |      8701.20 |    14.00s |     920.00s |     0.62s |\n|     13119 /     13119 |         0 |         0 |      9030.51 |    14.50s |     950.00s |     0.64s |\n|     13582 /     13582 |         0 |         0 |      9349.72 |    15.00s |     926.00s |     0.62s |\n|     14053 /     14053 |         0 |         0 |      9676.28 |    15.50s |     942.00s |     0.64s |\n|     14516 /     14516 |         0 |         0 |      9994.85 |    16.00s |     926.00s |     0.62s |\n|     14987 /     14987 |         0 |         0 |     10321.41 |    16.50s |     942.00s |     0.64s |\n|     15450 /     15450 |         0 |         0 |     10640.41 |    17.00s |     926.00s |     0.62s |\n|     15922 /     15922 |         0 |         0 |     10967.66 |    17.50s |     944.00s |     0.64s |\n|     16386 /     16386 |         0 |         0 |     11287.13 |    18.00s |     926.15s |     0.62s |\n|     16386 /     16386 |         0 |         0 |     11287.13 |    18.50s |       0.00s |     0.00s |\n|     16855 /     16855 |         0 |         0 |     11612.32 |    19.00s |     938.00s |     0.64s |\n|     17320 /     17320 |         0 |         0 |     11932.69 |    19.50s |     930.00s |     0.63s |\n|     17794 /     17794 |         0 |         0 |     12261.34 |    20.00s |     948.00s |     0.64s |\n\nBee: i-fb457323\nmax-parallel:           50\n1st-resp-ms-min:        10.036\nresponse-codes\n        200:            184\n        403:            17560\nseconds:                20.006\nconnect-ms-max:         8.729\n1st-resp-ms-max:        130.893\nbytes:                  16541472.0\n1st-resp-ms-mean:       40.5336982642\nend2end-ms-mean:        40.5487644838\nfetches-per-sec:        889.433170049\nconnect-ms-min:         0.0\nfetches:                17794\nbytes-per-sec:          826825.552334\nend2end-ms-min:         10.053\nend2end-ms-max:         130.907\nmean-bytes-per-conn:    929.609531303\nconnect-ms-mean:        0.0224927299369\n\n\nhurl http://can.192bf.transactcdn.com/00192BF/test.html/config.workflow:ContinueWarranty -p 50  -H \"Accept : text/html\" -H \"Content-Type : text/plain\" -o /tmp/tmp.PkGEethemi -l 20 -A 1000 -j\n\ni-fa457322\nec2-54-186-127-166.us-west-2.compute.amazonaws.com\nRunning 1 threads 50 parallel connections per thread with 1 reqests per connection\n+-----------/-----------+-----------+-----------+--------------+-----------+-------------+-----------+\n|    Cmpltd /     Total |    IdlKil |    Errors | kBytes Recvd |   Elapsed |       Req/s |      MB/s |\n+-----------/-----------+-----------+-----------+--------------+-----------+-------------+-----------+\n|         0 /         0 |         0 |         0 |         0.00 |     0.50s |       0.00s |     0.00s |\n|       499 /       499 |         0 |         0 |       309.51 |     1.00s |     998.00s |     0.60s |\n|       965 /       965 |         0 |         0 |       631.24 |     1.50s |     930.14s |     0.63s |\n|      1435 /      1435 |         0 |         0 |       957.58 |     2.00s |     940.00s |     0.64s |\n|      1900 /      1900 |         0 |         0 |      1279.25 |     2.50s |     930.00s |     0.63s |\n|      2373 /      2373 |         0 |         0 |      1607.67 |     3.00s |     946.00s |     0.64s |\n|      2834 /      2834 |         0 |         0 |      1925.72 |     3.50s |     922.00s |     0.62s |\n|      3306 /      3306 |         0 |         0 |      2253.45 |     4.00s |     944.00s |     0.64s |\n|      3781 /      3781 |         0 |         0 |      2581.00 |     4.50s |     950.00s |     0.64s |\n|      4246 /      4246 |         0 |         0 |      2903.87 |     5.00s |     928.14s |     0.63s |\n|      4701 /      4701 |         0 |         0 |      3217.97 |     5.50s |     910.00s |     0.61s |\n|      5181 /      5181 |         0 |         0 |      3551.25 |     6.00s |     960.00s |     0.65s |\n|      5666 /      5666 |         0 |         0 |      3885.54 |     6.50s |     970.00s |     0.65s |\n|      6135 /      6135 |         0 |         0 |      4210.28 |     7.00s |     938.00s |     0.63s |\n|      6605 /      6605 |         0 |         0 |      4535.90 |     7.50s |     940.00s |     0.64s |\n|      7072 /      7072 |         0 |         0 |      4857.96 |     8.00s |     934.00s |     0.63s |\n|      7553 /      7553 |         0 |         0 |      5191.88 |     8.50s |     962.00s |     0.65s |\n|      8020 /      8020 |         0 |         0 |      5514.13 |     9.00s |     932.14s |     0.63s |\n|      8497 /      8497 |         0 |         0 |      5845.30 |     9.50s |     954.00s |     0.65s |\n|      8966 /      8966 |         0 |         0 |      6168.91 |    10.00s |     938.00s |     0.63s |\n|      9437 /      9437 |         0 |         0 |      6495.93 |    10.50s |     942.00s |     0.64s |\n|      9921 /      9921 |         0 |         0 |      6829.96 |    11.00s |     968.00s |     0.65s |\n|     10389 /     10389 |         0 |         0 |      7154.90 |    11.50s |     936.00s |     0.63s |\n|     10389 /     10389 |         0 |         0 |      7154.90 |    12.00s |       0.00s |     0.00s |\n|     10857 /     10857 |         0 |         0 |      7477.18 |    12.50s |     936.00s |     0.63s |\n|     11323 /     11323 |         0 |         0 |      7800.74 |    13.00s |     932.00s |     0.63s |\n|     11790 /     11790 |         0 |         0 |      8124.22 |    13.50s |     932.14s |     0.63s |\n|     12257 /     12257 |         0 |         0 |      8448.48 |    14.00s |     934.00s |     0.63s |\n|     12718 /     12718 |         0 |         0 |      8765.89 |    14.50s |     922.00s |     0.62s |\n|     13191 /     13191 |         0 |         0 |      9094.32 |    15.00s |     946.00s |     0.64s |\n|     13670 /     13670 |         0 |         0 |      9424.86 |    15.50s |     958.00s |     0.65s |\n|     14133 /     14133 |         0 |         0 |      9746.34 |    16.00s |     926.00s |     0.63s |\n|     14603 /     14603 |         0 |         0 |     10070.01 |    16.50s |     940.00s |     0.63s |\n|     15067 /     15067 |         0 |         0 |     10392.18 |    17.00s |     928.00s |     0.63s |\n|     15532 /     15532 |         0 |         0 |     10712.58 |    17.50s |     928.14s |     0.62s |\n|     16012 /     16012 |         0 |         0 |     11045.87 |    18.00s |     960.00s |     0.65s |\n|     16487 /     16487 |         0 |         0 |     11372.79 |    18.50s |     950.00s |     0.64s |\n|     16963 /     16963 |         0 |         0 |     11703.30 |    19.00s |     952.00s |     0.65s |\n|     17451 /     17451 |         0 |         0 |     12040.73 |    19.50s |     976.00s |     0.66s |\n|     17919 /     17919 |         0 |         0 |     12365.67 |    20.00s |     936.00s |     0.63s |\n\nBee: i-fa457322\nmax-parallel:           50\n1st-resp-ms-min:        7.623\nresponse-codes\n        200:            183\n        403:            17686\nseconds:                20.006\nconnect-ms-max:         8.547\n1st-resp-ms-max:        140.328\nbytes:                  16676307.0\n1st-resp-ms-mean:       39.7687282444\nend2end-ms-mean:        39.7864872125\nfetches-per-sec:        895.681295611\nconnect-ms-min:         0.0\nfetches:                17919\nbytes-per-sec:          833565.280416\nend2end-ms-min:         7.636\nend2end-ms-max:         140.342\nmean-bytes-per-conn:    930.649422401\nconnect-ms-mean:        0.0225320387263\n\n\nOffensive complete.\n\nSummarized Results\n     Total bytes:               33217779\n     Seconds:                   20\n     Connect-ms-max:            8.729000\n     1st-resp-ms-max:           140.328000\n     1st-resp-ms-mean:          40.151213\n     Fetches/sec mean:          892.557233\n     connect-ms-min:            0.000000\n     Total fetches:             35713\n     bytes/sec mean:            830195.416375\n     end2end-ms-min mean:       8.844500\n     mean-bytes-per-conn:       930.129477\n     connect-ms-mean:           0.022512\n\nResponse Codes:\n     2xx:                       367\n     3xx:                       0\n     4xx:                       35246\n     5xx:                       0\n\nMission Assessment: Target crushed bee offensive.\nThe swarm is awaiting new orders.\nhurl http://can.192bf.transactcdn.com/00192BF/test.html/config.workflow:ContinueWarranty -p 50  -H \"Accept : text/html\" -H \"Content-Type : text/plain\" -o /tmp/tmp.L7eQsLiKs9 -l 20 -A 1000 -j\n\ni-9e6fc612\nec2-54-194-180-232.eu-west-1.compute.amazonaws.com\nRunning 1 threads 50 parallel connections per thread with 1 reqests per connection\n+-----------/-----------+-----------+-----------+--------------+-----------+-------------+-----------+\n|    Cmpltd /     Total |    IdlKil |    Errors | kBytes Recvd |   Elapsed |       Req/s |      MB/s |\n+-----------/-----------+-----------+-----------+--------------+-----------+-------------+-----------+\n|         0 /         0 |         0 |         0 |         0.00 |     0.50s |       0.00s |     0.00s |\n|       484 /       484 |         0 |         0 |       220.82 |     1.00s |     968.00s |     0.43s |\n|       957 /       957 |         0 |         0 |       463.33 |     1.50s |     944.11s |     0.47s |\n|      1409 /      1409 |         0 |         0 |       692.96 |     2.00s |     904.00s |     0.45s |\n|      1872 /      1872 |         0 |         0 |       930.33 |     2.50s |     926.00s |     0.46s |\n|      2345 /      2345 |         0 |         0 |      1170.94 |     3.00s |     946.00s |     0.47s |\n|      2803 /      2803 |         0 |         0 |      1405.75 |     3.50s |     916.00s |     0.46s |\n|      3259 /      3259 |         0 |         0 |      1637.65 |     4.00s |     912.00s |     0.45s |\n|      3729 /      3729 |         0 |         0 |      1878.61 |     4.50s |     940.00s |     0.47s |\n|      4190 /      4190 |         0 |         0 |      2112.86 |     5.00s |     922.00s |     0.46s |\n|      4645 /      4645 |         0 |         0 |      2346.13 |     5.50s |     908.18s |     0.45s |\n|      5108 /      5108 |         0 |         0 |      2581.82 |     6.00s |     926.00s |     0.46s |\n|      5566 /      5566 |         0 |         0 |      2816.64 |     6.50s |     916.00s |     0.46s |\n|      6035 /      6035 |         0 |         0 |      3055.19 |     7.00s |     938.00s |     0.47s |\n|      6501 /      6501 |         0 |         0 |      3294.11 |     7.50s |     932.00s |     0.47s |\n|      6928 /      6928 |         0 |         0 |      3510.92 |     8.00s |     854.00s |     0.42s |\n|      7381 /      7381 |         0 |         0 |      3743.17 |     8.50s |     906.00s |     0.45s |\n|      7851 /      7851 |         0 |         0 |      3981.82 |     9.00s |     940.00s |     0.47s |\n|      8308 /      8308 |         0 |         0 |      4216.12 |     9.50s |     912.18s |     0.46s |\n|      8757 /      8757 |         0 |         0 |      4444.00 |    10.00s |     898.00s |     0.45s |\n|      9218 /      9218 |         0 |         0 |      4680.35 |    10.50s |     922.00s |     0.46s |\n|      9690 /      9690 |         0 |         0 |      4920.66 |    11.00s |     944.00s |     0.47s |\n|     10158 /     10158 |         0 |         0 |      5159.12 |    11.50s |     936.00s |     0.47s |\n|     10627 /     10627 |         0 |         0 |      5398.73 |    12.00s |     938.00s |     0.47s |\n|     11104 /     11104 |         0 |         0 |      5641.39 |    12.50s |     954.00s |     0.47s |\n|     11576 /     11576 |         0 |         0 |      5883.38 |    13.00s |     944.00s |     0.47s |\n|     12056 /     12056 |         0 |         0 |      6126.94 |    13.50s |     960.00s |     0.48s |\n|     12528 /     12528 |         0 |         0 |      6368.94 |    14.00s |     942.12s |     0.47s |\n|     12997 /     12997 |         0 |         0 |      6606.86 |    14.50s |     938.00s |     0.46s |\n|     13447 /     13447 |         0 |         0 |      6837.57 |    15.00s |     900.00s |     0.45s |\n|     13899 /     13899 |         0 |         0 |      7067.20 |    15.50s |     904.00s |     0.45s |\n|     14378 /     14378 |         0 |         0 |      7312.78 |    16.00s |     958.00s |     0.48s |\n|     14815 /     14815 |         0 |         0 |      7534.72 |    16.50s |     874.00s |     0.43s |\n|     14815 /     14815 |         0 |         0 |      7534.72 |    17.00s |       0.00s |     0.00s |\n|     15290 /     15290 |         0 |         0 |      7778.25 |    17.50s |     950.00s |     0.48s |\n|     15743 /     15743 |         0 |         0 |      8008.39 |    18.00s |     904.19s |     0.45s |\n|     16218 /     16218 |         0 |         0 |      8251.92 |    18.50s |     950.00s |     0.48s |\n|     16676 /     16676 |         0 |         0 |      8484.63 |    19.00s |     916.00s |     0.45s |\n|     17138 /     17138 |         0 |         0 |      8721.49 |    19.50s |     924.00s |     0.46s |\n|     17597 /     17597 |         0 |         0 |      8954.50 |    20.00s |     918.00s |     0.46s |\n\nBee: i-9e6fc612\nmax-parallel:           50\n1st-resp-ms-min:        11.354\nresponse-codes\n        200:            198\n        403:            17349\nseconds:                20.006\nconnect-ms-max:         15.13\n1st-resp-ms-max:        376.028\nbytes:                  13111135.0\n1st-resp-ms-mean:       41.7301567789\nend2end-ms-mean:        41.7460330541\nfetches-per-sec:        879.586124163\nconnect-ms-min:         0.0\nfetches:                17597\nbytes-per-sec:          655360.141957\nend2end-ms-min:         11.37\nend2end-ms-max:         376.044\nmean-bytes-per-conn:    745.07785418\nconnect-ms-mean:        0.0346185102867\n\n\nhurl http://can.192bf.transactcdn.com/00192BF/test.html/config.workflow:ContinueWarranty -p 50  -H \"Accept : text/html\" -H \"Content-Type : text/plain\" -o /tmp/tmp.cDsfCaHBDo -l 20 -A 1000 -j\n\ni-9d6fc611\nec2-54-194-207-186.eu-west-1.compute.amazonaws.com\nRunning 1 threads 50 parallel connections per thread with 1 reqests per connection\n+-----------/-----------+-----------+-----------+--------------+-----------+-------------+-----------+\n|    Cmpltd /     Total |    IdlKil |    Errors | kBytes Recvd |   Elapsed |       Req/s |      MB/s |\n+-----------/-----------+-----------+-----------+--------------+-----------+-------------+-----------+\n|         0 /         0 |         0 |         0 |         0.00 |     0.50s |       0.00s |     0.00s |\n|       490 /       490 |         0 |         0 |       222.63 |     1.00s |     980.00s |     0.43s |\n|       958 /       958 |         0 |         0 |       461.10 |     1.50s |     936.00s |     0.47s |\n|      1437 /      1437 |         0 |         0 |       704.57 |     2.00s |     958.00s |     0.48s |\n|      1913 /      1913 |         0 |         0 |       948.61 |     2.50s |     952.00s |     0.48s |\n|      2394 /      2394 |         0 |         0 |      1192.90 |     3.00s |     962.00s |     0.48s |\n|      2868 /      2868 |         0 |         0 |      1435.92 |     3.50s |     948.00s |     0.47s |\n|      3327 /      3327 |         0 |         0 |      1669.34 |     4.00s |     918.00s |     0.46s |\n|      3802 /      3802 |         0 |         0 |      1912.88 |     4.50s |     948.10s |     0.47s |\n|      4265 /      4265 |         0 |         0 |      2147.93 |     5.00s |     926.00s |     0.46s |\n|      4737 /      4737 |         0 |         0 |      2389.92 |     5.50s |     944.00s |     0.47s |\n|      5192 /      5192 |         0 |         0 |      2620.67 |     6.00s |     910.00s |     0.45s |\n|      5663 /      5663 |         0 |         0 |      2862.15 |     6.50s |     942.00s |     0.47s |\n|      6100 /      6100 |         0 |         0 |      3084.09 |     7.00s |     874.00s |     0.43s |\n|      6568 /      6568 |         0 |         0 |      3324.03 |     7.50s |     936.00s |     0.47s |\n|      7030 /      7030 |         0 |         0 |      3558.57 |     8.00s |     924.00s |     0.46s |\n|      7502 /      7502 |         0 |         0 |      3800.57 |     8.50s |     942.12s |     0.47s |\n|      7960 /      7960 |         0 |         0 |      4033.27 |     9.00s |     916.00s |     0.45s |\n|      8410 /      8410 |         0 |         0 |      4263.98 |     9.50s |     900.00s |     0.45s |\n|      8886 /      8886 |         0 |         0 |      4505.92 |    10.00s |     952.00s |     0.47s |\n|      9359 /      9359 |         0 |         0 |      4748.42 |    10.50s |     946.00s |     0.47s |\n|      9809 /      9809 |         0 |         0 |      4977.03 |    11.00s |     900.00s |     0.45s |\n|     10269 /     10269 |         0 |         0 |      5212.87 |    11.50s |     920.00s |     0.46s |\n|     10737 /     10737 |         0 |         0 |      5450.49 |    12.03s |     894.84s |     0.44s |\n|     11184 /     11184 |         0 |         0 |      5679.66 |    12.53s |     894.00s |     0.45s |\n|     11643 /     11643 |         0 |         0 |      5913.09 |    13.03s |     918.00s |     0.46s |\n|     12109 /     12109 |         0 |         0 |      6152.01 |    13.53s |     932.00s |     0.47s |\n|     12575 /     12575 |         0 |         0 |      6388.81 |    14.03s |     932.00s |     0.46s |\n|     13041 /     13041 |         0 |         0 |      6627.73 |    14.53s |     932.00s |     0.47s |\n|     13497 /     13497 |         0 |         0 |      6859.83 |    15.03s |     912.00s |     0.45s |\n|     13946 /     13946 |         0 |         0 |      7090.03 |    15.53s |     898.00s |     0.45s |\n|     14412 /     14412 |         0 |         0 |      7326.42 |    16.03s |     930.14s |     0.46s |\n|     14861 /     14861 |         0 |         0 |      7556.62 |    16.53s |     898.00s |     0.45s |\n|     15319 /     15319 |         0 |         0 |      7789.74 |    17.03s |     916.00s |     0.46s |\n|     15779 /     15779 |         0 |         0 |      8025.58 |    17.53s |     920.00s |     0.46s |\n|     15779 /     15779 |         0 |         0 |      8025.58 |    18.03s |       0.00s |     0.00s |\n|     16239 /     16239 |         0 |         0 |      8259.31 |    18.53s |     920.00s |     0.46s |\n|     16704 /     16704 |         0 |         0 |      8497.29 |    19.03s |     928.14s |     0.46s |\n|     17175 /     17175 |         0 |         0 |      8737.09 |    19.53s |     942.00s |     0.47s |\n|     17628 /     17628 |         0 |         0 |      8968.28 |    20.03s |     906.00s |     0.45s |\n\nBee: i-9d6fc611\nmax-parallel:           50\n1st-resp-ms-min:        11.29\nresponse-codes\n        200:            208\n        403:            17370\nseconds:                20.027\nconnect-ms-max:         15.115\n1st-resp-ms-max:        390.853\nbytes:                  13132194.0\n1st-resp-ms-mean:       41.6057616907\nend2end-ms-mean:        41.6218236432\nfetches-per-sec:        880.211714186\nconnect-ms-min:         0.0\nfetches:                17628\nbytes-per-sec:          655724.471963\nend2end-ms-min:         11.303\nend2end-ms-max:         390.866\nmean-bytes-per-conn:    744.962219197\nconnect-ms-mean:        0.0350904539766\n\n\nOffensive complete.\n\nSummarized Results\n     Total bytes:               26243329\n     Seconds:                   20\n     Connect-ms-max:            15.130000\n     1st-resp-ms-max:           390.853000\n     1st-resp-ms-mean:          41.667959\n     Fetches/sec mean:          879.898919\n     connect-ms-min:            0.000000\n     Total fetches:             35225\n     bytes/sec mean:            655542.306960\n     end2end-ms-min mean:       11.336500\n     mean-bytes-per-conn:       745.020037\n     connect-ms-mean:           0.034854\n\nResponse Codes:\n     2xx:                       406\n     3xx:                       0\n     4xx:                       34719\n     5xx:                       0\n\nMission Assessment: Target crushed bee offensive.\nThe swarm is awaiting new orders.\nhurl http://can.192bf.transactcdn.com/00192BF/test.html/config.workflow:ContinueWarranty -p 50  -H \"Accept : text/html\" -H \"Content-Type : text/plain\" -o /tmp/tmp.jfVcPxIqE5 -l 20 -A 1000 -j\n\ni-02fead8c\nec2-52-77-216-107.ap-southeast-1.compute.amazonaws.com\nRunning 1 threads 50 parallel connections per thread with 1 reqests per connection\n+-----------/-----------+-----------+-----------+--------------+-----------+-------------+-----------+\n|    Cmpltd /     Total |    IdlKil |    Errors | kBytes Recvd |   Elapsed |       Req/s |      MB/s |\n+-----------/-----------+-----------+-----------+--------------+-----------+-------------+-----------+\n|         0 /         0 |         0 |         0 |         0.00 |     0.50s |       0.00s |     0.00s |\n|       100 /       100 |         0 |         0 |        23.95 |     1.00s |     200.00s |     0.05s |\n|       250 /       250 |         0 |         0 |        98.11 |     1.50s |     300.00s |     0.14s |\n|       388 /       388 |         0 |         0 |       167.60 |     2.00s |     276.00s |     0.14s |\n|       529 /       529 |         0 |         0 |       239.25 |     2.50s |     282.00s |     0.14s |\n|       673 /       673 |         0 |         0 |       311.39 |     3.00s |     287.43s |     0.14s |\n|       816 /       816 |         0 |         0 |       384.29 |     3.50s |     286.00s |     0.14s |\n|       953 /       953 |         0 |         0 |       452.42 |     4.00s |     274.00s |     0.13s |\n|      1091 /      1091 |         0 |         0 |       522.96 |     4.50s |     276.00s |     0.14s |\n|      1226 /      1226 |         0 |         0 |       591.12 |     5.00s |     270.00s |     0.13s |\n|      1372 /      1372 |         0 |         0 |       664.71 |     5.50s |     292.00s |     0.14s |\n|      1516 /      1516 |         0 |         0 |       736.85 |     6.00s |     288.00s |     0.14s |\n|      1656 /      1656 |         0 |         0 |       808.20 |     6.50s |     280.00s |     0.14s |\n|      1795 /      1795 |         0 |         0 |       877.78 |     7.00s |     278.00s |     0.14s |\n|      1937 /      1937 |         0 |         0 |       950.16 |     7.50s |     283.43s |     0.14s |\n|      2073 /      2073 |         0 |         0 |      1018.41 |     8.00s |     272.00s |     0.13s |\n|      2213 /      2213 |         0 |         0 |      1089.34 |     8.50s |     280.00s |     0.14s |\n|      2356 /      2356 |         0 |         0 |      1161.60 |     9.00s |     286.00s |     0.14s |\n|      2495 /      2495 |         0 |         0 |      1232.02 |     9.50s |     278.00s |     0.14s |\n|      2637 /      2637 |         0 |         0 |      1303.14 |    10.00s |     284.00s |     0.14s |\n|      2779 /      2779 |         0 |         0 |      1375.31 |    10.50s |     284.00s |     0.14s |\n|      2916 /      2916 |         0 |         0 |      1443.86 |    11.00s |     274.00s |     0.13s |\n|      3052 /      3052 |         0 |         0 |      1513.17 |    11.50s |     272.00s |     0.14s |\n|      3194 /      3194 |         0 |         0 |      1584.70 |    12.00s |     283.43s |     0.14s |\n|      3334 /      3334 |         0 |         0 |      1656.06 |    12.50s |     280.00s |     0.14s |\n|      3471 /      3471 |         0 |         0 |      1724.40 |    13.00s |     274.00s |     0.13s |\n|      3605 /      3605 |         0 |         0 |      1793.10 |    13.50s |     268.00s |     0.13s |\n|      3747 /      3747 |         0 |         0 |      1864.22 |    14.00s |     284.00s |     0.14s |\n|      3882 /      3882 |         0 |         0 |      1932.80 |    14.50s |     270.00s |     0.13s |\n|      4026 /      4026 |         0 |         0 |      2004.73 |    15.00s |     288.00s |     0.14s |\n|      4166 /      4166 |         0 |         0 |      2075.87 |    15.50s |     280.00s |     0.14s |\n|      4306 /      4306 |         0 |         0 |      2146.38 |    16.00s |     280.00s |     0.14s |\n|      4451 /      4451 |         0 |         0 |      2220.09 |    16.50s |     289.42s |     0.14s |\n|      4597 /      4597 |         0 |         0 |      2293.05 |    17.00s |     292.00s |     0.14s |\n|      4728 /      4728 |         0 |         0 |      2360.21 |    17.50s |     262.00s |     0.13s |\n|      4865 /      4865 |         0 |         0 |      2428.76 |    18.00s |     274.00s |     0.13s |\n|      5008 /      5008 |         0 |         0 |      2501.44 |    18.50s |     286.00s |     0.14s |\n|      5153 /      5153 |         0 |         0 |      2574.10 |    19.00s |     289.42s |     0.14s |\n|      5292 /      5292 |         0 |         0 |      2645.15 |    19.50s |     278.00s |     0.14s |\n|      5429 /      5429 |         0 |         0 |      2713.91 |    20.00s |     274.00s |     0.13s |\n\nBee: i-02fead8c\nmax-parallel:           50\n1st-resp-ms-min:        167.119\nresponse-codes\n        200:            208\n        403:            5171\nseconds:                20.005\nconnect-ms-max:         194.213\n1st-resp-ms-max:        402.352\nbytes:                  3995143.0\n1st-resp-ms-mean:       179.083958914\nend2end-ms-mean:        179.106678751\nfetches-per-sec:        271.382154461\nconnect-ms-min:         0.0\nfetches:                5429\nbytes-per-sec:          199707.223194\nend2end-ms-min:         167.165\nend2end-ms-max:         402.373\nmean-bytes-per-conn:    735.889298213\nconnect-ms-mean:        1.63028611266\n\n\nhurl http://can.192bf.transactcdn.com/00192BF/test.html/config.workflow:ContinueWarranty -p 50  -H \"Accept : text/html\" -H \"Content-Type : text/plain\" -o /tmp/tmp.Ze2gnkVVd2 -l 20 -A 1000 -j\n\ni-03fead8d\nec2-54-254-220-121.ap-southeast-1.compute.amazonaws.com\nRunning 1 threads 50 parallel connections per thread with 1 reqests per connection\n+-----------/-----------+-----------+-----------+--------------+-----------+-------------+-----------+\n|    Cmpltd /     Total |    IdlKil |    Errors | kBytes Recvd |   Elapsed |       Req/s |      MB/s |\n+-----------/-----------+-----------+-----------+--------------+-----------+-------------+-----------+\n|         0 /         0 |         0 |         0 |         0.00 |     0.50s |       0.00s |     0.00s |\n|       100 /       100 |         0 |         0 |        23.10 |     1.00s |     199.60s |     0.05s |\n|       250 /       250 |         0 |         0 |        98.11 |     1.50s |     300.00s |     0.15s |\n|       388 /       388 |         0 |         0 |       167.38 |     2.00s |     276.00s |     0.14s |\n|       526 /       526 |         0 |         0 |       237.71 |     2.50s |     276.00s |     0.14s |\n|       669 /       669 |         0 |         0 |       308.92 |     3.00s |     286.00s |     0.14s |\n|       810 /       810 |         0 |         0 |       380.79 |     3.50s |     282.00s |     0.14s |\n|       948 /       948 |         0 |         0 |       449.85 |     4.00s |     276.00s |     0.13s |\n|      1083 /      1083 |         0 |         0 |       518.65 |     4.50s |     270.00s |     0.13s |\n|      1224 /      1224 |         0 |         0 |       589.04 |     5.00s |     281.44s |     0.14s |\n|      1363 /      1363 |         0 |         0 |       659.88 |     5.50s |     278.00s |     0.14s |\n|      1503 /      1503 |         0 |         0 |       729.76 |     6.00s |     280.00s |     0.14s |\n|      1642 /      1642 |         0 |         0 |       800.81 |     6.50s |     278.00s |     0.14s |\n|      1775 /      1775 |         0 |         0 |       867.31 |     7.00s |     266.00s |     0.13s |\n|      1917 /      1917 |         0 |         0 |       939.69 |     7.50s |     284.00s |     0.14s |\n|      2060 /      2060 |         0 |         0 |      1011.53 |     8.00s |     286.00s |     0.14s |\n|      2197 /      2197 |         0 |         0 |      1081.14 |     8.50s |     274.00s |     0.14s |\n|      2331 /      2331 |         0 |         0 |      1147.94 |     9.00s |     268.00s |     0.13s |\n|      2474 /      2474 |         0 |         0 |      1221.26 |     9.50s |     285.43s |     0.14s |\n|      2618 /      2618 |         0 |         0 |      1292.98 |    10.00s |     288.00s |     0.14s |\n|      2756 /      2756 |         0 |         0 |      1363.52 |    10.50s |     276.00s |     0.14s |\n|      2893 /      2893 |         0 |         0 |      1432.07 |    11.00s |     274.00s |     0.13s |\n|      3035 /      3035 |         0 |         0 |      1504.45 |    11.50s |     284.00s |     0.14s |\n|      3170 /      3170 |         0 |         0 |      1572.19 |    12.00s |     270.00s |     0.13s |\n|      3301 /      3301 |         0 |         0 |      1639.35 |    12.50s |     262.00s |     0.13s |\n|      3439 /      3439 |         0 |         0 |      1707.99 |    13.00s |     276.00s |     0.13s |\n|      3574 /      3574 |         0 |         0 |      1777.21 |    13.50s |     269.46s |     0.13s |\n|      3712 /      3712 |         0 |         0 |      1845.64 |    14.00s |     276.00s |     0.13s |\n|      3851 /      3851 |         0 |         0 |      1916.90 |    14.50s |     278.00s |     0.14s |\n|      3987 /      3987 |         0 |         0 |      1984.94 |    15.00s |     272.00s |     0.13s |\n|      4124 /      4124 |         0 |         0 |      2054.97 |    15.50s |     274.00s |     0.14s |\n|      4266 /      4266 |         0 |         0 |      2125.88 |    16.00s |     284.00s |     0.14s |\n|      4408 /      4408 |         0 |         0 |      2198.26 |    16.50s |     284.00s |     0.14s |\n|      4539 /      4539 |         0 |         0 |      2263.73 |    17.00s |     262.00s |     0.13s |\n|      4679 /      4679 |         0 |         0 |      2334.88 |    17.50s |     280.00s |     0.14s |\n|      4822 /      4822 |         0 |         0 |      2406.50 |    18.00s |     285.43s |     0.14s |\n|      4962 /      4962 |         0 |         0 |      2478.07 |    18.50s |     280.00s |     0.14s |\n|      5101 /      5101 |         0 |         0 |      2548.07 |    19.00s |     278.00s |     0.14s |\n|      5240 /      5240 |         0 |         0 |      2618.91 |    19.50s |     278.00s |     0.14s |\n|      5382 /      5382 |         0 |         0 |      2689.82 |    20.00s |     284.00s |     0.14s |\n\nBee: i-03fead8d\nmax-parallel:           50\n1st-resp-ms-min:        167.37\nresponse-codes\n        200:            208\n        403:            5124\nseconds:                20.005\nconnect-ms-max:         197.678\n1st-resp-ms-max:        396.03\nbytes:                  3959940.0\n1st-resp-ms-mean:       180.888643473\nend2end-ms-mean:        180.91185859\nfetches-per-sec:        269.032741815\nconnect-ms-min:         0.0\nfetches:                5382\nbytes-per-sec:          197947.513122\nend2end-ms-min:         167.395\nend2end-ms-max:         396.049\nmean-bytes-per-conn:    735.774804905\nconnect-ms-mean:        1.66231845461\n\n\nOffensive complete.\n\nSummarized Results\n     Total bytes:               7955083\n     Seconds:                   20\n     Connect-ms-max:            197.678000\n     1st-resp-ms-max:           402.352000\n     1st-resp-ms-mean:          179.986301\n     Fetches/sec mean:          270.207448\n     connect-ms-min:            0.000000\n     Total fetches:             10811\n     bytes/sec mean:            198827.368158\n     end2end-ms-min mean:       167.280000\n     mean-bytes-per-conn:       735.832052\n     connect-ms-mean:           1.646302\n\nResponse Codes:\n     2xx:                       416\n     3xx:                       0\n     4xx:                       10295\n     5xx:                       0\n\nMission Assessment: Target crushed bee offensive.\nThe swarm is awaiting new orders.\n(trusty)rawm@localhost:~/beeswithmachineguns$\n</pre>\n\n\nAn example bees down\n\n<pre>\n$ ./bees down\nRead 2 bees from the roster: eu-west-1b\nConnecting to the hive.\nCalling off the swarm for eu-west-1b.\nStood down 2 bees.\nRead 2 bees from the roster: ap-southeast-1b\nConnecting to the hive.\nCalling off the swarm for ap-southeast-1b.\nStood down 2 bees.\nRead 2 bees from the roster: us-west-2b\nConnecting to the hive.\nCalling off the swarm for us-west-2b.\nStood down 2 bees.\n</pre>\n\nh2. The caveat! (PLEASE READ)\n\n(The following was cribbed from our \"original blog post about the bees\":http://blog.apps.chicagotribune.com/2010/07/08/bees-with-machine-guns/.)\n\nIf you decide to use the Bees, please keep in mind the following important caveat: they are, more-or-less a distributed denial-of-service attack in a fancy package and, therefore, if you point them at any server you don\u2019t own you will behaving *unethically*, have your Amazon Web Services account *locked-out*, and be *liable* in a court of law for any downtime you cause.\n\nYou have been warned.\n\nh2. Troubleshooting\n\nh3. EC2 Instances Out Of Sync\n\nIf you find yourself in a situation where 'bees report' seems to be out of sync with EC2 instances you know are (or are not) running:\n* You can reset the BWMG state by deleting ~/.bees.<region>.  Note that this will prevent BWMG from identifying EC2 instances that may now be orphaned by the tool\n* You can manually edit ~/.bees.<region> to add or remove instance IDs and force synchronization with the reality from your EC2 dashboard\n\nThis is helpful in cases where BWMG crashes, EC2 instances are terminated outside of the control of BWMG, or other situations where BWMG is out of sync with reality.\n\n\nh2. Bugs\n\nPlease log your bugs on the \"Github issues tracker\":http://github.com/newsapps/beeswithmachineguns/issues.\n\nh2. Credits\n\nThe bees are a creation of the News Applications team at the Chicago Tribune--visit \"our blog\":http://apps.chicagotribune.com/ and read \"our original post about the project\":http://blog.apps.chicagotribune.com/2010/07/%2008/bees-with-machine-guns/.\n\nInitial refactoring code and inspiration from \"Jeff Larson\":http://github.com/thejefflarson.\n\nMultiple url support from \"timsu\":https://github.com/timsu/beeswithmachineguns.\n\nThanks to everyone who reported bugs against the alpha release.\n\nh2. License\n\nMIT.\n"
 },
 {
  "repo": "voxmedia/meme",
  "language": "JavaScript",
  "readme_contents": "# Meme v.2\n\nContributors: Yuri Victor, Joshua Benton, Matt Montgomery, Ivar Vong, Steve Peters, Flip Stewart, Greg MacWilliam.\n\nMeme is a generator that Vox Media uses to create social sharing images. See working version at [http://www.sbnation.com/a/meme](http://www.sbnation.com/a/meme).\n\n![screenshot](readme.png)\n\n## What's new in version 2.0?\n\n* Refactored into a formal MV* app.\n* Fixed bugs with rendering state and repeat drag-n-drop images.\n* Improved initial rendering with loaded web fonts.\n* Improved cross-origin options: both for base64 images and CORS.\n* Highly (and easily!) customizable editor and theme options.\n* Watermark selector.\n\n## Install\n\n* `git clone https://github.com/voxmedia/meme.git`\n* `bundle install`\n* `bundle exec middleman`\n\nThis will start a local web server running at: `http://localhost:4567/`\n\n## Customization\n\n### Configuration\n\nSettings and controls are configured through `source/javascripts/settings.js.erb`. The [settings file](https://github.com/voxmedia/meme/blob/master/source/javascripts/settings.js.erb) has ample comments to document configuration.\n\n### Fonts\n\nInclude your own fonts in `stylesheets/_fonts.scss`, then add your font options into the [settings file](https://github.com/voxmedia/meme/blob/master/source/javascripts/settings.js.erb#L12).\n\n### Editor theme\n\nSet the [theme-color variable](https://github.com/voxmedia/meme/blob/master/source/stylesheets/_vars.scss#L3) in `source/stylesheets/_vars.scss`. That one color will be tinted across all editor controls.\n\n## Cross-Origin Resources (CORS)\n\nThis is an HTML5 Canvas-based application, and thus comes with some security restrictions when loading graphics across domains (ex: a canvas element on *http://tatooine.com* cannot export with an image hosted on *http://dagobah.com*).\n\nIf you're hosting this application on the same domain that serves your images, then congratulations! You have no problems. However, if you're going through a CDN, then you'll probably encounter some cross-domain security issues; at which time you have two options:\n\n1. Follow this [excellent MDN article](https://developer.mozilla.org/en-US/docs/Web/HTML/CORS_enabled_image) about configuring \"Access-Control-Allow-Origin\" headers. You'll need to enable these headers on your CDN, at which time the Meme app should be able to request images from it.\n\n2. Embed all of your watermark images as base64 data URIs within the `settings.js.erb` file. The asset pipeline's `asset_data_uri` helper method makes this very easy, and effectively embeds all image data within your JavaScript. The downside here is that your JavaScript will become a very large payload as you include more images. In the long term, getting CORS headers configured will be a better option.\n\n## Examples\n\n* http://www.sbnation.com/a/meme\n* https://twitter.com/voxdotcom/status/481671889094340608\n* https://twitter.com/voxdotcom/status/479228288221470721\n* https://twitter.com/voxdotcom/status/481619042545844225\n\n## Contributing\n\n1. Fork it ( https://github.com/voxmedia/meme/fork )\n2. Create your feature branch (`git checkout -b my-new-feature`)\n3. Commit your changes (`git commit -am 'Add some feature'`)\n4. Push to the branch (`git push origin my-new-feature`)\n5. Create a new Pull Request"
 },
 {
  "repo": "censusreporter/censusreporter",
  "language": "HTML",
  "readme_contents": "About Census Reporter\n=====================\n\nThe United States Census Bureau provides a massive amount of data about the American people, covering topics from demographics to poverty rates to educational attainment, and at geographical levels ranging from the entire country down to city blocks. As a product of the federal government, the <a href=\"https://data.census.gov/\">data is free to use</a>. But for working journalists&mdash;especially those who don't have experience with the particulars of Census data&mdash;navigating these datasets on deadline can be a challenge.\n\nCensus Reporter's goal is to make it easier for journalists to write stories using Census data. Our focus is on the American Community Survey; we want to help people understand what the survey covers and help them quickly find data from places they care about. Census Reporter received <a href=\"http://www.niemanlab.org/2012/10/knight-funding-expands-ires-journalist-friendly-census-site/\">funding from the Knight News Challenge</a>, and primary development took place from March 2013 through June 2014.\n\nThe <a href=\"https://censusreporter.org/\">Census Reporter website</a> includes three primary types of pages: <a href=\"https://censusreporter.org/profiles/16000US5367000-spokane-wa/\">geographical profiles</a>, which provide an overview of data indicators from a particular place; <a href=\"https://censusreporter.org/data/table/?table=B15002&geo_ids=050|04000US17&primary_geo_id=04000US17\">data comparisons</a>, which use tabular, map and distribution formats to show information from a table across a group of geographies; and <a href=\"https://censusreporter.org/topics/income/\">topical overviews</a>, which document the concepts and tables the ACS uses to cover specific subject areas.\n\nIn This Guide\n=============\n\n* [Setting up for local development](#setting-up-for-local-development)\n* [Getting data from our API (the basics)](#getting-data-from-our-api-the-basics)\n    * [Show data](#show-data)\n    * [Get geography metadata](#get-geography-metadata)\n    * [Get geography parents](#get-geography-parents)\n    * [Geography search](#geography-search)\n    * [Table search](#table-search)\n* [Profile pages](#profile-pages)\n    * [The profile page back end](#the-profile-page-back-end)\n    * [The profile page front end](#the-profile-page-front-end)\n        * [Profile map](#profile-map)\n        * [Profile charts](#profile-charts)\n        * [Responsive design](#responsive-design)\n\nSetting up for local development\n================================\n\nCensus Reporter is an open-source project, so not only is the data free to use, so is the code. Developers in South Africa forked this repository to build <a href=\"http://wazimap.co.za/\">Wazi</a>, for example, a site exploring South African data. We'd love it if you'd fork this repository, too, and maybe you even have features you'd like to contribute back!\n\nHere's what you need to know to get a local version of Census Reporter up and running. These instructions assume you're using <a href=\"http://virtualenv.readthedocs.org/en/latest/\">virtualenv</a> and <a href=\"http://virtualenvwrapper.readthedocs.org/en/latest/\">virtualenvwrapper</a> to manage your development environments.\n\nFirst, clone this repository to your machine and move into your new project directory:\n\n    >> git clone git@github.com:censusreporter/censusreporter.git\n    >> cd <your cloned repo dir>\n\nCreate the virtual environment for your local project, activate it and install the required libraries:\n\n    >> mkvirtualenv census --no-site-packages\n    >> workon census\n    >> pip install -r requirements.txt\n\nWith your development environment still active, make sure it has the path settings it will need:\n\n    >> add2virtualenv ./censusreporter\n    >> add2virtualenv ./censusreporter/apps\n\nAnd make sure your development environment knows the proper DJANGO_SETTINGS_MODULE by creating a `postactivate` script ...\n\n    >> cdvirtualenv bin\n    >> touch postactivate\n\n... and then using your favorite editor to add these two lines to your new `postactivate` script:\n\n    export DJANGO_SETTINGS_MODULE='config.dev.settings'\n    echo \"DJANGO_SETTINGS_MODULE set to $DJANGO_SETTINGS_MODULE\"\n\nSave and close the file. Reactivate your development environment so `postactivate` gets triggered, then fire up your local Census Reporter:\n\n    >> deactivate\n    >> workon census\n    >> cd <your cloned repo dir>\n    >> python manage.py runserver\n\nHurray! Your local copy of Census Reporter will be hitting our production data sources, so you should be able to search and browse just like you would on the live website. If you'd like to go a step further, you can <a href=\"https://github.com/censusreporter/census-api\">run the API locally</a>, and even <a href=\"http://censusreporter.tumblr.com/post/73727555158/easier-access-to-acs-data\">load the entire Postgres database locally</a>, as well. But if you're primarily interested in adding features to the Census Reporter website or just seeing how it works, the instructions above should get you going.\n\n**Note** for people who wish to develop for Census Reporter\n\nIf you are running the django server from within the vagrant box, provide the localhost explicitly \n\n    >> python manage.py runserver 0.0.0.0:8000\n\nThe application can now be accessed via localhost:8000 on your browser\n\nGetting data from our API (the basics)\n======================================\n\nAs part of the Census Reporter project, we've loaded ACS data into a Postgres database to make queries significantly easier. The Census Reporter website gets information from this database using the API at api.censusreporter.org. For more extensive API documentation, <a href=\"https://github.com/censusreporter/census-api/blob/master/README.md\">see the census-api repository and README</a>. Here is a basic introduction to the endpoints you're likely to use:\n\n### Show data\nThis endpoint does the heavy lifting for Census Reporter's profile and comparison pages. Given a release code, a table code, and a geography, it will return American Community Survey data. A common call to this endpoint might look like:\n\n    https://api.censusreporter.org/1.0/data/show/latest?table_ids=B01001&geo_ids=16000US5367000\n\nThis will return data for Spokane, WA, from the \"Sex By Age\" table, using the \"latest\" ACS release available. In this case, \"latest\" determines not only the year of release, but also the estimate used. The ACS provides <a href=\"http://www.census.gov/acs/www/guidance_for_data_users/estimates/\">three datasets per year</a>: the 1-year, which uses 12 months of data to arrive at estimates for areas with at least 65,000 residents; the 3-year, which uses 36 months of data and covers areas with at least 20,000 people; and the 5-year, which uses 60 months of data and covers areas of all sizes.\n\nFor this API endpoint, \"latest\" is a shortcut that asks for the most current estimate from the most recent release year, favoring 1-year over 3-year over 5-year data. Because Spokane, WA, has more than 65,000 residents, the API call above would return data from the ACS 2012 1-year release.\n\nYou can ask for a specific release and estimate by exchanging \"latest\" for a release code that looks like `acs{year}_{estimate}yr`. So a call to:\n\n    https://api.censusreporter.org/1.0/data/show/acs2012_5yr?table_ids=B01001&geo_ids=16000US5367000\n\n... would return data for Spokane, WA, from the \"Sex By Age\" table, using the ACS 2012 5-year release.\n\nYou can ask for multiple tables at a time by passing a comma-separated `table_ids` list:\n\n    https://api.censusreporter.org/1.0/data/show/latest?table_ids=B01001,B01002&geo_ids=16000US5367000\n\nAnd you can ask for multiple geographies by passing a comma-separated `geo_ids` list:\n\n    https://api.censusreporter.org/1.0/data/show/latest?table_ids=B01001,B01002&geo_ids=16000US5367000,16000US1714000\n\nOne particularly common use case is to request data for all geographies of a particular class within a particular parent geography, e.g. \"compare all counties in Washington state.\" Identifying each county's geoID individually would be unwieldy, so we added a shortcut for this type of request. Your comma-separated list of `geo_ids` can contain one or more items that use the pipe character to describe a comparison set like this: `{child_summary_level}|{parent_geoid}`.\n\nThe Census uses summary levels to identify classes of geographies (like counties or school districts or census tracts), and each one <a href=\"https://censusreporter.org/topics/geography/\">is represented by a three-digit code</a>. For this API endpoint, \"all counties in Washington state\" can be represented as `050|04000US53`, so a request to:\n\n    https://api.censusreporter.org/1.0/data/show/latest?table_ids=B01001&geo_ids=050|04000US53\n\n... would return \"Sex By Age\" data for all 39 counties in Washington. Great? Great! But hold on, let's talk about this for a second.\n\nSpecifying \"latest\" as the release parameter means we're asking for the most current estimate from the most recent release. Although some of the counties in Washington state are large enough to be in the 1-year release, many are not. Several counties fall into the 3-year release, and several more are small enough to only exist in the 5-year release. Data from different estimates should never be compared, so the API will return data from the lowest common denominator: in this case, the 2012 5-year release.\n\nYou won't have to guess which release you're getting data from, though; the API response from this endpoint will include four objects:\n\n* `release`: metadata about the ACS release that provided the data in this response\n* `tables`: metadata, including title and column information, about the tables requested\n* `data`: the actual data for the geographies requested, including estimates and margins of error, nested according to geoID > table code > estimate|error > column code\n* `geography`: metadata, including geoID and name, about the geographies requested\n\nThe entire response for the \"all counties in Washington\" example above <a href=\"https://gist.github.com/ryanpitts/71517ac65333c72ccb8e\">looks like this</a>; below is an excerpt so you can see what to expect.\n\n    {\n        \"release\": {\n            \"id\": \"acs2012_5yr\",\n            \"name\": \"ACS 2012 5-year\",\n            \"years\": \"2008-2012\"\n        },\n        \"tables\": {\n            \"B01001\": {\n                \"title\": \"Sex by Age\",\n                \"universe\": \"Total Population\",\n                \"denominator_column_id\": \"B01001001\",\n                \"columns\": {\n                    \"B01001001\": {\n                        \"name\": \"Total:\",\n                        \"indent\": 0\n                    },\n                    \"B01001002\": {\n                        \"name\": \"Male:\",\n                        \"indent\": 1\n                    },\n                    ...\n                    \"B01001049\": {\n                        \"name\": \"85 years and over\",\n                        \"indent\": 2\n                    }\n                }\n            }\n        },\n        \"data\": {\n            \"05000US53001\": {\n                \"B01001\": {\n                    \"estimate\": {\n                        \"B01001001\": 18575,\n                        \"B01001002\": 9453,\n                        ...\n                        \"B01001049\": 214\n                    },\n                    \"error\": {\n                        \"B01001001\": 0,\n                        \"B01001002\": 24,\n                        ...\n                        \"B01001049\": 53\n                    }\n                }\n            }\n        },\n        \"geography\": {\n            \"04000US53\": {\n                \"name\": \"Washington\"\n            },\n            \"05000US53047\": {\n                \"name\": \"Okanogan County, WA\"\n            },\n            ...\n            \"05000US53035\": {\n                \"name\": \"Kitsap County, WA\"\n            }\n        }\n    }\n\nNote that Washington state's data is also included in this API response. When you ask for a set of geographies&mdash;in this case, 050|04000US53&mdash;the API will automatically include the parent geography's data so you can perform comparisons.\n\n### Get geography metadata\n\nThis endpoint provides basic information about a specific geography, including name, summary level, land area, and population. A common call to this endpoint might look like:\n\n    https://api.censusreporter.org/1.0/geo/tiger2013/16000US5367000\n\nThis request uses the geoID for Spokane, WA, and returns this JSON:\n\n    {\n        \"geometry\": null,\n        \"type\": \"Feature\",\n        \"properties\": {\n            \"awater\": 1991595,\n            \"display_name\": \"Spokane, WA\",\n            \"simple_name\": \"Spokane\",\n            \"sumlevel\": \"160\",\n            \"population\": 208040,\n            \"full_geoid\": \"16000US5367000\",\n            \"aland\": 178021482\n        }\n    }\n\nIf you append a querystring paramater `geom=true` to your request, the API response's `geometry` <a href=\"https://gist.github.com/ryanpitts/750aacecc167233c5547\">will include geographical coordinates</a> suitable for mapping.\n\nThis endpoint returns Tiger 2012 data for all but one class of geography: congressional districts. Redistricting after the 2010 Decennial Census changed these boundaries, and some states gained or lost entire districts. If you request metadata for a congressional district, the API will return Tiger 2013 data <a href=\"http://www.census.gov/geo/maps-data/data/pdfs/tiger/How_do_I_choose_TIGER_vintage.pdf\">in order to accurately represent redistricting</a>.\n\n### Get geography parents\n\nThis endpoint takes a specific geography and provides metadata for it, along with a list of parent geographies that contain it. A common call to this endpoint might look like:\n\n    https://api.censusreporter.org/1.0/geo/tiger2013/16000US5367000/parents\n\n... which returns results that include name, geoID, summary level, geography class, and percentage of coverage:\n\n    {\n        \"parents\": [\n            {\n                \"sumlevel\": \"160\",\n                \"relation\": \"this\",\n                \"coverage\": 100,\n                \"display_name\": \"Spokane, WA\",\n                \"geoid\": \"16000US5367000\"\n            },\n            {\n                \"sumlevel\": \"050\",\n                \"relation\": \"county\",\n                \"coverage\": 100,\n                \"display_name\": \"Spokane County, WA\",\n                \"geoid\": \"05000US53063\"\n            },\n            {\n                \"sumlevel\": \"310\",\n                \"relation\": \"CBSA\",\n                \"coverage\": 100,\n                \"display_name\": \"Spokane, WA Metro Area\",\n                \"geoid\": \"31000US44060\"\n            },\n            {\n                \"sumlevel\": \"040\",\n                \"relation\": \"state\",\n                \"coverage\": 100,\n                \"display_name\": \"Washington\",\n                \"geoid\": \"04000US53\"\n            },\n            {\n                \"sumlevel\": \"010\",\n                \"relation\": \"nation\",\n                \"coverage\": 100,\n                \"display_name\": \"United States\",\n                \"geoid\": \"01000US\"\n            }\n        ]\n    }\n\n### Geography search\n\nThis endpoint returns metadata for geographies with names that match a text string. A common call to this endpoint might look like:\n\n    https://api.censusreporter.org/1.0/geo/search?q=spo\n\n... which returns results that include each matching geography's summary level, geoID and name:\n\n    {\n      \"results\": [\n        {\n          \"sumlevel\": \"160\",\n          \"full_geoid\": \"16000US5367000\",\n          \"full_name\": \"Spokane, WA\"\n        },\n        {\n          \"sumlevel\": \"160\",\n          \"full_geoid\": \"16000US5367167\",\n          \"full_name\": \"Spokane Valley, WA\"\n        },\n        {\n          \"sumlevel\": \"160\",\n          \"full_geoid\": \"16000US3469810\",\n          \"full_name\": \"Spotswood, NJ\"\n        },\n        ...\n        {\n          \"sumlevel\": \"795\",\n          \"full_geoid\": \"79500US5310504\",\n          \"full_name\": \"Spokane County (Outer)--Cheney City PUMA, WA\"\n        }\n      ]\n    }\n\nYou can limit the search to a particular set of summary levels by passing an optional, comma-separated list in a `sumlevs` argument. The Census Reporter website does this in most cases:\n\n    https://api.censusreporter.org/1.0/geo/search?q=spo&sumlevs=010,020,030,040,050,060,160,250,310,500,610,620,860,950,960,970\n\n### Table search\n\nThis endpoint returns metadata for tables with titles or column names that match a text string. A common call to this endpoint might look like:\n\n    https://api.censusreporter.org/1.0/table/search?q=heat\n\n... which would return metadata for matches that includes table name, table code, table universe and table topics. Column metadata is also included where they matched type is a column:\n\n    [\n        {\n            \"unique_key\": \"B25040\",\n            \"universe\": \"Occupied Housing Units\",\n            \"simple_table_name\": \"House Heating Fuel\",\n            \"id\": \"B25040\",\n            \"table_id\": \"B25040\",\n            \"table_name\": \"House Heating Fuel\",\n            \"type\": \"table\",\n            \"topics\": [\n                \"housing\",\n                \"physical characteristics\"\n            ]\n        },\n        {\n            \"unique_key\": \"B25117\",\n            \"universe\": \"Occupied Housing Units\",\n            \"simple_table_name\": \"Tenure by House Heating Fuel\",\n            \"id\": \"B25117\",\n            \"table_id\": \"B25117\",\n            \"table_name\": \"Tenure by House Heating Fuel\",\n            \"type\": \"table\",\n            \"topics\": [\n                \"housing\",\n                \"physical characteristics\",\n                \"tenure\"\n            ]\n        },\n        ...\n        {\n            \"unique_key\": \"B24126|B24126441\",\n            \"universe\": \"Full-time, Year-round Civilian Employed Female Population 16 Years and Over\",\n            \"simple_table_name\": \"Detailed Occupation for the Full-time, Year-round Civilian Employed Female Population\",\n            \"column_id\": \"B24126441\",\n            \"table_id\": \"B24126\",\n            \"id\": \"B24126441\",\n            \"table_name\": \"Detailed Occupation for the Full-time, Year-round Civilian Employed Female Population 16 Years and Over\",\n            \"type\": \"column\",\n            \"topics\": [\n                \"employment\"\n            ],\n            \"column_name\": \"Heat treating equipment setters, operators, and tenders, metal and plastic\"\n        }\n    ]\n\nMatches against table names will appear first in the response, followed by matches against column names within tables. The `unique_key` value is a combination of table code and column code (if necessary), and is useful as a key for autocomplete libraries like <a href=\"http://twitter.github.io/typeahead.js/\">Typeahead</a>.\n\nProfile pages\n=============\n\nGeographical profile pages provide an overview of important Census data indicators for a particular place. We've divided these data points into five categories: Demographics, Economics, Families, Housing and Social. Each category includes a mix of figures and charts that help you understand what life is like in a community. Profiles also include comparative data, so you can consider statistics from a city, for example, in the context of the metro area and state they're in.\n\nThe Census Reporter website generates profile pages for geographies at the <a href=\"https://censusreporter.org/profiles/01000US-united-states/\">national</a> and <a href=\"https://censusreporter.org/profiles/04000US17-illinois/\">state</a> levels all the way down to <a href=\"https://censusreporter.org/profiles/14000US17031808702-census-tract-808702-cook-il/\">census tracts</a>. If your browser supports geolocation, you can <a href=\"https://censusreporter.org/locate/\">use your current location</a> to easily profile any of the geographies that you're currently in.\n\n### The profile page back end\n\nEach profile page requires queries against a few dozen Census tables. To lighten the database load, profile data has been pre-computed and stored as JSON in an Amazon S3 bucket, so most of these pages should never touch the API. When the Census Reporter app sees a profile request, the <a href=\"https://github.com/censusreporter/censusreporter/blob/master/censusreporter/apps/census/views.py\">`GeographyDetail`</a> view <a href=\"https://github.com/censusreporter/censusreporter/blob/master/censusreporter/apps/census/views.py#L285\">checks for flat JSON data</a> first, and falls back to <a href=\"https://github.com/censusreporter/censusreporter/blob/master/censusreporter/apps/census/profile.py\">a profile generator</a> if necessary.\n\nThe profile generator at `profile.py` does a number of things:\n\n* establishes a connection to the Census Reporter API\n* takes the geoID from the URL and uses the [\"geography parents\" API endpoint](#get-geography-parents) to determine which geographies should be shown as comparative data\n* uses the [\"geography metadata\" endpoint](#get-geography-metadata) to get basic information about each parent as well as the requested geography\n* uses the [\"show data\" endpoint](#show-data) to query each table necessary for the charts and figures on the page\n    * When requesting the profiled geography's total population, a default ACS release is also stored. This lets us flag any data point that comes from a different release; there are cases where a place has a high enough population for the 1-year release, for example, but specific tables have smaller universes that force us to dip into 3-year or 5-year releases to find data.\n    * In many cases, columns are added together for display purposes. For example, creating the \"10-19\" category in the profile page's age distribution chart requires adding together six columns from Table B01001: the male and female versions of \"10 to 14 years,\" \"15 to 17 years,\" and \"18 and 19 years.\"\n    * In many cases, a denominator column is used for division so we can display a percentage.\n    * In some cases, data from more than one table is required. For example, determining a place's \"Mean travel time to work\" requires the total number of workers, found in Table B08006, as well as the aggregate minutes spent commuting, found in Table B08013.\n\nIf the profile generator is invoked, it returns a Python dictionary with this processed data from the API. The `GeographyDetail` view passes this into an `enhance_api_data` method to do a couple more things:\n\n* removes extraneous comparisons, limiting data to the profile geography plus at most two parent geographies\n* calculates index values, which are figures from the profile geography expressed as percentages of a parent geography's value. (These are ultimately used Madlib-style, for phrases like \"a little less than the figure in Washington\")\n* calculates margin of error ratios to determine which data points should be flagged as requiring extra care\n\nOnce this is done, `GeographyDetail` writes everything as flat JSON to an S3 bucket so this process never has to be repeated. Regardless of whether the data came straight from JSON or had to be generated, the `GeographyDetail` view's final job is to use `SafeString()` to hand everything to the template in a format suitable for use as Javascript variables.\n\nThis pattern&mdash;using a generator script to collect and shape data from multiple tables, then storing the results as flat JSON&mdash;is something that could be repeated for new Census Reporter features. We'd like to add deeper category profiles for each of the Demographics, Economics, Families, Housing and Social sections, for example, which could be done by copying and modifying the <a href=\"https://github.com/censusreporter/censusreporter/blob/master/censusreporter/apps/census/profile.py#L173\">`geo_profile` method</a> in `profile.py`.\n\n### The profile page front end\n\nThe skeleton of the profile page you see on the Census Reporter website is created by <a href=\"https://github.com/censusreporter/censusreporter/blob/master/censusreporter/apps/census/templates/profile/profile.html\">a Django template</a>. The map is filled in by one Javascript library: <a href=\"https://github.com/censusreporter/censusreporter/blob/master/censusreporter/apps/census/static/js/tilelayer.js\">`tilelayer.js`</a>, and the charts filled in by another: <a href=\"https://github.com/censusreporter/censusreporter/blob/master/censusreporter/apps/census/static/js/charts.js\">`charts.js`</a>.\n\n#### Profile map\n\nThe profile page uses Javascript to call the [\"geography metadata\" endpoint](#get-geography-metadata), using the `geom=true` argument to get boundary coordinates for the chosen place. A tile layer then adds shapes of nearby geographies at the same summary level, which can be used to navigate to their corresponding profile pages. The map will also do some smart centering to account for the box full of place metadata in that part of the page.\n\n#### Profile charts\n\nThe Django template for the profile page creates empty slots for each chart, which are filled on load by `charts.js`. These placeholders look something like:\n\n    <div class=\"column-half\" id=\"chart-histogram-demographics-age-distribution_by_decade-total\" data-stat-type=\"scaled-percentage\" data-chart-title=\"Population by age range\"></div>\n\nThe `column-*` class isn't really important here; that's just a structural setting that gives the block an appropriate amount of width that can be governed with media queries. What we really care about are the `id` and `data-*` attribute values. The `data` attributes provide a place to pass optional information into the chart constructor, and the `id` value tells the constructor what type of chart to draw and which data to use.\n\nAt the bottom of the profile page, we trigger all the charts at once. Profile data is assigned to a Javascript variable:\n\n    profileData = {{ profile_data_json }};\n\nAnd we grab all the chart placeholders with:\n\n    chartContainers = $('[id^=chart-]')\n\nThe <a href=\"https://github.com/censusreporter/censusreporter/blob/master/censusreporter/apps/census/templates/profile/profile.html#L437\">`makeCharts()` function</a> then loops through those containers, empties each one of any contents, and builds the variables required for a chart:\n\n    chartDataKey = chartID.replace('chart-','').replace('alt-','')\n    chartDataID = chartDataKey.split('-') #temporary variable\n    chartType = gracefulType(chartDataID[0])\n    chartData = profileData[chartDataID[1]]\n    geographyData = profileData['geography']\n\n##### Required variables\n\n`chartDataKey`: This tells us everything we need to know to recreate this particular chart from a given set of profile data, and we'll use it to populate embed code if a user asks for it. In the example above, this value would be `histogram-demographics-age-distribution_by_decade-total`.\n\n`chartType`: The first bit of our `chartDataKey`, in this case `histogram`, represents the type of chart we want. The `charts.js` library currently supports:\n\n* pie\n* column\n* grouped_column\n* histogram\n* bar\n* grouped_bar\n\nYou'll note that we actually pass this value through a function called `gracefulType`, which allows us to change chart types based on screen width. More on that in a moment.\n\n`chartData`: The rest of our `chartDataKey` provides the path to the data that should fill this chart. We start by assigning this variable a top-level item from `profileData`, in this case `demographics`. Then we use a loop to drill down based on the rest of our keys: `demographics` > `age` > `distribution_by_decade` > `total`. That's where we'll find the data to pass into the chart constructor.\n\n`geographyData`: We also reach into `profileData` for names and summary levels of the chosen place and its parent geographies.\n\n##### Optional variables\n\nPlaceholder containers can also use data attributes to pass optional information to the chart constructor. Our example container uses `data-stat-type` and `data-chart-title`:\n\n    <div class=\"column-half\" id=\"chart-histogram-demographics-age-distribution_by_decade-total\" data-stat-type=\"scaled-percentage\" data-chart-title=\"Population by age range\"></div>\n\nThe `makeCharts()` function will recognize:\n\n`data-chart-title`: A title to place above the chart elements, passed to the chart constructor as `chartChartTitle`. Defaults to `null`, although most charts on Census Reporter's profile pages do assign a value here.\n\n`data-initial-sort`: Used only by pie charts. Determines which category to highlight when the chart is initialized. A placeholder container with `data-initial-sort=\"-value\"` will display the highest data value in the chart on load. Otherwise the first value in the chart will serve as the default state.\n\n`data-stat-type`: Provides formatting hints for the chart's language and display. Standard chart behavior may be overriden with these values:\n\n* **percentage**: Adds a \"%\" character after figures in the chart. Sets chart domain to 0-100. Uses \"rate\" in comparison sentences.\n* **scaled-percentage**: Does the same things as \"percentage,\" but also scales the chart so that the highest category value takes up the full vertical space available.\n* **dollar**: Adds a \"$\" character before figures in the chart. Uses \"amount\" in comparison sentences.\n\n`data-qualifier`: Adds a trailing line below the chart, prepended with an \"*\" character. This is useful when charts require a little extra context. For example, the profile page's <a href=\"https://censusreporter.org/profiles/16000US5367000-spokane-wa/#race\">\"Race & Ethnicity\" column chart</a> adds this explanation: \"Hispanic includes respondents of any race. Other categories are non-Hispanic.\"\n\n#### Responsive design\n\nThe charts on Census Reporter's profile pages are responsive to browser width. They use a combination of CSS media queries and Javascript to accommodate various screen sizes. Media queries take care of changes like column widths and legend placements, and they help arrange the interactive hovercards that provide extra data when a user mouses over or taps a chart element.\n\nJavascript comes into play so we can completely change chart types that won't read well at certain widths. There are a couple functions <a href=\"https://github.com/censusreporter/censusreporter/blob/master/censusreporter/apps/census/templates/profile/profile.html#L425\">at the bottom of the profile template</a> that make this happen:\n\n`lazyRedrawCharts`: This function updates `window.browserWidth` and `window.browserHeight` variables whenever a page is resized, then triggers `makeCharts()` to redraw any charts on the page according to their new available widths. (This is why `makeCharts()` empties out the contents of each container first. So they can be filled again, maybe even with a different chart format.) Realistically, this most likely gets triggered by a user turning a phone or tablet from portrait to landscape orientation, but just in case, the redraw is debounced to avoid a crazy number of events firing off.\n\n    var lazyRedrawCharts = _.debounce(function() {\n        window.browserWidth = document.documentElement.clientWidth;\n        window.browserHeight = document.documentElement.clientHeight;\n        makeCharts();\n    }, 50);\n    $(window).resize(lazyRedrawCharts);\n\n\n`gracefulType`: This function checks `window.browserWidth`, and if it's too narrow to reasonably display a column chart, flips it to a bar chart. This is called by each chart inside the `makeCharts()` function, which is triggered on page load as well as resize thanks to `lazyRedrawCharts`.\n\n    var gracefulType = function(chartType) {\n        if (browserWidth <= 640) {\n            if (chartType == 'column' || chartType == 'histogram') {\n                return 'bar'\n            } else if (chartType == 'grouped_column') {\n                return 'grouped_bar'\n            }\n        }\n        return chartType\n    }\n\nBetween media queries and this Javascript, charts should be useful on phones as much as desktops.\n\nData comparison pages\n=====================\n\nTabular view, map view, distribution view. Docs soon.\n"
 },
 {
  "repo": "nprapps/app-template",
  "language": "JavaScript",
  "readme_contents": "nprviz's Project Template\n=========================\n\n* [About this template](#about-this-template)\n* [Assumptions](#assumptions)\n* [Copy the template](#copy-the-template)\n* [Configure the project](#configure-the-project)\n* [Bootstrap issues](#bootstrap-issues)\n* [Develop with the template](#develop-with-the-template)\n\nAbout this template\n-------------------\n\nThis template provides a a project skeleton suitable for NPR projects that are designed to be served as flat files. Facilities are provided for rendering html from data, compiling LESS into CSS, deploying to S3, installing cron jobs on servers, copy-editing via Google Spreadsheets and a whole bunch of other stuff.\n\nThis codebase is licensed under the [MIT open source license](http://opensource.org/licenses/MIT). See the ``LICENSE`` file for the complete license.\n\nPlease note: logos, fonts and other media referenced via url from this template are **not** covered by this license. Do not republish NPR media assets without written permission. Open source libraries in this repository are redistributed for convenience and are each governed by their own license.\n\nAlso note: Though open source, This project is not intended to be a generic solution. We strongly encourage those who love the app-template to use it as a basis for their own project template. We have no plans to remove NPR-specific code from this project.\n\nIf you want to setup a version of the app template for yourself, read [this blog post](http://blog.apps.npr.org/2014/09/08/how-to-setup-the-npr-app-template-for-you-and-your-news-org.html) about how to do so.\n\nAssumptions\n-----------\n\nThe following things are assumed to be true in this documentation.\n\n* You are running OSX.\n* You are using Python 2.7. (Probably the version that came with OSX.)\n* You have [virtualenv](https://pypi.python.org/pypi/virtualenv) and [virtualenvwrapper](https://pypi.python.org/pypi/virtualenvwrapper) installed and working.\n* You have NPR's AWS credentials stored as environment variables locally.\n\nFor more details on the technology stack used with the app-template, see our [development environment blog post](http://blog.apps.npr.org/2013/06/06/how-to-setup-a-developers-environment.html).\n\nCopy the template\n-----------------\n\nCreate a new repository on Github. Everywhere you see ``$NEW_PROJECT_NAME`` in the following script, replace it with the name of the repository you just created.\n\n```\ngit clone git@github.com:nprapps/app-template.git $NEW_PROJECT_NAME\ncd $NEW_PROJECT_NAME\n\nmkvirtualenv $NEW_PROJECT_NAME\npip install -r requirements.txt\nnpm install\n\nfab bootstrap\n```\n\nThis will setup the new repo and will replace `README.md` (this file) with `PROJECT_README.md`. See that file for usage documentation.\n\nBy default `bootstrap` will use `nprapps` as the Github username, and the current directory name as the repository name. **This is a best practice**, but you can override these defaults if you need to:\n\n```\nfab bootstrap:$GITHUB_USERNAME,$REPOSITORY_NAME\n```\n\n**Problems installing requirements?** You may need to run the pip command as ``ARCHFLAGS=-Wno-error=unused-command-line-argument-hard-error-in-future pip install -r requirements.txt`` to work around an issue with OSX.\n\nBootstrap issues\n----------------\n\nThe app-template can automatically setup your Github repo with our default labels and tickets by running ``fab issues.bootstrap``. You will be prompted for your Github username and password.\n\n"
 },
 {
  "repo": "TimeMagazineLabs/babynames",
  "language": "JavaScript",
  "readme_contents": "# Baby Names!\n\nFun with the Social Security Administration's [baby name data](http://www.ssa.gov/OACT/babynames/).\n\nv0.1.1\n\n[![Build Status](https://travis-ci.org/TimeMagazineLabs/babynames.svg)](https://travis-ci.org/TimeMagazineLabs/babynames)\n\n## Setup\n\nThis is a Node.js script, so you should consider [downloading Node](http://nodejs.org/) before attempting to run it.\n\nTo download the repo, simply clone it:\n\n\tgit clone https://github.com/TimeMagazineLabs/babynames.git\n\tcd babynames\n\nThen install the dependencies:\n\n\tnpm install\n\n## Data \n\nThe Social Security Administration organizes the baby name data, somewhat inconveniently, as year-by-year text files named `yob[year].txt`, beginning in 1880 for national data and 1910 for state data. The data for the previous calendar year is usually released around Mother's Day.\n\nAs of the 2018 data, there are 109,174 names, constituting any name-gender combination that appeared in at least one year a minimum of five times. The scripts in this repo download this raw data and provide tools for aggregating specific (or all) names in specific (or all) spans of years will the ability to exclude uncommon names.\n\n### Getting the Data\n\nThe raw data is not included in the repo. Instead, you need to download it from the [Social Security Administration](http://www.ssa.gov/OACT/babynames/) with a simple command or function. This script will download and unzip it for you into the [`data/national`](data/national) directory, and then delete the `.zip` file.\n\n\t# CLI\n\t./index.js download \n\t\n\t# Node\n\tconst babynames = require(\"babynames\");\n\tbabynames.download({ [opts] }, function() {\n\t\t// further commands to aggregate the flat files you just downloaded\n\t});\n\nYou can also get the state-by-state data, which extracts to the [`data/states`](data/states) directory\n\n\t# CLI\n\t./index.js download --states\n\n\t# Node\n\tconst babynames = require(\"babynames\");\n\tbabynames.download({ states: true }, function() { ... });\n\n### Total babies born each year\n\nThere is also a file called `extra/totals.json` with data on the total number of babies born (or at least, those issued a SSN) each year, [per the SSA](http://www.ssa.gov/oact/babynames/numberUSbirths.html), which is used for calculating frequencies as percentages. This is necessary because the totals are higher than the sum of each name in the name files, which don't include names that occur fewer than five times.\n\nIf you want to re-download this data--maybe it's a new year or you suspect there has been a revision--just run `./scripts/total_births.js`, which will scrape the page on the SSA website and overwrite the file in the repo.\n\n### Let's get started!\n\nNow that you've downloaded the raw data, the fun begins!\n\n## Extracting names\n\nOnce you've downloaded the data, you can aggregate it on a per-name basis and store it in a variety of formats:\n\n\t./index.js store --format=json --names=Edward,Nancy,Christopher\n\n### Options\n\nThese options are either passed as `--format`, e.g. from the command-line, or as key values in Node (see [test/test.js](test/test.js))\n\n| option | default | purpose |\n| -------- | --------- | ---------- |\n| `names` | `null`: Every name! | A comma-separated list of names to extract. If you don't append a name with `-f` or `-m`, it will search for both genders.|\n| `start` | `1880` | The first year of data to extract. The default, `1880`, is the first available year of data. |\n| `end` | Most recent year |  The last year of data to extract. The default is the previous calendar year starting in June, otherwise the year before that. |\n| `min` | `1` | Don't include names that don't show up at least this many times in at least one year. The default is functionally `5` in the data. |\n| `cutoff` | `1` | Don't include names that don't show up in at least this many individual years. |\n| `format` | Must be provided | Format out output. Options are `json`, `csv`, `jsonp`, `csvs`, `mongodb`. See following explanation. |\n\n#### Formats\n+ `json`: Each name is stored as an individual JSON file in the `/flat_files/individual/` directory.\n+ `jsonp`: Each name is stored as an individual JSON-P file in the `/flat_files/individual/` directory. It is wrapped in a callback function named `name_callback` by default, which you can override with `opts.callback` or `--callback`.\n+ `csv`: All names are packaged into one CSV file and stored in `/flat_files/names_[year_start]_[year_end].csv/`. This file will be able 30MB if you don't include limiting specifications above (`start`, `end`, `min`, `cutoff`).\n+ `csvs`: Each name is stored as an individual CSV file in the `/flat_files/individual/` directory.\n+ `mongodb`: All names are inserted into a MongoDB instance, using the slug `[name]-[m|f]` as the `_id`. *Note:* Because this is optional, the [mongodb](https://www.npmjs.org/package/mongodb) Node module is not included as a dependency, so you'll need to install it yourself as well as running a mongo server: `npm install mongodb`. You can pass a `--mongo_uri` argument, which defaults to `mongodb://localhost:27017`, as well as a `db_name` argument, which defaults to `babynames`.\n\n### How it Works\n\nFirst, the script reads every raw file from the SSA and stores the data on a per-name basis in memory. For each name, it records both the absolute number of babies with that name in a given year and the percentage of all babies of the same gender with that name. The denominator in that calculation is the gender-specific total number of babies [as reported on SSA.gov](http://www.ssa.gov/oact/babynames/numberUSbirths.html), NOT the calculated sum of all baby name frequencies (which will be lower than the actual number of children born in the United States, given that the data only counts names that appear at least five times). For JSON, the years are stored as keys in an object for fast retrieval. Here is the output of `./index.js store --name=Clifford-m --format=json --start=1960 --end=1980`, which would be writting to `flat_files/individuals/clifford-m.json`:\n\n\t{\n\t  \"id\": \"clifford-m\",\n\t  \"name\": \"Clifford\",\n\t  \"gender\": \"M\",\n\t  \"values\": {\n\t    \"1960\": 2465,\n\t    \"1961\": 2336,\n\t    \"1962\": 2183,\n\t    \"1963\": 2198,\n\t    \"1964\": 2021,\n\t    \"1965\": 1822,\n\t    \"1966\": 1612,\n\t    \"1967\": 1514,\n\t    \"1968\": 1608,\n\t    \"1969\": 1507,\n\t    \"1970\": 1581,\n\t    \"1971\": 1382,\n\t    \"1972\": 1268,\n\t    \"1973\": 1175,\n\t    \"1974\": 1148,\n\t    \"1975\": 1124,\n\t    \"1976\": 1065,\n\t    \"1977\": 973,\n\t    \"1978\": 1002,\n\t    \"1979\": 1046,\n\t    \"1980\": 1218\n\t  },\n\t  \"percents\": {\n\t    \"1960\": 0.00113831417748373,\n\t    \"1961\": 0.0010835080428208315,\n\t    \"1962\": 0.001038538187329834,\n\t    \"1963\": 0.0010643861620113896,\n\t    \"1964\": 0.0009969135634998424,\n\t    \"1965\": 0.0009614400281994266,\n\t    \"1966\": 0.0008867579315876157,\n\t    \"1967\": 0.0008507089990852349,\n\t    \"1968\": 0.0009054069348090115,\n\t    \"1969\": 0.0008235737746129422,\n\t    \"1970\": 0.0008297069733171276,\n\t    \"1971\": 0.0007600414008079977,\n\t    \"1972\": 0.0007571952960735124,\n\t    \"1973\": 0.0007279201679110487,\n\t    \"1974\": 0.0007039861066992741,\n\t    \"1975\": 0.0006925348562528072,\n\t    \"1976\": 0.0006520884587077995,\n\t    \"1977\": 0.0005690371283567635,\n\t    \"1978\": 0.0005862398622043789,\n\t    \"1979\": 0.0005837441214009189,\n\t    \"1980\": 0.0006565602388628679\n\t  }\n\t}\n\n### Formats\n\nYour choices are:\n\n+ `json`: Each name is stored as an individual JSON file in the `/flat/individual/` directory.\n+ `jsonp`: Each name is stored as an individual JSON-P file in the `/flat/individual/` directory. It is wrapped in a callback function named `ticallback` by default, which you can override with `opts.callback`.\n+ `csvs`: Each name is stored as an individual CSV file in the `/flat/individual/` directory.\n+ `csv`: All names are packaged into one CSV file and stored in `/flat/names.csv/`. This file will be able 30MB if you don't include limiting specifications (below). This preprocessed file is included in this repo.\n+ `mongodb`: All names are inserted into a MongoDB instance. You are responsible for running a Mongo server at `localhost:27017` or updating the source to point to your  instance. *Note:* Because this is optional, the [mongodb](https://www.npmjs.org/package/mongodb) Node module is not listed as a dependency, you you'll need to install it yourself.\n\n## Reducing the size\nAs of 2013, there are 102,691 names that show up in at least one year at least five times. Many users will not be interested in this volume of data. There are several ways to reduce the scope with command line options.\n\n### Limit the years\n\nUsing `--start` and `--end` can narrow the window of time:\n\n  `./index.js store --format=csv --start=1960 --end=1980`\n\n### Exclude uncommon names \n\n+ `min`: Don't include names that don't show up at least this many time in at least one year. Ex: `--min=25`. Default is `0`.\n+ `cutoff`: Don't include names that don't show up in at least this many individual years. Ex: `--cutoff=50`. Default is `0`.\n\n  `./index.js store --format=json --min=25 --cutoff=10`\n\n  \n## Analysis\n\n**This is still in the works**\n\nThe script comes with several options for basic analysis:\n\n+ `normalize`: Add a third property to each name that is the normalized value for the percentage figures, such that the peak percentage year is 1.\n+ `peak`: Find the peak value and year for both raw values and percents\n+ `maxima`: Identify all the local maxima -- points where every value 5 years before and after is lower. Only counts maxima that are at least 25 percent of peak value.\n+ `dense`: If a name does not appear in a year in the range specified between `start` and `end`, list that year in the data as `0`. Otherwise it is not included at all (a \"sparse\" format).\n\n### Types\n\nFor csv outputs, you can get the data back as either raw numbers of new babies each year with a given name (`--type=values`, which is the default) or as a percent (`--type=values`). JSON formats return both percents and values. \n\n### Phonemes\nYou can also pass a special type, `--type=phonemes`, to get back a JSON document of phoneme percents for each year for all names. By default, the script examines the first phoneme in each name. You can use `--N==TK` to aggregate around the TKth phonemes in the name. Use a negative value to start from the end.\n\n### Extras\nWe've now got British baby names going back to 1996, accessed on Oct. 5, 2016 from the U.K. [Office for National Statistics](http://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/adhocs/006073babynames1996to2015). The total number of live births was downloaded [here](http://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/datasets/birthsummarytables) from the same source.\n\n# License\n\nThis script is provided free and open-source by Time under the MIT license. If you use it, you are politely encouraged to acknowledge Time and link to this page.\n\nThe dictionary file [dict/2of12.txt](dict/2of12.txt) is from the [12 Dicts project](http://wordlist.aspell.net/12dicts-readme/), which is in the public domain.\n"
 },
 {
  "repo": "guardian/frontend",
  "language": "HTML",
  "readme_contents": "## We're hiring!\nEver thought about joining us?\nhttps://workforus.theguardian.com/careers/digital-development/\n\n# Frontend\nThe Guardian website frontend.\n\n**For everybody who engages with our journalism, [theguardian.com](https://www.theguardian.com) is an industry-best news website that is fast, accessible and easy to use. Unlike other ways of developing products, ours puts the audience first.**\n\nFrontend is [a set of Play Framework 2 Scala applications](docs/02-architecture/01-applications-architecture.md). It is built in two parts, using `make` for the client side asset build and SBT for the Play Framework backend.\n\n# Moving to main\n\nThe `master` branch in the frontend repository has now been renamed to `main`. If you work with this repository, there are two things you need to do!\n\nFirst, you need to make some changes to your local repository. We recommend you run the following sequence of commands, which will rename your master branch to main and set main as your default branch.\n\n```\ngit fetch --all\ngit remote set-head origin -a\ngit branch master --set-upstream-to origin/main\ngit branch -m master main\n```\n\n\nSecond, you\u2019ll need to rebase or merge from main on any branch you\u2019ve started working on before the rename. This is because frontend has a pre-push git hook that is hardcoded to look for `origin/master`. This has been patched in main, so you\u2019ll need to integrate that change into your branch to be able to push to github.\n\n# Documentation\n\n**[All documentation notes and useful items can be found in the `docs` folder](docs).**\n\n# Core Development Principles (lines in the sand)\nThese principles apply to all requests on `www.theguardian.com` and `api.nextgen.guardianapps.co.uk` (our Ajax URL)\n\n## On the server\n* Every request can be cached and has an appropriate Cache-Control header set.\n* Each request may only perform one I/O operation on the backend. (you cannot make two calls to the content API or any other 3rd party)\n* The average response time of any endpoint is less than 500ms.\n* Requests that take longer than two seconds will be terminated.\n\n# New developers\nWelcome! **[The best place to start is here](docs/01-start-here)**\n\nTo get set up, please follow [the installation guide](docs/01-start-here/01-installation-steps.md).\n\nFixes for common problems can be found [here](docs/01-start-here/04-troubleshooting.md).\n\nPlease read the [development tips](docs/01-start-here/05-development-tips.md) document to learn about more about development process.\n\n## Deploying\nFollow the steps described in the [How to deploy document](docs/01-start-here/03-how-to-deploy.md).\n"
 },
 {
  "repo": "dukechronicle/chronline",
  "language": "Ruby",
  "readme_contents": "Chronline [![Build Status](https://travis-ci.org/dukechronicle/chronline.svg?branch=master)](https://travis-ci.org/dukechronicle/chronline)\n=========\n\nThis is the code for the [Duke Chronicle](http://www.dukechronicle.com) website. The Chronicle is Duke's independent daily news organization. The site was written entirely by undergrad students at Duke. We are one of the few college news organizations that have a dedicated team responsible for building, maintaining, and running the online product. We want to build a platform for providing information and fostering discussion within the Duke community, and we take this mission very seriously. This is a project that will continue to improve as we try to build the *best* news platform out there.\n\nDevelopment\n=============\n\nDocumentation for development is in the [wiki](https://github.com/dukechronicle/chronline/wiki). To get the site up and running, check out the [setting up](https://github.com/dukechronicle/chronline/wiki/Setting-Up) page. We are very open to comments, suggestions, and pull requests.\n\nJoin Us\n-------\n\nThe Chronline team is a small group of undergrads who are passionate about programming and building something awesome. This is a great chance to get some real software engineering experience. If you are a current Duke student and are interested in helping us, you can send an email to [*recruitment@dukechronicle.com*](mailto:recruitment@dukechronicle.com). Anyone interested in web design should also definitely reach out!\n\nWriting Tests\n-------------\n\nWe are building up our test coverage slowly. We would like for all new code to have tests. Sometimes hard deadlines make this difficult, but we request that any pull requests come with tests. For more about our testing infrastructure, see the [testing](https://github.com/dukechronicle/chronline/wiki/Testing) wiki page.\n\nResources\n---------\n\nIn no particular order:\n\n - [Rails Guides](http://guides.rubyonrails.org/)\n - [Ruby Toolbox](https://www.ruby-toolbox.com/)\n - [HAML](http://haml.info/)\n - [CoffeeScript](http://coffeescript.org/)\n - [Sass](http://sass-lang.com/)\n - [Pro Git](http://git-scm.com/book)\n - [SimpleForm](http://simple-form.plataformatec.com.br/)\n - [RSpec Rails](https://www.relishapp.com/rspec/rspec-rails/docs)\n - [Cucumber](https://www.relishapp.com/cucumber/cucumber/docs)\n - [Capybara](http://jnicklas.github.com/capybara/)\n - [Pry](http://pryrepl.org/)\n\nSecurity Issues\n===============\n\nWe feel strongly about keeping this repository open source to show the kind of work we are doing. We try to abide by best practices to keep the site secure, but in the case that you find a security issue, please email [webmaster@dukechronicle.com](mailto:webmaster@dukechronicle.com). If you happen to find a bug, we will happily list your name here as a thank you.\n\nLicense\n=======\n\nThe MIT License (MIT)\n\nCopyright (c) 2014 Duke Student Publishing Company\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
 },
 {
  "repo": "BloombergMedia/whatiscode",
  "language": "JavaScript",
  "readme_contents": "## [What Is Code?](http://www.bloomberg.com/whatiscode)\n*Businessweek*, June 11, 2015  \nby [Paul Ford](https://twitter.com/ftrain) & [contributors](https://makerba.se/p/15js4s/whatiscode)\n\nReport an [issue](https://github.com/BloombergMedia/whatiscode/issues) or suggest changes by submitting a [pull request](https://github.com/BloombergMedia/whatiscode/pulls). Issues and pull requests labeled \u201c[text](https://github.com/BloombergMedia/whatiscode/labels/text)\u201d will be reviewed by an editor, though we make no guarantees about timeliness. Changes merged into master may take a day or so to push live to production, especially on weekends.\n\n#### LICENSE\n---\nThis repository contains a variety of content; some is owned by Bloomberg Finance LP, and some is from third-parties (various Javascript libraries).\n\nThe third-party content is distributed under the license provided by those parties.\n\nThe Javascript content owned by Bloomberg Finance LP is distributed under the Apache 2 license; the text of this license can be found in the [LICENSE](https://github.com/BloombergMedia/whatiscode/blob/master/LICENSE) file.\n\nThe article text, contained in the [index.html](https://github.com/BloombergMedia/whatiscode/blob/master/index.html) file, is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International license. For the full text of the license, please see [the Creative Commons site](https://creativecommons.org/licenses/by-nc-nd/4.0/).\n"
 },
 {
  "repo": "aduggin/accessibility-fails",
  "language": "HTML",
  "readme_contents": "# Accessibility Fails\n\nA HTML page with lots of accessibility fails to see which ones are picked up by automated accessibility testing tools.\n"
 },
 {
  "repo": "Heydon/REVENGE.CSS",
  "language": "CSS",
  "readme_contents": "## REVENGE.CSS\n\nThe premise of `revenge.css` is simple: A *CSS bookmarklet* that uses selectors to find bad markup, displaying ugly pink error messages in *comic sans serif* wherever you write bad HTML. If you activate the bookmarklet and the page gets lots of pink blotches, the author has included at least one of the following:\n\n* Misplaced &lt;div&gt;s\n* Deprecated elements\n* Malformed hyperlinks\n* Inaccessible forms\n* Empty elements\n* Inaccessible images\n* Missing ARIA landmarks\n* Badly authored sectioning elements\n* Erroneous lists\n* Obsolete attributes\n\n## Try the bookmarklet\n\nGitHub won't let me use javascript in my README. Pretty sensible. Go to the new <a href=\"http://heydonworks.com/revenge_css_bookmarklet/\">hot pink, skull-festooned demo page</a>.\n\n"
 },
 {
  "repo": "aseprite/aseprite",
  "language": "C++",
  "readme_contents": "# Aseprite\r\n\r\n[![Build Status](https://travis-ci.org/aseprite/aseprite.svg)](https://travis-ci.org/aseprite/aseprite)\r\n[![Build status](https://ci.appveyor.com/api/projects/status/kdu2gt7ls014i25h?svg=true)](https://ci.appveyor.com/project/dacap/aseprite)\r\n[![Discourse Community](https://img.shields.io/badge/discourse-community-brightgreen.svg?style=flat)](https://community.aseprite.org/)\r\n[![Discord Server](https://discordapp.com/api/guilds/324979738533822464/embed.png)](https://discord.gg/Yb2CeX8)\r\n\r\n## Introduction\r\n\r\n**Aseprite** is a program to create animated sprites. Its main features are:\r\n\r\n* Sprites are composed of [layers &amp; frames](https://www.aseprite.org/docs/timeline/) as separated concepts.\r\n* Support for [color profiles](https://www.aseprite.org/docs/color-profile/) and different [color modes](https://www.aseprite.org/docs/color-mode/): RGBA, Indexed (palettes up to 256 colors), Grayscale.\r\n* [Animation facilities](https://www.aseprite.org/docs/animation/), with real-time [preview](https://www.aseprite.org/docs/preview-window/) and [onion skinning](https://www.aseprite.org/docs/onion-skinning/).\r\n* [Export/import](https://www.aseprite.org/docs/exporting/) animations to/from [sprite sheets](https://www.aseprite.org/docs/sprite-sheet/), GIF files, or sequence of PNG files (and FLC, FLI, JPG, BMP, PCX, TGA).\r\n* [Multiple editors](https://www.aseprite.org/docs/workspace/#drag-and-drop-tabs) support.\r\n* [Layer groups](https://imgur.com/x3OKkGj) for organizing your work, and [reference layers](https://twitter.com/aseprite/status/806889204601016325) for rotoscoping.\r\n* Pixel-art specific tools like [Pixel Perfect freehand mode](https://imgur.com/0fdlNau), [Shading ink](https://www.aseprite.org/docs/shading/), [Custom Brushes](https://twitter.com/aseprite/status/1196883990080344067), [Outlines](https://twitter.com/aseprite/status/1126548469865431041), [Wide Pixels](https://imgur.com/1yZKUcs), etc.\r\n* Other special drawing tools like [Pressure sensitivity](https://twitter.com/aseprite/status/1253770784708886533), [Symmetry Tool](https://twitter.com/aseprite/status/659709226747625472), [Stroke and Fill](https://imgur.com/7JZQ81o) selection, [Gradients](https://twitter.com/aseprite/status/1126549217856622597).\r\n* [Tiled mode](https://twitter.com/pixel__toast/status/1132079817736695808) useful to draw patterns and textures.\r\n* [Transform multiple frames/layers](https://twitter.com/aseprite/status/1170007034651172866) at the same time.\r\n* [Lua scripting capabilities](https://www.aseprite.org/docs/scripting/).\r\n* [CLI - Command Line Interface](https://www.aseprite.org/docs/cli/) to automatize tasks.\r\n* [Quick Reference / Cheat Sheet](https://www.aseprite.org/quickref/) keyboard shortcuts ([customizable keys](https://imgur.com/rvAUxyF) and [mouse wheel](https://imgur.com/oNqFqVb)).\r\n* [Reopen closed files](https://twitter.com/aseprite/status/1202641475256881153) and [recover data](https://www.aseprite.org/docs/data-recovery/) in case of crash.\r\n* Undo/Redo for every operation and support for [non-linear undo](https://imgur.com/9I42fZK).\r\n* [More features &amp; tips](https://twitter.com/aseprite/status/1124442198651678720)\r\n\r\n## Issues\r\n\r\nThere is a list of\r\n[Known Issues](https://github.com/aseprite/aseprite/issues) (things\r\nto be fixed or that aren't yet implemented).\r\n\r\nIf you found a bug or have a new idea/feature for the program,\r\n[you can report them](https://github.com/aseprite/aseprite/issues/new).\r\n\r\n## Support\r\n\r\nYou can ask for help in:\r\n\r\n* [Aseprite Community](https://community.aseprite.org/)\r\n* [Aseprite Discord Server](https://discord.gg/Yb2CeX8)\r\n* Official support: [support@aseprite.org](mailto:support@aseprite.org)\r\n* Social networks and community-driven places:\r\n  [Twitter](https://twitter.com/aseprite/),\r\n  [Facebook](https://facebook.com/aseprite/),\r\n  [YouTube](https://www.youtube.com/user/aseprite),\r\n  [Instagram](https://www.instagram.com/aseprite/).\r\n\r\n## Authors\r\n\r\n[Igara Studio](https://www.igarastudio.com/) is developing Aseprite:\r\n\r\n* [David Capello](https://davidcapello.com/): Lead developer, fixing\r\n  issues, new features, and user support.\r\n* [Gaspar Capello](https://github.com/Gasparoken): Developer, fixing\r\n  issues and new features.\r\n\r\n## Credits\r\n\r\nThe default Aseprite theme was introduced in v0.8, created by:\r\n\r\n* [Ilija Melentijevic](https://ilkke.net/)\r\n\r\nAseprite includes color palettes created by:\r\n\r\n* [Richard \"DawnBringer\" Fhager](http://pixeljoint.com/p/23821.htm), [16 colors](http://pixeljoint.com/forum/forum_posts.asp?TID=12795),  [32 colors](http://pixeljoint.com/forum/forum_posts.asp?TID=16247).\r\n* [Arne Niklas Jansson](http://androidarts.com/), [16 colors](http://androidarts.com/palette/16pal.htm), [32 colors](http://wayofthepixel.net/index.php?topic=15824.msg144494).\r\n* [ENDESGA Studios](https://twitter.com/ENDESGA), [EDG16 and EDG32](https://forums.tigsource.com/index.php?topic=46126.msg1279124#msg1279124), and [other palettes](https://twitter.com/ENDESGA/status/865812366931353600).\r\n* [Hyohnoo Games](https://twitter.com/Hyohnoo), [mail24](https://twitter.com/Hyohnoo/status/797472587974639616) palette.\r\n* [Davit Masia](https://twitter.com/DavitMasia), [matriax8c](https://twitter.com/DavitMasia/status/834862452164612096) palette.\r\n* [Javier Guerrero](https://twitter.com/Xavier_Gd), [nyx8](https://twitter.com/Xavier_Gd/status/868519467864686594) palette.\r\n* [Adigun A. Polack](https://twitter.com/adigunpolack), [AAP-64](http://pixeljoint.com/pixelart/119466.htm), [AAP-Splendor128](http://pixeljoint.com/pixelart/120714.htm), [SimpleJPC-16](http://pixeljoint.com/pixelart/119844.htm), and [AAP-Micro12](http://pixeljoint.com/pixelart/121151.htm) palette.\r\n* [PineTreePizza](https://twitter.com/PineTreePizza), [Rosy-42](https://twitter.com/PineTreePizza/status/1006536191955623938) palette.\r\n\r\nIt tries to replicate some pixel-art algorithms:\r\n\r\n* [RotSprite](http://forums.sonicretro.org/index.php?showtopic=8848&st=15&p=159754&#entry159754) by Xenowhirl.\r\n* [Pixel perfect drawing algorithm](https://deepnight.net/blog/tools/pixel-perfect-drawing/) by [S\u00e9bastien B\u00e9nard](https://twitter.com/deepnightfr) and [Carduus](https://twitter.com/CarduusHimself/status/420554200737935361).\r\n\r\nThanks to [third-party open source projects](docs/LICENSES.md), to\r\n[contributors](https://www.aseprite.org/contributors/), and all the\r\npeople who have contributed ideas, patches, bugs report, feature\r\nrequests, donations, and help me to develop Aseprite.\r\n\r\n## License\r\n\r\nThis program is distributed under three different licenses:\r\n\r\n1. Source code and official releases/binaries are distributed under\r\n   our [End-User License Agreement for Aseprite (EULA)](EULA.txt). Please check\r\n   that there are [modules/libraries in the source code](src/README.md) that\r\n   are distributed under the MIT license\r\n   (e.g. [laf](https://github.com/aseprite/laf),\r\n   [clip](https://github.com/aseprite/clip),\r\n   [undo](https://github.com/aseprite/undo),\r\n   [observable](https://github.com/aseprite/observable),\r\n   [ui](src/ui), etc.).\r\n2. You can request a special\r\n   [educational license](https://www.aseprite.org/faq/#is-there-an-educational-license)\r\n   in case you are a teacher in an educational institution and want to\r\n   use Aseprite in your classroom (in-situ).\r\n3. Steam releases are distributed under the terms of the\r\n   [Steam Subscriber Agreement](http://store.steampowered.com/subscriber_agreement/).\r\n\r\nYou can get more information about Aseprite license in the\r\n[FAQ](https://www.aseprite.org/faq/#licensing-&-commercial).\r\n"
 },
 {
  "repo": "piskelapp/piskel",
  "language": "JavaScript",
  "readme_contents": "Piskel\n======\n\n[![Travis Status](https://api.travis-ci.org/piskelapp/piskel.png?branch=master)](https://travis-ci.org/piskelapp/piskel) [![Built with Grunt](https://cdn.gruntjs.com/builtwith.png)](https://gruntjs.com/)\n\nPiskel is an easy-to-use sprite editor. It can be used to create game sprites, animations, pixel-art...\nIt is the editor used in **[piskelapp.com](https://www.piskelapp.com)**.\n\n<img\n  src=\"https://screenletstore.appspot.com/img/95aaa0f0-37a4-11e7-a652-7b8128ce3e3b.png\"\n  title=\"Piskel editor screenshot\"\n  width=\"500\">\n\n## About Piskel\n\n### Built with\n\nThe Piskel editor is purely built in **JavaScript, HTML and CSS**.\n\nWe also use the following **libraries** :\n* [spectrum](https://github.com/bgrins/spectrum) : awesome standalone colorpicker\n* [gifjs](https://jnordberg.github.io/gif.js/) : generate animated GIFs in javascript, using webworkers\n* [supergif](https://github.com/buzzfeed/libgif-js) : modified version of SuperGif to parse and import GIFs\n* [jszip](https://github.com/Stuk/jszip) : create, read and edit .zip files with Javascript\n* [canvas-toBlob](https://github.com/eligrey/canvas-toBlob.js/) : shim for canvas toBlob\n* [jquery](https://jquery.com/) : used sporadically in the application\n* [bootstrap-tooltip](https://getbootstrap.com/javascript/#tooltips) : nice tooltips\n\nAs well as some **icons** from the [Noun Project](https://thenounproject.com/) :\n* Folder by Simple Icons from The Noun Project\n* (and probably one or two others)\n\n### Browser Support\n\nPiskel supports the following browsers:\n* **Chrome** (latest)\n* **Firefox** (latest)\n* **Edge** (latest)\n* **Internet Explorer** 11\n\n### Mobile/Tablets\n\nThere is no support for mobile.\n\n### Offline builds\n\nOffline builds are available. More details in the [dedicated wiki page](https://github.com/piskelapp/piskel/wiki/Desktop-applications).\n\n## Contributing ?\n\nHelp is always welcome !\n\n* **Issues** : Found a problem when using the application, want to request a feature, [open an issue](https://github.com/piskelapp/piskel/issues).\n* **Development** : Have a look at the [wiki](https://github.com/piskelapp/piskel/wiki) to set up the development environment\n\n## License\n\nCopyright 2017 Julian Descottes\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n"
 },
 {
  "repo": "jvalen/pixel-art-react",
  "language": "JavaScript",
  "readme_contents": "<p align=\"center\">\n  <img width=\"200\" src=\"screenshots/tree-pixelartcss.png\">\n</p>\n<h1 align=\"center\">Pixel Art to CSS</h1>\n<p align=\"center\">\n  <h3 align=\"center\">  \n    Animate pixel art and get CSS\n  </h3>\n</p>\n<p align=\"center\">\n  <a target='_blank' href='http://www.recurse.com' title='Made at the Recurse Center'><img src='https://cloud.githubusercontent.com/assets/2883345/11325206/336ea5f4-9150-11e5-9e90-d86ad31993d8.png' height='20px'/></a>\n  <a href=\"https://travis-ci.com/jvalen/pixel-art-react\"><img src=\"https://travis-ci.com/jvalen/pixel-art-react.svg?branch=master\" alt=\"travis ci\"></a>\n</p>\n\n## Did you know that you can create pixel art using CSS?\n\n**Pixel Art to CSS** is an online editor that helps you with that task.\n\nCombining the power of both **box-shadow** and **keyframes** CSS properties, you will get CSS code ready to use in your site.\n\nFurthermore, you can download your work in different formats such as a static image, animated GIF or sprite like image.\n\n:pencil2: [Try it out](https://www.pixelartcss.com/)\n\n<p align=\"center\">\n  <img width=\"600\" src=\"screenshots/screenshot-potion.png\">\n</p>\n\n**Pixel Art to CSS** aims to be an intuitive tool by its simplicity, however it is equipped with a wide range of features: customize your color palette, go back and forth in time, modify animation settings, save or load your projects, among others.\n\n## Example\n\nBy default, you will find the following project within the <b>LOAD</b> section:\n\n![Cat animation example](screenshots/animation-cat.gif)\n\nSee it live at [pixelartcss](https://www.pixelartcss.com/)\n\nYou can also import it directly submitting [this](examples/import-export/cat.txt) code.\n\n## Technical overview\n\nThis application has been built with the following technologies:\n\n- [React](https://facebook.github.io/react/): Library to build the UI.\n- [Redux](http://redux.js.org/): Implements a Flux like architecture.\n- [ImmutableJS](https://facebook.github.io/immutable-js/) Helps to keep the data immutable aiming to avoid side effects.\n- [PostCSS](https://github.com/postcss/postcss) Handle the app CSS.\n- [NodeJS](https://nodejs.org/en/) + [Express](http://expressjs.com/) (Server side to build an universal application, create and serve the generated drawings).\n\n## Installation\n\n```bash\nnpm install\n```\n\n## Development\n\nIf you just want to develop the interface with no need of the back-end side.\n\n```bash\nnpm run development\n```\n\n## Deploy\n\nCreate the final build and run the generated React HTML on the server using SSR.\n\n```bash\nnpm run deploy\n\nnpm run server\n```\n\nA `config.json` is needed for deployment with the Twitter and express keys.\n\n## Lint\n\nThere are several libraries used in the project that help us to keep our codebase healthy:\n\n- [ESlint](https://eslint.org/)\n- [stylelint](https://stylelint.io/)\n- [Prettier](https://prettier.io/)\n\nEvery time we commit something it will execute the linters and format the staged files if needed.\n\nIf you want to check them individually you could execute the following scripts:\n\n```bash\nnpm run lint\nnpm run csslint\nnpm run format\n```\n\n## Testing\n\nWe are using [Jest](https://jestjs.io/) as the testing platform.\n\n```bash\nnpm run test\n```\n\n## Contributing\n\n#### Help me to improve it :seedling:\n\nPlease create a GitHub issue if there is something wrong or to be improved. Pull requests are also welcome, they should be created to the **develop branch**.\n\n## License\n\n[MIT](https://opensource.org/licenses/mit-license.php)\nCopyright \u00a9 2016 Javier Valencia Romero (@jvalen)\n"
 },
 {
  "repo": "kitao/pyxel",
  "language": "C++",
  "readme_contents": "# <img src=\"images/pyxel_logo_152x64.png\">\n\n[ [English](README.md) | [\u4e2d\u6587](README.cn.md) | [Espa\u00f1ol](README.es.md) | [Italiano](README.it.md) | [\u65e5\u672c\u8a9e](README.ja.md) | [\ud55c\uad6d\uc5b4](README.ko.md) | [Portugu\u00eas](README.pt.md) | [\u0420\u0443\u0441\u0441\u043a\u0438\u0439](README.ru.md) ]\n\n**Pyxel**\u662f\u4e00\u4e2apython\u7684\u7ecf\u5178\u50cf\u7d20\u98ce\u6e38\u620f\u5236\u4f5c\u5f15\u64ce\u3002\n\n\u7531\u4e8e\u50cf\u7d20\u98ce\u6e38\u620f\u7684\u673a\u5236\u975e\u5e38\u7b80\u5355\uff08\u5982\uff1a\u6700\u591a\u53ea\u80fd\u663e\u793a16\u79cd\u989c\u8272\u3001\u64ad\u653e4\u79cd\u58f0\u97f3\u7b49\uff09\uff0c\u73b0\u5728\u4f60\u4e5f\u53ef\u4ee5\u8f7b\u677e\u5730\u4eab\u53d7\u8fd9\u79cd\u6e38\u620f\u7684\u5236\u4f5c\u8fc7\u7a0b\u3002\n\n<a href=\"pyxel/examples/01_hello_pyxel.py\" target=\"_blank\">\n<img src=\"pyxel/examples/screenshots/01_hello_pyxel.gif\" width=\"48%\">\n</a>\n\n<a href=\"pyxel/examples/02_jump_game.py\" target=\"_blank\">\n<img src=\"pyxel/examples/screenshots/02_jump_game.gif\" width=\"48%\">\n</a>\n\n<a href=\"pyxel/examples/03_draw_api.py\" target=\"_blank\">\n<img src=\"pyxel/examples/screenshots/03_draw_api.gif\" width=\"48%\">\n</a>\n\n<a href=\"pyxel/examples/04_sound_api.py\" target=\"_blank\">\n<img src=\"pyxel/examples/screenshots/04_sound_api.gif\" width=\"48%\">\n</a>\n\n<a href=\"pyxel/editor/screenshots/image_tilemap_editor.gif\" target=\"_blank\">\n<img src=\"pyxel/editor/screenshots/image_tilemap_editor.gif\" width=\"48%\">\n</a>\n\n<a href=\"pyxel/editor/screenshots/sound_music_editor.gif\" target=\"_blank\">\n<img src=\"pyxel/editor/screenshots/sound_music_editor.gif\" width=\"48%\">\n</a>\n\n\u6e38\u620f\u63a7\u5236\u53f0\u4ee5\u53caAPI\u7684\u8bbe\u8ba1\u53c2\u8003\u4e86\u7ecf\u5178\u7684[PICO-8](https://www.lexaloffle.com/pico-8.php)\u4ee5\u53ca[TIC-80](https://tic.computer/)\u3002\n\nPyxel\u662f\u5f00\u6e90\u7684\uff0c\u5927\u5bb6\u53ef\u4ee5\u514d\u8d39\u4f7f\u7528\u3002\u73b0\u5728\u5c31\u8ba9\u6211\u4eec\u4e00\u8d77\u7528Pyxel\u5236\u4f5c\u81ea\u5df1\u7684\u6e38\u620f\u5427\uff01\n\n## \u8bf4\u660e\n\n- \u9700\u8981\u5728Windows\u3001Mac\u6216Linux\u4e0a\u8fd0\u884c\n- \u9700\u8981Python3\n- \u5185\u7f6e16\u8272\u8c03\u8272\u677f\n- 3\u4e2a256x256\u7684\u56fe\u50cf\u5e93\n- 8\u4e2a256x256\u7684\u74e6\u7247\u5730\u56fe\n- 4\u4e2a\u58f0\u9053\u5404\u542b\u670964\u4e2a\u53ef\u9009\u97f3\u8c03\n- \u53ef\u4efb\u610f\u7ec4\u54088\u4e2a\u97f3\u4e50\n- \u652f\u6301\u952e\u76d8\u3001\u9f20\u6807\u53ca\u6e38\u620f\u624b\u67c4\u8f93\u5165\n- \u56fe\u50cf\u548c\u97f3\u9891\u7f16\u8f91\u5668\n\n### \u8c03\u8272\u677f\n\n<img src=\"pyxel/examples/screenshots/05_color_palette.png\">\n<br><br>\n<img src=\"images/pyxel_palette.png\">\n\n## \u5982\u4f55\u5b89\u88c5\n\n### Windows\n\n\u7b2c\u4e00\u6b65\uff0c\u5b89\u88c5[Python3](https://www.python.org/)(3.6.8\u6216\u66f4\u9ad8\u7248\u672c)\u3002\n\n\u5982\u679c\u4f7f\u7528\u5b98\u65b9\u5b89\u88c5\u5668\u6765\u5b89\u88c5python\uff0c\u4e0d\u8981\u5fd8\u8bb0\u52fe\u9009\u4e0b\u56fe\u9009\u9879**\u5c06python\u6dfb\u52a0\u5230\u73af\u5883\u53d8\u91cf\uff1a**\n\n<img src=\"images/python_installer.png\">\n\n\u7b2c\u4e8c\u6b65, \u5728\u547d\u4ee4\u63d0\u793a\u7b26\u4e2d\u8f93\u5165\u4ee5\u4e0b`pip`\u6307\u4ee4\u76f4\u63a5\u5b89\u88c5pyxel\uff1a\n\n```sh\npip install -U pyxel\n```\n\n### Mac\n\n\u7b2c\u4e00\u6b65\uff0c\u5728\u5df2\u7ecf\u5b89\u88c5\u4e86[Homebrew](https://brew.sh/)\u5305\u7ba1\u7406\u5de5\u5177\u7684\u73af\u5883\u4e0b\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5[Python3](https://www.python.org/)(3.6.8\u6216\u66f4\u9ad8\u7248\u672c)\u548c\u5fc5\u9700\u7684\u8f6f\u4ef6\u5305\uff1a\n\n```sh\nbrew install python3 gcc sdl2 sdl2_image gifsicle\n```\n\n\u4f60\u53ef\u4ee5\u7528\u5176\u4ed6\u65b9\u5f0f\u5b89\u88c5Python3\uff0c\u4f46\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4f60\u5fc5\u987b\u5b89\u88c5\u5176\u4ed6\u5e93\u3002\n\n\u7b2c\u4e8c\u6b65\uff0c**\u91cd\u542f\u7ec8\u7aef**\uff0c\u5e76\u4f7f\u7528`pip3`\u547d\u4ee4\u5b89\u88c5Pyxel\uff1a\n\n```sh\npip3 install -U pyxel\n```\n\n### Linux\n\n\u4e3a\u5404linux\u53d1\u884c\u7248\u5b89\u88c5[Python3](https://www.python.org/)(3.6.8\u6216\u66f4\u9ad8\u7248\u672c)\u53ca\u5176\u4f9d\u8d56\u5305\u3002\n\n**Ubuntu:**\n\n```sh\nsudo apt install python3 python3-pip libsdl2-dev libsdl2-image-dev gifsicle\nsudo -H pip3 install -U pyxel\n```\n\n### \u5176\u4ed6\u73af\u5883\n\n\u4e3a\u9664\u4e0a\u8ff0\u5916\u5176\u4ed6\u73af\u5883(32\u4f4dLinux\u3001\u6811\u8393\u6d3e\u7b49)\u5b89\u88c5Pyxel\uff0c\u8bf7\u6309\u4ee5\u4e0b\u6b65\u9aa4\u8fdb\u884c\u6784\u5efa\uff1a\n\n#### \u5b89\u88c5\u6240\u9700\u7684\u5de5\u5177\u53ca\u4f9d\u8d56\u5305\n\n- C++\u6784\u5efa\u5de5\u5177\u94fe\uff08\u5305\u542bgcc\u548cmake\u547d\u4ee4\uff09\n- libsdl2-dev\u548clibsdl2-image-dev\n- [Python3](https://www.python.org/)(3.6.8\u6216\u66f4\u9ad8\u7248\u672c)\u548cpip\u5de5\u5177\n\n#### \u4efb\u610f\u6587\u4ef6\u5939\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\n\n```sh\ngit clone https://github.com/kitao/pyxel.git\ncd pyxel\nmake -C pyxel/core clean all\npip3 install .\n```\n\n### \u5b89\u88c5\u4f8b\u7a0b\n\n\u5b89\u88c5Pyxel\u540e\uff0c\u53ef\u4ee5\u7528\u4ee5\u4e0b\u547d\u4ee4\u5c06Pyxe\u4f8b\u7a0b\u590d\u5236\u5230\u5f53\u524d\u6587\u4ef6\u5939\uff1a\n\n```sh\ninstall_pyxel_examples\n```\n\n\u4f8b\u7a0b\u5305\u542b\uff1a\n\n- [01_hello_pyxel.py](pyxel/examples/01_hello_pyxel.py) - \u6700\u7b80\u5355\u7684\u5e94\u7528\n- [02_jump_game.py](pyxel/examples/02_jump_game.py) - \u7528Pyxel\u5236\u4f5c\u7684\u8df3\u8dc3\u6e38\u620f\n- [03_draw_api.py](pyxel/examples/03_draw_api.py) - \u7ed8\u753bAPI\u7684\u793a\u4f8b\n- [04_sound_api.py](pyxel/examples/04_sound_api.py) - \u58f0\u97f3API\u7684\u793a\u4f8b\n- [05_color_palette.py](pyxel/examples/05_color_palette.py) - \u8c03\u8272\u677f\u5217\u8868\n- [06_click_game.py](pyxel/examples/06_click_game.py) - \u9f20\u6807\u70b9\u51fb\u6e38\u620f\n- [07_snake.py](pyxel/examples/07_snake.py) - \u5e26BGM\u7684\u8d2a\u5403\u86c7\u6e38\u620f\n- [08_triangle_api.py](pyxel/examples/08_triangle_api.py) - \u4e09\u89d2\u5f62\u7ed8\u56fe\u793a\u4f8b\n- [09_shooter.py](pyxel/examples/09_shooter.py) - \u5c4f\u5e55\u8fc7\u6e21\u5c04\u51fb\u6e38\u620f\n\n\u8fd9\u4e9b\u4f8b\u7a0b\u53ef\u4ee5\u50cf\u6267\u884c\u6b63\u5e38python\u7a0b\u5e8f\u4e00\u6837\u8fd0\u884c\uff1a\n\n**Windows:**\n\n```sh\ncd pyxel_examples\npython 01_hello_pyxel.py\n```\n\n**Mac / Linux:**\n\n```sh\ncd pyxel_examples\npython3 01_hello_pyxel.py\n```\n\n## \u4f7f\u7528\u6559\u7a0b\n\n### \u521b\u5efaPyxel\u5e94\u7528\n\n\u5728python\u4ee3\u7801\u5bfc\u5165Pyxel\u6a21\u5757\u540e\uff0c\u9996\u5148\u7528`init`\u51fd\u6570\u6307\u5b9a\u7a97\u53e3\u5927\u5c0f\uff0c\u7136\u540e\u7528`run`\u51fd\u6570\u542f\u52a8Pyxel\u5e94\u7528\u3002\n\n```python\nimport pyxel\n\npyxel.init(160, 120)\n\ndef update():\n    if pyxel.btnp(pyxel.KEY_Q):\n        pyxel.quit()\n\ndef draw():\n    pyxel.cls(0)\n    pyxel.rect(10, 10, 20, 20, 11)\n\npyxel.run(update, draw)\n```\n\n`run`\u51fd\u6570\u7684\u4e24\u4e2a\u53c2\u6570`update`\u51fd\u6570\u548c`draw`\u51fd\u6570\u5206\u522b\u7528\u6765\u5728\u9700\u8981\u65f6\u66f4\u65b0\u5e27\u548c\u7ed8\u5236\u753b\u9762\u3002\n\n\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5efa\u8bae\u5c06pyxel\u4ee3\u7801\u5c01\u88c5\u6210\u5982\u4e0b\u7c7b\uff1a\n\n```python\nimport pyxel\n\nclass App:\n    def __init__(self):\n        pyxel.init(160, 120)\n        self.x = 0\n        pyxel.run(self.update, self.draw)\n\n    def update(self):\n        self.x = (self.x + 1) % pyxel.width\n\n    def draw(self):\n        pyxel.cls(0)\n        pyxel.rect(self.x, 0, 8, 8, 9)\n\nApp()\n```\n\n\u6709\u65f6\u4e5f\u53ef\u7b80\u5355\u4f7f\u7528`show`\u548c`flip`\u753b\u51fa\u7b80\u5355\u7684\u753b\u9762\u548c\u52a8\u753b\u3002\n\n`show`\u51fd\u6570\u53ef\u4ee5\u663e\u793a\u753b\u9762\u76f4\u5230`ESC`\u952e\u6309\u4e0b\u3002\n\n```python\nimport pyxel\n\npyxel.init(120, 120)\npyxel.cls(1)\npyxel.circb(60, 60, 40, 7)\npyxel.show()\n```\n\n`flip`\u51fd\u6570\u53ef\u4ee5\u66f4\u65b0\u4e00\u6b21\u753b\u9762\u3002\n\n```python\nimport pyxel\n\npyxel.init(120, 80)\n\nwhile True:\n    pyxel.cls(3)\n    pyxel.rectb(pyxel.frame_count % 160 - 40, 20, 40, 40, 7)\n    pyxel.flip()\n```\n\n### \u5feb\u6377\u952e\n\n\u4ee5\u4e0b\u5feb\u6377\u952e\u53ef\u4ee5\u5728Pyxel\u8fd0\u884c\u65f6\u4f7f\u7528\uff1a\n\n- `Esc`<br>\n\u9000\u51fa\u5e94\u7528\n- `Alt(Option)+1`<br>\n\u622a\u5c4f\u5e76\u4fdd\u5b58\u5728\u684c\u9762\n- `Alt(Option)+2`<br>\n\u91cd\u7f6e\u5c4f\u5e55\u5f55\u5236\u7684\u5f00\u59cb\u65f6\u95f4\n- `Alt(Option)+3`<br>\n\u4fdd\u5b58\u5c4f\u5e55\u5f55\u5236\u52a8\u56fe\uff08gif\uff09\u5230\u684c\u9762\uff08\u6700\u591a30\u79d2\uff09\n- `Alt(Option)+0`<br>\n\u5207\u6362\u6027\u80fd\u76d1\u63a7\uff08fps\uff0c\u66f4\u65b0\u65f6\u95f4\uff0c\u753b\u9762\u7ed8\u5236\u65f6\u95f4\uff09\n- `Alt(Option)+Enter`<br>\n\u5207\u6362\u5168\u5c4f\n\n### \u5982\u4f55\u521b\u5efa\u6e90\u6587\u4ef6\n\n\u5185\u7f6ePyxel\u7f16\u8f91\u5668\u53ef\u4ee5\u4e3aPyxel\u5e94\u7528\u521b\u5efa\u56fe\u7247\u548c\u97f3\u9891\u3002\n\n\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u542f\u52a8Pyxel\u7f16\u8f91\u5668\uff1a\n\n```sh\npyxeleditor [pyxel_resource_file]\n```\n\n\u82e5\u6307\u5b9aPyxel\u6e90\u6587\u4ef6\uff08.pyxres\uff09\u5b58\u5728\uff0c\u5219\u52a0\u8f7d\u6587\u4ef6\uff0c\u82e5\u4e0d\u5b58\u5728\uff0c\u5219\u4ee5\u6307\u5b9a\u6587\u4ef6\u540d\u65b0\u5efa\u6587\u4ef6\u3002\n\n\u82e5\u672a\u6307\u5b9a\u6e90\u6587\u4ef6\uff0c\u5219\u547d\u540d\u4e3a`my_resource.pyxres`\u3002\n\n\u542f\u52a8Pyxel\u7f16\u8f91\u5668\u540e\uff0c\u53ef\u4ee5\u901a\u8fc7\u62d6\u653e\u6765\u5207\u6362\u6587\u4ef6\u3002\u82e5\u5728\u6309\u4e0b``Ctrl``(``Cmd``)\u952e\u7684\u540c\u65f6\u62d6\u653e\u6e90\u6587\u4ef6\uff0c\u5219\u53ea\u6709\u5f53\u524d\u6b63\u5728\u7f16\u8f91\u7684\u7c7b\u578b(image/tilemap/sound/music)\u4f1a\u88ab\u52a0\u8f7d\u3002\u901a\u8fc7\u672c\u64cd\u4f5c\u53ef\u4ee5\u5c06\u591a\u4e2a\u6e90\u6587\u4ef6\u5408\u5e76\u4e3a\u4e00\u4e2a\u3002\n\n\u521b\u5efa\u540e\u7684\u6e90\u6587\u4ef6\u53ef\u7528`load`\u51fd\u6570\u6765\u52a0\u8f7d\u3002\n\nPyxel\u7f16\u8f91\u5668\u6709\u4ee5\u4e0b\u7f16\u8f91\u6a21\u5f0f\u3002\n\n**\u56fe\u50cf\u7f16\u8f91\u5668\uff1a**\n\n\u6b64\u6a21\u5f0f\u7528\u6765\u7f16\u8f91\u56fe\u50cf\u5e93\u3002\n\n<img src=\"pyxel/editor/screenshots/image_editor.gif\">\n\n\u901a\u8fc7\u62d6\u52a8png\u6587\u4ef6\u81f3\u56fe\u50cf\u7f16\u8f91\u5668\u754c\u9762\uff0c\u53ef\u4ee5\u5c06\u56fe\u50cf\u52a0\u8f7d\u81f3\u5f53\u9009\u62e9\u524d\u56fe\u50cf\u5e93\u3002\n\n**\u74e6\u7247\u5730\u56fe(Tilemap)\u7f16\u8f91\u5668\uff1a**\n\n\u6b64\u6a21\u5f0f\u7528\u6765\u7f16\u8f91\u74e6\u7247\u5730\u56fe\uff0c\u5176\u4e2d\u56fe\u50cf\u5e93\u7684\u56fe\u50cf\u4ee5\u74e6\u7247\u7684\u6837\u5f0f\u6392\u5217\u3002\n\n<img src=\"pyxel/editor/screenshots/tilemap_editor.gif\">\n\n**\u97f3\u9891\u7f16\u8f91\u5668\uff1a**\n\n\u6b64\u6a21\u5f0f\u7528\u6765\u7f16\u8f91\u97f3\u9891\u3002\n\n<img src=\"pyxel/editor/screenshots/sound_editor.gif\">\n\n**\u97f3\u4e50\u7f16\u8f91\u5668\uff1a**\n\n\u6b64\u6a21\u5f0f\u7528\u6765\u7f16\u8f91\u5c06\u5f55\u97f3\u6709\u5e8f\u7f16\u6392\u5f62\u6210\u7684\u97f3\u4e50\u3002\n\n<img src=\"pyxel/editor/screenshots/music_editor.gif\">\n\n### \u5176\u4ed6\u521b\u5efa\u6e90\u6587\u4ef6\u7684\u65b9\u6cd5\n\nPyxel\u56fe\u50cf\u548c\u74e6\u7247\u5730\u56fe\u8fd8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u6cd5\u521b\u5efa\uff1a\n\n- \u5728`Image.set`\u6216`Tilemap.set`\u51fd\u6570\u4e2d\u901a\u8fc7\u5b57\u7b26\u4e32list\u6765\u751f\u6210\u56fe\u50cf\n- \u5728Pyxel\u8c03\u8272\u677f\u4e2d\u7528`Image.load`\u51fd\u6570\u52a0\u8f7dpng\u6587\u4ef6\n\nPyxel\u97f3\u9891\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u6cd5\u521b\u5efa\uff1a\n\n- \u5728`Sound.set`\u6216`Music.set`\u51fd\u6570\u4e2d\u901a\u8fc7\u5b57\u7b26\u4e32\u6765\u751f\u6210\u97f3\u9891\n\n\u8fd9\u4e9b\u51fd\u6570\u7684\u5177\u4f53\u7528\u6cd5\u8bf7\u67e5\u9605API\u53c2\u8003\u624b\u518c\u3002\n\n### \u5982\u4f55\u521b\u5efa\u72ec\u7acb\u53ef\u6267\u884c\u6587\u4ef6\n\n\u4f7f\u7528\u5185\u7f6e\u7684Pyxel  Packager\u521b\u5efa\u72ec\u7acb\u7684\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u5728\u6ca1\u6709python\u7684\u73af\u5883\u4e0b\u4e5f\u53ef\u4ee5\u6267\u884c\u3002\n\n\u8981\u521b\u5efa\u72ec\u7acb\u7684\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u8bf7\u5728\u5b89\u88c5\u4e86[PyInstaller](https://www.pyinstaller.org/)\u7684\u73af\u5883\u4e2d\uff0c\u4f7f\u7528`pyxelpackager`\u547d\u4ee4\u6307\u5b9a\u7528\u4e8e\u542f\u52a8\u5e94\u7528\u7a0b\u5e8f\u7684Python\u6587\u4ef6\uff0c\u5982\u4e0b\u6240\u793a\uff1a\n\n```sh\npyxelpackager python_file\n```\n\n\u8fdb\u7a0b\u7ed3\u675f\u540e\uff0c\u53ef\u6267\u884c\u6587\u4ef6\u4fbf\u4f1a\u751f\u6210\u5728`dist`\u6587\u4ef6\u5939\u4e0b\u3002\n\n\u82e5\u5e94\u7528\u5fc5\u987b\u5305\u542b.pyxres\u548c.png\u6587\u4ef6\uff0c\u5c06\u5176\u653e\u5728`assets`\u6587\u4ef6\u5939\u4e0b\uff0c\u4ed6\u4eec\u4fbf\u4f1a\u88ab\u6253\u5305\u8fdb\u53ef\u6267\u884c\u6587\u4ef6\u4e2d\u3002\n\n\u53ef\u4ee5\u4f7f\u7528``-i icon_file``\u6307\u4ee4\u81ea\u5b9a\u4e49\u5e94\u7528\u56fe\u6807\u3002\n\n## API\u53c2\u8003\u624b\u518c\n\n### \u7cfb\u7edf\n\n- `width`, `height`<br>\n\u753b\u9762\u7684\u5bbd\u548c\u9ad8\n\n- `frame_count`<br>\n\u7ecf\u8fc7\u7684\u5e27\u6570\n\n- `init(width, height, [caption], [scale], [palette], [fps], [quit_key], [fullscreen])`<br>\n\u521d\u59cb\u5316Pyxel\u5e94\u7528\u7684\u753b\u9762\u5c3a\u5bf8\u3002\u753b\u9762\u7684\u5bbd\u548c\u9ad8\u7684\u6700\u5927\u503c\u662f256\u3002<br>\n\u540c\u65f6\u53ef\u4ee5\u7528`caption`\u6307\u5b9a\u7a97\u53e3\u6807\u9898\uff0c`scale`\u8bbe\u5b9a\u653e\u5927\u500d\u6570\uff0c`palette`\u8bbe\u5b9a\u8272\u8c03\uff0c`fps`\u8bbe\u5b9a\u5e27\u7387\uff0c`quit_key`\u53ef\u6307\u5b9a\u9000\u51fa\u952e, `fullscreen`\u8bbe\u7f6e\u662f\u5426\u5168\u5c4f\u542f\u52a8\u3002\u5176\u4e2d`palette`\u4e3a16\u4e2a24\u4f4d\u771f\u5f69\u8272\u5143\u7d20\u7684list\u3002<br>\n\u4f8b\uff1a`pyxel.init(160, 120, caption=\"Pyxel with PICO-8 palette\", palette=[0x000000, 0x1D2B53, 0x7E2553, 0x008751, 0xAB5236, 0x5F574F, 0xC2C3C7, 0xFFF1E8, 0xFF004D, 0xFFA300, 0xFFEC27, 0x00E436, 0x29ADFF, 0x83769C, 0xFF77A8, 0xFFCCAA], quit_key=pyxel.KEY_NONE, fullscreen=True)`\n\n- `run(update, draw)`<br>\n\u542f\u52a8Pyxel\u5e94\u7528\u5e76\u8c03\u7528`update`\u66f4\u65b0\u5e27\u3001`draw`\u7ed8\u5236\u753b\u9762\u3002\n\n- `quit()`<br>\n\u5f53\u524d\u5e27\u7ed3\u675f\u540e\u9000\u51faPyxel\u5e94\u7528\u3002\n\n- `flip()`<br>\n\u5f3a\u5236\u7ed8\u5236\u753b\u9762\uff08\u901a\u5e38\u5e94\u7528\u4e2d\u4e0d\u4f1a\u4f7f\u7528\uff09\u3002\n\n- `show()`<br>\n\u7ed8\u5236\u753b\u9762\u5e76\u4e00\u76f4\u7b49\u5f85\uff08\u901a\u5e38\u5e94\u7528\u4e2d\u4e0d\u4f1a\u4f7f\u7528\uff09\u3002\n\n### \u6e90\u6587\u4ef6\n\n- `save(filename)`<br>\n\u4fdd\u5b58\u6e90\u6587\u4ef6\uff08.pyxres\uff09\u5230\u6267\u884c\u811a\u672c\u7684\u76ee\u5f55\u4e0b\u3002\n\n- `load(filename, [image], [tilemap], [sound], [music])`<br>\n\u4ece\u6267\u884c\u811a\u672c\u7684\u76ee\u5f55\u4e0b\u8bfb\u53d6\u6e90\u6587\u4ef6\uff08.pyxres\uff09\u3002\u5982\u679c\u67d0\u4e00\u6e90\u6587\u4ef6\u7c7b\u578b\u6307\u5b9a\u4e3aFalse\uff0c\u5219\u5bf9\u5e94\u7c7b\u578b\u4e0d\u4f1a\u88ab\u52a0\u8f7d\u3002\n\n### \u8f93\u5165\n- `mouse_x`, `mouse_y`<br>\n\u5f53\u524d\u9f20\u6807\u6307\u9488\u7684\u4f4d\u7f6e\u3002\n\n- `mouse_wheel`<br>\n\u5f53\u524d\u9f20\u6807\u6eda\u8f6e\u7684\u503c\u3002\n\n- `btn(key)`<br>\n\u5982\u679c`key`\u88ab\u6309\u4e0b\u5219\u8fd4\u56de`True`\uff0c\u5426\u5219\u8fd4\u56de`False`([\u6309\u952e\u5b9a\u4e49\u5217\u8868](pyxel/__init__.py))\u3002\n\n- `btnp(key, [hold], [period])`<br>\n\u5982\u679c`key`\u88ab\u6309\u4e0b\u5219\u8fd4\u56de`True`\u3002\u82e5\u8bbe\u7f6e\u4e86`hold`\u548c`period`\u53c2\u6570\uff0c\u5219\u5f53`key`\u88ab\u6309\u4e0b\u6301\u7eed`hold`\u5e27\u65f6\uff0c\u5728`period`\u5e27\u95f4\u9699\u8fd4\u56de`True`\u3002\n\n- `btnr(key)`<br>\n\u5982\u679c`key`\u88ab\u677e\u5f00\uff0c\u5219\u5728\u6b64\u5e27\u8fd4\u56de`True`\uff0c\u5426\u5219\u8fd4\u56de`False`\u3002\n\n- `mouse(visible)`<br>\n\u5982\u679c`visible`\u4e3a`True`\u5219\u663e\u793a\u9f20\u6807\u6307\u9488\uff0c\u4e3a`False`\u5219\u4e0d\u663e\u793a\u3002\u5373\u4f7f\u9f20\u6807\u6307\u9488\u4e0d\u663e\u793a\uff0c\u5176\u4f4d\u7f6e\u540c\u6837\u4f1a\u88ab\u66f4\u65b0\u3002\n\n### \u663e\u793a\n\n- `image(img, [system])`<br>\n\u64cd\u4f5c\u56fe\u50cf\u5e93`img`(0-2)\uff08\u53c2\u8003Image\u7c7b\uff09\u3002\u82e5`system`\u6307\u5b9a\u4e3a`True`\uff0c\u5219\u56fe\u50cf\u5e93\u53ef\u5b58\u53d6\u3002\n3\u5bf9\u5e94\u5b57\u4f53\u548c\u6e90\u6587\u4ef6\u7f16\u8f91\u5668\uff0c4\u5bf9\u5e94\u663e\u793a\u753b\u9762\u3002<br>\n\u4f8b\uff1a`pyxel.image(0).load(0, 0, \"title.png\")`\n\n- `tilemap(tm)`<br>\n\u64cd\u4f5c\u74e6\u7247\u5730\u56fe`tm`(0-7)\uff08\u53c2\u8003Tilemap\u7c7b\uff09\n\n- `clip(x, y, w, h)`<br>\n\u8bbe\u7f6e\u753b\u9762\u7ed8\u5236\u533a\u57df\u4e3a\u4ece(`x`, `y`)\u5f00\u59cb\u7684\u5bbd\u5ea6`w`\u3001\u9ad8\u5ea6\u4e3a`h`\u7684\u533a\u57df\u3002`clip()`\u53ef\u4ee5\u5c06\u7ed8\u5236\u533a\u57df\u91cd\u7f6e\u4e3a\u5168\u5c4f\u3002\n\n- `pal(col1, col2)`<br>\n\u7ed8\u5236\u65f6\u7528`col1`\u989c\u8272\u4ee3\u66ff`col2`\u989c\u8272\u3002`pal()`\u53ef\u4ee5\u91cd\u7f6e\u4e3a\u521d\u59cb\u8272\u8c03\u3002\n\n- `cls(col)`<br>\n\u7528`col`\u989c\u8272\u6e05\u7a7a\u753b\u9762\u3002\n\n- `pget(x, y)`<br>\n\u83b7\u53d6(`x`, `y`)\u5904\u7684\u50cf\u7d20\u989c\u8272\u3002\n\n- `pset(x, y, col)`<br>\n\u7528`col`\u989c\u8272\u5728(`x`, `y`)\u5904\u7ed8\u5236\u4e00\u4e2a\u50cf\u7d20\u70b9\u3002\n\n- `line(x1, y1, x2, y2, col)`<br>\n\u7528`col`\u989c\u8272\u753b\u4e00\u6761\u4ece(`x1`, `y1`)\u5230(`x2`, `y2`)\u7684\u76f4\u7ebf\u3002\n\n- `rect(x, y, w, h, col)`<br>\n\u7528`col`\u989c\u8272\u7ed8\u5236\u4e00\u4e2a\u4ece(`x`, `y`)\u5f00\u59cb\u7684\u5bbd\u4e3a`w`\u3001\u9ad8\u4e3a`h`\u7684\u77e9\u5f62\u3002\n\n- `rectb(x, y, w, h, col)`<br>\n\u7528`col`\u989c\u8272\u7ed8\u5236\u4ece(`x`, `y`)\u5f00\u59cb\u7684\u5bbd\u4e3a`w`\u3001\u9ad8\u4e3a`h`\u7684\u77e9\u5f62\u8fb9\u6846\u3002\n\n- `circ(x, y, r, col)`<br>\n\u7528`col`\u989c\u8272\u7ed8\u5236\u5706\u5fc3\u4e3a(`x`, `y`)\uff0c\u534a\u5f84\u4e3a`r`\u7684\u5706\u5f62\u3002\n\n- `circb(x, y, r, col)`<br>\n\u7528`col`\u989c\u8272\u7ed8\u5236\u5706\u5fc3\u4e3a(`x`, `y`)\uff0c\u534a\u5f84\u4e3a`r`\u7684\u5706\u5f62\u8fb9\u6846\u3002\n\n- `tri(x1, y1, x2, y2, x3, y3, col)`<br>\n\u7528`col`\u989c\u8272\u7ed8\u5236\u9876\u70b9\u5206\u522b\u4e3a(`x1`, `y1`)\uff0c(`x2`, `y2`)\uff0c(`x3`, `y3`)\u7684\u4e09\u89d2\u5f62\u3002\n\n- `trib(x1, y1, x2, y2, x3, y3, col)`<br>\n\u7528`col`\u989c\u8272\u7ed8\u5236\u9876\u70b9\u5206\u522b\u4e3a(`x1`, `y1`)\uff0c(`x2`, `y2`)\uff0c(`x3`, `y3`)\u7684\u4e09\u89d2\u5f62\u8fb9\u6846\u3002\n\n- `blt(x, y, img, u, v, w, h, [colkey])`<br>\n\u5c06\u5c3a\u5bf8\u4e3a(`w`, `h`)\u7684\u533a\u57df\u4ece\u56fe\u50cf\u5e93\u7684(`u`, `v`)\u590d\u5236\u5230(`x`, `y`)\u3002\u82e5`w`\u6216`h`\u4e3a\u8d1f\u503c\uff0c\u5219\u5728\u6c34\u5e73\u6216\u5782\u76f4\u65b9\u5411\u4e0a\u7ffb\u8f6c\u3002\u82e5\u6307\u5b9a\u4e86`colkey`\u7684\u503c\uff0c\u5219\u89c6\u4f5c\u900f\u660e\u989c\u8272\u3002\n\n<img src=\"images/image_bank_mechanism.png\">\n\n- `bltm(x, y, tm, u, v, w, h, [colkey])`<br>\n\u6839\u636e\u4ece(`u`, `v`)\u5f00\u59cb\u7684\u5c3a\u5bf8\u4e3a(`w`, `h`)\u7684tail\u4fe1\u606f\uff0c\u5c06\u74e6\u7247\u5730\u56fe(tilemap)`tm`(0-7)\u7ed8\u5236\u5230(`x`, `y`)\u5904\u3002\u82e5\u6307\u5b9a\u4e86`colkey`\u7684\u503c\uff0c\u5219\u89c6\u4f5c\u900f\u660e\u989c\u8272\u3002\u74e6\u7247\u5730\u56fe(tilemap)\u4e2d\u4e00\u4e2atail\u5c3a\u5bf8\u4e3a8x8\u3002\u82e5tail\u7f16\u53f7\u4e3a0\uff0c\u4ee3\u8868\u56fe\u50cf\u5e93\u4e2d(0, 0)-(7, 7)\u7684\u533a\u57df\uff0c\u82e5\u7f16\u53f7\u4e3a1\uff0c\u4ee3\u8868(8, 0)-(15, 0)\u7684\u533a\u57df\u3002\n\n<img src=\"images/tilemap_mechanism.png\">\n\n- `text(x, y, s, col)`<br>\n\u7528`col`\u989c\u8272\u5728(`x`, `y`)\u7ed8\u5236\u5b57\u7b26\u4e32`s`\u3002\n\n### \u58f0\u97f3\n\n- `sound(snd, [system])`<br>\n\u64cd\u4f5c\u97f3\u9891`snd`(0-63)\uff08\u53c2\u8003Sound\u7c7b\uff09\u3002\u82e5`system`\u4e3a`True`\uff0c\u5219sound 64\u53ef\u5b58\u53d6<br>\n\u793a\u4f8b\uff1a`pyxel.sound(0).speed = 60`\n\n- `music(msc)`<br>\n\u64cd\u4f5c\u97f3\u4e50`msc`(0-7)\uff08\u53c2\u8003Music\u7c7b\uff09\n\n- `play_pos(ch)`<br>\n\u83b7\u53d6`ch`\u58f0\u9053\u7684\u97f3\u9891\u5f53\u524d\u64ad\u653e\u5230\u7684\u4f4d\u7f6e\u3002\u4e2a\u4f4d\u6570\u548c\u5341\u4f4d\u6570\u8868\u793anote\u7684\u503c\uff0c\u767e\u4f4d\u6570\u548c\u5343\u4f4d\u6570\u8868\u793asound\u7684\u6570\u5b57\u3002\u5f53\u64ad\u653e\u505c\u6b62\u65f6\uff0c\u8fd4\u56de-1\u3002\n\n- `play(ch, snd, loop=False)`<br>\n\u5728\u58f0\u9053`ch`(0-3)\u64ad\u653e\u97f3\u9891`snd`(0-63)\u3002\u5f53`snd`\u662f\u5217\u8868\u65f6\uff0c\u6309\u987a\u5e8f\u64ad\u653e\u3002\n\n- `playm(msc, loop=False)`<br>\n\u64ad\u653e\u97f3\u4e50`msc`(0-7)\n\n- `stop([ch])`<br>\n\u505c\u6b62\u6240\u6709\u58f0\u9053\u7684\u64ad\u653e\u3002\u82e5\u6307\u5b9a\u4e86`ch`(0-3)\uff0c\u5219\u53ea\u505c\u6b62\u5bf9\u5e94\u58f0\u9053\u3002\n\n### Image\u7c7b\n\n- `width`, `height`<br>\n\u56fe\u50cf\u7684\u5bbd\u548c\u9ad8\u3002\n\n- `data`<br>\n\u56fe\u50cf\u4e2d\u7684\u6570\u636e\uff08256x256\u7684\u4e8c\u7ef4\u5217\u8868\uff09\u3002\n\n- `get(x, y)`<br>\n\u83b7\u53d6\u56fe\u50cf\u4e2d(`x`, `y`)\u4f4d\u7f6e\u7684\u503c\u3002\n\n- `set(x, y, data)`<br>\n\u5c06\u56fe\u50cf\u4e2d(`x`, `y`)\u4f4d\u7f6e\u7684\u503c\u8bbe\u7f6e\u4e3a\u5b57\u7b26\u4e32\u5217\u8868\u7684\u503c\u3002<br>\n\u793a\u4f8b\uff1a`pyxel.image(0).set(10, 10, [\"1234\", \"5678\", \"9abc\", \"defg\"])`\n\n- `load(x, y, filename)`<br>\n\u4ece\u6267\u884c\u811a\u672c\u6240\u5728\u7684\u6587\u4ef6\u5939\u52a0\u8f7dpng\u6587\u4ef6\u5230(`x`, `y`)\n\n- `copy(x, y, img, u, v, w, h)`<br>\n\u5c06\u56fe\u50cf\u5e93`img`(0-2)\u4e2d\u4ece(`u`, `v`)\u5f00\u59cb\u7684\u5c3a\u5bf8\u4e3a(`w`, `h`)\u7684\u533a\u57df\u590d\u5236\u5230(`x`, `y`)\n\n### Tilemap\u7c7b\n\n- `width`, `height`<br>\n\u74e6\u7247\u5730\u56fe(tilemap)\u7684\u5bbd\u548c\u9ad8\u3002\n\n- `data`<br>\n\u74e6\u7247\u5730\u56fe\u4e2d\u7684\u6570\u636e\uff08256x256\u7684\u4e8c\u7ef4\u5217\u8868\uff09\n\n- `refimg`<br>\n\u74e6\u7247\u5730\u56fe\u4e2d\u5f15\u7528\u7684\u56fe\u50cf\u5e93\u3002\n\n- `get(x, y)`<br>\n\u83b7\u53d6\u74e6\u7247\u5730\u56fe\u4e2d(`x`, `y`)\u4f4d\u7f6e\u7684\u503c\u3002\n\n- `set(x, y, data)`<br>\n\u5c06\u74e6\u7247\u5730\u56fe\u4e2d(`x`, `y`)\u4f4d\u7f6e\u7684\u503c\u8bbe\u7f6e\u4e3a\u5b57\u7b26\u4e32\u5217\u8868\u7684\u503c\u3002<br>\n\u793a\u4f8b\uff1a`pyxel.tilemap(0).set(0, 0, [\"000102\", \"202122\", \"a0a1a2\", \"b0b1b2\"])`\n\n- `copy(x, y, tm, u, v, w, h)`<br>\n\u5c06\u74e6\u7247\u5730\u56fe`tm`(0-7)\u4e2d\u4ece(`u`, `v`)\u5f00\u59cb\u7684\u5c3a\u5bf8\u4e3a(`w`, `h`)\u7684\u533a\u57df\u590d\u5236\u5230(`x`, `y`)\n\n### Sound\u7c7b\n\n- `note`<br>\nnote\uff08\u97f3\u7b26\uff09\u5217\u8868(0-127) (33 = 'A2' = 440Hz)\n\n- `tone`<br>\ntone\uff08\u97f3\u8c03\uff09\u5217\u8868(0:Triangle / 1:Square / 2:Pulse / 3:Noise)\n\n- `volume`<br>\nvolume\uff08\u97f3\u91cf\uff09\u5217\u8868(0-7)\n\n- `effect`<br>\neffect\uff08\u97f3\u6548\uff09\u5217\u8868(0:None / 1:Slide / 2:Vibrato / 3:FadeOut)\n\n- `speed`<br>\n\u4e00\u4e2anote\uff08\u97f3\u7b26\uff09\u7684\u957f\u5ea6(120 = 1 second per tone)\n\n- `set(note, tone, volume, effect, speed)`<br>\n\u7528\u5b57\u7b26\u4e32\u6765\u8bbe\u7f6enote\uff0ctone\uff0cvolume\u548ceffect\u3002\u82e5tone\uff0cvolume\uff0c\u548ceffect\u7684\u957f\u5ea6\u6bd4note\u77ed\uff0c\u5219\u5c06\u5176\u5faa\u73af\u5904\u7406\u3002\n\n- `set_note(note)`<br>\n\u7528'CDEFGAB'+'#-'+'0123'\u6216'R'\u7ec4\u6210\u7684\u5b57\u7b26\u4e32\u6765\u8bbe\u7f6enote\u3002\u4e0d\u533a\u5206\u5927\u5c0f\u5199\uff0c\u4e0d\u8ba1\u5165\u7a7a\u683c\u3002<br>\n\u793a\u4f8b\uff1a`pyxel.sound(0).set_note(\"G2B-2D3R RF3F3F3\")`\n\n- `set_tone(tone)`<br>\n\u7528'TSPN'\u7ec4\u6210\u7684\u5b57\u7b26\u4e32\u8bbe\u7f6etone\u3002\u4e0d\u533a\u5206\u5927\u5c0f\u5199\uff0c\u4e0d\u8ba1\u5165\u7a7a\u683c\u3002<br>\n\u793a\u4f8b\uff1a`pyxel.sound(0).set_tone(\"TTSS PPPN\")`\n\n- `set_volume(volume)`<br>\n\u7528'01234567'\u7ec4\u6210\u7684\u5b57\u7b26\u4e32\u8bbe\u7f6evolume\u3002\u4e0d\u533a\u5206\u5927\u5c0f\u5199\uff0c\u4e0d\u8ba1\u5165\u7a7a\u683c\u3002<br>\n\u793a\u4f8b\uff1a`pyxel.sound(0).set_volume(\"7777 7531\")`\n\n- `set_effect(effect)`<br>\n\u7528'NSVF'\u7ec4\u6210\u7684\u5b57\u7b26\u4e32\u8bbe\u7f6eeffect\u3002\u4e0d\u533a\u5206\u5927\u5c0f\u5199\uff0c\u4e0d\u8ba1\u5165\u7a7a\u683c\u3002<br>\n\u793a\u4f8b\uff1a`pyxel.sound(0).set_effect(\"NFNF NVVS\")`\n\n### Music\u7c7b\n\n- `ch0`<br>\n\u58f0\u90530\u4e2d\u64ad\u653e\u7684sound(0-63)\u5217\u8868\u3002\u82e5\u5217\u8868\u4e3a\u7a7a\uff0c\u5219\u6b64\u58f0\u9053\u672a\u88ab\u4f7f\u7528\u3002\n\n- `ch1`<br>\n\u58f0\u90531\u4e2d\u64ad\u653e\u7684sound(0-63)\u5217\u8868\u3002\u82e5\u5217\u8868\u4e3a\u7a7a\uff0c\u5219\u6b64\u58f0\u9053\u672a\u88ab\u4f7f\u7528\u3002\n\n- `ch2`<br>\n\u58f0\u90532\u4e2d\u64ad\u653e\u7684sound(0-63)\u5217\u8868\u3002\u82e5\u5217\u8868\u4e3a\u7a7a\uff0c\u5219\u6b64\u58f0\u9053\u672a\u88ab\u4f7f\u7528\u3002\n\n- `ch3`<br>\n\u58f0\u90533\u4e2d\u64ad\u653e\u7684sound(0-63)\u5217\u8868\u3002\u82e5\u5217\u8868\u4e3a\u7a7a\uff0c\u5219\u6b64\u58f0\u9053\u672a\u88ab\u4f7f\u7528\u3002\n\n- `set(ch0, ch1, ch2, ch3)`<br>\n\u8bbe\u7f6e\u6240\u6709\u58f0\u9053\u7684\u97f3\u9891sound(0-63)\u64ad\u653e\u5217\u8868\u3002\u82e5\u6307\u5b9a\u4e86\u7a7a\u5217\u8868\uff0c\u5219\u5bf9\u5e94\u58f0\u9053\u672a\u88ab\u4f7f\u7528\u3002<br>\n\u793a\u4f8b\uff1a`pyxel.music(0).set([0, 1], [2, 3], [4], [])`\n\n- `set_ch0(data)`<br>\n\u8bbe\u7f6e\u58f0\u90530\u7684\u97f3\u9891sound(0-63)\u64ad\u653e\u5217\u8868\u3002\n\n- `set_ch1(data)`<br>\n\u8bbe\u7f6e\u58f0\u90531\u7684\u97f3\u9891sound(0-63)\u64ad\u653e\u5217\u8868\u3002\n\n- `set_ch2(data)`<br>\n\u8bbe\u7f6e\u58f0\u90532\u7684\u97f3\u9891sound(0-63)\u64ad\u653e\u5217\u8868\u3002\n\n- `set_ch3(data)`<br>\n\u8bbe\u7f6e\u58f0\u90533\u7684\u97f3\u9891sound(0-63)\u64ad\u653e\u5217\u8868\u3002\n\n## \u5982\u4f55\u53c2\u4e0e\n\n### \u63d0\u4ea4\u95ee\u9898\n\u4f7f\u7528[issue tracker](https://github.com/kitao/pyxel/issues)\u6765\u63d0\u4ea4bug\u62a5\u544a\u6216\u529f\u80fd\u9700\u6c42\u3002\n\u63d0\u4ea4\u95ee\u9898\u4e4b\u524d\uff0c\u8bf7\u641c\u7d22issue tracker\u4ee5\u786e\u8ba4\u6ca1\u6709\u4eba\u63d0\u51fa\u8fc7\u7c7b\u4f3c\u7684\u95ee\u9898\u3002\n\n\u63d0\u4ea4\u62a5\u544a\u65f6\uff0c\u4ece[\u8fd9\u91cc](https://github.com/kitao/pyxel/issues/new/choose)\u9009\u53d6\u5408\u9002\u7684\u6a21\u677f\u3002\n\n### \u624b\u52a8\u6d4b\u8bd5\n\n\u6b22\u8fce\u5927\u5bb6\u624b\u52a8\u6d4b\u8bd5\u4ee3\u7801\u5e76\u63d0\u4ea4bug\uff0c\u6216\u8005\u63d0\u51fa\u6539\u8fdb\u610f\u89c1\uff01\n\n### \u63d0\u4ea4pull request\n\n\u53ef\u4ee5\u901a\u8fc7pull requests(PRs)\u5f62\u5f0f\u6765\u63d0\u4ea4\u8865\u4e01\u6216\u4fee\u590d\u3002\u8bf7\u786e\u8ba4\u4f60\u7684pull request\u5bf9\u5e94\u7684issue\u5730\u5740\u5728issue tracker\u4e2d\u4f9d\u7136\u662fopen\u72b6\u6001\u3002\n\n\u4e00\u65e6\u63d0\u4ea4pull request\uff0c\u5219\u9ed8\u8ba4\u540c\u610f\u5728[MIT license](LICENSE)\u7684\u8bb8\u53ef\u4e0b\u53d1\u5e03\u3002\n\n## \u5176\u4ed6\u4fe1\u606f\n\n- [Wiki](https://github.com/kitao/pyxel/wiki)\n- [Subreddit](https://www.reddit.com/r/pyxel/)\n- [Discord server (English)](https://discord.gg/FC7kUZJ)\n- [Discord server (Japanese - \u65e5\u672c\u8a9e\u7248)](https://discord.gg/qHA5BCS)\n\n## \u8bb8\u53ef\u8bc1\n\nPyxel\u5f00\u6e90\u5728[MIT license](http://en.wikipedia.org/wiki/MIT_License)\u4e0b\uff0c\u4f60\u53ef\u4ee5\u5c06pyxel\u7528\u5728\u4f60\u7684\u8f6f\u4ef6\u4e2d\uff0c\u4f46\u540c\u65f6\u6240\u8ff0\u8f6f\u4ef6\u7684\u6240\u6709\u7248\u672c\u90fd\u5fc5\u987b\u5305\u542bMIT License\u8bb8\u53ef\u6761\u6b3e\u53ca\u7248\u6743\u58f0\u660e\u3002\n\nPyxel\u4f7f\u7528\u4ee5\u4e0b\u8f6f\u4ef6\uff1a\n\n- [SDL2](https://www.libsdl.org/)\n- [miniz-cpp](https://github.com/tfussell/miniz-cpp)\n- [Gifsicle](https://www.lcdf.org/gifsicle/)\n"
 },
 {
  "repo": "cloudhead/rx",
  "language": "Rust",
  "readme_contents": "\n\n        \u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588\n         \u2588\u2588\u2588 \u2588\u2588  \u2588\u2588\u2588\n         \u2588\u2588     \u2588\u2588 \u2588\u2588\n         \u2588\u2588    \u2588\u2588   \u2588\u2588\n\n\n    `rx` is a modern and minimalist pixel editor.\n\n  Designed with great care and love\n    with pixel artists and animators in mind.\n\nOVERVIEW\n\n  `rx` is an extensible, modern and minimalist pixel editor implemented\n  in rust[0]. rx is free software, licensed under the GPLv3.\n\n  Most of the information on how to use rx is on the website:\n  https://rx.cloudhead.io. There, you can also find a user guide\n  (https://rx.cloudhead.io/guide.html), installation instructions,\n  and binary download links.\n\n  To build rx from source, see the build sections below.\n\n  Once inside rx, enter the `:help` command to get a list of key bindings\n  and commands.\n\n  [0]: https://rust-lang.org\n\nREQUIREMENTS\n\n  At a minimum, OpenGL 3.3 support is required.\n\nBUILD DEPENDENCIES\n\n  * rust (https://www.rust-lang.org/tools/install)\n  * cmake (https://cmake.org/download/)\n\n  On macOS, `Xcode` and the `Xcode Command Line Tools` are required.\n  The latter can be obtained by running `xcode-select --install`\n  CMake can be installed with `brew install cmake`.\n\nBUILD & INSTALLATION\n\n  Before proceeding, make sure the BUILD DEPENDENCIES have been installed.\n\n  Then, clone this repository and from its root, run:\n\n    $ cargo install --locked --path .\n\n  This will install rx under `~/.cargo/bin/rx`.  If you prefer a different\n  install location, you can specify it via the `--root <prefix>` flag, where\n  <prefix> is for example '/usr/local'.\n\nCONTRIBUTING\n\n  See the CONTRIBUTING file for details. Contributions are appreciated.\n\nUSAGE\n\n  See the guide at https://rx.cloudhead.io/guide.html.\n\nTROUBLESHOOTING\n\n  If something isn't working like it's supposed to, there are various things\n  you can do to debug the problem:\n\n  * Run `rx` with verbose logging, by using the `-v` command-line flag. This\n    will log errors from the underlying libraries that are usually suppressed,\n    and will show debug output that might indicate what the problem is.\n  * If `rx` is crashing, run it with `RUST_BACKTRACE=1` set in your environment\n    to show a backtrace on crash.\n  * It could be that the issue is related to your configuration - in that case\n    the program can be run without loading the initialization script like so:\n\n        rx -u -\n\nLICENSE\n\n  This software is licensed under the GPL. See the LICENSE file for more details.\n\nCOPYRIGHT\n\n  (c) 2019 Alexis Sellier\n"
 },
 {
  "repo": "godotengine/godot",
  "language": "C++",
  "readme_contents": "# Godot Engine\n\n<p align=\"center\">\n  <a href=\"https://godotengine.org\">\n    <img src=\"logo.svg\" width=\"400\" alt=\"Godot Engine logo\">\n  </a>\n</p>\n\n## 2D and 3D cross-platform game engine\n\n**[Godot Engine](https://godotengine.org) is a feature-packed, cross-platform\ngame engine to create 2D and 3D games from a unified interface.** It provides a\ncomprehensive set of common tools, so that users can focus on making games\nwithout having to reinvent the wheel. Games can be exported in one click to a\nnumber of platforms, including the major desktop platforms (Linux, macOS,\nWindows), mobile platforms (Android, iOS), as well as Web-based platforms\n(HTML5) and\n[consoles](https://docs.godotengine.org/en/latest/tutorials/platform/consoles.html).\n\n## Free, open source and community-driven\n\nGodot is completely free and open source under the very permissive MIT license.\nNo strings attached, no royalties, nothing. The users' games are theirs, down\nto the last line of engine code. Godot's development is fully independent and\ncommunity-driven, empowering users to help shape their engine to match their\nexpectations. It is supported by the [Software Freedom Conservancy](https://sfconservancy.org/)\nnot-for-profit.\n\nBefore being open sourced in February 2014, Godot had been developed by Juan\nLinietsky and Ariel Manzur (both still maintaining the project) for several\nyears as an in-house engine, used to publish several work-for-hire titles.\n\n![Screenshot of a 3D scene in Godot Engine](https://raw.githubusercontent.com/godotengine/godot-design/master/screenshots/editor_tps_demo_1920x1080.jpg)\n\n## Getting the engine\n\n### Binary downloads\n\nOfficial binaries for the Godot editor and the export templates can be found\n[on the homepage](https://godotengine.org/download).\n\n### Compiling from source\n\n[See the official docs](https://docs.godotengine.org/en/latest/development/compiling/)\nfor compilation instructions for every supported platform.\n\n## Community and contributing\n\nGodot is not only an engine but an ever-growing community of users and engine\ndevelopers. The main community channels are listed [on the homepage](https://godotengine.org/community).\n\nTo get in touch with the engine developers, the best way is to join the\n[#godotengine-devel IRC channel](https://webchat.freenode.net/?channels=godotengine-devel)\non Freenode.\n\nTo get started contributing to the project, see the [contributing guide](CONTRIBUTING.md).\n\n## Documentation and demos\n\nThe official documentation is hosted on [ReadTheDocs](https://docs.godotengine.org).\nIt is maintained by the Godot community in its own [GitHub repository](https://github.com/godotengine/godot-docs).\n\nThe [class reference](https://docs.godotengine.org/en/latest/classes/)\nis also accessible from the Godot editor.\n\nThe official demos are maintained in their own [GitHub repository](https://github.com/godotengine/godot-demo-projects)\nas well.\n\nThere are also a number of other\n[learning resources](https://docs.godotengine.org/en/latest/community/tutorials.html)\nprovided by the community, such as text and video tutorials, demos, etc.\nConsult the [community channels](https://godotengine.org/community)\nfor more information.\n\n[![Code Triagers Badge](https://www.codetriage.com/godotengine/godot/badges/users.svg)](https://www.codetriage.com/godotengine/godot)\n[![Translate on Weblate](https://hosted.weblate.org/widgets/godot-engine/-/godot/svg-badge.svg)](https://hosted.weblate.org/engage/godot-engine/?utm_source=widget)\n[![Total alerts on LGTM](https://img.shields.io/lgtm/alerts/g/godotengine/godot.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/godotengine/godot/alerts)\n[![TODOs](https://badgen.net/https/api.tickgit.com/badgen/github.com/godotengine/godot)](https://www.tickgit.com/browse?repo=github.com/godotengine/godot)\n"
 },
 {
  "repo": "turbulenz/turbulenz_engine",
  "language": "TypeScript",
  "readme_contents": "================\nTurbulenz Engine\n================\n\nTurbulenz is an HTML5 game engine and server-side APIs available in JavaScript and TypeScript for building and distributing 2D and 3D games that run on platforms that support HTML5 features such as modern browsers without the need for plugins.\n\n.. contents::\n    :local:\n\n\nExamples using the Turbulenz Engine\n===================================\n\nGames\n-----\n\n* `Polycraft <https://turbulenz.com/games/polycraft>`__ - 3D\n* `Save the day <https://turbulenz.com/games/save-the-day>`__ - 2D\n* `Denki Blocks! <https://turbulenz.com/games/denkiblocks>`__ - 2D\n* `Denki Word Quest <https://turbulenz.com/games/denkiwordquest>`__ - 2D\n* `Score Rush <https://turbulenz.com/games/scorerush>`__ - 2D\n* `Score Rush MP <https://turbulenz.com/games/scorerush-mp>`__ - multiplayer\n* `Space Ark <https://turbulenz.com/games/space-ark>`__ - 3D\n* `Apathy <http://apathy.plankhead.com/>`__ - `src <https://github.com/Zacqary/Apathy>`__ - 2D\n* Salty Dogs - `src <https://github.com/zacqary/salty-dogs>`__\n* `Gargantia Sky Courier <http://fly.gargantia.jp>`__ - `src <https://github.com/turbulenz/gargantia_editor>`__ - 3D\n\nApps\n----\n\n* Sample app - `src <apps/sampleapp/scripts/sampleappmain.js>`__\n* Touch, keyboard, mouse and pad controller input app - `src <apps/inputapp/tsscripts/inputapp>`__\n* Viewer - `src <apps/viewer/scripts/viewer.js>`__\n* Realtime multiplayer worm app - `src <apps/multiworm/tsscripts/worm>`__\n* Asynchronous turn based multiplayer tic-tac-toe app - `src <apps/tictactoe/tsscripts/tictactoe>`__\n\nPrototyping\n-----------\n\n* Prototyping basic app - `jsfiddle <http://jsfiddle.net/jamesaustin/WF47C/>`__\n* Prototyping sample app - `src <apps/protolibsampleapp/scripts/protolibsampleapp.js>`__\n* Prototyping template app - `src <apps/protolibtemplateapp/scripts/app.js>`__\n\nSamples\n-------\n\n* Setup:\n\n  * `Device Initialization <http://biz.turbulenz.com/sample_assets/device_initialization.canvas.release.html>`__ - `src <samples/tsscripts/templates/device_initialization.ts>`__\n  * `Basic Loop <http://biz.turbulenz.com/sample_assets/basic_loop.canvas.release.html>`__ - `src <samples/tsscripts/templates/basic_loop.ts>`__\n\n* 2D Rendering:\n\n  * `2D Canvas <http://biz.turbulenz.com/sample_assets/2dcanvas.canvas.release.html>`__ - `src <samples/tsscripts/templates/2dcanvas.ts>`__ - `jsfiddle <http://jsfiddle.net/jamesaustin/HRAw7/>`__\n  * `Draw2D <http://biz.turbulenz.com/sample_assets/draw2d.canvas.release.html>`__ - `src <samples/tsscripts/templates/draw2d.ts>`__\n  * `TextureEffects <http://biz.turbulenz.com/sample_assets/textureeffects.canvas.release.html>`__ - `src <samples/tsscripts/templates/textureeffects.ts>`__\n  * `Immediate Mode Rendering <http://biz.turbulenz.com/sample_assets/immediate_mode_rendering.canvas.release.html>`__ - `src <samples/tsscripts/templates/immediate_mode_rendering.ts>`__\n  * Loading - `src <samples/tsscripts/templates/loading.ts>`__\n  * SVG - `src <samples/tsscripts/templates/svg.ts>`__\n\n* 3D Rendering:\n\n  * `Camera <http://biz.turbulenz.com/sample_assets/camera.canvas.release.html>`__ - `src <samples/tsscripts/templates/camera.ts>`__\n  * `Material <http://biz.turbulenz.com/sample_assets/material.canvas.release.html>`__ - `src <samples/tsscripts/templates/material.ts>`__\n  * `Forward rendering <http://biz.turbulenz.com/sample_assets/forward_rendering.canvas.release.html>`__ - `src <samples/tsscripts/templates/forward_rendering.ts>`__\n  * `Morphing <http://biz.turbulenz.com/sample_assets/morphing.canvas.release.html>`__ - `src <samples/tsscripts/templates/morphing.ts>`__\n  * `Post effects <http://biz.turbulenz.com/sample_assets/postfx.canvas.release.html>`__ - `src <samples/tsscripts/templates/postfx.ts>`__\n  * `Particles <http://biz.turbulenz.com/sample_assets/particles.canvas.release.html>`__ - `src <samples/tsscripts/templates/particles.ts>`__\n  * Deferred Rendering - `src <samples/tsscripts/templates/deferred_rendering.ts>`__\n  * Load model - `src <samples/tsscripts/templates/load_model.ts>`__\n  * Scene loading - `src <samples/tsscripts/templates/scene_loading.ts>`__\n\n* Animation:\n\n  * `Animation <http://biz.turbulenz.com/sample_assets/animation.canvas.release.html>`__ - `src <samples/tsscripts/templates/animation.ts>`__\n  * `Multiple animations <http://biz.turbulenz.com/sample_assets/multiple_animations.canvas.release.html>`__ - `src <samples/tsscripts/templates/multiple_animations.ts>`__\n\n* 2D Physics:\n\n  * `2D Physics <http://biz.turbulenz.com/sample_assets/physics2d.canvas.release.html>`__ - `src <samples/tsscripts/templates/physics2d.ts>`__\n  * `2D Physics constraints <http://biz.turbulenz.com/sample_assets/physics2d_constraints.canvas.release.html>`__ - `src <samples/tsscripts/templates/physics2d_constraints.ts>`__\n  * `2D Physics callbacks <http://biz.turbulenz.com/sample_assets/physics2d_callbacks.canvas.release.html>`__ - `src <samples/tsscripts/templates/physics2d_callbacks.ts>`__\n\n* 3D Physics:\n\n  * `3D Physics <http://biz.turbulenz.com/sample_assets/physics.canvas.release.html>`__ - `src <samples/tsscripts/templates/physics.ts>`__\n  * `3D Physics <benchmark(http://biz.turbulenz.com/sample_assets/physics_benchmark.canvas.release.html>`__ - `src <samples/tsscripts/templates/physics_benchmark.ts>`__\n  * `3D Physics collision mesh <http://biz.turbulenz.com/sample_assets/physics_collisionmesh.canvas.release.html>`__ - `src <samples/tsscripts/templates/physics_collisionmesh.ts>`__\n  * 3D Physics constraints - `src <samples/tsscripts/templates/physics_constraints.ts>`__\n\n* Sound:\n\n  * `Sound <http://biz.turbulenz.com/sample_assets/sound.canvas.release.html>`__ - `src <samples/tsscripts/templates/sound.ts>`__\n\n* Video:\n\n  * `Video playback <http://biz.turbulenz.com/sample_assets/video.canvas.release.html>`__ - `src <samples/tsscripts/templates/video.ts>`__\n\n* Services:\n\n  * Leaderboards - `src <samples/tsscripts/templates/leaderboards.ts>`__\n  * Multichat - `src <samples/tsscripts/templates/multichat.ts>`__\n  * Payments - `src <samples/tsscripts/templates/payments.ts>`__\n  * Userdata - `src <samples/tsscripts/templates/userdata.ts>`__\n\nFeatures\n========\n\nLow-level API\n-------------\n\n**Graphics**\n\n- Simple shader-based immediate mode API:\n\n  - A Shader may contain multiple Techniques, either single or multi-pass.\n  - Once a shader Technique is set on the Device, the parameters required by the program code can be updated by a TechniqueParameter object:\n  - TechniqueParameter objects hold multiple references to Textures, TechniqueParameterBuffers or individual values.\n  - Multiple TechniqueParameters can be set on the Device at once.\n\n- Vertex buffers, Index buffers and Textures can be created, updated and destroyed dynamically.\n- Multiple Streams of Vertex buffers can be used at the same time.\n- Support for 1D, 2D, 3D and Cube textures: Any pixel format supported by the hardware.\n- Asynchronous resource loading: multiple resource files can be downloaded on the fly, JavaScript code will be notified when resource is available for usage.\n- Multiple image file formats: DDS, JPG, PNG and TGA.\n- Support for textures archives containing multiple image files: less flexibility than individual files but better for optimal bandwidth usage.\n- Occlusion queries:\n\n  - Number of pixels rendered can be queried for a section of rendering.\n  - Available in plugin mode only.\n\n- Fullscreen support (Supported platforms).\n- Take screenshot feature:\n- Video playback support:\n\n  - WebM, MP4.\n  - Render video as texture.\n  - Playback controls play, pause, stop, resume, rewind.\n\n**Math**\n\n- Math types:\n\n  - *Vector2* *Vector3*, *Vector4*\n  - *Matrix33*, *Matrix34*, *Matrix43*, *Matrix44*\n  - *Quaternion*, *QuatPos*\n  - *AABB*\n\n- Storage format optimized based on available support\n- Optimized operations support *destination parameters*, reducing object allocation.\n- Array to/from Math type conversion utilities.\n\n**Physics**\n\n**3D**\n\n- Easy-to-use efficient physics simulation.\n\n  - Optimized JavaScript implementation.\n  - In plugin mode, this is a lightweight wrapper around the Bullet Physics Library http://bulletphysics.org/wordpress/\n\n- Rigid bodies and collision objects: Plane, Box, Sphere, Capsule, Cylinder, Cone, Triangle Mesh, Convex Hull.\n- Constraints: Point to Point, Hinge, Cone Twist, 6DOF, Slider.\n- Ray and convex sweep queries: Returning closest point of impact and surface normal.\n- Character representation.\n\n  - For use with 1st/3rd person games.\n  - Includes properties for velocity, position, crouch, jump height, death, on ground.\n\n- Contact callbacks.\n\n  - Rigidbodies, characters, collision objects.\n  - Called on presolve, added, processed, removed.\n  - Filter responses by mask.\n  - Triggers with no collision response.\n\n**2D**\n\n- Efficient 2D physics simulation written specifically for JavaScript.\n- Shapes:\n\n  - Circle, Box, Rectangle, Regular Polygon, Custom Polygon.\n  - Create shapes as sensors.\n  - Shape grouping and mask interactions.\n\n- Collision detection:\n\n  - Sweep & Prune, Box Tree Broadphases.\n  - Utilities for Raytest, Signed Distance, Intersection, Contains Point, Sweep Test.\n\n- Simulation world:\n\n  - Multiple simulation groups.\n  - Optional gravity.\n  - Customisable simulation iterations.\n\n- Rigid body simulation: Dynamic, Static, Kinematic objects.\n- Materials: Elasticity, Static/Dynamic/Rolling Friction, Density.\n- Arbiters:\n\n  - Contact grouping.\n  - Contact information: Position, Penetration, Normal/Tangent Impulse\n\n- Constraints: Point to Point, Distance, Weld, Angle, Motor, Line, Pulley, Custom Constraint.\n- Debug rendering:\n\n  - Rigid Bodies, Constraints, Worlds, Lines, Curves, Rectangles, Circles, Spirals, Linear/Spiral Springs.\n  - Enabling and disabling of rendering types.\n  - Scaling for Draw2D viewport.\n\n**Sound**\n\n- Easy-to-use efficient wrapper of hardware audio features: Utilizes Web Audio, <Audio> tag, `OpenAL <http://connect.creativelabs.com/openal/default.aspx>`__ dependent on platform support.\n- 3D sound sources: Position, Direction, Velocity, Gain, Pitch, Loop.\n- Emulated 3D sound for stereo setups.\n- Asynchronous sound files loading: Multiple resource files can be downloaded on the fly, JavaScript code will be notified when resource is available for usage.\n- Uncompress audio dynamically.\n- Multiple sound file formats: OGG, WAV, MP3.\n- Supported query for platform capabilities: Load the best audio format for the platform.\n- Effect/Filter support: Reverb, Echo, Low Pass\n\n**Networking**\n\n- Bi-directional, full-duplex communications channels, over a TCP socket:\n\n  - Utilizes browser Websocket support.\n  - Efficient native implementation of WebSockets for platforms without support:\n\n    - http://en.wikipedia.org/wiki/WebSocket\n    - http://dev.w3.org/html5/websockets/\n\n- HTTP-compatible handshake so that HTTP servers can share their default HTTP and HTTPS ports (80 and 443) with a WebSocket server.\n- Support for secure connections as part of the standard.\n- Support for data compression with the extension `deflate-frame`.\n\n**Input**\n\n- Access to input types: Keyboard, Mouse, Xbox360 Pad, Joysticks, Wheels, Touch, Multi-touch\n- Asynchronous event system when state changes:\n\n  - JavaScript code is notified when input changes.\n  - Events for keydown, keyup, mousedown, mouseup, mousewheel, mousemove,\n    mouseover, mouseenter, mouseleave, paddown, padup, focus, blur, mouselocklost,\n    touchstart, touchend, touchmove, touchmove, touchenter, touchleave, touchcancel.\n\n- Additional mouse features: hiding/showing platform icon, locking/unlocking (supported platforms).\n- Language independent keymapping.\n\nHigh-level API\n--------------\n\n**Scene Graph**\n\n- Flexible JSON file format: Could describe either a whole scene or individual meshes.\n- Asynchronous loading of external references:\n\n  - If a scene contains references to external meshes they are all loaded in parallel and attached to the main scene when ready.\n  - Support for optimal reuse of same mesh on different locations.\n\n- Pluggable renderer system:\n\n  - Links between geometries, effects and materials are resolved at\n    runtime.\n  - Easy swap of multiple rendering techniques for same assets.\n\n- Geometry sharing: Geometry information can be optimally reused on multiple scene locations with different rendering effects.\n- Flexible scene hierarchy nodes: Lights, Geometries, Animation, Physics.\n- Visibility queries: Portals, Frustum, Overlapping Box.\n- Sorting and grouping: Visible nodes are sorted and grouped for optimal rendering: Opaque, Transparent, Decal.\n- Lazy evaluation of node updates.\n\n**Animation**\n\n- 3D animation for scene geometry.\n- Skeleton/Skinning animation.\n- Animation controllers:\n\n  - Interpolation, Overloaded Node, Reference, Transition, Blend, Mask, Pose, Skin, GPU Skin, Skinned Node.\n  - Controllers can be combined for desired effect.\n\n- Dynamically update scene data.\n\n**Resource Manager**\n\n- Asynchronous loading avoiding duplicates: Additional remapping layer for easy URL redirection.\n- Provide default resources if missing: Game can provide custom default resource to be used when a required one is missing or still loading.\n- Multiple managers for individual needs: Animations, Effects, Fonts, Shaders, Sounds, Textures.\n- Bandwidth and hardware scaling by selecting different assets and effects depending on machine and Internet connection performance.\n- Client-side asset cache for optimizing and reusing requests.\n\n**Server Requests**\n\n- HTTP & AJAX request functionality:\n\n  - Automatic retry and error handling.\n  - Cross-browser support.\n  - Encrypted API support.\n\n**Deferred Renderer**\n\n- Unlimited number of lights: Point, Spot, Directional, Ambient.\n- Texture based light falloff: Allows multi-colored lights and cheap fake shadows, for example the typical fan under a light source.\n- Materials with multiple texture maps: Specular color and intensity, Normal vector, Glow color, Alpha.\n- Pluggable post effects:\n\n  - Easy set-up for full screen post effects as part of the final deferred shading.\n  - Copy, Fade in, Modulate, Bicolor, Blend.\n\n- Exponential shadow maps:\n\n  - Reuse of texture shadow maps to save video memory.\n  - Gaussian blur for smooth results.\n  - Exponential depth information to avoid light bleeding.\n\n- Volumetric fog.\n- 4 weight GPU skinning.\n- UV animation.\n- Wireframe mode.\n- Callbacks for additional passes: decals, transparency, debug\n- Available in plugin mode only until draw buffers are added to WebGL http://www.khronos.org/registry/webgl/extensions/WEBGL_draw_buffers/\n\n**Forward Renderer**\n\n- Unlimited number of lights: Point, Spot, Directional, Ambient.\n- Texture based light falloff: Allows multi-colored lights and cheap fake shadows, for example the typical fan under a light source.\n- Materials with multiple texture maps: Specular color and intensity, Normal vector, Glow color, Alpha.\n- Pluggable post effects:\n\n  - Easy set-up for full screen post effects as part of the final\n    deferred shading.\n  - Copy, Fade in, Modulate, Bicolor, Blend.\n\n- Exponential shadow maps:\n\n  - Reuse of texture shadow maps to save video memory.\n  - Gaussian blur for smooth results.\n  - Exponential depth information to avoid light bleeding.\n\n- 4 weight GPU skinning.\n- UV animation.\n- Wireframe mode.\n- Callbacks for additional passes: decals, transparency, debug\n\n**Default Renderer**\n\n- Single point and ambient light.\n- Pixel-based lighting.\n- Materials with multiple texture maps: Specular color and intensity, Normal vector, Glow color, Alpha.\n- Optimzed for speed and compatibility on a wide range of hardware.\n- 4 weight GPU skinning.\n- UV animation.\n- Wireframe mode.\n- Callbacks for additional passes: decals, transparency, debug\n\n**Simple Renderer**\n\n- Single point and ambient light.\n- Vertex-based lighting.\n- Materials with multiple texture maps: Specular color and intensity, Normal vector, Glow color, Alpha.\n- Optimzed for speed and compatibility on a wide range of hardware.\n- 4 weight GPU skinning.\n- UV animation.\n- Wireframe mode.\n- Callbacks for additional passes: decals, transparency, debug\n\n**2D Rendering**\n\n**Draw2D**\n\n- 2D sprite-based renderer: Batches sprites for efficiency.\n- Draw modes:\n\n  - **Draw:** Draw object literal,\n  - **DrawRaw:** Draw buffer data,\n  - **DrawSprite:** Draw sprite reference.\n\n- Scalable viewport: Input coordinate mapping.\n- Sort modes: Immediate, Deferred, Texture.\n- Blend modes: Opaque, Additive, Alpha.\n- Custom shader support.\n- Render-to-target support.\n- Texture effects: Distort, Gaussian Blur, Bloom, Color, Grey Scale, Sepia, Negative, Saturation, Hue, Brightness, Contrast.\n- Recording performance data.\n\n**Canvas2D**\n\n- Accelerated implementation of `canvas 2D API <http://www.w3.org/html/wg/drafts/2dcontext/html5_canvas/>`__.\n- Runs on WebGL/OpenGL depending on platform.\n- SVG rendering.\n- Text rendering via FontManager.\n- For complete implementation see `canvas element specification <http://www.whatwg.org/specs/web-apps/current-work/multipage/the-canvas-element.html#the-canvas-element>`__\n\n**Utilities**\n\n- Allocation and management of graphics buffers: Vertex buffers, Index buffers.\n- API controlled JavaScript profiling:\n\n  - Per-function millisecond accuracy timing.\n  - Record top-down or bottom-up function trees.\n  - Calculate the time spent by an individual function or\n    the total spent by sub-functions.\n  - Identify the source file and line number of problematic areas.\n\n- Memory usage identification:\n\n  - Retrieve the object count of constructed object types.\n  - Take snapshots and compare memory fluctuations.\n\n- Encryption and decryption of server-side requests for TZO formats.\n- Debug utility with function stripping for performance:\n\n  - assert, log, abort.\n  - Complete stacktrace.\n  - Supports adding custom functions.\n\n- Network Simulator:\n\n  - Simulates latency and network behaviour.\n  - Client-side manipulation of multiplayer session messages.\n  - Simulates spikes in network traffic.\n\nTurbulenz Service API\n---------------------\n\n**Leaderboards**\n\n- Submitting/retrieving ranked friend/global leaderboards.\n- Sort by either higher or lower scores.\n- Infinitely scrollable scoreboards.\n- Friend's score notifications.\n- Aggregate scores.\n- Default score entries.\n\n**Badges**\n\n- Achievement system for awarding game progress.\n- Custom badge shape and design\n- Progression badges.\n- Achievement notification.\n\n**Payments**\n\n- Payments API: In game, On website, App stores.\n- Payment methods: Single purchase, Micro transactions.\n- Purchasable items: Ownable, Consumeable.\n\n**Userdata**\n\n- Per-user save game information.\n- Key-value pair data storage: Settings, Preferences, Personal items.\n\n**Userprofile**\n\n- Game player's profile information: Username, Display name, Language, Age, Country, Guest user.\n\n**Gameprofile**\n\n- Game status of a player:\n\n  - Viewable by other players a game.\n  - Custom field information decided by game.\n\n**Multiplayer**\n\n- Real-time session match-making between friends and public users.\n- Session creation/joining.\n- Multiplayer session invite and notification.\n\n**Datashares**\n\n- Shared key-value store for turn based games and user generated content.\n- Read only and read and write access.\n- Find other users public shares or filter by username.\n\n**Notifications**\n\n- Send delayed notifications to the current user.\n- Send instant notifications to other users.\n- Notification types: Email, website pop-up and in-game.\n\n**Metrics**\n\n- Custom event submission:\n\n  - Can be used to gather progress during game.\n  - Exportable from developer services.\n  - Events identifiable by custom key.\n  - Allows additional numerical data.\n\n**Bridge**\n\n- Bi-directional communication channel between game and webpage.\n- Allows messages to be exchanged.\n- Live updating: Badge progress, notifications, loading/saving status.\n\n**Utilities**\n\n- Mapping between game resources references and content distribution network.\n- Uniquely identifiable gamesession.\n- Service availability notification.\n\nWhat Are the Design Goals of the Turbulenz Engine\n=================================================\n\nThe main design goals of the Turbulenz Engine are performance, modularity and customizability. Users of the engine should be able to build any kind of game without limitations, in an efficient manner and with an end product that performs optimally when loading and during play.\n\nTo achieve this target the Turbulenz team followed these rules when writing code:\n\n**Modularity**\n\n- Users should be able to pick what they want and replace what they don't.\n- When possible new functionality should be orthogonal to existing one.\n\n**High performance**\n\n- Strict coding standards to keep code efficient.\n- Keep memory allocations to minimum, reuse existing objects or arrays whenever possible, use scratch pads, combine multiple separate objects into a single one.\n- Use most efficient storage for each data, Typed Arrays when possible.\n- Reduce function calls when possible: write functions that handle arrays of objects instead of loops that make a function call per element, games rarely do a single thing to a single object.\n- Be aware of performance differences between browsers.\n- Profile often.\n\n**Asynchronous loading**\n\n- No API should block waiting for a response from the server, avoid polling whenever possible, use callbacks or Promises, to notify of data availability.\n\n**Data driven**\n\n- The target should be to make the game a simple dumb player of data, all functionality defined by simple data files.\n\n**Simple well documented file formats**\n\n- Define simple, easy to create asset formats that can trivially be connected to any tool chain.\n\n**Scalability**\n\n- Design interfaces that can be implemented with different level of detail or quality settings in order to scale from mobile to desktops.\n\n**Power without control is nothing**\n\n- Make sure users can do exactly what they want, with a helper layer put on top if required, document performance implications at every level.\n\n**Fault tolerant**\n\n- The engine should keep going even if any type of asset fails to load. The application is able to provide sensible defaults for all asset types making it easier to stay productive and diagnose issues.\n\n**Fast loading**\n\n- Reduce amount of data to be downloaded, compress data efficiently.\n- Use the browser cache efficiently, use unique file names based on content and tell the browser to cache forever.\n\n**Maintainability**\n\n- Strict coding standards to keep code readable, easy to maintain and debug.\n- Write unit tests, samples and documentation for every new code path.\n\n**Targeted**\n\n- This is a game engine, for games.\n\n\nHistory\n=======\n\nThe Engine was created and is maintained by `Turbulenz Limited <http://biz.turbulenz.com>`__ and was open sourced\nin April 2013.\n\nThe latest release is 1.3.2 which is tagged in the repository or a tarball/zip can be can be downloaded from\n`here <https://github.com/turbulenz/turbulenz_engine/archive/release_1.3.2.tar.gz>`__\n\nA full history of changes can be found in the `Changelog <docs/source/changelog.rst>`__\n\n\nPre-Requisites\n==============\n\nThe pre-requisites for the open source version of the Turbulenz Engine allowing you to use the various\ncommands are\n\n- Python 2.7.x.\n\n  - For Windows we recommend a 32bit install of Python.\n  - If you have multiple Python versions installed e.g. 3.x you may need to run commands with ``python2.7``\n  - On Windows if you didn't add Python to your path in the installer you may need to run ``C:\\Python27\\python.exe``\n  You can check your version with\n  ::\n\n        $ python --version\n        Python 2.7.3\n\n- VirtualEnv - version 1.9.1 or higher recommended\n  You can check your version with\n  ::\n\n        $ virtualenv --version\n        1.9.1\n\n- UglifyJS, turbulenz_build, DefinitelyTyped and NvTriStrip which are included via Git submodules contained\n  within the Turbulenz Engine repository.\n\n- Additional Python packages which will be automatically installed during the initial environment creation\n  using a Python package manager.\n\nPre-requisites for building the tools cgfx2json and NvTriStrip via ``python manage.py tools``\n\n- Compiler Toolchain\n\n  - Windows : Any one of\n\n    - Microsoft Visual Studio 2008 with SP1\n    - Microsoft Visual Studio 2010\n    - Visual C++ 2010 Express\n    - Microsoft Visual Studio 2012 with update 2\n    - Microsoft Visual Studio Express 2012 for Windows Desktop with update 2\n\n  - Mac OSX : Xcode with the command line tools\n\n  - Linux : GCC 4.6.x or higher\n\n- `NVIDIA CgToolkit <https://developer.nvidia.com/cg-toolkit>`__ version 3.1 or higher. The repository\n  includes the binaries for Windows, if you're developing on Mac OSX or Linux please download and install it.\n\n- OpenGL development libraries, these are included on Windows and Mac OSX with the compiler toolchains. For\n  debian based linux distributions the libgl1-mesa-dev package will provide the required files (e.g. ``sudo\n  apt-get install libgl1-mesa-dev``), for other linux distributions find the package supplying GL/gl.h and libGL.so\n\n\nSetup Guide\n===========\n\nThere are two ways to get up and running with the Turbulenz Engine, you can downloaded a packaged fully QA'd\nsnapshot release from the `Turbulenz Hub <https://hub.turbulenz.com>`__. These installers are available for\nWindows, Mac OSX and Linux and will install all the required packages and dependencies to get started,\na full guide can be found at `<http://docs.turbulenz.com/installing.html>`__\n\n*Note: SDK versions prior to 0.26.0 were released under a non open source license.*\n\nIf you want to run with the latest version or would like to contribute to the open source project the steps for\ngetting setup are included below. Use of the open source repository is tested against Windows, Mac OSX and Linux\nbut may also work on other unix-like operating systems.\n\nSetup\n-----\n\n1. Clone the repository `<http://github.com/turbulenz/turbulenz_engine>`__ (or if you wish you can fork the repository\n   on GitHub and clone that). To clone the repository maintained by Turbulenz use\n   ::\n\n        $ git clone git://github.com/turbulenz/turbulenz_engine.git\n\n2. The Turbulenz Engine submodules the following technology in the external folder\n\n   + tzbuild: https://github.com/turbulenz/turbulenz_build\n   + DefinitelyTyped: https://github.com/borisyankov/DefinitelyTyped\n   + UglifyJS: https://github.com/mishoo/UglifyJS.git\n\n   Initialize the Git submodules with\n   ::\n\n        $ git submodule update --init\n\n3. Check you have the `pre-requisites`_ installed\n\n4. From the cloned repository create a VirtualEnv environment to install the required Python packages and NodeJS,\n   allowing you to use all the features of the Turbulenz Engine. Note if Python is not on your shell's path you\n   will need to specify the full path for this first command.\n   ::\n\n        $ python manage.py env\n\n5. Activate the environment in your shell.\n   ::\n\n        $ source env/bin/activate - for bash and similar shells\n        > env\\scripts\\activate.bat - for Windows\n\n6. If you want to move onto the API tutorial section next then your final command is to build the JavaScript sources\n   from the TypeScript sources. The next section will detail some of the additional actions you can perform or you\n   can move onto `Getting Started With The API`_\n   ::\n\n        $ python manage.py jslib\n\nWorking With The Open Source Project\n------------------------------------\n\nThe manage.py script at the top level of the repository provides a set of commands for managing the Engine, the\nscript should be run as ``python manage.py command`` on Windows but can usually be shortcut to ``./manage.py command``\non unix shells. Running the script with ``--help`` will give a list of commands available, most of these are\ndescribed below. All the commands other than the env command expect to have the VirtualEnv environment activated\nas described in the setup section.\n\n- **JavaScript Sources** - The Turbulenz Engine source is written in TypeScript. To generate the JavaScript version\n  of the engine source run the command\n  ::\n\n    $ python manage.py jslib\n\n- **Tools** - The Turbulenz Engine includes a number of Python tools which are installed during the env command.\n  In addition the Engine includes a CGFX shader conversion tool which can be built with the following command.\n  See the `pre-requisites`_ section for details of required compiler toolchains.\n  ::\n\n    $ python manage.py tools\n\n- **Documentation** - The Turbulenz Engine documentation is based on restructured text sources. To build the html\n  documentation run the command\n  ::\n\n    $ python manage.py docs\n\n- **Samples** - Various samples are included with the Turbulenz Engine. These can be built from their TypeScript\n  sources with the command below. This generates a set of html files, JavaScript and asset JSON files which can\n  be served with a web server such as the Turbulenz Local Development Server.\n  ::\n\n    $ python manage.py samples\n\n- **Applications** - The Turbulenz Engine project includes a few larger applications and some templates for building\n  your own application. These can be found in the apps folder, and can be built with the command\n  ::\n\n    $ python manage.py apps\n\n  You can also build individual apps by specifying their name e.g.\n  ::\n\n    $ python manage.py apps multiworm\n\n- **Command Line Tools** - Various command line tools for processing code and assets are installed as part of the\n  virtual environment. These are available at the command line e.g. running ``dae2json`` will execute the dae2json\n  tool used to convert Collada assets to a Turbulenz Engine JSON asset format. See the\n  `tools <http://docs.turbulenz.com/tools/index.html>`__ section in the documentation for more details on the tools.\n\n- **Local Development Server** - Setting up the environment also includes a locally hosted web server which can be\n  used for development of HTML5 games and applications. See the\n  `Local Server <https://github.com/turbulenz/turbulenz_local>`__ repository for more details.\n\n\nGetting Started With The API\n============================\n\nTo try the Turbulenz APIs requires only a text editor and a browser such as Google Chrome or Mozilla Firefox.\nCreate an HTML file with the following content and place it in the root of the Turbulenz directory::\n\n    <html>\n    <head>\n        <title>Turbulenz - API - Clear Screen Example</title>\n        <script src=\"jslib/debug.js\"></script>\n        <script src=\"jslib/webgl/turbulenzengine.js\"></script>\n        <script src=\"jslib/webgl/graphicsdevice.js\"></script>\n    </head>\n    <body>\n        <canvas id=\"canvas\" width=\"640px\" height=\"480px\"/>\n        <script>\n            TurbulenzEngine = WebGLTurbulenzEngine.create({\n                canvas: document.getElementById(\"canvas\")\n            });\n            var graphicsDevice = TurbulenzEngine.createGraphicsDevice({});\n\n            var bgColor = [1.0, 1.0, 0.0, 1.0];\n\n            function update() {\n                if (graphicsDevice.beginFrame()) {\n                    graphicsDevice.clear(bgColor, 1.0);\n                    graphicsDevice.endFrame();\n                }\n            }\n\n            TurbulenzEngine.setInterval(update, 1000 / 60);\n        </script>\n    </body>\n    </html>\n\nAfter defining a <canvas> element of 640x480 pixels, this code will create the TurbulenzEngine and request the GraphicDevice module.\nUsing an update function called at a frequency of 60fps, the GraphicsDevice will clear the screen yellow.\nTo run the example, open the HTML file in your browser.\nYou should see a yellow rectangle.\n\nTo use assets such as images you will need to host a HTML file and assets on a webserver.\nAny webserver will work, a quick way to try is to activate the Turbulenz environment in the root of the Turbulenz directory and run::\n\n    python -m SimpleHTTPServer\n\nThis command will host the contents of the Turbulenz directory on your machine as a webserver.\n\nTo demonstrate loading an asset you can try loading an image file and drawing it as a textured sprite using the Draw2D API.\nCreate another HTML file with the following content and also place it in the root of the Turbulenz directory::\n\n    <html>\n    <head>\n        <title>Turbulenz - API - Textured Sprite Example</title>\n        <script src=\"jslib/debug.js\"></script>\n        <script src=\"jslib/webgl/turbulenzengine.js\"></script>\n        <script src=\"jslib/webgl/graphicsdevice.js\"></script>\n        <script src=\"jslib/draw2d.js\"></script>\n    </head>\n    <body>\n        <canvas id=\"canvas\" width=\"640px\" height=\"480px\"/>\n        <script>\n            TurbulenzEngine = WebGLTurbulenzEngine.create({\n                canvas: document.getElementById(\"canvas\")\n            });\n            var graphicsDevice = TurbulenzEngine.createGraphicsDevice({});\n            var draw2D = Draw2D.create({\n                graphicsDevice: graphicsDevice\n            });\n\n            var bgColor = [1.0, 1.0, 0.0, 1.0];\n\n            var sprite = Draw2DSprite.create({\n                width: 100,\n                height: 100,\n                x: graphicsDevice.width / 2,\n                y: graphicsDevice.height / 2,\n                color: [1.0, 1.0, 1.0, 1.0],\n                rotation: Math.PI / 4\n            });\n\n            var texture = graphicsDevice.createTexture({\n                src: \"assets/textures/crate.jpg\",\n                mipmaps: true,\n                onload: function (texture)\n                {\n                    if (texture)\n                    {\n                        sprite.setTexture(texture);\n                        sprite.setTextureRectangle([0, 0, texture.width, texture.height]);\n                    }\n                }\n            });\n\n            var PI2 = Math.PI * 2;\n            var rotateAngle = PI2 / 360; // 1 deg per frame\n\n            function update() {\n\n                sprite.rotation += rotateAngle;\n                sprite.rotation %= PI2; // Wrap rotation at PI * 2\n\n                if (graphicsDevice.beginFrame()) {\n                    graphicsDevice.clear(bgColor, 1.0);\n\n                    draw2D.begin();\n                    draw2D.drawSprite(sprite);\n                    draw2D.end();\n\n                    graphicsDevice.endFrame();\n                }\n            }\n\n            TurbulenzEngine.setInterval(update, 1000 / 60);\n        </script>\n    </body>\n    </html>\n\nThis time, instead of opening the file in the browser, navigate your browser to *http://127.0.0.1:8000* or *http://localhost:8000* and select the HTML file you created.\nYou should see a spinning textured box in the middle of a yellow rectangle.\n\nThe next step is render a simple textured mesh in 3D.\nTo do this you will need to build some assets from their source files.\nMake sure you have run the *tools* command to build the tools for your platform::\n\n    $ python manage.py tools\n\n*Note: The requirements for building the tools is different per platform. See the* `Pre-Requisites`_ *section.*\n\nFor this example you should use the `Protolib <http://docs.turbulenz.com/protolib/protolib_api.html>`__ library, which is ideal for prototyping games using Turbulenz.\nYou will need these assets::\n\n    - models/duck.dae\n    - textures/duck.png\n    - textures/default_light.png\n    - shaders/shadowmapping.cgfx\n    - shaders/zonly.cgfx\n    - shaders/forwardrendering.cgfx\n    - shaders/forwardrenderingshadows.cgfx\n    - shaders/debug.cgfx\n    - shaders/font.cgfx\n    - shaders/simplesprite.cgfx\n    - fonts/opensans-8.fnt\n    - fonts/opensans-16.fnt\n    - fonts/opensans-32.fnt\n    - fonts/opensans-64.fnt\n    - fonts/opensans-128.fnt\n    - textures/opensans-8_0.png\n    - textures/opensans-16_0.png\n    - textures/opensans-32_0.png\n    - textures/opensans-64_0.png\n    - textures/opensans-128_0.png\n\nCopy this text into a file called \"deps.yaml\" and place it in the root of the Turbulenz directory.\nHaving built the tools you can now run this command with the Turbulenz environment activated::\n\n    $ python scripts/buildassets.py --root . --assets-path assets\n\nThis will build the assets listed in the deps.yaml and output a \"staticmax\" directory and \"mapping_table.json\" file containing the processed assets and a mapping to them for the webserver.\nWhen a library tries to request one of these files, it will be able to find it in the staticmax directory.\nNow you can create the mesh example HTML file and place it at the root of the Turbulenz directory::\n\n    <html>\n    <head>\n        <title>Turbulenz - API - Textured Mesh Example</title>\n        <script>\n            var TurbulenzEngine = {};\n        </script>\n        <script src=\"jslib/debug.js\"></script>\n        <script src=\"jslib/vmath.js\"></script>\n        <script src=\"jslib/webgl/turbulenzengine.js\"></script>\n        <script src=\"jslib/webgl/graphicsdevice.js\"></script>\n        <script src=\"jslib/webgl/inputdevice.js\"></script>\n        <script src=\"jslib/webgl/sounddevice.js\"></script>\n        <script src=\"jslib/webgl/mathdevice.js\"></script>\n\n        <script src=\"jslib/aabbtree.js\"></script>\n        <script src=\"jslib/assettracker.js\"></script>\n        <script src=\"jslib/camera.js\"></script>\n        <script src=\"jslib/draw2d.js\"></script>\n        <script src=\"jslib/effectmanager.js\"></script>\n        <script src=\"jslib/fontmanager.js\"></script>\n        <script src=\"jslib/forwardrendering.js\"></script>\n        <script src=\"jslib/geometry.js\"></script>\n        <script src=\"jslib/indexbuffermanager.js\"></script>\n        <script src=\"jslib/light.js\"></script>\n        <script src=\"jslib/loadingscreen.js\"></script>\n        <script src=\"jslib/material.js\"></script>\n        <script src=\"jslib/observer.js\"></script>\n        <script src=\"jslib/renderingcommon.js\"></script>\n        <script src=\"jslib/requesthandler.js\"></script>\n        <script src=\"jslib/resourceloader.js\"></script>\n        <script src=\"jslib/scene.js\"></script>\n        <script src=\"jslib/scenenode.js\"></script>\n        <script src=\"jslib/shadermanager.js\"></script>\n        <script src=\"jslib/shadowmapping.js\"></script>\n        <script src=\"jslib/soundmanager.js\"></script>\n        <script src=\"jslib/texturemanager.js\"></script>\n        <script src=\"jslib/utilities.js\"></script>\n        <script src=\"jslib/vertexbuffermanager.js\"></script>\n\n        <script src=\"jslib/services/turbulenzbridge.js\"></script>\n        <script src=\"jslib/services/turbulenzservices.js\"></script>\n        <script src=\"jslib/services/gamesession.js\"></script>\n        <script src=\"jslib/services/mappingtable.js\"></script>\n\n        <script src=\"protolib/duimanager.js\"></script>\n        <script src=\"protolib/jqueryextend.js\"></script>\n        <script src=\"protolib/simplesprite.js\"></script>\n        <script src=\"protolib/simplefonts.js\"></script>\n        <script src=\"protolib/simplesceneloader.js\"></script>\n        <script src=\"protolib/debugdraw.js\"></script>\n        <script src=\"protolib/sceneloader.js\"></script>\n        <script src=\"protolib/soundsourcemanager.js\"></script>\n        <script src=\"protolib/protolib.js\"></script>\n\n    </head>\n    <body>\n        <canvas id=\"canvas\" width=\"640px\" height=\"480px\"/>\n        <script>\n            TurbulenzEngine = WebGLTurbulenzEngine.create({\n                canvas: document.getElementById(\"canvas\")\n            });\n            var mathDevice = null;\n\n            var mesh = null;\n            var rotationMatrix = null;\n            var rotationAngleMatrix = null;\n\n            var protolib = Protolib.create({\n                onInitialized: function onIntializedFn(protolib)\n                {\n                    mathDevice = protolib.getMathDevice();\n                    protolib.setCameraPosition(mathDevice.v3Build(0, 1, -2));\n                    protolib.setCameraDirection(mathDevice.v3Build(0, 0, 1));\n                    protolib.setAmbientLightColor(mathDevice.v3Build(1, 1, 1));\n                    protolib.addPointLight({\n                        v3Position: mathDevice.v3Build(-1, 1, -1),\n                        v3Color: mathDevice.v3Build(1, 1, 1),\n                        radius: 10\n                    });\n                    mesh = protolib.loadMesh({\n                        mesh: \"models/duck.dae\"\n                    });\n                    rotationMatrix = mathDevice.m43BuildIdentity();\n                    rotationAngleMatrix = mathDevice.m43BuildIdentity();\n                    mathDevice.m43SetAxisRotation(rotationAngleMatrix,\n                                                  mathDevice.v3Build(0, 1, 0),\n                                                  (Math.PI * 2) / 360);\n                }\n            })\n\n            function update() {\n\n                if (protolib.beginFrame())\n                {\n                    if (mesh)\n                    {\n                        mesh.getRotationMatrix(rotationMatrix);\n                        mathDevice.m43Mul(rotationMatrix, rotationAngleMatrix, rotationMatrix);\n                        mesh.setRotationMatrix(rotationMatrix);\n                    }\n                    protolib.endFrame();\n                }\n            }\n\n            TurbulenzEngine.setInterval(update, 1000 / 60);\n        </script>\n    </body>\n    </html>\n\nThis file is quite similar to the previous examples, but it requires a few more Turbulenz libraries to run.\nThis time you should see a spinning duck with a yellow texture on a white background and lit by a static point light.\n\nFor more information on how to build your own assets see the `assets section <http://docs.turbulenz.com/starter/getting_started_guide.html#assets>`__ in the getting started guide.\n\nIf you would like to learn more or work through this example step-by-step (with troubleshooting hints), see the `Getting Started Guide <http://docs.turbulenz.com/starter/getting_started_guide.html>`__ in the documentation.\n\nFor more information on the various APIs, see the following links:\n\n* `Low-level API <http://docs.turbulenz.com/jslibrary_api/low_level_api.html>`__, `2D Physics API <http://docs.turbulenz.com/jslibrary_api/physics2d_api.html>`__, `3D Physics API <http://docs.turbulenz.com/jslibrary_api/physics3d_api.html>`__\n* `High-level API <http://docs.turbulenz.com/jslibrary_api/high_level_api.html>`__\n* `Turbulenz Services API <http://docs.turbulenz.com/turbulenz_services/index.html>`__\n* `Protolib API <http://docs.turbulenz.com/protolib/protolib_api.html>`__\n\n\nDocumentation\n=============\n\nFull documentation for the Turbulenz Engine can be found at `<http://docs.turbulenz.com/index.html>`__\n\nThis documentation is built from the source restructured text in the docs/source folder of the repository, the latest\nversion online is maintained from the latest release tag in the repository. If you wish to build up to date\ndocumentation follow the setup guide and the run the ``manage.py docs`` command, this will generate html docs in the\nbuild/docs/html folder.\n\n\nKnown Issues\n============\n\nThe following known issues exist with using the open source repository version of the Turbulenz Engine,\nadditional known issues also existing in the SDK releases of the engine can be found here\n`<http://docs.turbulenz.com/known_issues/index.html>`__\n\n* The application and JavaScript build process will currently fail if the repository is cloned to a path with\n  spaces in it.\n\n\nLicensing\n=========\n\nThe Turbulenz Engine is licensed under the `MIT license <LICENSE>`__\n\n\nContributing\n============\n\nOur contributors are listed `here <docs/source/contributors.rst>`__\n\nContributions are always encouraged whether they are small documentation tweaks, bug fixes or suggestions for larger\nchanges. You can check the `issues <http://github.com/turbulenz/turbulenz_engine/issues>`__ or `discussion forums\n<https://groups.google.com/group/turbulenz-engine-users>`_ first to see if anybody else is undertaking similar changes.\n\nIf you'd like to contribute any changes simply fork the project on Github and send us a pull request or send a Git\npatch to the discussion forums detailing the proposed changes. If accepted we'll add you to the list of contributors.\n\nWe include a .pylintrc file in the repository which allows you to check your code conforms to our standards. Our\ndocumentation is built from restructured text sources in the docs folder so please consider how your changes may affect\nthe documentation.\n\nNote: by contributing code to the Turbulenz Engine project in any form, including sending a pull request via Github,\na code fragment or patch via private email or public discussion groups, you agree to release your code under the\nterms of the MIT license that you can find in the `LICENSE <LICENSE>`__ file included in the source distribution.\n\n\nLinks\n=====\n\n| Turbulenz game site - `turbulenz.com <https://turbulenz.com>`__\n| Turbulenz developer service and SDK download - `hub.turbulenz.com <https://hub.turbulenz.com>`__\n| Documentation for this module and the SDK - `docs.turbulenz.com <http://docs.turbulenz.com>`__\n| About Turbulenz - `biz.turbulenz.com <http://biz.turbulenz.com>`__\n"
 },
 {
  "repo": "Gamua/Starling-Framework",
  "language": "ActionScript",
  "readme_contents": "# Starling Framework [![Build Status](https://travis-ci.org/Gamua/Starling-Framework.svg)](https://travis-ci.org/Gamua/Starling-Framework)\n\nThe Cross Platform Game Engine\n------------------------------\n\nThe Starling Framework allows you to create hardware accelerated apps in ActionScript 3. The main target is the creation of 2D games, but Starling may be used for any graphical application. Thanks to Adobe AIR, Starling-based applications can be deployed to mobile devices (iOS, Android), the desktop (Windows, OS X), and to the browser (via the Flash plugin).\n\nWhile Starling mimics the classic display tree architecture of Adobe AIR/Flash, it provides a much better performance: all objects are rendered directly by the GPU (using the Stage3D API). The complete architecture was designed for working well with the GPU; common game development tasks were built right into its core. Starling hides Stage3D internals from developers, but makes it easy to access them for those who need full performance and flexibility.\n\nStarling aims to be as lightweight and easy to use as possible. As an Open Source project, much care was taken to make the source code easy to read, understand and extend. With under 20k lines of code, experienced developers can easily grasp it in its entirety, or modify it to their needs.\n\nQuick Links\n-----------\n\n* [Official Homepage](http://www.starling-framework.org)\n* [Instruction Manual](http://manual.starling-framework.org)\n* [API Reference](http://doc.starling-framework.org)\n* [Support Forum](http://forum.starling-framework.org)\n* [Starling Wiki](http://wiki.starling-framework.org)\n  * [Showcase](http://wiki.starling-framework.org/games/start)\n  * [Extensions](http://wiki.starling-framework.org/extensions/start)\n\nNews and Updates\n----------------\n\n* [Twitter](https://twitter.com/gamua)\n* [Facebook](https://facebook.com/gamua.co)\n* [LinkedIn](https://www.linkedin.com/company/gamua)\n\nMinimum Requirements\n--------------------\n\n* Adobe AIR or Flash Player 19.0\n"
 },
 {
  "repo": "jMonkeyEngine/jmonkeyengine",
  "language": "Java",
  "readme_contents": "jMonkeyEngine\n=============\n\n[![Build Status](https://github.com/jMonkeyEngine/jmonkeyengine/workflows/Build%20jMonkeyEngine/badge.svg)](https://github.com/jMonkeyEngine/jmonkeyengine/actions)\n\njMonkeyEngine is a 3-D game engine for adventurous Java developers. It\u2019s open-source, cross-platform, and cutting-edge. 3.2.4 is the latest stable version of the jMonkeyEngine 3 SDK, a complete game development suite. We'll release 3.2.x updates until the major 3.3 release arrives.\n\nThe engine is used by several commercial game studios and computer-science courses. Here's a taste:\n\n![jME3 Games Mashup](https://i.imgur.com/nF8WOW6.jpg)\n\n - [jME powered games on IndieDB](http://www.indiedb.com/engines/jmonkeyengine/games)\n - [Maker's Tale](http://steamcommunity.com/sharedfiles/filedetails/?id=93461954t)\n - [Boardtastic 2](https://boardtastic-2.fileplanet.com/apk)\n - [Attack of the Gelatinous Blob](https://attack-gelatinous-blob.softwareandgames.com/)\n - [Mythruna](http://mythruna.com/)\n - [PirateHell (on Steam)](https://store.steampowered.com/app/321080/Pirate_Hell/)\n - [3089 (on Steam)](http://store.steampowered.com/app/263360/)\n - [3079 (on Steam)](http://store.steampowered.com/app/259620/)\n - [Lightspeed Frontier (on Steam)](https://store.steampowered.com/app/548650/Lightspeed_Frontier/)\n - [Skullstone](http://www.skullstonegame.com/)\n - [Spoxel (on Steam)](https://store.steampowered.com/app/746880/Spoxel/)\n - [Nine Circles of Hell (on Steam)](https://store.steampowered.com/app/1200600/Nine_Circles_of_Hell/)\n - [Leap](https://gamejolt.com/games/leap/313308)\n - [Jumping Jack Flag](http://timealias.bplaced.net/jack/)\n\n## Getting started\n\nGo to https://github.com/jMonkeyEngine/sdk/releases to download the jMonkeyEngine SDK.\n[Read the wiki](https://jmonkeyengine.github.io/wiki) for a complete install guide. Power up with some SDK Plugins and AssetPacks and you are off to the races. At this point you're gonna want to [join the forum](http://hub.jmonkeyengine.org/) so our tribe can grow stronger.\n\nNote: The master branch on GitHub is a development version of the engine and is NOT MEANT TO BE USED IN PRODUCTION, it will break constantly during development of the stable jME versions!\n\n### Technology Stack\n\n - Java\n - NetBeans Platform\n - Gradle\n\nPlus a bunch of awesome libraries & tight integrations like Bullet, NiftyGUI and other goodies.\n\n### Documentation\n\nDid you miss it? Don't sweat it, [here it is again](https://jmonkeyengine.github.io/wiki).\n\n### Contributing\n\nRead our [contribution guide](https://github.com/jMonkeyEngine/jmonkeyengine/blob/master/CONTRIBUTING.md).\n\n### License\n\nNew BSD (3-clause) License. In other words, you do whatever makes you happy!\n\n"
 },
 {
  "repo": "SFTtech/openage",
  "language": "Python",
  "readme_contents": "[![openage](/assets/logo/banner.png)](http://openage.sft.mx)\n============================================================\n\n**openage**: a volunteer project to create a free engine clone of the *Genie Engine* used by *Age of Empires*, *Age of Empires II (HD)* and *Star Wars: Galactic Battlegrounds*, comparable to projects like [OpenMW](https://openmw.org/), [OpenRA](http://openra.net/),  [OpenSAGE](https://github.com/OpenSAGE/OpenSAGE/), [OpenTTD](https://openttd.org/) and [OpenRCT2](https://openrct2.org/). At the moment we focus our efforts on the integration of *Age of Empires II*, while being primarily aimed at POSIX platforms such as **GNU/Linux**.\n\nopenage uses the original game assets (such as sounds and graphics), but (for obvious reasons) doesn't ship them.\nTo play, you require *an original AoE II: TC or [AoE II: HD](http://store.steampowered.com/app/221380/)* installation\n(via [Wine](https://www.winehq.org/) or [Steam-Linux](doc/media_convert.md#how-to-use-the-original-game-assets)).\n\n[![github stars](https://img.shields.io/github/stars/SFTtech/openage.svg)](https://github.com/SFTtech/openage/stargazers)\n[![#sfttech on Freenode](https://img.shields.io/badge/chat-on%20freenode-brightgreen)](https://webchat.freenode.net/?channels=sfttech)\n[![#sfttech on matrix.org](https://img.shields.io/badge/matrix-%23sfttech-blue.svg)](https://app.element.io/#/room/#sfttech:matrix.org)\n[![quality badge](https://img.shields.io/badge/cuteness-overload-orange.svg)](http://www.emergencykitten.com/)\n\n\n\nContact\n-------\nContact          | Where?\n-----------------|-------\nIssue Tracker    | [SFTtech/openage](https://github.com/SFTtech/openage/issues)\nDevelopment Blog | [blog.openage.dev](https://blog.openage.dev)\nForum            | [<img src=\"https://www.redditstatic.com/about/assets/reddit-logo.png\" alt=\"reddit\" height=\"22\"/> /r/openage](https://www.reddit.com/r/openage/)\nMatrix Chat      | [`#sfttech:matrix.org`](https://app.element.io/#/room/#sfttech:matrix.org)\nIRC Chat         | [`irc.freenode.net #sfttech`](https://webchat.freenode.net/?channels=sfttech)\nMoney Sink       | [![money sink](https://liberapay.com/assets/widgets/donate.svg)](https://liberapay.com/SFTtech)\n\n\nTechnical foundation\n--------------------\n\nTechnology     | Component\n---------------|----------\n**C++17**      | Engine core\n**Python3**    | Scripting, media conversion, in-game console, code generation\n**Qt5**        | Graphical user interface\n**Cython**     | Glue code\n**CMake**      | Build system\n**OpenGL3.3**  | Rendering, shaders\n**SDL2**       | Cross-platform Audio/Input/Window handling\n**Opus**       | Audio codec\n[**nyan**](https://github.com/SFTtech/nyan) | Content Configuration and Modding\n**Humans**     | Mixing together all of the above\n\n\nGoals\n-----\n\n* Fully authentic look and feel\n    * This can only be approximated, since the behaviour of the original game is mostly undocumented,\n    and guessing/experimenting can only get you this close\n    * We will not implement useless artificial limitations (max 30 selectable units...)\n* An easily-moddable content format: [**nyan** yet another notation](https://github.com/SFTtech/nyan)\n* An integrated Python console and API, comparable to [blender](https://www.blender.org/)\n* AI scripting in Python, you can use [machine learning](http://scikit-learn.org/stable/)\n    * here is some [additional literature](http://www.deeplearningbook.org/)\n* Re-creating [free game assets](https://github.com/SFTtech/openage-data)\n* Multiplayer (obviously)\n* Matchmaking and ranking with a [haskell masterserver](https://github.com/SFTtech/openage-masterserver)\n* Optionally, [improvements](/doc/ideas/) over the original game\n* Awesome infrastructure such as our own [Kevin CI service](https://github.com/SFTtech/kevin)\n\nBut beware, for sanity reasons:\n\n* No network compatibility with the original game.\n  You really wanna have the same problems again?\n* No binary compatibility with the original game.\n  A one-way script to convert maps/savegames/missions to openage is planned though.\n\n\nCurrent State of the Project\n----------------------------\n\n**Important notice**: Gameplay is currently non-functional as the internal simulation is replaced by a more sophisticated implementation. You also might experience errors when running a build. Gameplay will return in a later update. Detailed explanations can be found in this [blog post](https://blog.openage.dev/new-gamestate-2020.html).\n\n* What features are currently implemented?\n    * See [status page](https://github.com/SFTtech/openage/projects).\n\n* What's the plan?\n    * See [doc/milestones.md](/doc/milestones.md). We also have [lists of crazy xor good ideas](/doc/ideas) and a [technical overview for requested features](/doc/ideas/fr_technical_overview.md).\n\n\nInstallation Packages\n---------------------\n\n**Supported Platforms:** Linux, Windows 10, MacOS X 10.14\n \n* For **Linux** check at [repology](https://repology.org/project/openage/versions) if your distribution has any packages available or [here](https://bintray.com/simonsan/openage-packages/openage-linux-releases) for any future updates on *.deb and AppImage packages.\n* For **Windows** check our [release page](https://github.com/SFTtech/openage/releases) for the latest installer or [here](https://dl.bintray.com/simonsan/openage-packages/) for nightly builds.\n\n    * **NOTE:** If you have any problems starting conversion or starting *openage* take a look into the [package instructions](doc/build_instructions/packages.md) and our [troubleshooting guide](/doc/troubleshooting.md).\n\n    * **NOTE:** For **MacOSX** we currently don't have any packages.\n\n\nQuickstart\n----------------------------------\n\n  Operating System    | Build status\n  :------------------:|:--------------:\n  Debian Sid          | [Todo: Kevin #11](https://github.com/SFTtech/kevin/issues/11)\n  macOS               | [![macOS build status](https://github.com/SFTtech/openage/workflows/macOS-CI/badge.svg)](https://github.com/SFTtech/openage/actions?query=workflow%3AmacOS-CI)\n  Windows 10 - x64    | [![Build status](https://ci.appveyor.com/api/projects/status/66sx35key94u740e?svg=true)](https://ci.appveyor.com/project/simonsan/openage-sl215)\n\n\n* How do I get this to run on my box?\n    * See [doc/building.md](/doc/building.md).\n\n* I compiled everything. Now how do I run it?\n    * Execute `bin/run`.\n    * [The convert script](/doc/media_convert.md) will transform original assets into openage formats, which are a lot saner and more moddable.\n    * Use your brain and react to the things you'll see.\n\n* Waaaaaah! It\n    * segfaults\n    * prints error messages I don't want to read\n    * ate my dog\n\nAll of those are features, not bugs.\n\nTo turn them off, use `./run --dont-segfault --no-errors --dont-eat-dog`.\n\n\nIf this still does not help, try our [troubleshooting guide](/doc/troubleshooting.md), the [contact section](#contact)\nor the [bug tracker](https://github.com/SFTtech/openage/issues).\n\n\nContributing\n============\n\nYou might ask yourself now \"Yeah, this sounds cool and all, but how do *I* participate\nand ~~get famous~~ contribute useful features?\".\n\nFortunately for you, there is a lot to do and we are very grateful for help.\n\n## Where do I start?\n\n* The engine has several [core parts](https://github.com/SFTtech/openage/projects) that need help.\n  You can look at the project related issues and find something for you, for example:\n    * **Asset Converter:** Converts whatever properietary format used by a Age of Empires 2 into\n    open formats. Written mostly in Python 3. There are a lot of TODOs and beginner issues available\n    right now, so it's a good place to get your feet wet.\n    * **Game simulation:** Also known as the gameplay implementation. Written in C++, using the\n    Entity-Component-System paradigm in addition to an event-driven simulation.\n    * **Documentation:** We not only document code, but also anything technical about the Genie engine\n    and its games. If you like documenting [file formats](/doc/media)\n    or thouroughly investigating [game mechanics](/doc/reverse_engineering),\n    then this might be the right place to start.\n* **Check the issues** [labelled with good first issues](https://github.com/SFTtech/openage/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22). These are tasks that you can start right away\n  and don't require much previous knowledge.\n* **Ask us** in the [chat](https://app.element.io/#/room/#sfttech:matrix.org). Someone there could need\n  help with something.\n* You can also **take the initiative** and fix a bug you found, create an issue for discussion or\n  implement a feature that we never though of, but always wanted.\n\n\n## Ok, I found something. What now?\n\n* **[Tell us](#contact)**, if you haven't already. Chances are that we have additional information\n  and directions.\n* **[Read the docs](/doc)**. They will answer most \"administrative\"\n  questions like what code style is used and how the engine core parts are connected.\n* **Read the code** and get familiar with the engine component you want to work with.\n* Do not hesitate to **[ask us for help](#contact)** if you do not understand something.\n\n\n## How do I contribute my features/changes?\n\n* Read the **[contributing guide](/doc/contributing.md)**.\n* You can upload work in progress (WIP) revisions or drafts of your contribution to get feedback or support.\n* Tell us (again) when you want us to review your work.\n\n## I want to help, but I'm not a programmer...\n\nThen openage might be a good reason to become one! We have many issues and tasks for beginners. You\njust have to ask and we'll find something. Alternatively, lurking is also allowed.\n\n----\n\nCheers, happy hecking.\n\n\nDevelopment Process\n-------------------\n\nWhat does openage development look like in practice?\n\n* extensive [synchronization](#contact)!\n* [doc/development.md](/doc/development.md).\n\nHow can I help?\n\n* [doc/contributing.md](/doc/contributing.md).\n\nAll documentation is also in this repo:\n\n* Code documentation is embedded in the sources for Doxygen (see [doc readme](/doc/README.md)).\n* Have a look at the [doc directory](/doc/). This folder tends to get outdated when code changes.\n\n\nLicense\n-------\n\n**GNU GPLv3** or later; see [copying.md](copying.md) and [legal/GPLv3](/legal/GPLv3).\n\nI know that probably nobody is ever gonna look at the `copying.md` file,\nbut if you want to contribute code to openage, please take the time to\nskim through it and add yourself to the authors list.\n"
 },
 {
  "repo": "MonoGame/MonoGame",
  "language": "C#",
  "readme_contents": "\ufeff# MonoGame\n\nOne framework for creating powerful cross-platform games.  The spiritual successor to XNA with thousands of titles shipped across desktop, mobile, and console platforms.  [MonoGame](http://www.monogame.net/) is a fully managed .NET open source game framework without any black boxes.  Create, develop and distribute your games your way.\n\n[![Join the chat at https://discord.gg/tsuucV4](https://img.shields.io/discord/355231098122272778?color=%237289DA&label=MonoGame&logo=discord&logoColor=white)](https://discord.gg/tsuucV4) [![Join the chat at https://gitter.im/MonoGame/MonoGame](https://badges.gitter.im/MonoGame/MonoGame.svg)](https://gitter.im/MonoGame/MonoGame?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\n * [Build Status](#build-status)\n * [Supported Platforms](#supported-platforms)\n * [Support and Contributions](#support-and-contributions)\n * [Source Code](#source-code)\n * [Helpful Links](#helpful-links)\n * [License](#license)\n \n\n## Build Status\n\nOur [build server](http://teamcity.monogame.net/?guest=1) builds, tests, and packages the latest MonoGame changes.  The table below shows the current build status for the develop branch.\n\n| Name  | Status |\n|:---|--------|\n| Build Windows, Web, and Android | [![Build Status](http://teamcity.monogame.net/app/rest/builds/buildType:MonoGame_DevelopWin/statusIcon)](http://teamcity.monogame.net/viewType.html?buildTypeId=MonoGame_DevelopWin&guest=1) |\n| Build Mac, iOS, and Linux | [![Build Status](http://teamcity.monogame.net/app/rest/builds/buildType:MonoGame_DevelopMac/statusIcon)](http://teamcity.monogame.net/viewType.html?buildTypeId=MonoGame_DevelopMac&guest=1) |\n| Windows Tests | [![Build Status](http://teamcity.monogame.net/app/rest/builds/buildType:MonoGame_TestWindows/statusIcon)](http://teamcity.monogame.net/viewType.html?buildTypeId=MonoGame_TestWindows&guest=1) |\n| Mac Tests | [![Build Status](http://teamcity.monogame.net/app/rest/builds/buildType:MonoGame_TestMac/statusIcon)](http://teamcity.monogame.net/viewType.html?buildTypeId=MonoGame_TestMac&guest=1) |\n\n\n## Supported Platforms\n\nWe support a growing list of platforms across the desktop, mobile, and console space.  If there is a platform we don't support, please [make a request](https://github.com/MonoGame/MonoGame/issues) or [come help us](CONTRIBUTING.md) add it.\n\n * Desktop PCs\n   * Windows 10 Store Apps (UWP)\n   * Windows Win32 (OpenGL & DirectX)\n   * Linux (OpenGL)\n   * Mac OS X (OpenGL)\n * Mobile/Tablet Devices\n   * Android (OpenGL)\n   * iPhone/iPad (OpenGL)\n   * Windows Phone 10 (UWP)\n * Consoles (for registered developers)\n   * PlayStation 4\n   * PlayStation Vita\n   * Xbox One (both UWP and XDK)\n   * Nintendo Switch\n   * Google Stadia\n * Other\n   * tvOS (OpenGL)\n\n\n## Support and Contributions\n\nIf you think you have found a bug or have a feature request, use our [issue tracker](https://github.com/MonoGame/MonoGame/issues). Before opening a new issue, please search to see if your problem has already been reported.  Try to be as detailed as possible in your issue reports.\n\nIf you need help using MonoGame or have other questions we suggest you post on our [community forums](http://community.monogame.net).  Please do not use the GitHub issue tracker for personal support requests.\n\nIf you are interested in contributing fixes or features to MonoGame, please read our [contributors guide](CONTRIBUTING.md) first.\n\n### Subscription\n\nIf you'd like to help the project by supporting us financially, consider supporting us via a subscription for the price of a monthly coffee.\n\nMoney goes towards hosting, new hardware and if enough people subscribe a dedicated developer.\n\nThere are several options on our [Donation Page](http://www.monogame.net/donate/).\n\n\n## Source Code\n\nThe full source code is available here from GitHub:\n\n * Clone the source: `git clone https://github.com/MonoGame/MonoGame.git`\n * Set up the submodules: `git submodule update --init`\n * Open the solution for your target platform to build the game framework.\n * Open the Tools solution for your development platform to build the pipeline and content tools.\n\nFor the prerequisites for building from source, please look at the [Requirements](REQUIREMENTS.md) file.\n\nA high level breakdown of the components of the framework:\n\n * The game framework is found in [MonoGame.Framework](MonoGame.Framework).\n * The content pipeline is located in [MonoGame.Framework.Content.Pipeline](MonoGame.Framework.Content.Pipeline).\n * Project templates are in [Templates](Templates).\n * See [Tests](Tests) for the framework unit tests.\n * See [Tools/Tests](Tools/MonoGame.Tools.Tests) for the content pipeline and other tool tests.\n * [mgcb](Tools/MonoGame.Content.Builder) is the command line tool for content processing.\n * [mgfxc](Tools/MonoGame.Effect.Compiler) is the command line effect compiler tool.\n * The [mgcb-editor](Tools/MonoGame.Content.Builder.Editor) tool is a GUI frontend for content processing.\n\n\n## Helpful Links\n\n * The official website is [monogame.net](http://www.monogame.net).\n * Our [issue tracker](https://github.com/MonoGame/MonoGame/issues) is on GitHub.\n * Use our [community forums](http://community.monogame.net/) for support questions.\n * You can [chat live](https://gitter.im/mono/MonoGame?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) with the core developers and other users.\n * The [official documentation](http://www.monogame.net/documentation/) is on our website.\n * Download release and development [installers and packages](http://www.monogame.net/downloads/).\n * Follow [@MonoGameTeam](https://twitter.com/monogameteam) on Twitter.\n\n## License\n\nThe MonoGame project is under the [Microsoft Public License](https://opensource.org/licenses/MS-PL) except for a few portions of the code.  See the [LICENSE.txt](LICENSE.txt) file for more details.  Third-party libraries used by MonoGame are under their own licenses.  Please refer to those libraries for details on the license they use.\n"
 },
 {
  "repo": "gabrielecirulli/2048",
  "language": "CSS",
  "readme_contents": "# 2048\nA small clone of [1024](https://play.google.com/store/apps/details?id=com.veewo.a1024), based on [Saming's 2048](http://saming.fr/p/2048/) (also a clone).\n\nMade just for fun. [Play it here!](http://gabrielecirulli.github.io/2048/)\n\nThe official app can also be found on the [Play Store](https://play.google.com/store/apps/details?id=com.gabrielecirulli.app2048) and [App Store!](https://itunes.apple.com/us/app/2048-by-gabriele-cirulli/id868076805)\n\n### Contributions\n\n[Anna Harren](https://github.com/iirelu/) and [sigod](https://github.com/sigod) are maintainers for this repository.\n\nOther notable contributors:\n\n - [TimPetricola](https://github.com/TimPetricola) added best score storage\n - [chrisprice](https://github.com/chrisprice) added custom code for swipe handling on mobile\n - [marcingajda](https://github.com/marcingajda) made swipes work on Windows Phone\n - [mgarciaisaia](https://github.com/mgarciaisaia) added support for Android 2.3\n\nMany thanks to [rayhaanj](https://github.com/rayhaanj), [Mechazawa](https://github.com/Mechazawa), [grant](https://github.com/grant), [remram44](https://github.com/remram44) and [ghoullier](https://github.com/ghoullier) for the many other good contributions.\n\n### Screenshot\n\n<p align=\"center\">\n  <img src=\"https://cloud.githubusercontent.com/assets/1175750/8614312/280e5dc2-26f1-11e5-9f1f-5891c3ca8b26.png\" alt=\"Screenshot\"/>\n</p>\n\nThat screenshot is fake, by the way. I never reached 2048 :smile:\n\n## Contributing\nChanges and improvements are more than welcome! Feel free to fork and open a pull request. Please make your changes in a specific branch and request to pull into `master`! If you can, please make sure the game fully works before sending the PR, as that will help speed up the process.\n\nYou can find the same information in the [contributing guide.](https://github.com/gabrielecirulli/2048/blob/master/CONTRIBUTING.md)\n\n## License\n2048 is licensed under the [MIT license.](https://github.com/gabrielecirulli/2048/blob/master/LICENSE.txt)\n\n## Donations\nI made this in my spare time, and it's hosted on GitHub (which means I don't have any hosting costs), but if you enjoyed the game and feel like buying me coffee, you can donate at my BTC address: `1Ec6onfsQmoP9kkL3zkpB6c5sA4PVcXU2i`. Thank you very much!\n"
 },
 {
  "repo": "ellisonleao/clumsy-bird",
  "language": "JavaScript",
  "readme_contents": "Clumsy Bird\n===========\n\nA MelonJS made \"Flappy Bird\" clone.\n\n![](http://i.imgur.com/Slbvt65.png)\n\nPlay online at http://ellisonleao.github.io/clumsy-bird/\n\n[![Deploy](https://www.herokucdn.com/deploy/button.png)](https://heroku.com/deploy?template=https://github.com/ellisonleao/clumsy-bird/tree/gh-pages)\n\n## Running Locally\n\n- Install [Node](http://nodejs.org/download/) and [Grunt](http://gruntjs.com/)\n- Install the dependencies\n\n```\nnpm install\n```\n\nThen just type on your shell:\n\n```\ngrunt connect\n```\n\nOpen your browser at `http://localhost:8001/`\n\n## Making your customization\n\nSee [CUSTOMIZING](https://github.com/ellisonleao/clumsy-bird/blob/master/CUSTOMIZING.md)\n\n## Some nice games made with this project\n\n[Checkout here](https://github.com/ellisonleao/clumsy-bird/wiki/Games-using-clumsy-bird-code)\n\nSome thoughts about this code you can find on [my blog post](https://medium.com/@ellisonleao/clumsy-bird-an-open-source-flappy-bird-clone-cf615724730f)\n"
 },
 {
  "repo": "mozilla/BrowserQuest",
  "language": "JavaScript",
  "readme_contents": "BrowserQuest\n============\n\nBrowserQuest is a HTML5/JavaScript multiplayer game experiment.\n\n\nDocumentation\n-------------\n\nDocumentation is located in client and server directories.\n\n\nLicense\n-------\n\nCode is licensed under MPL 2.0. Content is licensed under CC-BY-SA 3.0.\nSee the LICENSE file for details.\n\n\nCredits\n-------\nCreated by [Little Workshop](http://www.littleworkshop.fr):\n\n* Franck Lecollinet - [@whatthefranck](http://twitter.com/whatthefranck)\n* Guillaume Lecollinet - [@glecollinet](http://twitter.com/glecollinet)"
 },
 {
  "repo": "AlexNisnevich/untrusted",
  "language": "JavaScript",
  "readme_contents": "**Untrusted \u2014or\u2014 the Continuing Adventures of Dr. Eval** is an exciting Meta-JavaScript Adventure Game wherein you guide the dashing, steadfast Dr. Eval through a mysterious MACHINE CONTINUUM, wherein, using only his trusty computer and the TURING-COMPLETE power of JavaScript, he must literally ALTER HIS REALITY in order to find his freedom! You must literally edit and re-execute the very JavaScript running the game in your browser to save Dr. Eval from this dark and confusing reality!\n\n### Overview\n\nThe game presents you with a roguelike-like playing environment and a console window  with the JavaScript code generating each level. As loaded, each level is unbeatable, and most of the JavaScript is blocked from editing. The challenge is to open a path to the next level using only the limited tools left open to you.\n\n### Development\n\nRun\n```\nmake\n```\nto merge the JavaScript files into `scripts/build/untrusted.js` (and enables debug features).\n\n```\nmake release\n```\nmerges and minifies the JavaScript files into `scripts/build/untrusted.min.js` (and disables debug features).\n\nTo run the game locally, you need to set up a local server to serve `index.html` (this step is necessary due to Access-Control-Allow-Origin restrictions).\n\nFirst install [http-server](https://github.com/nodeapps/http-server/#installing-globally) if you haven't already:\n\n```\nnpm install http-server\n```\n\nThen run:\n\n```\nmake runlocal\n```\n\nBuild your own mod in the `mods` directory:\n\n```\nmake mod=example_mod\n```\n\n### Contributing Levels\n\nTo add a new level, create a jsx file in [/levels/bonus](https://github.com/AlexNisnevich/untrusted/tree/master/levels/bonus) and add the level filename to the `bonusLevels` array in [game.js](https://github.com/AlexNisnevich/untrusted/blob/master/scripts/game.js#L40).\n\nIf you are adding any new commands that the player can use, make sure to add them to `reference.js`.\n\n#### The .jsx file format\n\njsx files are like regular JavaScript files, but have some additional syntax:\n- `#BEGIN_EDITABLE#` and `#END_EDITABLE#` surround editable lines\n- `#{#` and `#}#` wrap editable sections (parts of lines)\n- `#BEGIN_PROPERTIES#` and `#END_PROPERTIES#` surround the properties object at the start of the file. Available properties include:\n  - `commandsIntroduced`: array of new commands introduced in the level (see `reference.js`)\n  - `mapProperties`: optionally contains any of the following:\n     - `allowOverwrite`: if true, placed static objects can be overwritten by other objects\n     - `keyDelay`: specifies the lag, in milliseconds, between player keystrokes (default: 0)\n     - `refreshRate`: the refresh rate of the level, in milliseconds (required for dynamic objects with `interval` properties to work correctly)\n     - `showDrawingCanvas`: if true, the drawing canvas overlay is displayed\n     - `showDummyDom`: if true, a dummy DOM will be displayed instead of the regular map\n  - `music`: name of the background track for the level (see `sound.js`)\n  - `startingMessage`: message displayed at the bottom of the screen when the level starts (if any)\n  - `version`: increase the level version whenever you update a level\n- `#START_OF_START_LEVEL#` and `#END_OF_START_LEVEL#` should be the first and last line of the `startLevel` method, respectively\n\n#### Adding music\n\nTo add a new background music track, add an MP3 file (that you have permission to use) to the [/music](https://github.com/AlexNisnevich/untrusted/tree/master/music) and add a new entry to the `tracks` array in [sound.js](https://github.com/AlexNisnevich/untrusted/blob/master/scripts/sound.js).\n\n### Acknowledgements\n\nUntrusted is a game by [Alex Nisnevich](http://alex.nisnevich.com/) and [Greg Shuflin](https://github.com/neunenak).\n\nWe'd like to thank:\n\n- [Dmitry Mazin](https://github.com/dmazin) for design assistance and for the implementation of multiline editing\n- [Jordan Arnesen](https://github.com/extrajordanary) for playtesting and design of lvl17\n- [Natasha Hull-Richter](http://nhull.com) for extensive playtesting and assistance in level design\n- Alex Bolotov, Colin Curtin, Conrad Irwin, Devin C-R, Eugene Evans, Gilbert Hsyu, Jacob Nisnevich, James Silvey, Jason Jiang, Jimmy Hack, Philip Shao, Ryan Fitzgerald, Stephen Liu, Yayoi Ukai, and Yuval Gnessin for playtesting and feedback\n- [Ond\u0159ej \u017d\u00e1ra](https://github.com/ondras) for his [rot.js](http://ondras.github.io/rot.js/) library\n- [Marijn Haverbeke](https://github.com/marijnh) for his [CodeMirror](http://codemirror.net/) library\n- [Brian Harvey](http://www.cs.berkeley.edu/~bh/) for allowing us to use his likeness in lvl19\n\n#### Soundtrack\n\nYou can [listen to the full soundtrack here](https://soundcloud.com/untrusted/sets/untrusted-soundtrack).\n\nThe music that appears in Untrusted, in order, is:\n\n- \"The Green\" - [Jonathan Holliday](http://www.soundclick.com/bands/default.cfm?bandID=836578) (used with permission)\n- \"Dmitry's Thing #2\" - [Dmitry Mazin](https://soundcloud.com/dmitry-mazin) (written for Untrusted)\n- \"Obscure Terrain\" - [Revolution Void](http://revolutionvoid.com/) (CC-BY-NC-SA)\n- \"coming soon\" - [Fex](http://artistserver.com/Fex) (public domain)\n- \"cloudy sin\" - [iNTRICATE](https://soundcloud.com/stk13) (used with permission)\n- \"Dynamic Punctuality\" - [Dmitry Mazin](https://soundcloud.com/dmitry-mazin) (written for Untrusted)\n- \"Y\" - [Tortue Super Sonic](https://soundcloud.com/tss-tortue-super-sonic) (CC-BY-NC-SA)\n- \"Night Owl\" - [Broke for Free](http://brokeforfree.com/) (CC-BY)\n- \"The Waves Call Her Name\" - [Sycamore Drive](http://sycamoredrive.bandcamp.com/) (CC-BY-NC-SA)\n- \"Come and Find Me - B mix\" - [Eric Skiff](http://ericskiff.com/) (CC-BY)\n- \"Conspiracy\" - [Mike and Alan](https://www.facebook.com/MicAndAlan) (used with permission)\n- \"Messeah\" - [RoccoW](https://soundcloud.com/roccow) (CC-BY)\n- \"Searching\" - [Eric Skiff](http://ericskiff.com/) (CC-BY)\n- \"Da Funk Do You Know 'bout Chip?\" - [That Andy Guy](https://soundcloud.com/that-andy-guy) (used with permission)\n- \"Soixante-8\" - [Obsibilo](http://freemusicarchive.org/music/Obsibilo/) (CC-BY-NC-SA)\n- \"Tart (Pts 1 and 2)\" - [BLEO feat KeFF](http://bleo.dummydrome.com/) (CC-BY-NC-SA)\n- \"Beach Wedding Dance\" - [Rolemusic](https://soundcloud.com/rolemusic) (CC-BY-NC-SA)\n- \"Boss Loop 1\" - [Essa](http://www.youtube.com/user/Essasmusic) (free to use)\n- \"Adversity\" - [Seropard](https://soundcloud.com/seropard) (free to use)\n- \"Comme Des Orages\" - [Obsibilo](http://freemusicarchive.org/music/Obsibilo/) (CC-BY-NC-SA)\n- \"Brazilicon Alley\" - [Vernon Lenoir](http://vernonlenoir.wordpress.com/) (CC-BY-NC-SA), based on \"Aquarela do Brazil\" by Ary Barroso\n\n### License\nThis work is dual-licensed.\n\n- Untrusted and the Untrusted soundtrack are licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/3.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License (CC-BY-NC-SA 3.0)</a>. In other words, you are free to use and modify Untrusted for non-commercial purposes, provided that you credit us and your work is also licensed under CC-BY-NC-SA.\n- Additionally, the Untrusted code *without the soundtrack* is licenced under a commercial license. This means that you are able to use Untrusted for commercial purposes under some conditions, provided that you do not use any of the music. Please [contact us](mailto:alex.nisnevich@gmail.com,greg.shuflin@gmail.com) for details.\n"
 },
 {
  "repo": "doublespeakgames/adarkroom",
  "language": "JavaScript",
  "readme_contents": "A Dark Room\n===========\n> \"awake. head throbbing. vision blurry. come light the fire.\"\n\na minimalist text adventure game for your browser\n\n[Click to play](http://adarkroom.doublespeakgames.com)\n\n<table>\n<tr><th colspan=4>Available Languages</tr>\n<tr>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=zh_cn\">Chinese (Simplified)</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=zh_tw\">Chinese (Traditional)</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=en\">English</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=fr\">French</a></td>\n</tr><tr>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=de\">German</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=el\">Greek</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=id\">Indonesian</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=it\">Italian</a></td>\n</tr><tr>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=ja\">Japanese</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=ko\">Korean</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=nb\">Norwegian</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=pl\">Polish</a></td>\n</tr><tr>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=pt\">Portuguese</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=pt_br\">Portuguese (Brazil)</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=ru\">Russian</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=es\">Spanish</a></td>\n</tr><tr>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=sv\">Swedish</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=th\">Thai</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=tr\">Turkish</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=uk\">Ukrainian</a></td>\n</tr><tr>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=vi\">Vietnamese</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=lt_LT\">Lithuanian</a></td>\n\t<td><a href=\"http://adarkroom.doublespeakgames.com/?lang=gl\">Galician</a></td>\n</tr>\n</table>\n\nor play the latest on [GitHub](http://doublespeakgames.github.io/adarkroom)\n\n<a href=\"https://itunes.apple.com/us/app/a-dark-room/id736683061\"><img src=\"http://i.imgur.com/DMdnDYq.png\" height=\"50\"></a>\n<a href=\"https://play.google.com/store/apps/details?id=com.yourcompany.adarkroom\"><img src=\"http://i.imgur.com/bLWWj4r.png\" height=\"50\"></a>\n"
 },
 {
  "repo": "Hextris/hextris",
  "language": "JavaScript",
  "readme_contents": "Hextris\n==========\n\n<img src=\"images/twitter-opengraph.png\" width=\"100px\"><br>\n\nAn addictive puzzle game inspired by Tetris. Play it at [www.hextris.io](http://www.hextris.io), or [https://hextris.github.io/hextris](https://hextris.github.io/hextris).\n\nBy:\n - Logan Engstrom ([@lengstrom](http://loganengstrom.com/))\n - Garrett Finucane ([@garrettdreyfus](http://github.com/garrettdreyfus))\n - Noah Moroze ([@nmoroze](http://github.com/nmoroze))\n - Michael Yang ([@themichaelyang](http://github.com/themichaelyang))\n \n ## Citation\nDid you use Hextris in your research? Cite us as follows:\n```\n  @misc{engstrom2015hextris,\n    author = {Logan Engstrom, Garrett Finucane, Noah Moroze, Michael Yang},\n    title = {hextris},\n    year = {2015},\n    howpublished = {\\url{https://github.com/hextris/hextris/}},\n    note = {commit xxxxxxx}\n  }\n```\n\n\n# Contributions\nThis project is not very actively maintained, as we are all very busy these days. But feel free to open an issue or PR, and we'll eventually take a look.\n\n# About\nHextris was created by a group of high school friends in 2014.\n\n## Press kit\nhttp://hextris.github.io/presskit/info.html\n\n## License\nCopyright (C) 2018 Logan Engstrom\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <https://www.gnu.org/licenses/>.\n"
 },
 {
  "repo": "Greenstand/Development-Overview",
  "language": null,
  "readme_contents": "# Welcome to Greenstand's Treetracker project\n\nWe are here to make the world a cooler, richer and greener place.  \n\n**Basic Overview:** The [treetracker-android](https://github.com/Greenstand/treetracker-android) app is a tool to allow planters to verify tree planting and tree survival with geo-tagged, time-stamped images. The tree images and data points are to be analyzed using the [treetracker-admin](https://github.com/Greenstand/treetracker-admin) and then displayed and shared via tokens/links on the [treetracker-web-map](https://github.com/Greenstand/treetracker-web-map). This map can be embedded on other organizations' websites displaying their planted and mapped trees. \n\nJoin our teams: https://github.com/Greenstand/Development-Overview/wiki/Teams\n\n## Development needs as of Aug 2020\n\nHere are immediate and long-term needs. Here are a few, if you don\u2019t see a project that fits, come check in on [Slack link](https://join.slack.com/t/greenstand/shared_invite/enQtMjcyMzgyMjk4NzU3LWE3N2UwYjYyNWJiNzNlMzgzYjUyZDEwODVhNDVhYTZhNmJlZDc2NTM2MTkyODcxM2U3OWJlZWMxN2FhNWJkNWU) and let\u2019s figure out how to leverage your skills. Don't be shy. There are lots to a do and a great community to plug into!\n\n\n- React.js - Our admin panel is build using React.js, and our web map uses jquery but needs to be ported to React.js.   We have new designs for the web map that need to be implemented and made mobile compliant.\n\n- Node.js, Loopback - our APIs are build on node.js express and loopback.  We need API engineers with some database experience to add features.  We also need engineers to improve our unit testing coverage.\n\n- Android - Need Android UI/UX engineers and individuals with experience working with cloud integrations in offline scenarios.  Experience with enhancing GPS accuracy on the Android platform is also useful.\n\n- iOS - We need to build a clone of our Android app in iOS.  \n\n- DevOps - We use ansible and travis CI for build and deployment automation.  Our automation coverage is partial and we need engineer with experience on these platforms to fix problems and streamline the process.\n\n- Cloud - Scaling and security analysis is needed.\n\n- Database - Database adminstration is need to continue refining our backend systems.\n\n- Image processing - Duplicate image detection, species detection. Let's get our analysis pipeline rolling.\n\n- Quality Control - Need quality control engineers to understand use cases and be available for robust testing on builds before rollout. This is often testing new features on our admin panel, webmap or mobile app. If you can find a broken link or a button that doesn't work, you have found a great way to contribute and get to know the project. \n\n## [Roadmap](https://github.com/Greenstand/Development-Overview/blob/master/Roadmap.md) for our feature development plans\n\n# Contributing to The Cause\nHelp us star and fork our repositories.\n\n### Join The team\n* Fill out the [Join-the-tech-team-form](https://docs.google.com/forms/d/e/1FAIpQLSe61HDJKVH16vtTxhXpbwCH-wTVN1e6XoVU1riWjJ-ne5SIiA/viewform?usp=sf_link)\n\n* Ask to be added to the team on Github. \n\n* If you already know how, feel free to just fork, build and pull.\n\n* If the words 'fork and pull' doesn't make sense, please read up on Github below. \n\n**Think Agile - Small Iterations - Clearly Defined Commits.**\n  \n * Keep your commits small - if you worked on three issues make three commits etc. \n \n * Small commits help the reviewer\n \n * Lots of small commits make your profile look cooler.\n \n * When in doubt, ask.\n\n## Your first contribution\n* Find a task you would like to work on by browsing the repositories' issue lists below. \n* Let others know what you are working on - post your intentions in the Github comments or relevant Slack channel. \n* If you don't find something now, be persistent. \n\n## Complete list of [Issues/Tickets](https://github.com/Greenstand/Development-Overview/blob/master/Issues-lndex.md)**\n\n   * [General Issues](https://github.com/Greenstand/Development-Overview/issues)\n   * Kotlin Issues: [Android App](https://github.com/Greenstand/treetracker-android/issues)\n       \n   * Java Script Issues:\n          [Admin Panel](https://github.com/Greenstand/treetracker-admin/issues), \n          [Web map issues](https://github.com/Greenstand/treetracker-web/issues), \n          [Mobile App API](https://github.com/Greenstand/treetracker-mobile-api/issues), \n    \n   * React Issues:\n          [Admin Panel](https://github.com/Greenstand/treetracker-admin/issues), \n   * Data Related: \n          [Data Base Issues](https://github.com/Greenstand/treetracker-database/issues)\n          \n## Project Repositories\n\n*In its most basic form, this platform is designed to collect tree images from an android application and display these images on web-maps that can be served on other organizations' websites. In its more advanced form, we are verifying individual planter's efforts by tracking individual trees over time and creating results-based employment. Each Data point gets turned into a tradeable token*\n\n#### [Treetracker-Android Application](https://github.com/Greenstand/treetracker-android)\n- [Treetracker_Android User Story](https://github.com/Greenstand/treetracker-android/wiki/User-Story)\n- [Treetracker Android Project Board](https://github.com/orgs/Greenstand/projects/5)\n- [Treetracker Android Wiki](https://github.com/Greenstand/treetracker-android/wiki)\n#### [Treetracker-Web Map](https://github.com/Greenstand/treetracker-web)\n- [Treetracker-Web-Map User Story](https://github.com/Greenstand/treetracker-web-map/wiki)\n- [Treetracker Web Map Project Board](https://github.com/orgs/Greenstand/projects/4)\n- [Treetracker Web Map Wiki](https://github.com/Greenstand/treetracker-web-map/wiki)\n#### [Treetracker-Admin panel](Https://github.com/Greenstand/treetracker-admin)\n- [Treetracker-Admin User Story]\n- [Treetracker Admin Project Board](https://github.com/orgs/Greenstand/projects/6)\n- [Treetracker Admin Wiki](https://github.com/Greenstand/treetracker-admin/wiki)\n#### [Database Migrations](https://github.com/Greenstand/treetracker-database-migrations)\n- [Database Migrations Docs](https://db-migrate.readthedocs.io/en/latest/Getting%20Started/configuration/)\n \n#### [Development overview](https://github.com/Greenstand/Development-Overview)\n- [Development Overview Project Board]\n- [Development Overview Wiki](https://github.com/Greenstand/Development-Overview/wiki)\n\nAPI's \n* [Repository - Api for working with the android mobile segment](https://github.com/Greenstand/treetracker-mobile-api)\n* [Repository - Api for working with admin-panel](https://github.com/Greenstand/treetracker-admin-api)\n* [Some json and scripts for working with the API](https://github.com/Greenstand/treetracker-json)\n* [Tree tracker server scripts](https://github.com/Greenstand/treetracker-server-scripts)\n\n## Basic Git Hub skills required!\nIf you have not heard of a **pull request** it is time for you to join over 27 million developers and learn this version control platform. There are lots of cool articles to get you up to speed: [Github Introduction](https://guides.github.com/introduction/flow/) [Forking a Repository in Github](https://help.github.com/articles/fork-a-repo/), [Synicing a Fork in Github](https://help.github.com/articles/syncing-a-fork/)\n\n**We use the Fork and Pull model.**\nPro Tip: Keep pull requests small and focused on the issue you are solving. (Large changes in code are much harder to accept)\n\nBasics: \n* Fork the repository\n* All changes you make must be submitted for review via a pull request to the appropriate branch.\n* Ask for clarification if needed (via Git pull requests or Slack). \n* Report problems.\n\n### Stay up to date with Git!\nTo avoid merge conflicts help us out by keeping your fork up to date. Rebase your forked and local repositories from the Greenstand repository before you start coding and before a pull request.\n1. Switch to your local master branch.\n```\ngit switch master\n```\n2. Add the Greenstand repository as a remote and call it upstream.\n```\ngit remote add upstream https://github.com/greenstand/repo.git\n```\n3. Fetch all branches of the remote repository.\n```\ngit fetch upstream\n```\n4. Rebase your local master with the upstream master branch.\n```\ngit rebase upstream/master\n```\n5. Push updates to your forked master.\n```\ngit push origin master\n```\n6. Switch to your branch then merge the local master to your branch.\n```\ngit switch yourbranch\ngit merge master\n```\n8. Resolve merge conflicts and complete merge.\n9. Resume coding or create a pull request.\n\n### Slack Basics\n[Slack link](https://join.slack.com/t/greenstand/shared_invite/enQtMjcyMzgyMjk4NzU3LWZmNjM3YzY5N2Q0MzQ5YTM4OGZkMWJhM2U4MTkyYjI2NjhkN2YxNTRiMDIwNWQ5ZTVlNDczYzBjZmMxYzM2ZjU)\nSlack is our main form of communication. Here are some Slack basics: \nTo add yourself to the desired channels: on the left window in Slack find **channel +**... \n\nOur popular channels are #development, #android, #nodejs-api, #tree-talk etc..\n\n* Speak out! Public messages build our team and keep us motivated. Use Public channels as much as possible. \n* Keep your messages in the relevant channels - talk about android in the android channel etc..\n* Ask any question - we are friendly and all learning together. General tech questions go in #development, other questions in #general, Android questions in #android etc.\n* Pointless chatter goes in #random channel\n* Use *Threads* to reply keeps channels cleaner.\n* A list of ALL channels is found pinned to the Genneral Channel if you are not in a channel that you think you need access to, reach out.\n\n## Helpful Links: \n\n[GreenStand Coding Style Guide](Coding-Style-Guide.md) for coding style rules \n\n## Contacts: \n- [Slack Link](https://join.slack.com/t/greenstand/shared_invite/enQtMjcyMzgyMjk4NzU3LWZmNjM3YzY5N2Q0MzQ5YTM4OGZkMWJhM2U4MTkyYjI2NjhkN2YxNTRiMDIwNWQ5ZTVlNDczYzBjZmMxYzM2ZjU)\n\n- Info@greenstand.org\n"
 },
 {
  "repo": "GliaX/Stethoscope",
  "language": "Ruby",
  "readme_contents": "Stethoscope\n===========\n\nThis project aims to create a research-validated stethoscope whose plans are \navailable freely and openly. The goal is for the bell to cost ~USD$1-2 to produce, \nand the rest of the stethoscope to cost approximately the same. You can see the peer-reviewed publication relating to this stethoscope's validation here:\n\nhttp://journals.plos.org/plosone/article?id=10.1371/journal.pone.0193087\n\nCurrently, the stethoscope resulting from this project functions as well as the \nmarket gold standard, the Littmann Cardiology III.\n\n\nBill of Materials\n=================\n\n**Printed parts:**\n* 1 stethoscope head (head.stl)\n* 2 ear tubes (eartube.stl)\n* 1 Y-piece (y_piece.stl)\n* 1 Spring (spring.stl)\n* 1 Ring (ring.stl)\n\n**Other hardware:**\nSome vendors are suggested.\n* 40cm - 50cm Silicone 13mm (preferred) or 12mm OD, 8mm ID, 50 durometer\n  * [8MM I.D X 13MM O.D NGP60 Clear Translucent Silicone Hose Pipe Tubing](https://www.advancedfluidsolutions.co.uk/8mm-id-x-13mm-od-clear-transulcent-silicone-hose-pipe-tubing-2482-p.asp) (USD$7.34/meter)\n\n* 20cm Silicone 8mm (preferred) or 6mm OD, 4mm ID (cut into 10cm pieces), 60 durometer\n  * [4mm ID x 8mm OD](https://www.advancedfluidsolutions.co.uk/4mm-id-x-8mm-od-clear-transulcent-silicone-hose-pipe-tubing-2454-p.asp)\n\n* Diaphragm: 40mm diameter cut from a report cover with approx 0.35mm plastic sheet\n  * [Staples 21639 report cover (UPC 718103160223)](https://www.staples.ca/products/780953-en-staples-swing-lock-report-cover-clear-with-black-spine-5pack) (USD$8.84)\n  * You can cut the diaphragm by hand or with a stamp like [this one](https://www.amazon.ca/Karujimu-ki-jumbo-craft-circle-CN45004/dp/B001CBY41W)\n\n* Earbuds / Eartips: Any large-sized standard earbuds will do.\n  * [Silicone Earbuds 7 Pairs - Large size](https://www.amazon.ca/gp/product/B006VELFJY)\n \n**Optional hardware**:\n* Metal spring for holding ear tubes together\n\n\nPrint Instructions\n==================\n**INFILL MUST BE 100%** **INFILL MUST BE 100%** **INFILL MUST BE 100%** **INFILL MUST BE 100%**\n\n* Use PETG or ABS\n* Layer height: 0.2mm\n* Use PrusaSlicer 2.0 or above to import 3MF file\n* Modify filament and printer settings as needed.\n* **DO NOT MODIFY PRINT SETTINGS**\n* Export and print\n\n\nTroubleshooting\n===============\n\n* If the spring and eartubes do not fit well, go ahead and scale the spring as needed\n* If the head and ring do not fit well, use caution as you may be modifying the acoustics. You can scale the head a little.\n\n\nNotes\n=====\n\n* We do not use PLA due to deformation in heat and poor plastic quality in the spring causing early failure. PLA may be used, but the lifetime of the stethoscope will decrease significantly.\n\n* We do not use brims, but you may print the eartubes and Y-pieces with a brim of 2mm to ensure that none of the parts lift off.\n\n**INFILL MUST BE 100%** **INFILL MUST BE 100%** **INFILL MUST BE 100%.** Otherwise, the stethoscope will not produce a correct sound.\n\n\nAssembly Instructions\n=====================\n\nSee [this instructional video](https://www.youtube.com/watch?v=u-KNTc0POLA) for assembly instructions.\n\n* Attach the diaphragm (40mm) to the stethoscope head.\n* Attach the stethoscope head to the silicone tube.\n* Attach the silicone tube to the Y-Piece.\n* Attach spring to ear tubes.\n* Attach the Y-piece to the ear tubes.\n* Attach the ear tubes to the eartips / earbuds.\n* Test the stethoscope as per the validation instructions.\n\n\nPrinting the inserts\n====================\nThe inserts are included in the `manual` directory. Using 8.5 x 11 (Legal) paper,\nthey can be printed at 8.25\" page width with 95% scale for the top print and \n90% scale for the bottom print using GIMP.\n\nThe top insert is cut at 14.5cm and again at 1cm creating two labels of 13.5cm height.\nThese inserts are printed on adhesive material.\n\nThe bottom insert is cut at a width of 20cm with a height cuts at 25cm, 15cm, \n13.5cm and 3.5cm creating 10cm x 20cm inserts.\n\n\nChanging and creating SCAD files\n================================\n\n[CrystalSCAD](https://github.com/Joaz/CrystalScad) and [OpenSCAD](http://www.openscad.org/) \nwere used to create all STL files. To recreate the stethoscope head, simply do:\n\n``` shell\ngem install crystalscad\nruby source_files/stethoscope_head/stethoscope_head.rb\n```\n\nThe SCAD files output from CrystalSCAD are found in `source_files/stethoscope_head/output` and are named as follows:\n* PrintableStethoscopeHead1Assembly_output.scad - The head\n\n\nMass Manufacturing\n==================\nWe generally print 4 stethoscopes per plate to ensure that each stethoscope is created out of the same material.\n\nOur serial numbering system consists of two parts. The last number part is the total number of unique stethoscopes created since day 1. All numbers before that dash are spool identifiers involved in that stethoscope. For example:\n\n001-010 would be the tenth stethoscope made with the first spool in our inventory. If the first spool makes a total of 15 stethoscopes, then the first stethoscope of the second spool would be 002-016. If the twentieth stethoscope uses plastic from spool 002 and spool 003, it would be 002-003-020.\n\n\nOther stethoscopes\n==================\nOthers have made 3D printed stethoscopes too. See:\n* https://www.youmagine.com/designs/stethoscope-chestpiece\n\n\nLicensing notes\n===============\nAs per our understanding, hardware is not covered by copyright. However, we present\nour work under the TAPR OHL license insofar as it applies.\n"
 },
 {
  "repo": "HospitalRun/hospitalrun-frontend",
  "language": "TypeScript",
  "readme_contents": "# HospitalRun Frontend\n\n<div align=\"center\">\n\n![Status](https://img.shields.io/badge/Status-developing-brightgree) [![Release](https://img.shields.io/github/release/HospitalRun/hospitalrun-frontend.svg)](https://github.com/HospitalRun/hospitalrun-frontend/releases) [![Version](https://img.shields.io/github/package-json/v/hospitalrun/hospitalrun-frontend)](https://github.com/HospitalRun/hospitalrun-frontend/releases)\n[![GitHub CI](https://github.com/HospitalRun/frontend/workflows/GitHub%20CI/badge.svg)](https://github.com/HospitalRun/frontend/actions) [![Coverage Status](https://coveralls.io/repos/github/HospitalRun/hospitalrun-frontend/badge.svg?branch=master)](https://coveralls.io/github/HospitalRun/hospitalrun-frontend?branch=master) [![Language grade: JavaScript](https://img.shields.io/lgtm/grade/javascript/g/HospitalRun/hospitalrun-frontend.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/HospitalRun/hospitalrun-frontend/context:javascript) ![Code scanning](https://github.com/HospitalRun/hospitalrun-frontend/workflows/Code%20scanning/badge.svg?branch=master) [![Documentation Status](https://readthedocs.org/projects/hospitalrun-frontend/badge/?version=latest)](https://hospitalrun-frontend.readthedocs.io)\n[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2FHospitalRun%2Fhospitalrun-frontend.svg?type=shield)](https://app.fossa.io/projects/git%2Bgithub.com%2FHospitalRun%2Fhospitalrun-frontend?ref=badge_large) [![Commitizen friendly](https://img.shields.io/badge/commitizen-friendly-brightgreen.svg)](http://commitizen.github.io/cz-cli/)\n![dependabot](https://api.dependabot.com/badges/status?host=github&repo=HospitalRun/hospitalrun-frontend) [![Slack](https://hospitalrun-slack.herokuapp.com/badge.svg)](https://hospitalrun-slack.herokuapp.com)\n[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/HospitalRun/hospitalrun-frontend)\n\n</div>\n\nReact frontend for [HospitalRun](http://hospitalrun.io/): free software for developing world hospitals.\n\n---\n\n# Are you a user? If yes...\n\n[Visit this page for general information on the HospitalRun application](https://github.com/HospitalRun/hospitalrun/blob/master/README.md) including:\n\n- How can I deploy 1.0.0-beta?\n- Where do I report a bug or request a feature?\n- How can I contribute? (There are several other ways besides coding)\n- What is the project structure?\n- What is the application infrastructure?\n- Who is behind HospitalRun? etc.\n\n# Would you like to contribute? If yes...\n\n[Get started by checking out the Frontend Contributing Guide](https://github.com/HospitalRun/hospitalrun-frontend/blob/master/.github/CONTRIBUTING.md) for:\n- What's the tech stack?\n- Where can I become familiar with the technologies?\n- Where do I browse issues?\n- How do I set up my local environment?\n- How do I run tests locally?\n- How do I submit my changes?\n- etc.\n\n# License\n\nReleased under the [MIT license](LICENSE).\n"
 },
 {
  "repo": "get-alex/alex",
  "language": "JavaScript",
  "readme_contents": "<!--lint disable no-html first-heading-level no-shell-dollars-->\n\n<h1 align=\"center\">\n  <img width=\"300\" src=\"https://raw.githubusercontent.com/get-alex/alex/a192b46/media/logo-alex-purple.svg?sanitize=true\" alt=\"alex\">\n  <br>\n  <br>\n</h1>\n\n> \ud83d\udcdd **alex** \u2014 Catch insensitive, inconsiderate writing.\n\n[![Build][build-badge]][build]\n[![Coverage][coverage-badge]][coverage]\n[![First timers friendly][first-timers-badge]][first-timers]\n\nWhether your own or someone else\u2019s writing, **alex** helps you find gender\nfavoring, polarizing, race related, religion inconsiderate, or other **unequal**\nphrasing in text.\n\nFor example, when `We\u2019ve confirmed his identity` is given, **alex** will warn\nyou and suggest using `their` instead of `his`.\n\nGive **alex** a spin on the [Online demo \u00bb][demo].\n\n## Why\n\n*   [x] Helps to get better at considerate writing\n*   [x] Catches many possible offences\n*   [x] Suggests helpful alternatives\n*   [x] Reads plain text, HTML, MDX, or markdown as input\n*   [x] Stylish\n\n## Install\n\nUsing [npm][] (with [Node.js][node]):\n\n```sh\n$ npm install alex --global\n```\n\nUsing [yarn][]:\n\n```sh\n$ yarn global add alex\n```\n\nOr you can follow this step-by-step tutorial:\n[Setting up alex in your project][setup-tutorial]\n\n<!--alex disable wacko stupid-->\n\n## Contents\n\n*   [Checks](#checks)\n*   [Integrations](#integrations)\n*   [Ignoring files](#ignoring-files)\n    *   [`.alexignore`](#alexignore)\n*   [Control](#control)\n*   [Configuration](#configuration)\n*   [CLI](#cli)\n*   [API](#api)\n    *   [`alex(value, config)`](#alexvalue-config)\n    *   [`alex.markdown(value, config)`](#alexmarkdownvalue-config)\n    *   [`alex.mdx(value, config)`](#alexmdxvalue-config)\n    *   [`alex.html(value, config)`](#alexhtmlvalue-config)\n    *   [`alex.text(value, config)`](#alextextvalue-config)\n*   [Workflow](#workflow)\n*   [FAQ](#faq)\n    *   [This is stupid!](#this-is-stupid)\n    *   [alex didn\u2019t check \u201cX\u201d!](#alex-didnt-check-x)\n    *   [Why is this named alex?](#why-is-this-named-alex)\n*   [Further reading](#further-reading)\n*   [Contribute](#contribute)\n*   [Origin story](#origin-story)\n*   [Acknowledgments](#acknowledgments)\n*   [License](#license)\n\n## Checks\n\n**alex** checks things such as:\n\n*   Gendered work-titles (if you write `garbageman` alex suggests `garbage\n    collector`; if you write `landlord` alex suggests `proprietor`)\n*   Gendered proverbs (if you write `like a man` alex suggests `bravely`; if you\n    write `ladylike` alex suggests `courteous`)\n*   Ableist language (if you write `learning disabled` alex suggests `person\n    with learning disabilities`)\n*   Condescending language (if you write `obviously` or `everyone knows` alex\n    warns about it)\n*   Intolerant phrasing (if you write `master` and `slave` alex suggests\n    `primary` and `replica`)\n*   Profanities (if you write `butt` \ud83c\udf51 alex warns about it)\n\n\u2026and much more!\n\nNote: alex assumes good intent: that you don\u2019t mean to offend!\n\nSee [`retext-equality`][equality] and [`retext-profanities`][profanities] for\nall rules.\n\n**alex** ignores words meant literally, so `\u201che\u201d`, `He \u2014 ...`, and [the\nlike][literals] are not warned about.\n\n## Integrations\n\n*   Atom \u2014 [`get-alex/atom-linter-alex`](https://github.com/get-alex/atom-linter-alex)\n*   Sublime \u2014 [`sindresorhus/SublimeLinter-contrib-alex`](https://github.com/sindresorhus/SublimeLinter-contrib-alex)\n*   Gulp \u2014 [`dustinspecker/gulp-alex`](https://github.com/dustinspecker/gulp-alex)\n*   Slack \u2014 [`keoghpe/alex-slack`](https://github.com/keoghpe/alex-slack)\n*   Ember \u2014 [`yohanmishkin/ember-cli-alex`](https://github.com/yohanmishkin/ember-cli-alex)\n*   Probot \u2014 [`swinton/linter-alex`](https://github.com/swinton/linter-alex)\n*   GitHub Actions \u2014 [`brown-ccv/alex-recommends`](https://github.com/marketplace/actions/alex-recommends)\n*   GitHub Actions (reviewdog) \u2014 [`reviewdog/action-alex`](https://github.com/marketplace/actions/run-alex-with-reviewdog)\n*   Vim \u2014 [`w0rp/ale`](https://github.com/w0rp/ale)\n*   Browser extension \u2014 [`skn0tt/alex-browser-extension`](https://github.com/skn0tt/alex-browser-extension)\n*   Contentful - [`stefanjudis/alex-js-contentful-ui-extension`](https://github.com/stefanjudis/alex-js-contentful-ui-extension)\n*   Figma - [`nickradford/figma-plugin-alex`](https://github.com/nickradford/figma-plugin-alex)\n*   VSCode - [`tlahmann/vscode-alex`](https://github.com/tlahmann/vscode-alex)\n\n## Ignoring files\n\nThe CLI searches for files with a markdown or text extension when given\ndirectories (so `$ alex .` will find `readme.md` and `path/to/file.txt`).\nTo prevent files from being found, create an [`.alexignore`][alexignore] file.\n\n### `.alexignore`\n\nThe CLI will sometimes [search for files][ignoring-files].\nTo prevent files from being found, add a file named `.alexignore` in one of the\ndirectories above the current working directory (the place you run `alex` from).\nThe format of these files is similar to [`.eslintignore`][eslintignore] (which\nin turn is similar to `.gitignore` files).\n\nFor example, when working in `~/path/to/place`, the ignore file can be in\n`to`, `place`, or `~`.\n\nThe ignore file for [this project itself][.alexignore] looks like this:\n\n```txt\n# `node_modules` is ignored by default.\nexample.md\n```\n\n## Control\n\nSometimes **alex** makes mistakes:\n\n```markdown\nA message for this sentence will pop up.\n```\n\nYields:\n\n```txt\nreadme.md\n  1:15-1:18  warning  `pop` may be insensitive, use `parent` instead  dad-mom  retext-equality\n\n\u26a0 1 warning\n```\n\nHTML comments in Markdown can be used to ignore them:\n\n```markdown\n<!--alex ignore dad-mom-->\n\nA message for this sentence will **not** pop up.\n```\n\nYields:\n\n```txt\nreadme.md: no issues found\n```\n\n`ignore` turns off messages for the thing after the comment (in this case, the\nparagraph).\nIt\u2019s also possible to turn off messages after a comment by using `disable`, and,\nturn those messages back on using `enable`:\n\n```markdown\n<!--alex disable dad-mom-->\n\nA message for this sentence will **not** pop up.\n\nA message for this sentence will also **not** pop up.\n\nYet another sentence where a message will **not** pop up.\n\n<!--alex enable dad-mom-->\n\nA message for this sentence will pop up.\n```\n\nYields:\n\n```txt\nreadme.md\n  9:15-9:18  warning  `pop` may be insensitive, use `parent` instead  dad-mom  retext-equality\n\n\u26a0 1 warning\n```\n\nMultiple messages can be controlled in one go:\n\n```md\n<!--alex disable he-her his-hers dad-mom-->\n```\n\n\u2026and all messages can be controlled by omitting all rule identifiers:\n\n```md\n<!--alex ignore-->\n```\n\n## Configuration\n\nYou can control **alex** through `.alexrc` configuration files:\n\n```json\n{\n  \"allow\": [\"boogeyman-boogeywoman\"]\n}\n```\n\n\u2026you can use YAML if the file is named `.alexrc.yml` or `.alexrc.yaml`:\n\n```yml\nallow:\n  - dad-mom\n```\n\n\u2026you can also use JavaScript if the file is named `.alexrc.js`:\n\n```js\n// But making it random like this is a bad idea!\nexports.profanitySureness = Math.floor(Math.random() * 3)\n```\n\n\u2026and finally it is possible to use an `alex` field in `package.json`:\n\n```txt\n{\n  \u2026\n  \"alex\": {\n    \"noBinary\": true\n  },\n  \u2026\n}\n```\n\nThe `allow` field should be an array of rules or `undefined` (the default is\n`undefined`).  When provided, the rules specified are skipped and not reported.\n\nThe `deny` field should be an array of rules or `undefined` (the default is\n`undefined`).  When provided, *only* the rules specified are reported.\n\nYou cannot use both `allow` and `deny` at the same time.\n\nThe `noBinary` field should be a boolean (the default is `false`).\nWhen turned on (`true`), pairs such as `he and she` and `garbageman or\ngarbagewoman` are seen as errors.\nWhen turned off (`false`, the default), such pairs are okay.\n\nThe `profanitySureness` field is a number (the default is `0`).\nWe use [`cuss`][cuss], which has a dictionary of words that have a rating\nbetween 0 and 2 of how likely it is that a word or phrase is a profanity (not\nhow \u201cbad\u201d it is):\n\n| Rating | Use as a profanity | Use in clean text | Example |\n| ------ | ------------------ | ----------------- | ------- |\n| 2      | likely             | unlikely          | asshat  |\n| 1      | maybe              | maybe             | addict  |\n| 0      | unlikely           | likely            | beaver  |\n\nThe `profanitySureness` field is the minimum rating (including) that you want to\ncheck for.\nIf you set it to `1` (maybe) then it will warn for level `1` *and* `2` (likely)\nprofanities, but not for level `0` (unlikely).\n\n## CLI\n\n<!--alex enable wacko stupid-->\n\n![][screenshot]\n\nLet\u2019s say `example.md` looks as follows:\n\n```markdown\nThe boogeyman wrote all changes to the **master server**. Thus, the slaves\nwere read-only copies of master. But not to worry, he was a cripple.\n```\n\nNow, run **alex** on `example.md`:\n\n```sh\n$ alex example.md\n```\n\nYields:\n\n```txt\nexample.md\n   1:5-1:14  warning  `boogeyman` may be insensitive, use `boogeymonster` instead                boogeyman-boogeywoman  retext-equality\n  1:42-1:48  warning  `master` / `slaves` may be insensitive, use `primary` / `replica` instead  master-slave           retext-equality\n  1:69-1:75  warning  Don\u2019t use `slaves`, it\u2019s profane                                           slaves                 retext-profanities\n  2:52-2:54  warning  `he` may be insensitive, use `they`, `it` instead                          he-she                 retext-equality\n  2:61-2:68  warning  `cripple` may be insensitive, use `person with a limp` instead             gimp                   retext-equality\n\n\u26a0 5 warnings\n```\n\nSee `$ alex --help` for more information.\n\n> When no input files are given to **alex**, it searches for files in the\n> current directory, `doc`, and `docs`.\n> If `--mdx` is given, it searches for `mdx` extensions.\n> If `--html` is given, it searches for `htm` and `html` extensions.\n> Otherwise, it searches for `txt`, `text`, `md`, `mkd`, `mkdn`, `mkdown`,\n> `ron`, and `markdown` extensions.\n\n## API\n\n[npm][]:\n\n```sh\n$ npm install alex --save\n```\n\n**alex** is also available as an AMD, CommonJS, and globals module,\n[uncompressed and compressed][releases].\n\n### `alex(value, config)`\n\n### `alex.markdown(value, config)`\n\nCheck Markdown (ignoring syntax).\n\n###### Parameters\n\n*   `value` ([`VFile`][vfile] or `string`) \u2014 Markdown document\n*   `config` (`Object`, optional) \u2014 See the [Configuration][] section\n\n###### Returns\n\n[`VFile`][vfile].\nYou are probably interested in its [`messages`][vfile-message] property, as\nshown in the example below, because it holds the possible violations.\n\n###### Example\n\n```js\nalex('We\u2019ve confirmed his identity.').messages\n```\n\nYields:\n\n```js\n[\n  [1:17-1:20: `his` may be insensitive, when referring to a person, use `their`, `theirs`, `them` instead] {\n    message: '`his` may be insensitive, when referring to a ' +\n      'person, use `their`, `theirs`, `them` instead',\n    name: '1:17-1:20',\n    reason: '`his` may be insensitive, when referring to a ' +\n      'person, use `their`, `theirs`, `them` instead',\n    line: 1,\n    column: 17,\n    location: { start: [Object], end: [Object] },\n    source: 'retext-equality',\n    ruleId: 'her-him',\n    fatal: false,\n    actual: 'his',\n    expected: [ 'their', 'theirs', 'them' ]\n  }\n]\n```\n\n### `alex.mdx(value, config)`\n\nCheck [MDX][] (ignoring syntax).\n\n> Note: the syntax for [MDX\\@2][mdx-next], while currently in beta, is used in\n> alex.\n\n###### Parameters\n\n*   `value` ([`VFile`][vfile] or `string`) \u2014 MDX document\n*   `config` (`Object`, optional) \u2014 See the [Configuration][] section\n\n###### Returns\n\n[`VFile`][vfile].\n\n###### Example\n\n```js\nalex.mdx('<Component>He walked to class.</Component>').messages\n```\n\nYields:\n\n```js\n[\n  [1:12-1:14: `He` may be insensitive, use `They`, `It` instead] {\n    reason: '`He` may be insensitive, use `They`, `It` instead',\n    line: 1,\n    column: 12,\n    location: { start: [Object], end: [Object] },\n    source: 'retext-equality',\n    ruleId: 'he-she',\n    fatal: false,\n    actual: 'He',\n    expected: [ 'They', 'It' ]\n  }\n]\n```\n\n### `alex.html(value, config)`\n\nCheck HTML (ignoring syntax).\n\n###### Parameters\n\n*   `value` ([`VFile`][vfile] or `string`) \u2014 HTML document\n*   `config` (`Object`, optional) \u2014 See the [Configuration][] section\n\n###### Returns\n\n[`VFile`][vfile].\n\n###### Example\n\n```js\nalex.html('<p class=\"black\">He walked to class.</p>').messages\n```\n\nYields:\n\n```js\n[\n  [1:18-1:20: `He` may be insensitive, use `They`, `It` instead] {\n    message: '`He` may be insensitive, use `They`, `It` instead',\n    name: '1:18-1:20',\n    reason: '`He` may be insensitive, use `They`, `It` instead',\n    line: 1,\n    column: 18,\n    location: { start: [Object], end: [Object] },\n    source: 'retext-equality',\n    ruleId: 'he-she',\n    fatal: false,\n    actual: 'He',\n    expected: [ 'They', 'It' ]\n  }\n]\n```\n\n### `alex.text(value, config)`\n\nCheck plain text (as in, syntax is checked).\n\n###### Parameters\n\n*   `value` ([`VFile`][vfile] or `string`) \u2014 Text document\n*   `config` (`Object`, optional) \u2014 See the [Configuration][] section\n\n###### Returns\n\n[`VFile`][vfile].\n\n###### Example\n\n```js\nalex('The `boogeyman`.').messages // => []\n\nalex.text('The `boogeyman`.').messages\n```\n\nYields:\n\n```js\n[\n  [1:6-1:15: `boogeyman` may be insensitive, use `boogeymonster` instead] {\n    message: '`boogeyman` may be insensitive, use `boogeymonster` instead',\n    name: '1:6-1:15',\n    reason: '`boogeyman` may be insensitive, use `boogeymonster` instead',\n    line: 1,\n    column: 6,\n    location: Position { start: [Object], end: [Object] },\n    source: 'retext-equality',\n    ruleId: 'boogeyman-boogeywoman',\n    fatal: false,\n    actual: 'boogeyman',\n    expected: [ 'boogeymonster' ]\n  }\n]\n```\n\n## Workflow\n\nThe recommended workflow is to add **alex** to `package.json` and to run it with\nyour tests in Travis.\n\nYou can opt to ignore warnings through [alexrc][configuration] files and\n[control comments][control].\n\nA `package.json` file with [npm scripts][npm-scripts], and additionally using\n[AVA][] for unit tests, could look like so:\n\n```json\n{\n  \"scripts\": {\n    \"test-api\": \"ava\",\n    \"test-doc\": \"alex\",\n    \"test\": \"npm run test-api && npm run test-doc\"\n  },\n  \"devDependencies\": {\n    \"alex\": \"^1.0.0\",\n    \"ava\": \"^0.1.0\"\n  }\n}\n```\n\nIf you\u2019re using Travis for continuous integration, set up something like the\nfollowing in your `.travis.yml`:\n\n```diff\n script:\n - npm test\n+- alex --diff\n```\n\nMake sure to still install alex though!\n\nIf the `--diff` flag is used, and Travis is detected, lines that are not changes\nin this push are ignored.\nUsing this workflow, you can merge PRs if it has warnings, and then if someone\nedits an entirely different file, they won\u2019t be bothered about existing\nwarnings, only about the things they added!\n\n## FAQ\n\n<!--lint disable no-heading-punctuation-->\n\n<!--alex ignore wacko stupid-->\n\n### This is stupid!\n\nNot a question.\nAnd yeah, alex isn\u2019t very smart.\nPeople are much better at this.\nBut people make mistakes, and alex is there to help.\n\n### alex didn\u2019t check \u201cX\u201d!\n\nSee [`contributing.md`][contributing] on how to get \u201cX\u201d checked by alex.\n\n### Why is this named alex?\n\nIt\u2019s a nice unisex name, it was free on npm, I like it!  :smile:\n\n<!--lint enable no-heading-punctuation-->\n\n## Further reading\n\nNo automated tool can replace studying inclusive communication and listening to\nthe lived experiences of others.\nAn error from `alex` can be an invitation to learn more.\nThese resources are a launch point for deepening your own understanding and\neditorial skills beyond what `alex` can offer:\n\n*   The [18F Content Guide](https://content-guide.18f.gov/our-style/inclusive-language/)\n    has a helpful list of links to other inclusive language guides used in\n    journalism and academic writing.\n*   The [Conscious Style Guide](https://consciousstyleguide.com/articles/) has\n    articles on many nuanced topics of language.  For example, the terms race\n    and ethnicity mean different things, and choosing the right word is up to\n    you.\n    Likewise, a sentence that overgeneralizes about a group of people\n    (e.g. \u201cDevelopers love to code all day\u201d) may not be noticed by `alex`, but\n    it is not inclusive.  A good human editor can step up to the challenge and\n    find a better way to phrase things.\n*   Sometimes, the only way to know what is inclusive is to ask.\n    In [Disability is a nuanced thing](https://incl.ca/disability-language-is-a-nuanced-thing/),\n    Nicolas Steenhout writes about how person-first language, such as\n    \u201ca person with a disability,\u201d is not always the right choice.\n*   Language is always evolving.  A term that is neutral one year ago can be\n    problematic today.  Projects like the\n    [Self-Defined Dictionary](https://github.com/selfdefined/web-app) aim to\n    collect the words that we use to define ourselves and others, and connect\n    them with the history and some helpful advice.\n*   Unconsious bias is present in daily decisions and conversations and can show\n    up in writing.\n    [Textio](https://textio.com/blog/4-overlooked-types-of-bias-in-business-writing/27521593662)\n\n    offers some examples of how descriptive adjective choice and tone can push\n    some people away, and how regional language differences can cause confusion.\n*   Using complex sentences and uncommon vocabulary can lead to less inclusive\n    content.  This is described as literacy exclusion in\n    [this article by Harver](https://harver.com/blog/inclusive-job-descriptions/).\n    This is critical to be aware of if your content has a global audience,\n    where a reader\u2019s strongest language may not be the language you are writing\n    in.\n\n## Contribute\n\nSee [`contributing.md`][contributing] in [`get-alex/.github`][health] for ways\nto get started.\nSee [`support.md`][support] for ways to get help.\n\nThis project has a [Code of conduct][coc].\nBy interacting with this repository, organization, or community you agree to\nabide by its terms.\n\n## Origin story\n\nThanks to [**@iheanyi**][iheany] for [raising the problem][tweet] and\n[**@sindresorhus**][sindre] for inspiring me ([**@wooorm**][wooorm]) to do\nsomething about it.\n\nWhen alex launched, it got some traction on [twitter][] and [producthunt][].\nThen there was a [lot][tnw] [of][dailydot] [press][vice] [coverage][bustle].\n\n## Acknowledgments\n\nPreliminary work for alex was done [in 2015][preliminary].\nThe project was authored by [**@wooorm**][wooorm].\n\nLots of [people helped since][contributors]!\n\n## License\n\n[MIT][license] \u00a9 [Titus Wormer][author]\n\n<!-- Definitions. -->\n\n[build]: https://github.com/get-alex/alex/actions\n\n[build-badge]: https://github.com/get-alex/alex/workflows/main/badge.svg\n\n[coverage]: https://codecov.io/github/get-alex/alex\n\n[coverage-badge]: https://img.shields.io/codecov/c/github/get-alex/alex.svg\n\n[first-timers]: https://www.firsttimersonly.com/\n\n[first-timers-badge]: https://img.shields.io/badge/first--timers--only-friendly-blue.svg\n\n[node]: https://nodejs.org/en/download/\n\n[npm]: https://docs.npmjs.com/cli/install\n\n[yarn]: https://yarnpkg.com/\n\n[setup-tutorial]: https://dev.to/meeshkan/setting-up-the-alex-js-language-linter-in-your-project-3bpl\n\n[demo]: http://alexjs.com/#demo\n\n[screenshot]: screenshot.png\n\n[releases]: https://github.com/get-alex/alex/releases\n\n[vfile]: https://github.com/vfile/vfile\n\n[profanities]: https://github.com/retextjs/retext-profanities/blob/main/rules.md\n\n[equality]: https://github.com/retextjs/retext-equality/blob/main/rules.md\n\n[vfile-message]: https://github.com/vfile/vfile#vfilemessages\n\n[literals]: https://github.com/syntax-tree/nlcst-is-literal#isliteralparent-index\n\n[eslintignore]: http://eslint.org/docs/user-guide/configuring.html#ignoring-files-and-directories\n\n[cuss]: https://github.com/words/cuss\n\n[npm-scripts]: https://docs.npmjs.com/misc/scripts\n\n[ava]: http://ava.li\n\n[author]: http://wooorm.com\n\n[health]: https://github.com/get-alex/.github\n\n[contributing]: https://github.com/get-alex/.github/blob/master/contributing.md\n\n[support]: https://github.com/get-alex/.github/blob/master/support.md\n\n[coc]: https://github.com/get-alex/.github/blob/master/code-of-conduct.md\n\n[tweet]: https://twitter.com/kwuchu/status/618799087006130176\n\n[twitter]: https://twitter.com/wooorm/status/639123753490907136\n\n[producthunt]: https://www.producthunt.com/posts/alex\n\n[tnw]: http://thenextweb.com/apps/2015/09/11/alex-stops-you-from-publishing-inconsiderate-content/\n\n[vice]: https://www.vice.com/en_us/article/nzeawx/meet-alex-the-javascript-tool-to-make-your-code-less-offensive\n\n[bustle]: https://www.bustle.com/articles/108684-alex-javascript-tool-corrects-harmful-language-in-your-writing-because-there-are-some-mistakes-spell-check\n\n[dailydot]: https://www.dailydot.com/debug/alex-coding-tool-offensive/\n\n[iheany]: https://github.com/iheanyi\n\n[sindre]: https://github.com/sindresorhus\n\n[wooorm]: https://github.com/wooorm\n\n[preliminary]: https://github.com/get-alex/alex/commit/3621b0a\n\n[contributors]: https://github.com/get-alex/alex/graphs/contributors\n\n[.alexignore]: .alexignore\n\n[license]: license\n\n[control]: #control\n\n[configuration]: #configuration\n\n[ignoring-files]: #ignoring-files\n\n[alexignore]: #alexignore\n\n[mdx]: https://mdxjs.com\n\n[mdx-next]: https://github.com/mdx-js/mdx/issues/1041\n"
 },
 {
  "repo": "coralproject/talk",
  "language": "TypeScript",
  "readme_contents": "<p align=\"center\">\n  <a href=\"https://coralproject.net\" target=\"_blank\"><img width=\"250\" src=\"https://docs.coralproject.net/coral/images/coralproject_by_voxmedia.svg\" alt=\"Coral by Vox Media\" /></a>\n</p>\n\n<p align=\"center\">\n  A better commenting experience from <a href=\"https://product.voxmedia.com/\" target=\"_blank\">Vox Media</a>.\n</p>\n\n<p align=\"center\">\n  <a href=\"https://circleci.com/gh/coralproject/talk\" target=\"_blank\"><img src=\"https://img.shields.io/circleci/build/gh/coralproject/talk?style=flat-square\" alt=\"CircleCI\" /></a>\n  <a href=\"https://hub.docker.com/r/coralproject/talk\" target=\"_blank\"><img src=\"https://img.shields.io/docker/v/coralproject/talk?label=docker%20hub&sort=semver&style=flat-square\" alt=\"Docker Image Version\" /></a>\n  <a href=\"https://hub.docker.com/r/coralproject/talk\" target=\"_blank\"><img src=\"https://img.shields.io/docker/image-size/coralproject/talk?label=docker%20image%20size&sort=semver&style=flat-square\" alt=\"Docker Image Size\" /></a>\n  <a href=\"https://twitter.com/coralproject\" target=\"_blank\"><img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/coralproject?style=flat-square\"></a>\n</p>\n\n## Description\n\nOnline comments are broken. Our open-source commenting platform,\n[Coral](https://coralproject.net), rethinks how moderation, comment display, and\nconversation function, creating the opportunity for safer, smarter discussions\naround your work.\n\nWe offer hosting and support packages for Coral, as well as exclusive,\ncustomer-only features. [Contact us](https://coralproject.net/pricing/) for more\ninformation or [sign up for a webinar](https://coralproject.net).\n\n## Documentation\n\nIf you're new to Coral, the [Coral documentation](https://docs.coralproject.net/)\nis a great place to start running and developing with Coral.\n\nYou\u2019ve installed Coral, and you\u2019re preparing to launch it on your site. The real\ncommunity work starts now, before you go live. You have a unique opportunity\npre-launch to set your community up for success. Read our\n[Community Guides](https://guides.coralproject.net/start-here/) to learn more.\n\n## Support\n\nWe can help you set up Coral, migrate your comments from another system,\nintegrate your registration platform, pair with your programmers, and help you\nwith bespoke installs. To learn more, [contact us](https://coralproject.net/pricing/).\n\n## Contributing\n\nCoral is a Apache-2.0 licensed open-source project built with <3 by the Coral\nteam, a part of [Vox Media](https://product.voxmedia.com/).\n\nIf you are interested in contributing to Coral, check out our [Contributor's Guide](CONTRIBUTING.md).\n\n## License\n\nCoral is [Apache-2.0 licensed](LICENSE).\n"
 },
 {
  "repo": "hotosm/tasking-manager",
  "language": "JavaScript",
  "readme_contents": "# Tasking Manager\n\nThe most popular tool for teams to coordinate mapping on OpenStreetMap.\n\n[<img src=\"screenshot.jpg\" />](./screenshot.jpg)\n\nWith this web application an area of interest can be defined and divided up into smaller tasks that can be completed rapidly. It shows which areas need to be mapped and which areas need a review for quality assurance. You can see the tool in action: log into the widely used [HOT Tasking Manager](https://tasks.hotosm.org/) and start mapping.\n\n[<img src=\"./docs/assets/project-view.gif\" />](./docs/assets/project-view.gif)\n\nThis is Free and Open Source Software. You are welcome to use the code and set up your own instance. The Tasking Manager has been initially designed and built by and for the [Humanitarian OpenStreetMap Team](https://www.hotosm.org/), and is nowadays used by many communities and organizations.\n\n## Get involved!\n\n* Get familiar with our [contributor guidelines](./docs/contributing.md)\n* Join the [working groups](./docs/working-groups.md)\n* Help us to [translate the user interface](./docs/contributing-translation.md)\n\n## Developers\n\n* [Install TM with Docker](./docs/setup-docker.md)\n* [Setup the TM for development](./docs/setup-development.md)\n* [Learn about migrations between major versions](./docs/migration.md)\n* Help us and submit [pull requests](https://github.com/hotosm/tasking-manager/pulls)\n"
 },
 {
  "repo": "OptiKey/OptiKey",
  "language": "C#",
  "readme_contents": "# OptiKey\n\nOptiKey is an on-screen keyboard that is designed to help Motor Neuron Disease (MND) patients interact with Windows computers. When used with an eye-tracking device, OptiKey's on-screen keyboard allows MND patients to complete tasks, such as email composition, using only their eyes. OptiKey can also be used with a mouse or webcam. Unlike expensive and unreliable Alternative and Augmentative Communication (AAC) products that are difficult to use, Optikey is free, reliable, and easy to use.\n\n# Getting Started\n\n[**The OptiKey Wiki**](https://github.com/OptiKey/OptiKey/wiki) contains OptiKey's user guides, installation and system requirements, and additional support information. OptiKey's Windows installer can be downloaded from the [latest release](https://github.com/JuliusSweetland/OptiKey/releases/latest). To get an understanding of OptiKey's use, users should watch [Optikey's introduction video](https://www.youtube.com/watch?v=HLkyORh7vKk).\n\n# Supported Platforms\n\nOptiKey uses the .Net 4.6 Framework and is designed to run on Windows 8 / 8.1 / 10.\n\n# Problems?\n\nIf users encounter an issue with OptiKey, such as a software crash or an unexpected behaviour, users should add an issue ticket to [OptiKey's issue tracker](https://github.com/OptiKey/OptiKey/issues). Users are encouraged to check if their issue is already being tracked by OptiKey before creating a new issue ticket.\n\nThe following information should be specified in an issue ticket:\n\n* How OptiKey was being used\n* What the user expected to happen\n* What the user actually experienced\n* Steps to reproduce the issue\n* Any other information that helps to describe and/or reproduce the problem (ex. screenshots)\n\n# License\n\nLicensed under the GNU GENERAL PUBLIC LICENSE (Version 3, 29th June 2007)\n\n# Contributing\n\nContributions to this project are always welcome. If you'd like to help translate Optikey to another language, fix bugs or add new features, check out our [Contributing page](https://github.com/OptiKey/OptiKey/wiki/Contribute). \n\n# Contact\n\nTo ask a question, or to discuss information that is not on the [**OptiKey Wiki**](https://github.com/JuliusSweetland/OptiKey/wiki/), please use <optikeyfeedback@gmail.com> to contact Julius.\n\n[![Build status](https://dev.azure.com/optikey/optikey/_apis/build/status/OptiKey.OptiKey)](https://dev.azure.com/optikey/optikey/_build/latest?definitionId=-1)\n"
 },
 {
  "repo": "ifmeorg/ifme",
  "language": "Ruby",
  "readme_contents": "[![CircleCI](https://circleci.com/gh/ifmeorg/ifme/tree/main.svg?style=svg)](https://circleci.com/gh/ifmeorg/ifme/tree/main)\n[![Code Climate](https://codeclimate.com/github/ifmeorg/ifme/badges/gpa.svg)](https://codeclimate.com/github/ifmeorg/ifme)\n[![Test Coverage](https://api.codeclimate.com/v1/badges/f9444a4d4116720518fe/test_coverage)](https://codeclimate.com/github/ifmeorg/ifme/test_coverage)\n[![Backers on Open Collective](https://opencollective.com/ifme/backers/badge.svg)](#backers)\n[![Sponsors on Open Collective](https://opencollective.com/ifme/sponsors/badge.svg)](#sponsors)\n\nREADME en: [Ingles](https://github.com/ifmeorg/ifme/blob/main/README.md), [Portugu\u00e9s](https://github.com/ifmeorg/ifme/blob/main/README-PT.md), [Franc\u00e9s](https://github.com/ifmeorg/ifme/blob/main/README-FR.md), [Coreano](https://github.com/ifmeorg/ifme/blob/main/README-KO.md), [Indonesio](https://github.com/ifmeorg/ifme/blob/main/README-ID.md), [Turco](https://github.com/ifmeorg/ifme/blob/main/README-TR.md)\n\n# if-me.org\n\n[if-me.org](https://www.if-me.org/) una comunidad para experiencias de salud mental que alienta a las personas a compartir sus historias personales con aliados de confianza. Los aliados de confianza son las personas con las que interactuamos a diario, incluidos amigos, familiares, compa\u00f1eros de trabajo, maestros y trabajadores de salud mental.\n\nTratar con la salud mental es lo que nos hace humanos. Pero para muchos de nosotros, es una lucha ser abierto al respecto. No todos son consejeros o terapeutas. Las personas con las que interactuamos todos los d\u00edas dan forma a nuestras emociones y comportamiento. Involucrarlos en el tratamiento de la salud mental es la clave para la recuperaci\u00f3n.\n\nEl sitio en vivo se puede encontrar en [if-me.org](https://www.if-me.org/). Los sistemas de dise\u00f1o en vivo se pueden encontrar un [design.if-me.org](http://design.if-me.org/).\n\nUsamos el maravilloso [Pacto de Colaboradores](http://contributor-covenant.org) for\nopara nuestro c\u00f3digo de conducta. Por favor\n[l\u00e9elo](https://github.com/ifmeorg/ifme/blob/main/code_of_conduct.md)\nantes de unirte a nuestro proyecto.\n\n**Lea sobre nuestros objetivos de proyecto y c\u00f3mo contribuir (no solo como desarrollador) [aqu\u00ed](https://github.com/ifmeorg/ifme/blob/main/CONTRIBUTING.md).**\n\n## Documentaci\u00f3n\n\nPor favor, consulte nuestra [Wiki](https://github.com/ifmeorg/ifme/wiki)  para la documentaci\u00f3n completa.\n\n### Instalaci\u00f3n\n\nInformaci\u00f3n sobre la instalaci\u00f3n y configuraci\u00f3n de la aplicaci\u00f3n [aqu\u00ed](https://github.com/ifmeorg/ifme/wiki/Installation). Las instancias de prueba, desarrollo y producci\u00f3n est\u00e1n cubiertas.\n\n### Colaborador Blurb\n\nSe recomienda a todos que se [agreguen](https://github.com/ifmeorg/ifme/wiki/Contributor-Blurb) a nuestra p\u00e1gina de Contribuci\u00f3n.\n\n## Donar\n\nTambi\u00e9n agradecemos las contribuciones financieras con total transparencia en nuestro\n[Open Collective](https://opencollective.com/ifme).\n . Cualquiera puede presentar un gasto. Si el gasto tiene sentido para el desarrollo de la comunidad, ser\u00e1 \"fusionado\" en el libro mayor de nuestro Colectivo Abierto por los contribuyentes principales y la persona que present\u00f3 el gasto ser\u00e1 reembolsada.\n\nTambi\u00e9n tenemos una p\u00e1gina de [Patreon](https://www.patreon.com/ifme) donde puedes hacer donaciones mensuales.\n\n### Patrocinadores\n\n\u00a1Gracias a nuestros patrocinadores de Patreon, [Rob Drimmie](https://www.patreon.com/user?u=3251857),\n[Joseph D. Marhee](https://www.patreon.com/user?u=2899171) y\n[Carol Willing](https://www.patreon.com/user?u=202458)!\n\n\u00a1Gracias a todos nuestros patrocinadores de Open Collective!\n[Become a backer!](https://opencollective.com/ifme#backer)\n\n<a href=\"https://opencollective.com/ifme#backers\" target=\"_blank\"><img src=\"https://opencollective.com/ifme/backers.svg?width=890\"></a>\n\n### Los patrocinadores\n\n\u00a1Gracias a todos nuestros sponsors! (Por favor, p\u00eddale a su compa\u00f1\u00eda que tambi\u00e9n respalde este proyecto de c\u00f3digo abierto al  [convertirse en patrocinador](https://opencollective.com/ifme#sponsor))\n\n<section role=\"presentation\">\n <a href=\"https://opencollective.com/ifme/sponsor/0/website\" target=\"_blank\"><img src=\"https://opencollective.com/ifme/sponsor/0/avatar.svg\"></a>\n <a href=\"https://opencollective.com/ifme/sponsor/1/website\" target=\"_blank\"><img src=\"https://opencollective.com/ifme/sponsor/1/avatar.svg\"></a>\n <a href=\"https://opencollective.com/ifme/sponsor/2/website\" target=\"_blank\"><img src=\"https://opencollective.com/ifme/sponsor/2/avatar.svg\"></a>\n <a href=\"https://opencollective.com/ifme/sponsor/3/website\" target=\"_blank\"><img src=\"https://opencollective.com/ifme/sponsor/3/avatar.svg\"></a>\n <a href=\"https://opencollective.com/ifme/sponsor/4/website\" target=\"_blank\"><img src=\"https://opencollective.com/ifme/sponsor/4/avatar.svg\"></a>\n <a href=\"https://opencollective.com/ifme/sponsor/5/website\" target=\"_blank\"><img src=\"https://opencollective.com/ifme/sponsor/5/avatar.svg\"></a>\n <a href=\"https://opencollective.com/ifme/sponsor/6/website\" target=\"_blank\"><img src=\"https://opencollective.com/ifme/sponsor/6/avatar.svg\"></a>\n <a href=\"https://opencollective.com/ifme/sponsor/7/website\" target=\"_blank\"><img src=\"https://opencollective.com/ifme/sponsor/7/avatar.svg\"></a>\n <a href=\"https://opencollective.com/ifme/sponsor/8/website\" target=\"_blank\"><img src=\"https://opencollective.com/ifme/sponsor/8/avatar.svg\"></a>\n <a href=\"https://opencollective.com/ifme/sponsor/9/website\" target=\"_blank\"><img src=\"https://opencollective.com/ifme/sponsor/9/avatar.svg\"></a>\n</section>\n\n## Licencia\n\nEl c\u00f3digo fuente est\u00e1 licenciado bajo GNU AGPLv3. Para obtener m\u00e1s informaci\u00f3n, consulte http://www.gnu.org/licenses/agpl-3.0.txt or [LICENSE.txt](https://github.com/ifmeorg/ifme/blob/main/LICENSE.txt).\n"
 },
 {
  "repo": "RefugeRestrooms/refugerestrooms",
  "language": "Ruby",
  "readme_contents": "Production CI: [![Build Status](https://travis-ci.com/RefugeRestrooms/refugerestrooms.svg?branch=master)](https://travis-ci.com/RefugeRestrooms/refugerestrooms)\n\nDevelop CI: [![Build Status](https://travis-ci.com/RefugeRestrooms/refugerestrooms.svg?branch=develop)](https://travis-ci.com/RefugeRestrooms/refugerestrooms)\n\nCode Climate: [![Maintainability](https://api.codeclimate.com/v1/badges/a641d46a4ad2c2f01932/maintainability)](https://codeclimate.com/github/RefugeRestrooms/refugerestrooms/maintainability) [![Test Coverage](https://api.codeclimate.com/v1/badges/a641d46a4ad2c2f01932/test_coverage)](https://codeclimate.com/github/RefugeRestrooms/refugerestrooms/test_coverage)\n\n\nWaffle.io: [![Stories in Ready](https://badge.waffle.io/RefugeRestrooms/refugerestrooms.png?label=ready)](https://waffle.io/RefugeRestrooms/refugerestrooms)\n\nBugsnag Open Source Bug Tracking:\n[Bugsnag](https://www.bugsnag.com)\n<br>\n<img src=\"https://global-uploads.webflow.com/5c741219fd0819540590e785/5c741219fd0819856890e790_asset%2039.svg\" width=\"250\" />\n\n# REFUGE restrooms\n\nProviding safe restroom access to transgender, intersex, and gender nonconforming individuals.\n\nREFUGE is an effort to fill the void left by the now-defunct Safe2Pee website. It provides a free resource to trans\\* and queer individuals in need of gender neutral and other safe restrooms.\n\nThis project is open source. Feel free to contribute. We could use the help.\n\n## Deployed Environments\nProduction: [Link](http://www.refugerestrooms.org)\n\nStaging: [Link](http://staging.refugerestrooms.org)\n\n## Contributing\n\nFor more information on how to contribute to Refuge Restrooms, or how the technology works, see the [Wiki](https://github.com/RefugeRestrooms/refugerestrooms/wiki).\n\nIf you just want to get your environment set up for making changes locally and testing, you can head directly to [CONTRIBUTING.md](https://github.com/RefugeRestrooms/refugerestrooms/blob/develop/CONTRIBUTING.md).\n\nPlease also read our [Code of Conduct](https://github.com/RefugeRestrooms/refugerestrooms/blob/develop/CODE_OF_CONDUCT.md), which gives guidance on our standards of community and interaction.\n\n## Tech\n\n* Ruby Version - ruby-2.5.8\n* Ruby on Rails\n* RSpec\n* Javascript\n* HTML / SASS\n* Postgres\n* Geocoder Gem\n* Google Maps API\n* Twitter Bootstrap Framework\n* Deployed on Heroku\n\n## Links to Refuge project on other platforms\n\n- [SMS messaging Twilio Application](https://github.com/RefugeRestrooms/refugerest_sms)\n- [Android Native Application](https://github.com/RefugeRestrooms/refugerestrooms-android)\n- [iOS Native Application](https://github.com/RefugeRestrooms/refuge-ios)\n- [Yo Application](https://github.com/raptortech-js/YoRestrooms)\n\n## Slack\n\nIf you want to join the Refuge Restrooms Slack channel, you can do so by [clicking on this link.](https://join.slack.com/t/refugelgbt/shared_invite/zt-3zaagpad-DvyfAPcepuRzFBJix1uYkg)\n\n## License\n\nCopyright (C) 2014\u20132017 Teagan Widmer and contributors\n\nThis program is free software; you can redistribute and/or modify\nit under the terms of the GNU Affero General Public License as published by\nthe Free Software Foundation; either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n"
 },
 {
  "repo": "hurricane-response/florence-api",
  "language": "Ruby",
  "readme_contents": "# The Hurricane Response API\n\nThis API Server is the backend data management dashboard for mapping community resources such as Shelter and Food Distribution Points in disaster-effected regions.  It has importers for data from a FEMA managed GeoServer that contains data supplied by the RedCross, and user interfaces for manually updating.\n\nThere are planned features to have crowdsourced update tools to help the community update the data in real time as shelter/distribution point records become stale.\n\n## History\n\nThe Hurricane API was born out of necessity during Hurricane Harvey in 2017 when Houston, TX was innundated with a storm and people struggled to location shelters.  The developers at Sketch City took it upon themselves to crowdsource a solution.  This software is the product of their efforts and has continued to be developed by other Code for America brigade members.\n\n## Objective\n\n* We serve GeoJSON to mapping front-ends\n* We help client applications help those affected by hurricanes and other natural disasters.\n\n## Developer Quick Links\n\n* [CONTRIBUTORS](https://github.com/hurricane-response/florence-api/graphs/contributors)\n* [GETTING STARTED/CONTRIBUTING](CONTRIBUTING.md)\n* [API ENDPOINTS](API_ENDPOINT_SPECIFICATION.md)\n* [LICENSE](#license)\n* [CODE OF CONDUCT](CODE_OF_CONDUCT.md)\n* [CHANGELOG](CHANGELOG.md)\n\n## Thanks to\n\n\n\nBut the API wouldn't mean anything without our volunteers:\n\n* [Entire Sketch-City organization](http://sketchcity.org/)\n* [Code for America](https://www.codeforamerica.org/)\n* [Source Code Collaborators](https://api.hurricane-response.org/contributors.html)\n\n## LICENSE\n\n### Software Code\n\nThis system's software code is licensed under the GPLv3.\n\nFull license available in [LICENSE](LICENSE)\n\n### Data and Content\n\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img\nalt=\"Creative Commons License\" style=\"border-width:0\"\nsrc=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This\nwork is licensed under a <a rel=\"license\"\nhref=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons\nAttribution-NonCommercial-ShareAlike 4.0 International License</a>.\n"
 },
 {
  "repo": "Terrastories/terrastories",
  "language": "Ruby",
  "readme_contents": "![Terrastories](https://www.amazonteam.org/wp-content/uploads/2018/09/logo-1170x164.png)\n\n## Table of Contents\n\n1. [About Terrastories](#about-terrastories)\n\n2. [Setup Terrastories](#setup)\n\n3. [Customize Terrastories](#customize-terrastories)\n\n4. [Developing with Terrastories](#developing-with-terrastories)\n\n5. [Contributing Guidelines](#contributing)\n\n## About Terrastories\n\n**Terrastories** is a geostorytelling application built to enable indigenous and other local communities to locate and map their own oral storytelling traditions about places of significant meaning or value to them. Community members can add places and stories through a user-friendly interface, and make decisions about designating certain stories as private or restricted. It is a dockerized Rails app that uses [**Mapbox**](https://mapbox.com) to help users locate content geographically on an interactive map. Terrastories is designed to be entirely offline-compatible, so that remote communities can access the application entirely without needing internet connectivity. \n\nThe Terrastories interface is principally composed of an interactive map and a sidebar with media content. Users can explore the map and click on activated points to see the stories associated with those points. Alternatively, users can interact with the sidebar and click on stories to see where in the landscape these narratives took place. Through an administrative back end, users can also add, edit, and remove stories, or set them as restricted so that they are viewable only with a special login. Users can design and customize the content of the interactive map entirely, and the interface itself is customizable with a color scheme and design reflecting the style of the community.\n\nLearn more about Terrastories at [https://terrastories.io/](https://terrastories.io/).\n\n![](terrastories.gif)\n###### *Terrastories: Matawai Konde 1.0 (October 2018)*\n\n## Setup\n\nBefore you install Terrastories, you should consider the hosting environment for the application. Will it be hosted on an online server? If so, you are likely going to need to set up Terrastories on a Linux server. Are you installing Terrastories on your local machine, either for development or for demoing the app? Depending on what operating system you use, there are different setup guides, below. Lastly, if you are installing Terrastories to work fully offline (i.e. no online maps), there is a special guide for that use case as well.\n\nTo install and run a streamlined version of Terrastories with access to an online map on Mapbox.com, visit one of these links:\n1. [Setup for Mac](documentation/SETUP-MAC.md)\n2. [Setup for Windows](documentation/SETUP-WINDOWS.md)\n3. [Setup for Linux](documentation/SETUP-LINUX.md)\n\nTo install and run Terrastories for offline \"Field Kit\" usage, visit:\n\n4. [Setup for offline](documentation/SETUP-OFFLINE.md)\n\n## Customize Terrastories\n\nTo set up Terrastories with a custom map, languages, visual assets, and to import data, see our [customization guide](documentation/CUSTOMIZATION.md).\n\n## Developing with Terrastories\n\nTo find out how to develop with the Terrastories app, read our [developer guide](documentation/DEVELOPMENT.md) and check out our [Developer Community](https://terrastories.io/community/) pages on the Terrastories website.\n\nFor a general overview of the application as well as a Vision statement and Roadmap, please see our [Wiki](https://github.com/Terrastories/terrastories/wiki).\n\n## Contributing\n\nWe \u2665 contributors! By participating in this project, you agree to abide by the Ruby for Good [Code of Conduct](documentation/CODE_OF_CONDUCT.md).\n\n**First:** if you're unsure or afraid of *anything*, just ask or submit the issue or pull request anyways. You won't be yelled at for giving your best effort. The worst that can happen is that you'll be politely asked to change something. We appreciate any sort of contributions, and don't want a wall of rules to get in the way of that.\n\n### How To Contribute To Terrastories\n\n**Step 1: Learn a little about the app**\nOne of our core contributors @mirandawang wrote a really nice [outline of the app](https://docs.google.com/document/d/1azfvU7tXLv2EHGrc3Hs5SPmB32MkyYuhXTB4JjymlV4/edit). Unless you are working on something related to Docker containers or to map cartography then you will benefit from taking a couple minutes to get acquainted with the app. \n\n**Step 2: Find an issue to work on**\nPlease find an [issue](https://github.com/Terrastories/terrastories/issues) that you would like to take on and comment to assign yourself if no one else has done so already. All issues with the label `status: help wanted` are up for grabs! We will add the `status: claimed` label to the issue to mark it as assigned to you. Also, feel free to ask questions in the issues, and we will get back to you ASAP!\n\n**Step 3: Fork the repo**\nClick the \"fork\" button in the upper right of the Github repo page. A fork is a copy of the repository that allows you to freely explore & experiment without changing the original project. You can learn more about forking a repo in [this article](https://help.github.com/articles/fork-a-repo/).\n\n**Step 4: Create a branch**\nCheckout a new branch for your issue - this branch can be named anything, but we encourage the format  `XXX-brief-description-of-feature`  where  `XXX`  is the issue number.\n\n**Step 5: Happy Hacking!**\nFollow the instructions in the [Setup Document](#setup) to set up your local environment. Feel free to discuss any questions on the issues as needed, and we will get back to you! Don't forget to write some tests to verify your code. Commit your changes locally, using descriptive messages and please be sure to note the parts of the app that are affected by this commit.\n\n**Step 6: Pushing your branch and creating a pull request**\nPush your branch up and create a pull request! Please indicate which issue your PR addresses in the title.\n\n### Code Reviews & Pull Request Merging\nOnce you've submitted a pull request, a core contributor will work with you on doing a code review (typically pretty minor unless it's a very significant PR). If the reviewer gives a \u2705 to the PR merging, then huzzah! Merge into master! If your feature branch was in this main repository (and not forked), please delete your branch after it has been merged.\n\n### Stay Scoped\nTry to keep your PRs limited to one particular issue and don't make changes that are out of scope for that issue. If you notice something that needs attention but is out-of-scope, put a TODO, FIXME, or NOTE comment above it.\n\n### Work In Progress Pull Requests\nSometimes we want to get a PR up there and going so that other people can review it or provide feedback, but maybe it's incomplete. This is OK, but if you do it, please tag your PR with an  `in-progress`  label so that we know not to review / merge it.\n\n### Becoming a Core Contributor\nUsers that are frequent contributors and are involved in discussion may be given direct Contributor access to the Repo so they can submit Pull Requests directly, instead of Forking first. You can join us in Slack [here](https://t.co/kUtI3lnpW1), and find us in the channel #terrastories! :) \n"
 },
 {
  "repo": "rubyforgood/diaper",
  "language": "Ruby",
  "readme_contents": "# Welcome Contributors!\nThanks for checking us out!\n\nIf you're new here, here are some things you should know:\n - We actively curate issues and try to make them as self-contained as possible for people new to the application; those ones are tagged \"Help Wanted\"\n - We're actively watching for Pull Requests and you shouldn't have to wait very long for a review. Try to make sure your build passes (`rubocop -a` is a frequent need) and that you've addressed the requirements in the issue\n - There is a wiki article called [Application Overview](https://github.com/rubyforgood/diaper/wiki/Application-Overview). It needs a tiny bit of updating, but is mostly still accurate. It will introduce you to some vocabulary and general concepts, if you find something confusing and want to find the answer on your own.\n - Check the `CONTRIBUTING.md` file for a guide on how to get started\n - This is a 100% volunteer-supported project, please be patient with your correspondence. We do handle issues and PRs with more fervor during Hacktoberfest & Conferences, but most (all?) of us have day jobs and so responses to questions / pending PRs may not be immediate. Please be patient, we'll get to you! :)\n\nPlease feel free to join us on Slack! You can sign up at https://rubyforgood.herokuapp.com We're in #diaper\n\nThe core team leads are: @edwinmak @albert @gia @sean @scott\nThere are numerous other folks that can chime in and answer questions -- please ask and someone will probably be there to help!\n\n# README\n\n[![Maintainability](https://api.codeclimate.com/v1/badges/f100428ab2af34c142b7/maintainability)](https://codeclimate.com/github/rubyforgood/diaper/maintainability)\n[![Test Coverage](https://api.codeclimate.com/v1/badges/f100428ab2af34c142b7/test_coverage)](https://codeclimate.com/github/rubyforgood/diaper/test_coverage)\n[![Build Status](https://travis-ci.org/rubyforgood/diaper.svg?branch=main)](https://travis-ci.org/rubyforgood/diaper) [![View performance data on Skylight](https://badges.skylight.io/status/LrXHcxDK7Be9.svg)](https://oss.skylight.io/app/applications/LrXHcxDK7Be9)\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Frubyforgood%2Fdiaper.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Frubyforgood%2Fdiaper?ref=badge_shield)\n\n## About\n\nThis application is an inventory management system that is built to address the needs of [Diaper Banks](https://nationaldiaperbanknetwork.org/diaper-need-facts/) as directly and explicitly as possible. Diaper Banks maintain inventory, receive donations and other means of intaking diapers (and related supplies), and issue Distributions to community partner organizations. Like any non-profit, they also need to perform reports on this data, and have day-to-day operational information they need as well. This application aims to serve all those needs, as well as facilitate, wherever possible the general operations of the Diaper Bank themselves (eg. through using barcode readers, scale weighing, inventory audits).\n\nFor a general overview of the application, please see the [Application Overview](https://github.com/rubyforgood/diaper/wiki/Application-Overview) wiki article.\n\n### Origins\n\nThis project took what we built for the [Portland Diaper Bank in 2016](https://github.com/rubyforgood/pdx_diaper) and turned it into a multitenant application, something that all diaper banks can use. We re-used models, code and other documentation where applicable as well as implemented new features and functionality requested by the prime stakeholder (PDXDB). We're super excited to have had Rachel Alston, the director of the Portland Diaper Bank, attending our event in 2017, providing guidance and giving us the best chance of success!\n\n## Development\n\n### Installation Instructions\n\nThe `installation.md` file ([https://github.com/rubyforgood/diaper/blob/main/installation.md](https://github.com/rubyforgood/diaper/blob/main/installation.md)) has detailed instructions for installation and configuration of an Ubuntu host to run this software. Although there is not a document for Mac OS, it may be helpful for that as well.\n\n### Ruby Version\nThis app uses Ruby version 2.7.2, indicated in `/.ruby-version` and `Gemfile`, which will be auto-selected if you use a Ruby versioning manager like `rvm`, `rbenv`, or `asdf`.\n\n### Yarn Installation\nIf you don't have Yarn installed, you can install with Homebrew on macOS `brew install yarn` or visit [https://yarnpkg.com/en/docs/install](https://yarnpkg.com/en/docs/install). Be sure to run `yarn install` after installing Yarn. NOTE: It's possible that Node version 12 may cause you problems, see issue #751. Node 10 or 11 seem to be fine.\n\n### Create your .env with database credentials\nBe sure to create a `.env` file in the root of the app that includes the following lines (change to whatever is appropriate for your system):\n```\nPG_USERNAME=username\nPG_PASSWORD=password\n```\nIf you're getting the error `PG::ConnectionBad: fe_sendauth: no password supplied`, it's because you have probably not done this.\n\n### Database Configuration\nThis app uses PostgreSQL for all environments. You'll also need to create the `dev` and `test` databases, the app is expecting them to be named `diaper_dev` and `diaper_test`, respectively. This should all be handled with `rails db:setup`.\nCreate a `database.yml` file on `config/` directory with your database configurations. You can also copy the existing file called `database.yml.example` as an example and just change the credentials.\n\n## Seed the database\nFrom the root of the app, run `bundle exec rails db:seed`. This will create some initial data to use while testing the app and developing new features, including setting up the default user.\n\n## Start the app\nRun `bundle exec rails s` and browse to http://localhost:3000/\n\n## Login\nTo login to the web application, use these default credentials:\n\n    Organization Admin\n      Email: org_admin1@example.com\n      Password: password\n\n    User\n      Email: user_1@example.com\n      Password: password\n      \n## Connecting Diaper & Partner Apps Together Locally\n\nDepending on the issues or features you decided to undertake, you may need to get both the `diaper` and `partner` app running locally. This is true if you want to build a feature that depends on code changes in both repositories.\n\nPlease follow this [guide](connecting-the-partner-and-diaper.md) to get your local environment setup properly.\n\n## Contributing\n\nPlease feel free to contribute! While we welcome all contributions to this app, pull-requests that address outstanding Issues *and* have appropriate test coverage for them will be strongly prioritized. In particular, addressing issues that are tagged with the next milestone should be prioritized higher.\n\nTo contribute, do these things:\n\n * **Identify an issue** you want to work on that is not currently assigned to anyone\n * **Assign it** to yourself (so that no one else works on it while you are)\n * (If not already a Contributor, fork the repo first)\n * **Checkout a new issue branch** -- there's no absolute requirements on this, but we encourage the branch name format `XXX-brief-description-of-feature` where `XXX` is the issue number.\n * **Do the work** -- discuss any questions on the Issues as needed (we try to be pretty good about answering questions!)\n * (If you created a new model, run `bundle exec annotate` from the root of the app)\n * **Create tests** to provide proof that your work fixes the Issue (if you need help with this, please reach out!)\n * **Commit locally**, using descriptive commit messages that acknowledge, to the best of your ability, the parts of the app that are affected by the commit.\n * **Run the tests** and make sure they run green; if they don't, fix whatever broke so that the tests pass\n * **Final commit** if any tests had to be fixed\n * **Push** up the branch\n * **Create a Pull Request** - Please indicate which issue it addresses in your pull-request title.\n\n### Squashing Commits\nSquashing your own commits before pushing is totally fine. Please don't squash other people's commits. (Everyone who contributes here deserves credit for their work! :) ). Also, consider the balance of \"polluting the git log with commit messages\" vs. \"providing useful detail about the history of changes in the git log\". If you have several (or many) smaller commits that all serve one purpose, and these can be squashed into a single commit whose message describes the thing, you're encouraged to squash.\n\nThere's no hard and fast rule here about this (for now), just use your best judgement.\n\n### Pull Request Merging\n\nAt that point, someone will work with you on doing a code review (typically pretty minor unless it's a very significant PR). If TravisCI gives :+1: to the PR merging, we can then merge your code in; if your feature branch was in this main repository, the branch will be deleted after the PR is merged.\n\n### Stay Scoped\n\nTry to keep your PRs limited to one particular issue and don't make changes that are out of scope for that issue. If you notice something that needs attention but is out-of-scope, [please create a new issue.](https://github.com/rubyforgood/diaper/issues/new)\n\n### Testing\n\nRun all the tests with:\n\n  `bundle exec rspec`\n\nThis app uses RSpec, Capybara, and FactoryBot for testing. Make sure the tests run clean & green before submitting a Pull Request. If you are inexperienced in writing tests or get stuck on one, please reach out so one of us can help you. :)\n\nThe one situation where you probably don't need to write new tests is when simple re-stylings are done (ie. the page may look slightly different but the Test suite is unaffected by those changes).\n\nTip: If you need to skip a failing test, place `pending(\"Reason you are skipping the test\")` into the `it` block rather than skipping with `xit`. This will allow rspec to deliver the error message without causing the test suite to fail.\n\nexample:\n```ruby\n  it \"works!\" do\n    pending(\"Need to implement this\")\n    expect(my_code).to be_valid\n  end\n```\n\n##### Feature specs\n\nIf you need to see a feature spec run in the browser, you can use the following env variable:\n\n```\nNOT_HEADLESS=true bundle exec rspec\n```\n\nKeep in mind that you need js to be enabled. For example:\n\n```\ndescribe \"PickupSheet\", type: :feature, js: true do\n```\n\n### In-flight Pull Requests\n\nSometimes we want to get a PR up there and going so that other people can review it or provide feedback, but maybe it's incomplete. This is OK, but if you do it, please tag your PR with `in-progress` label so that we know not to review / merge it.\n\n### Additional Notes\n\n* The generated `schema.rb` file may include or omit `id: :serial` for `create table`, and `null: false` for `t.datetime`. According to Aaron, this can safely be ignored, and it is probably best to commit the schema.rb only if you have committed anything that would change the DB schema (i.e. a migration).\n* If you have trouble relating to SSL libraries installing Ruby using `rvm` or `rbenv` on a Mac, you may need to add a command line option to specify the location of the SSL libraries. Assuming you are using `brew`, this will probably result in a command looking something like:\n\n ```rvm install 2.6.4 --with-openssl-dir=`brew --prefix openssl` ```.\n\n### Becoming a Repo Contributor\n\nUsers that are frequent contributors and are involved in discussion (join the slack channel! :)) may be given direct Contributor access to the Repo so they can submit Pull Requests directly, instead of Forking first.\n\n# Deployment Process\nThe diaper & partner application should be deployed ideally on a weekly or bi-weekly schedule. However, this depends on the amount of updates that we have merged into main. Assuming there is updates that we want to ship into deploy, this is the process we take to getting updates from our `main` branch deployed to our servers.\n\n#### Requirements\n- You will need SSH access to our servers. Access is usually only given to core maintainers of the diaper & partner projects.\n- Login credentials to our [Mailchimp](https://mailchimp.com/) account\n\n#### Tag & Release\n1. You'll need to push up a tag with the proper semantic versioning. Check out the [releases](https://github.com/rubyforgood/diaper/releases) to get the correct semantic versioning tag to use. For example, if the last release was `2.1.0` and the update is a hotfix then the next one should be `2.1.1`\n```sh\ngit tag x.y.z\ngit push --tags\n```\n2. Publish a release associated to that tag pushed up in the previous step. You can do that [here](https://github.com/rubyforgood/diaper/releases/new). Make sure to include details on what the release's updates achieves (we use this to notify our stakeholders on updates via email).\n\n#### Deploying\nStart deploying the latest update by using capistrano and specifying the correct tag\n```sh\nTAG=x.y.z cap production deploy\n```\n\n#### Send Update Email To Diaperbase Users\nWe will now want to inform the stakeholders that we've recently made a deployment and include details on what was updated. This is achieved by accessing all the user records and sending out a email via our Mailchimp account.\n\n1. Fetch all the emails of our users by accessing our diaperbase production database\n```ruby\ncap production rails:console\nUser.all.pluck(:email) # Copy the output of this!\n```\n2. Use the list of the emails copied from the output from the previous step to send a update email via [Mailchimp](https://mailchimp.com/)\n\n# Acknowledgements\n\nThanks to Rachel (from PDX Diaperbank) for all of her insight, support, and assistance with this application, and Sarah ( http://www.sarahkasiske.com/ ) for her wonderful design and CSS work at Ruby For Good '17!\n\n## License\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Frubyforgood%2Fdiaper.svg?type=large)](https://app.fossa.com/projects/git%2Bgithub.com%2Frubyforgood%2Fdiaper?ref=badge_large)\n\n"
 },
 {
  "repo": "ebimodeling/ghgvc",
  "language": "Ruby",
  "readme_contents": "\n\n# Ecosystem Climate Regulation Services Calculator\n\nThis is the source code repository for the Ecosystem Climate Regulation\nServices Calculator. Contributions, comments, bug reports, and\nfeature requests are welcome!\n\n# Setup & Installation\n\nThis project uses Docker to manage dependencies.\n\n1. Install [Docker Community Edition](https://store.docker.com/search?offering=community&type=edition) for your OS\n\n2. Clone the rails application:\n\n    git clone git@github.com:rubyforgood/ghgvc.git --depth 1 && cd ghgvc\n\n# Building & running the application\n\nEnsure Docker is running, the ghgvc app is cloned, and you've navigated to your ghgvc repo.\n\n1. Build Docker:\n\n    docker-compose build\n\n2. Download the required netcdf files\n\n    docker-compose run --rm ghgvcr ./download-netcdf.sh\n\n    > This stores the netcdf data in a volume that will persist across containers\n\n3. Bundle install:\n\n    docker-compose run --rm app bundle\n\n    > This stores the downloaded gems in a volume that will persist across containers\n\n4. Run the application:\n\n    docker-compose up app\n\n5. Navigate to http://localhost:3000/ in your web browser.\n\n  *  If clicking on the map does not return ecosystems, ensure that data was downloaded:\n\n  ```\ndocker-compose run --rm ghgvcr bash\ncd data\nls\n```\n  * This should show a file `name_indexed_ecosystems.json`, and two directories: `maps` and `netcdf`. If it is empty, try:\n  ```\n  docker-compose down # stop everything\n  docker-compose up get_data # should re-download & un-zip the data\n  ```\n  * If there is still an issue, try:\n  ```\n  docker-compose down # stop everything\n  docker-compose volume rm ghgvc_netcdf-data # removes the volume\n  docker-compose up get_data # should re-download & un-zip the data\n  ```\n  * `ghgvc_netcdf-data` name should be the name of the volume. If that command fails, try `docker volume ls` and look for one that matches on the netcdf-data\n   * If all of the above fails (if you can't force it to stop with docker-compose or docker commands), try restarting either docker or your machine (sometimes both; it usually means the container was put into a state that it shouldn't be).\n\n# Development & Test\n\n* Enter the Rails console:\n```\ndocker-compose run --rm app bin/rails c\n```\n* Run all tests:\n```\ndocker-compose run --rm app rspec\n```\n* Run a singular test:\n```\ndocker-compose run --rm app rspec spec/<test file path>\n```\n\n# About the Ecosystem Climate Regulation Services Calculator\n\nEcosystems regulate climate through both greenhouse-gas exchange with\nthe atmosphere (biogeochemical mechanisms) and regulation of land\nsurface energy and water balances (biophysical mechanisms). The exchange\nof carbon dioxide (CO~2~) and other greenhouse gases (N~2~O, CH~4~)\nbetween ecosystems and the atmosphere influences climate. For example,\nforests remove CO~2~ from the atmosphere as they grow, croplands release\nthe potent greenhouse gas N~2~O as a byproduct of fertilization, and\ndeforestation releases large amounts of CO~2~ and other greenhouse\ngasses to the atmosphere. Beyond this, ecosystems also influence climate\nthrough absorption of incoming solar radiation (dependent upon their\nreflectivity, or *albedo*) and the transfer of heat by evaporation\n(latent heat flux-a process analogous to sweating). Efforts aimed at\nclimate change mitigation through land management quantify greenhouse\ngas exchange, but do not account for the biophysical exchanges, which in\nsome cases can be quite significant.\n\nRecently, researchers proposed an integrated index of the climate\nregulation value (CRV) of terrestrial ecosystems (Anderson-Teixeira *et\nal.*, 2012a; Hungate & Hampton, 2012), which combines a previous metric\nof the greenhouse gas value of ecosystems (GHGV; Anderson-Teixeira &\nDeLucia, 2011) with biophysical climate regulation services to show the\nclimate regulation services of ecosystems in CO~2~ equivalents - a\ncommon currency for carbon accounting. This is the most comprehensive\nexisting metric of ecosystem climate regulation services, and it sets\nthe stage for thorough accounting of climate regulation services in\ninitiatives aimed at climate protection through land management\n(Anderson-Teixeira *et al.*, 2011; Hungate & Hampton, 2012).\n\nThe CRV calculator is a publically available web-based tool for\nestimating *CRV* (or *GHGV*) for ecosystems globally. It uses global\nmaps of climatically significant ecosystem properties (for example,\nbiomass, soil carbon, biophysical services) to provide location-specific\nCRV estimates.\n\n# Applications\n\nThe Ecosystem Climate Regulation Services Calculator has potential\napplications in a variety of fields. Below are some examples.\n\n## Conservation\n\nThis calculator can be used to determine which areas of potential\nconservation interest are the most beneficial in terms of their net\neffect on the climate. This information can then be used to help make\nland conservation decisions and inform the general public about the\nclimate benefits of conserving lands.\n\n## Sustainability Science\n\nThe calculator can be used to evaluate the climate consequences of\nvarious land use decisions. For instance, the calculator can be used to\nevaluate the impacts of various bioenergy production strategies\n(Anderson-Teixeira *et al.*, 2012b; Buckeridge *et al.*, 2012). It could\nalso be used in determining the value of land when designing\ninfrastructure projects, such as dams or highways.\n\n## Education\n\nThe calculator can be used to educate students or the general public\nabout the climate regulation services of ecosystems around the globe.\nFor example, by using the calculator to research ecosystems in areas\nwhere land use change is occurring, students will gain a greater\nunderstanding of the issues surrounding land use and conservation\ndecisions. They can also use the calculator to learn more about the\nlocal ecosystems with which they are familiar.\n\n## Business\n\nIncreasing public interest in sustainable business practices creates a\nneed for conscientious businesses to evaluate the climate impact of\nbusiness decisions, including those that affect land use patterns. For\nexample, the calculator might be used to evaluate the climate impacts of\nland use change related to bioenergy production.\n\n## Policy\n\nPolicy decisions regarding the conservation of domestic lands or those\naffecting international land use patterns can benefit from the most\ncomplete information possible regarding the impact of those decisions on\nclimate. Policies aimed at climate protection through land management,\nincluding REDD+ and bioenergy sustainability standards, account for\ngreenhouse gasses but not for biophysical processes that can sometimes\noutweigh greenhouse gas effects (Anderson-Teixeira *et al.*, 2011,\n2012a). This calculator incorporates both greenhouse gases and\nbiophysical climate regulation services, thereby providing a better\nunderstanding of the climate impacts of various policies.\n\n## Further Reading\n\nAnderson-Teixeira KJ, Snyder PK, DeLucia EH (2011) Do biofuels life\ncycle analyses accurately quantify the climate impacts of\nbiofuels-related land use change? *Illinois Law Review*, 2011, 589-622.\n\nAnderson-Teixeira KJ, Snyder PK, Twine TE, Cuadra SV, Costa MH, DeLucia\nEH (2012a) Climate-regulation services of natural and agricultural\necoregions of the Americas. *Nature Climate Change*, 2, 177-181.\n\nAnderson-Teixeira KJ, Duval BD, Long SP, DeLucia EH (2012b) Biofuels on\nthe landscape: Is land sharing? preferable to land sparing? *Ecological\nApplications*, 22, 2035-2048.\n\nAnderson-Teixeira KJ, DeLucia EH (2011) The greenhouse gas value of\necosystems. *Global Change Biology*, 17, 425-438.\n\nBuckeridge MS, Souza AP, Arundale RA, Anderson-Teixeira KJ, DeLucia E\n(2012) Ethanol from sugarcane in Brazil: a \"midway\"? strategy for\nincreasing ethanol production while maximizing environmental benefits.\n*GCB Bioenergy*, 4, 119-126.\n\nHungate BA, Hampton HM (2012) Ecosystem services: Valuing ecosystems for\nclimate. *Nature Climate Change*, 2, 151-152.\n"
 },
 {
  "repo": "raksha-life/rescuekerala",
  "language": "Python",
  "readme_contents": "<h1 align=\"center\">KeralaRescue</h1>\n\n[![Build Status - Travis][0]][1] [![Open Source Helpers](https://www.codetriage.com/ieeekeralasection/rescuekerala/badges/users.svg)](https://www.codetriage.com/ieeekeralasection/rescuekerala)\n\n<p align=\"center\">The Website for co-ordinating the rehabilitation of the people affected in the Kerala Floods.</p>\n\n[![Join Kerala Rescue Slack channel](https://i.imgur.com/V7jxjak.png)](http://bit.ly/rebuildearth)\n\n## Table of Contents\n- [Requirements](#requirements)\n    - [Docker](#docker)\n    - [Python 3](#python-3)\n    - [Postgres](#postgres)\n    - [Git](#git)\n    - [Redis](#redis)\n    - [Setting up an S3 Account](#setting-up-an-s3-account)\n- [Getting started](#getting-started)\n    - [Setting up a development environment](#setting-up-a-development-environment)\n    - [Setup using docker-compose](#setup-using-docker-compose)\n- [Creating migration files](#creating-migration-files)\n- [Running tests](#running-tests)\n- [Enable HTTPS connections](#enable-https-connections)\n- [How can you help?](#how-can-you-help)\n    - [Verification of Rescue Requests](#verification-of-rescue-requests)\n    - [Contribution Guidelines](#contribution-guidelines)\n    - [Testing PRs](#by-testing)\n    - [Submitting PRs](#submitting-pull-requests)\n\n<hr>\n\n### Requirements\n[^top](#table-of-contents)\n\n#### Docker\n- Check out this [Wiki](https://github.com/raksha-life/rescuekerala/wiki/Using-Docker) to see how to run docker for this project.\n\n#### [Python 3](https://www.python.org/downloads/)\n\n#### [Postgres](https://www.postgresql.org/download/)\n\n#### [Git](https://git-scm.com/downloads)\n\n#### [Redis](https://redis.io/)\n\n#### Setting up an S3 Account\n\n- Follow https://django-storages.readthedocs.io/en/latest/backends/amazon-S3.html and https://www.caktusgroup.com/blog/2014/11/10/Using-Amazon-S3-to-store-your-Django-sites-static-and-media-files/ to setup s3 bucket, and download access keys.\n\n</details>\n\n<hr>\n\n### Getting Started\n[^top](#table-of-contents)\n\n### Setting up a development environment\n\n<details>\n<summary>1. Create database and user in Postgres for keralarescue and give privileges. </summary>\n\n    psql user=postgres\n    Password:\n    psql (10.4 (Ubuntu 10.4-0ubuntu0.18.04))\n    Type \"help\" for help.\n\n    postgres=# CREATE DATABASE rescuekerala;\n    CREATE DATABASE\n    postgres=# CREATE USER rescueuser WITH PASSWORD 'password';\n    CREATE ROLE\n    postgres=# GRANT ALL PRIVILEGES ON DATABASE rescuekerala TO rescueuser;\n    GRANT\n    postgres=# \\q\n\n</details>\n\n<details>\n<summary>2. Clone the repo.</summary>\n\n    git clone https://github.com/raksha-life/rescuekerala.git\n    cd rescuekerala\n</details>\n\n<details>\n<summary>3. Copy the sample environment file and configure it as per your local settings.</summary>\n\n        cp .env.example .env\n\n> Note: If you cannot copy the environment or you're facing any difficulty in starting the server, copy the settings file from\nhttps://github.com/vigneshhari/keralarescue_test_settings for local testing.\n</details>\n\n<details>\n<summary>4. Install dependencies.</summary>\n    \n```\npip3 install -r requirements_debug.txt\n```\n\n\n    \n</details>\n\n<details>\n<summary>5. Run database migrations.</summary>\n\n        python3 manage.py migrate\n</details>\n\n<details>\n<summary>6. Setup static files.</summary>\n\n        python3 manage.py collectstatic\n</details>\n\n<details>\n<summary>7. Run the server.</summary>\n\n        python3 manage.py runserver\n</details>\n\n<details>\n<summary>8. Now open localhost:8000 in the browser</summary>\nThat's it!\n</details>\n\n<hr>\n\n### Setup using docker-compose\n#### Pre-requisites\n* [Docker](https://docs.docker.com/install/)\n* [Docker Compose](https://docs.docker.com/compose/install/)\n\nRun the scripts in the root directory\n\n* `docker-start.sh` - to start the services. \n\n* `docker-stop.sh` - to stop the services. \n\n* `docker-restart.sh` - to rebuild and restart the services. \n\n### Creating migration files\n[^top](#table-of-contents)\n\nIf your code changes anything in models.py, you might need to make changes in database schema, or other constraints. To create migrations files, run python3 manage.py makemigrations --settings=floodrelief.settings after making your changes. Also make sure to add these files in the commit.\n\n### Running tests\n[^top](#table-of-contents)\n\nWhen running tests, Django creates a test replica of the database in order for the tests not to change the data on the real database. Because of that, you need to alter the Postgres user that you created and add to it the `CREATEDB` privilege:\n\n```\nALTER USER rescueuser CREATEDB;\n```\n\nTo run the tests, run this command:\n\n```\npython3 manage.py test --settings=floodrelief.test_settings\n```\n\n<hr>\n\n### Enable HTTPS connections\n[^top](#table-of-contents)\n\nCertain features (example: GPS location collection) only work with HTTPS connections.  To enable HTTPS connections,follow the below steps.\n\nCreate self-signed certificate with openssl\n\n```\n$openssl req -x509 -newkey rsa:4096 -keyout key.key -out certificate.crt -days 365 -subj '/CN=localhost' -nodes\n```\n[https://stackoverflow.com/questions/10175812/how-to-create-a-self-signed-certificate-with-openssl#10176685]\n\nInstall django-sslserver\n\n```\n$pip3 install django-sslserver\n```\n\nUpdate INSTALLED_APPS with sslserver by editing the file floodrelief/settings.py (diff below)\n\n```diff\n INSTALLED_APPS = [\n+    'sslserver',\n     'mainapp.apps.MainappConfig',\n     'django.contrib.admin',\n```\n#### Note: Make sure that this change is removed before pushing your changes back to git\nRun the server\n\n```\npython3 manage.py runsslserver 10.0.0.131:8002  --certificate /path/to/certificate.crt --key /path/to/key.key\n```\nIn the above example the server is being run on a local IP address on port 8002 to enable HTTPS access from mobile/laptop/desktop for testing.\n\n<hr>\n\n## How can you help?\n[^top](#table-of-contents)\n\n#### Verification of Rescue Requests\n\nYou can help us with verifying user submitted request from our [Ushahidi volunteer](https://volunteers.keralarescue.in/) portal. Please follow the usermanual available in either [English](https://github.com/raksha-life/rescuekerala/files/2300176/Kerala.Rescue.Volunteers.Manual.Draft.pdf) or [Malayalam](https://github.com/raksha-life/rescuekerala/files/2299875/default.pdf)\n\n#### Contribution Guidelines\nCheck out this [Wiki](https://github.com/raksha-life/rescuekerala/wiki/Contribution-Guidelines) for our contribution guidelines.\n\nPlease find issues that we need help [here](https://github.com/raksha-life/rescuekerala/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22). Go through the comments in the issue to check if someone else is already working on it. Don't forget to drop a comment when you start working on it.\n\n\n<details>\n<summary>Testing PRs\n\nWe have a lot of [Pull Requests](https://github.com/raksha-life/rescuekerala/pulls) that requires testing. Pick any PR that you like, try to reproduce the original issue and fix. Also join `#testing` channel in our slack and drop a note that you\nare working on it.\n</summary>\n\n#### Testing Pull Requests\nNote: If you have cloned a fork of raksha-life/rescuekerala, replace ```origin``` with ```upstream```\n\n1. Checkout the Pull Request you would like to test by\n      ```\n      git fetch origin pull/ID/head:BRANCHNAME`\n      git checkout BRANCHNAME\n     ```\n2. Example\n    ```\n    git fetch origin pull/406/head:jaseem\n    git checkout jaseem1\n    ```\n3. Run Migration\n</details>\n\n<details>\n<summary>Submitting Pull Requests\n\nPlease find issues that we need help [here](https://github.com/raksha-life/rescuekerala/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22). Go through the comments in the issue to check if someone else is already working on it. Don't forget to drop a comment when you start working on it.</summary>\n\nAlways start your work in a new git branch. **Don't start to work on the\nmaster branch**. Before you start your branch make sure you have the most\nup-to-date version of the master branch then, make a branch that ideally\nhas the bug number in the branch name.\n\n1. Before you begin, Fork the repository. This is needed as you might not have permission to push to the main repository\n\n2. If you have already clone this repository, create a remote to track your fork by\n     ```\n     git remote add origin2 git@github.com:tessie/rescuekerala.git\n     ```\n3. If you have not yet cloned, clone your fork\n    ```\n    git clone git@github.com:tessie/rescuekerala.git\n    ```\n4. Checkout a new branch by\n     ```\n     git checkout -b issues_442\n     ```\n4. Make your changes.\n\n5. Ensure your feature is working as expected.\n\n6. Push your code.\n      ```\n      git push origin2 issues_442\n      ```\n7. Compare and create your pull request.\n\n[0]: https://travis-ci.org/raksha-life/rescuekerala.svg?branch=master\n[1]: https://travis-ci.org/raksha-life/rescuekerala\n</details>\n\n<hr>\n\n### Flood Map\nYou can find the repo for the Flood Map here : https://github.com/aswinmohanme/kerala-flood-map\n"
 },
 {
  "repo": "Data4Democracy/ethics-resources",
  "language": null,
  "readme_contents": "# Global Data Ethics Pledge (GDEP)\n---\n\n## Working Groups for v2.0 of the Pledge are being formed right now and you can read more about them and join on the [Working Groups page](https://github.com/Data4Democracy/ethics-resources/blob/master/working-groups.md).\n\n\n## The 5 Principles:\n<ul>\n    <li>I COMMIT TO FAIRNESS</li>\n    <li>I COMMIT TO OPENNESS</li>\n    <li>I COMMIT TO RELIABILITY</li>\n    <li>I COMMIT TO TRUST</li>\n    <li>I COMMIT TO SOCIAL BENEFIT</li>\n</ul>\n\n[__Click here to join the movement today__](http://datafordemocracy.org/pledge). Become a data defender for life.\n\n    Fairness:\n    I will make make a dedicated effort to understand, mitigate and communicate the presence of bias in both data practice and consumption.\n\n    Openness:\n    I will practice humility and openness. Transparent practices, community engagement, and responsible communications are an integral part of my data ethics practice.\n\n    Reliability:\n    I will ensure that every effort is made to glean a complete understanding of what is contained within data, where it came from, and how it was created. I will also extend this effort for future users of all data and derivative data.\n\n    Trust:\n    I wish to build public confidence in data practitioners. I will make every effort to use data and algorithms in ways that maximize the informed participation of people around the world.\n\n    Social Benefit:\n    I set people before data and intend to be responsible for maximizing social benefit and minimizing harm. I will consider the impact of my work on communities of people, other living beings, ecosystems and the world-at-large.\n\nCommit to the Pledge and help us build a movement that collectively works towards a more accountable, equitable, open, inclusive, and transparent data community. The more of us who commit to The Pledge and adopt these principles, the more leverage we have to create change in the collective work we produce and how we produce it. We can affect hearts and minds and drive policy to a better place for all.\n\nData for Democracy\u2019s Global Data Ethics Pledge is a global initiative aimed at creating the industry\u2019s most widely adopted framework and community for ethics in data science.\n\nData for Democracy partnered with Bloomberg and BrightHive to develop a **code of ethics for data scientists, software developers and data analyzers of all types, (that means you :smile: ) **. This code aims to define values and priorities for overall ethical behavior, in order to **guide anyone handling data to be a thoughtful, responsible agent of positive change**. The code of ethics is being __developed through a community-driven approach__.\n\nBy hosting discussions among our data-focused community, we hope to better capture the diverse interests, needs, and concerns that are at play in the community, and put together a code that is truly created by the data community, for the data community.\n\nRead more [here](http://datafordemocracy.org/projects/ethics.html).\n\n### [SIGN UP HERE](https://docs.google.com/spreadsheets/d/1_D49VvbAA7vhWEJfaj3_RVTiUXsM_jRaLyZfzUK6zCM/edit?usp=sharing)\n\n## Ethics Principles\n---\n<ul>\n    <li>Bias will exist. It's my responsibility to understand, mitigate and communicate the presence of bias in algorithms.</li>\n    <li>It's my responsibility to increase social benefit while minimizing harm.</li>\n    <li>I will practice humility and openness.</li>\n    <li>I will know my data and help future users know it as well.</li>\n    <li>I will make reasonable efforts to know and document its origins and document its transformation.</li>\n    <li>I will document transparently, accessibly, responsibly,  reproducibly, and communicate.</li>\n    <li>I will strive to engage \"the whole community\" using, or impacted by, my work. I will try to clarify or redefine the definition of this community, on an ongoing basis. I will use skepticism and an open mind to expand this definition.</li>\n    <li>I will put people before data.</li>\n    <li>I will not over/under represent findings.</li>\n    <li>Data scientists should use a _question-driven approach_ rather than a _data-driven_ or _methods_ approach. Data scientists should consider personal safety and <a href=\"https://en.wikipedia.org/wiki/Golden_Rule\">treat others the way they want to be treated.</a></li>\n    <li>Open by default - use of data should be transparent and fair.</li>\n    <li>I am part of an ecosystem and it is my ethical obligation to produce considerate analysis conducive to standards of minimal impact and risk to communities, people, and the world</li>\n    <li>I will respect others' data even more than my own. I will seek to understand the sources of the data and the consequences of my actions.</li>\n    <li>I will protect individual and institutional privacy.</li>\n    <li>Diversity for inclusivity.</li>\n    <li>Attention to bias.</li>\n    <li>Respect for others/persons.</li>\n    <li>I will respect human dignity.</li>\n    <li>I will agitate my organizations and colleagues to act ethically and respect human dignity.</li>\n</ul>\n\n## What has been done so far?\n---\nWe conducted a preliminary scan in the Data for Democracy community, by posting discussion questions on Slack and Twitter, and collecting feedback and input from our 2,000-plus members. We then **identified recurring themes** that our community members highlighted as important, and arranged these in a systematic framework. This was made by a [list of resources](https://docs.google.com/document/d/1XwXmfIkQxXPidDT7domqEOC7KLcBVLAmSP_7V3g47j8/edit?usp=sharing) addressing the topic.\n\nAfter that, seven groups of work, formed by volunteers are doing an in-depth discussion of each topic area. Each group is meeting once every 2 weeks, for 1 to 2 hours each time to convey these analysis. Finally, a selected group of advisors will review the notes and gives feedback.\n\nThe aim was to have a draft version by February 6th to be presented at the Data for Good Exchange celebrated in San Francisco ([D4GX](https://www.bloomberg.com/company/d4gx/))\n\nThe topic areas that the volunteers are discussing are:\n- Data Ownership and Provenance\n- Bias and Mitigation\n- Responsible Communications\n- Privacy and Security\n- Transparency and Openness\n- Questions and Answers\n- Thought Diversity\n\n\n## How can you contribute with ideas?\n---\nIn order for this process to be as transparent and open as possible, we are making use of GitHub to collect ideas and suggestions from the community as a whole. This provides a quick and easy way for you to do the following-\n\n1. **Submit titles or links of literature/resources** that you have found useful in thinking about ethics and data science. You can do so by forking the repo and making the change in your text editor and submitting a pull request or directly in the [resources.md](https://github.com/Data4Democracy/ethics-resources/blob/master/resources.md) file.\n\n```\nIf you are not familiar with GitHub. (.md stands for Markdown, a way to format writing on the web easily-cheatsheet link below). If you click on the link, it\u2019ll take you to a page that only loads the resources.md file. On the right side, above the beginning of the document, there is a button with a pencil icon. This allows you edit the file and add your links without having to fork the repo to your computer, use the command line, or any other thing but the content management system built into GitHub.\n```\n\n2. **Submit links including comments on what matters to you** when creating a data science code of ethics. Open a pull request from your forked repo.\n\n```\nIf you click the \u201cCommit changes\u201d button when using GitHub's content management system, you\u2019ll get a notification from Github saying that you can\u2019t directly contribute to the code so it has forked the repository for you and has made your changes on your branch. That might sound a bit confusing, it only means that we aren\u2019t allowed to directly change the existing/live code or document without going through the process that verifies any proposed changes. In order for this to happen, everything is copied to your GitHub account and you make the changes on your GitHub account. If you want the changes to appear in the main project page/repo, you need to submit a pull request by following steps provided.\n```\n\n3. **Browse all the suggestions, comments and links** submitted by your fellow community members in the Discussions section. All results of these contributions can be viewed by the public, [here](https://github.com/orgs/Data4Democracy/teams/ethics) and in the master version of the resources.md file, including you.\n\n4. **Indicate which suggestions and comments you agree with** through voting via :thumbsup: or :thumbsdown: emojis in the Discussions section located **[here](https://github.com/orgs/Data4Democracy/teams/ethics)**.\n```\nThe setup of GitHub presents you with all suggestions. However, please keep in mind that none of these suggestions are mutually exclusive; we are not pitting ideas against each other or using the number of votes to eliminate suggestions. We are simply using this as one convenient metric to determine which ideas have the most resonance in the community, or which resources/literature have been useful to a large number of people. If all suggestions presented are important to you, feel free to :thumbsup: all of them. If you have a more detailed response or would like to express your thoughts on someone else's idea, you can submit this comment in the Ethics Team's Discussion area or comment directly on their open pull request.\n```\nThis document is written and edited using GitHub flavored Markdown. It's not scary, it's very simple and they even provide a cheatsheet which you can find [*here*](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet).\n\nShout out to [Ashley Blewer's blog post](https://ablwr.github.io/blog/2014/11/04/non-technical-persons-guide-to-becoming-an-open-source-software-contributor-via-github/) for breaking down the pull request so succinctly.\n"
 },
 {
  "repo": "civicdata/civicdata.github.io",
  "language": "HTML",
  "readme_contents": "# [Civic Data Alliance](civicdata.github.io)\n===================\n\nThis is the repository that powers http://www.civicdataalliance.org.  \nPull request are welcome :)  tho we don't rebase around here  \n\n## All the Data for We the People\n\nThe Civic Data Alliance (CDA) is Louisville\u2019s Civic Tech org and we are focused on advocating for inclusion, diversity, equity, accessibility, openness, transparency and accountability in government, civic tech and open data.\n\nCivic Data Alliance is interested in ethically liberating, improving, gathering, defining, and reporting on public data and the state of civic tech, while fostering an inclusive and diverse civic tech community.\n\nWe STRONGLY believe that our government should uphold and advocate for ethical principles regarding Open Data and be accountable for their Open Data practices, including the ethical collection, implementation and sharing of The People\u2019s data.\nCommunity Partners\n\nBlack Lives Matter Louisville/Stand Up Sundays | New Roots Inc | IDEAS xLab | Code Louisville | American Printing House for the Blind | KY Youth Advocates | Louisville Public Media | Metro Safe and Healthy Neighborhoods | Network Center for Community Change | TARC | The Courier-Journal\nCode of Conduct\n\nAll members, volunteers, and event participants agree to abide by [Civic Data Alliance Code of Conduct](https://github.com/civicdata/civicdata.github.io/blob/master/CODE_OF_CONDUCT.md)\n\n\nThis site uses Jekyll. If you are not familiar with a Jekyll install, to get this project\nrunning please visit the [__Jekyll docs__](https://jekyllrb.com/docs/installation/)\n"
 },
 {
  "repo": "EFForg/action-center-platform",
  "language": "Ruby",
  "readme_contents": "[![Build Status](https://travis-ci.org/EFForg/action-center-platform.svg?branch=master)](https://travis-ci.org/EFForg/action-center-platform)\n\n\nAction Center Platform\n======================\n\nThe Action Center Platform is an online organizing tool maintained by [EFF](https://www.eff.org/). Administrators can create targeted campaigns where users sign petitions, contact legislators, and engage on social media.\n\n\n## Setup\n\nFollow these instructions to run the Action Center using Docker (recommended). To run the Action Center without Docker, see [setup without Docker](https://github.com/EFForg/action-center-platform/wiki/Setup-without-Docker).\n\n1. Install Docker ([instructions](https://docs.docker.com/engine/installation/)) and Docker Compose ([instructions](https://docs.docker.com/compose/install/)).\n2. `git clone https://github.com/EFForg/action-center-platform.git`\n3. Copy `docker-compose.yml.example` to `docker-compose.yml`, and `.env.example` to `.env`. Fill in the variables in `.env` according to the instructions in that file. See [notable dependencies](#notable-dependencies) for hints.\n4. Build the docker image: `sudo docker-compose build`\n5. Run the application: `sudo docker-compose up`\n6. In a new tab, get a bash shell with access to your app: `sudo docker-compose exec app bash`.\n    1. If you aren't running migrations automatically, run `rake db:migrate` to migrate the database.\n    2. Run `rake congress:update` to populate CongressMember table.\n\n\n### Notable Dependencies\n\n* Amazon S3 secret key and key id\n  * Allows admins to upload images for the ActionPages\n* [SmartyStreets API](https://smartystreets.com/account/create) key and id\n  * Allows Congress members to be looked up for users\n* [Phantom of the Capitol](https://github.com/efforg/phantom-of-the-capitol) whitelisting on server side?\n  * Allows users to submit e-messages to congress\n* [Call Congress](https://github.com/EFForg/call-congress) url and API key\n  * Connects calls between citizens and their congress person using the Twilio API\n\n\n## Using the Action Center\n\nAction Center administrators can create four types of actions:\n* **Call Action**\n  * A user is connected to a political leader by phone, leaving a message or sometimes speaking to an aid.\n  * The user is shown a script to read and enters their phone number and email\n  * When they submit their info, they are called and the process begins.\n* **Congress Action**\n  * A user stepped through a four-part form to submit their comments to their congress person.\n  * Page 1 asks for the user's street address and zip code.\n  * Based on this information, page 2 displays the user's representatives. User can select which representatives they wish to contact. Then they choose what to fill in for whatever required fields for each representative selected.\n  * Page 3 asks the user to customize the message to be delivered.\n  * Page 4 is the Thank You page with share links for social media.\n  * When creating the action, admin can choose House, Senate, Both or specific legislators\n* **Petition Action**\n  * A user signs a petition, leaving an email address and sometimes location data.\n  * Optionally, users can petition local institutions (like universities) and see signatures by institution.\n* **Tweet Action**\n  * A user is invited to join a tweet action using their twitter account.\n* **Email Action**\n  * A user e-mails a target *or*\n  * A user submits comments to a congressperson via [Phantom of the Capitol](https://github.com/efforg/phantom-of-the-capitol).\n\n### Shared Elements of All Actions\n\n* User is presented with a Thank You page at the end where they are invited to share the action via social media.\n* When creating an action, admin can add partner organizations.\n* Admin can also customize share messages and thank you email.\n* Admin chooses a banner image form the library\n* The title for the action form is always \"Take Action\".\n\n### Administering Users\n\nTo get started using the Action Center, create a user and grant them admin privileges. Administrators can create, track, and manage campaigns.\n\n**To create an admin user**:\n\n1. If the user doesn't exist yet, create them through the web interface by following the `register` link in the top nav.\n2. Run the rake task to grant them admin access (including square brackets):\n```\nrake users:add_admin[youremail@example.org]\n```\nNew users will need to complete an e-mail confirmation in order to log in. Administrators can access admin features by clicking `admin` in the nav.\n\n**To remove an admin user**:\n```\nrake users:remove_admin[youremail@example.org]\n```\n\n**To list all admin users**:\n```\nrake users:list_admins\n```\n\n### Delayed Jobs and Cron\n\nAction Center uses [DelayedJob](https://github.com/collectiveidea/delayed_job) to perform certain tasks outside of a web request context. See that repository for information regarding how to run a delayed job worker. If you are deploying with Docker, our [docker-compose.yml.example](https://github.com/EFForg/action-center-platform/blob/master/docker-compose.yml.example) shows how to create a service which processes the job queue indefinitely.\n\nYou may also want to automate certain other tasks (such as `rake signatures:deduplicate` and `rake congress:update`) to run occasionally. For those deploying with Docker, docker-compose.yml.example illustrates how to create a service which runs these commands periodically using cron.\n\n### Embedding Actions\n\nEmbedding actions is simple. Just include the following HTML on the page you want the action to be embedded:\n\n    <script type=\"text/javascript\" src=\"https://act.eff.org/action/embed\"></script>\n    <a class=\"action-center-widget\" href=\"https://act.eff.org/action/shut-the-nsa-s-backdoor-to-the-internet\">Take part in the action!</a>\n\nThe link href should point to the action page you wish to embed. You may add `?nosignup=1` to the URL to get newsletter signup fields omitted from the action.\n\nIf you want to get fancy, you can modify the embed code to include some of the following parameters, all of which are optional:\n\n    <script type=\"text/javascript\">\n        var ac_embed = {};\n        ac_embed.css = \"https://example.com/hello.css\"; // specify a css file url. File must be globally available (i.e. on the Internet, not local or staging).\n        ac_embed.width = 500; // specify a width manually\n        ac_embed.no_css = true; // remove all default styles\n        ac_embed.css_content = \"#some_elem\"; // specify an element which itself contains some styles\n        ac_embed.bioguide_ids = [\"ID1\", \"ID2\"...] // bioguide IDs of congress members to target\n    </script>\n    <script id=\"some_div\" type=\"text/x-css-content\">\n        body{\n            background-color: blue;\n        }\n    </script>\n    <script type=\"text/javascript\" src=\"https://act.eff.org/action/embed\"></script>\n    <a id=\"action-center-widget\" href=\"https://act.eff.org/action/shut-the-nsa-s-backdoor-to-the-internet\">Take part in the action!</a>\n\n\n## Testing\n\nTo run the full test suite, simply run `rake` with no arguments.\n\nRspec tests are used for unit testing the app, and some integration testing. Cucumber tests are used for testing API keys, javascript tests, and feature tests.\n\nWe use [WebMock](https://github.com/bblimke/webmock) to stub backend requests to third party services and [Puffing Billy](https://github.com/oesmith/puffing-billy) to stub frontend (Ajax) requests. Puffing Billy will [cache](https://github.com/oesmith/puffing-billy#caching) unrecognized requests and play them back during future test runs. To prevent Puffing Billy from making any new requests, set `DISABLE_PUFFING_BILLY_REQUESTS=true`.\n\n## Linting\n\n`rake` will also run our linting:\n\n### [Rubocop](https://github.com/bbatsov/rubocop)\nRubocop checks for consistent style across the Ruby areas of the codebase.  We use a modified version of [Rubocop-Github](https://github.com/github/rubocop-github) to stay consistent with [SEC](https://github.com/EFForg/sec).\n\n### [Sass Lint] (https://github.com/sasstools/sass-lint)\nSass-lint checks for consistent style across the stylesheets. Our\n.sass-lint file is derived from [SEC](https://github.com/EFForg/sec).\n\n## Deployment\n\nFor notes related to deploying Action Center in production, see [the project wiki](https://github.com/EFForg/action-center-platform/wiki/Deployment-Notes).\n\n## Acknowledgements\n\nThis project was created by Lilia Kai, Thomas Davis, and Sina Khanifar. Large portions of the codebase are directly attributable to them, while under the employ or contractorship of the Electronic Frontier Foundation in 2014. Thank you Lilia, Thomas, and Sina! The Action Center is currently maintained by the EFF Engineering and Design team.\n\n\n## Styling\n\nThe styling is done with SCSS. The partials files are in the stylesheets directory. Admin files are in their own subdirectory.\n\nBootstrap is also used for much of the styling. Please see the stylesheets/application/bootstrap-custom.scss for what styles may be used.\n\nBootstrap is used only for styling, not for javascript.\n\n## File Structure\n\n* The home page layout is in views/welcome/index.html.erb.\n* The main internal layout is in views/layouts/application.html.erb.\n* Individiual action views are in views/tools/.\n* User pages are in views/devise.\n* All admin layouts are in views/admin.\n\n\n## Licensing\n\nSee the `LICENSE` file for licensing information. This is applicable to the entire project, sans any 3rd party libraries that may be included.\n"
 },
 {
  "repo": "fightforthefuture/battleforthenet",
  "language": "HTML",
  "readme_contents": "Battle for the Net!\n-------------------\n\n[![CircleCI](https://circleci.com/gh/fightforthefuture/battleforthenet/tree/master.svg?style=svg&circle-token=cc0477227a0fdae4c035fee910b31001f463909f)](https://circleci.com/gh/fightforthefuture/battleforthenet/tree/master)\n\n**This is the source code for [battleforthenet.com][1]!**\n\nDon't let the FCC kill net neutrality! The Battle for the Net is a\ncollaboration between [Fight for the Future][2], [Demand Progress][3], [Free Press Action Fund][4]  and a\ncoalition of people and companies who care about keeping the Internet free.\nIf you have a web site, you can get involved by embedding our FCC Contact\nForm. Or, if you have technical skills and time to volunteer, you can help us\nwin one of the biggest fights for Internet freedom EVER!\n\n\n### Use our Day of Action widget\n\nWe've created a tool that lets people directly contact the FCC and Congress to stop net neutrality rules from being rolled back.\n\n[Follow these instructions](https://github.com/fightforthefuture/redalert-widget) to install it on your site.\n\n### Contributing to our code\n\nWe are looking for skilled web developers to help us build the front page on\nour [battleforthenet.com][1] site. If you're interested in volunteering, contact <sarah@fightforthefuture.org>\n\n[1]: https://www.battleforthenet.com\n[2]: https://www.fightforthefuture.org\n[3]: http://www.demandprogress.org\n[4]: http://freepress.net/\n"
 },
 {
  "repo": "fightforthefuture/battleforthenet-widget",
  "language": "JavaScript",
  "readme_contents": "[![CircleCI](https://circleci.com/gh/fightforthefuture/battleforthenet-widget/tree/master.svg?style=svg)](https://circleci.com/gh/fightforthefuture/battleforthenet-widget/tree/master)\n\n# How to install the widget\n\nAdd this to any page, and you're golden: ([**See the demo!**](https://widget.battleforthenet.com/demos/modal.html))\n\n```html\n<script src=\"https://widget.battleforthenet.com/widget.js\" async></script>\n```\n\nOr, follow these [easy instructions for Tumblr](http://tumblr.fightforthefuture.org/post/162878793988/how-to-stand-up-for-netneutrality-on-tumblr).\n\nThe goal of this project is to allow anyone with a web site to run their own campaign to save net neutrality. Simply add one line of JavaScript and you're good to go! The modal animation will show up front-and-center on your page, prompting\nvisitors to contact Congress and the FCC.\n\nIf you have any problems or questions regarding the widget, please [submit an issue](https://github.com/fightforthefuture/battleforthenet-widget/issues).\n\n\n# How it works\n\nThe widget is designed to appear once per user, per device, per day, but can be configured to display at a different interval. If you'd like to force it to show up on your page for testing, reload the page with `#ALWAYS_SHOW_BFTN_WIDGET` at the end of the URL.\n\nPlease take a look at [**widget.js**](https://github.com/fightforthefuture/battleforthenet-widget/blob/master/widget.js) if you want to see exactly what you'll\nbe embedding on your page.\n\n* Compatible with Firefox, Chrome, Safari and Internet Explorer 11+.\n* Embed the widget JavaScript code on your page.\n* Optionally pass in customization parameters (see below), or defaults are used.\n* Widget checks to make sure it should be shown (hasn't been shown to this user recently and user hasn't initiated a call or clicked a donate link recently, via cookie). You can override this check for testing purposes.\n* Widget preloads any images required for the chosen animation.\n* Widget injects a floating `iframe` onto your page. All but the most trivial styles and interactions take place in the `iframe` so as not to interfere with your CSS and JavaScript.\n* Animation displays in floating `iframe`.\n* The user can dismiss the `iframe` and a cookie is written so it won't show again until cookie expires (unless you override).\n\n\n#### Modal customization options:\n\nIf you define an object called `_bftn_options` before including the widget code,\nyou can pass some properties in to customize the default behavior.\n\n```html\n<script type=\"text/javascript\">\n  var _bftn_options = {\n    /*\n     * Choose from 'take-action', 'capitol', 'onemorevote', 'countdown', 'glitch', 'money', 'stop', 'slow', 'without'.\n     * Default is 'take-action'.\n     */\n    theme: 'onemorevote', // @type {string}\n    \n    /*\n     * Or, if you want your own custom theme, specify its properties here.\n     * Unspecified options will fall back to the default values.\n     */\n    theme: {\n      className: 'money', // @type {string} will be applied to iframe body tag\n      logos: ['images/money.png', 'images/stop.png'], // @type {Array} img src values\n      headline: 'Your headline here.', // @type {string} modal headline text\n      body: 'Your body here.' // @type {string} modal body text\n    },\n    \n    /*\n     * Choose from 'fp' for Free Press, 'dp' for Demand Progress or\n     * 'fftf' for Fight for the Future. Omit this property to randomly split\n     * form submissions between all organizations in the Battle for the Net \n     * coalition.\n     */\n    org: 'fftf', // @type {string}\n    \n    /*\n     * Specify a delay (in milliseconds) before showing the widget. Defaults to one \n     * second.\n     */\n    delay: 1000, // @type {number}\n    \n    /*\n     * Specify a date for the countdown theme. Defaults to November 23rd, 2017\n     * (when the FCC is expected to announce a vote) if omitted. ISO-8601 dates are\n     * UTC time, three-argument dates (with a zero-based month) are local time.\n     */\n    date: new Date(2017, 10, 23), // @type {Date}\n\n    /*\n     * Specify view cookie expiration. After initial view, modal will not be\n     * displayed to a user again until after this cookie expires. Defaults to one\n     * day.\n     */\n    viewCookieExpires: 1, // @type {number}\n\n    /*\n     * Specify action cookie expiration. After initiating a call or clicking a\n     * donate link, modal will not be displayed to a user again until after this\n     * cookie expires. Defaults to one week.\n     */\n    actionCookieExpires: 7, // @type {number}\n    \n    /*\n     * If you show the modal on your homepage, you should let users close it to\n     * access your site. However, if you launch a new tab to open the modal, closing\n     * the modal just leaves the user staring at a blank page. Set this to true to\n     * prevent closing the modal - the user can close the tab to dismiss it. Defaults\n     * to false.\n     */\n    uncloseable: false, // @type {boolean}\n\n    /*\n     * Prevents the widget iframe from loading Google Analytics. Defaults to false.\n     */\n    disableGoogleAnalytics: false, // @type {boolean}\n    \n    /*\n     * Always show the widget. Useful for testing.\n     */\n    always_show_widget: true // @type {boolean}\n  };\n</script>\n<script src=\"https://widget.battleforthenet.com/widget.js\" async></script>\n```\n\n"
 },
 {
  "repo": "mariechatfield/call-my-congress",
  "language": "HTML",
  "readme_contents": "# DEPRECATED: Call My Congress\n\n---\n\n_Call My Congress is no longer supported._\n\nIf you are attempting to access your represenatives, try:\n\n- [U.S. Senate](https://www.senate.gov/general/contact_information/senators_cfm.cfm)\n- [U.S. House of Representatives](https://www.house.gov/representatives/find-your-representative)\n- [GovTrack](https://www.govtrack.us/congress/members)\n- [5 Calls](https://5calls.org/)\n\n---\n\nIf you are looking for the original source code of this application, refer to the `archived` branch."
 },
 {
  "repo": "mozilla/advocacy.mozilla.org",
  "language": "JavaScript",
  "readme_contents": "# Overview\n\nThis is the repository for [https://advocacy.mozilla.org](https://advocacy.mozilla.org)\n\nAs of April 2016, there are three parts to the site.\n\n[Homepage](https://advocacy.mozilla.org) and [Open Web Fellows](https://advocacy.mozilla.org/open-web-fellows/) are an index.html file statically generated by a server at build time. This single index page then renders the requested page on the client side.\n\n[Encrypt Campaign](https://advocacy.mozilla.org/encrypt/) doesn't use an html file and nothing is built on the server, instead the server generates the pages as requested and passed to the client. The result page is cached on the server in memory after the first request, so the next request is not generated again.\n\n# Getting Started\n\nClone a copy of the repository using something like [git](http://git-scm.com/).\n\nTo get a local version of the site running, you'll need [node](http://nodejs.org/) 8 or higher installed on your local machine.\n\n## Build\n\nTo start local development, install and run the following commands:\n\n``` bash\n$ cp sample.env .env\n$ npm install\n$ npm start\n```\n\n## Develop Workflow\n\nAfter successfully building and running the server, go to [localhost:8080/](http://localhost:8080/)\n\nYou can now start changing files in the `/src` directory, such that saving your changes and refreshing the browser will show your changes.\n\nFiles outside of /src require npm start to be rerun\n\n## Sitemap\n\n```\n/ -> /src/pages/home.js\n/encrypt/ -> /src/pages/encrypt/index.js\n/open-web-fellows/ -> /src/pages/open-web-fellows/overview.js\n/open-web-fellows/info/ -> /src/pages/open-web-fellows/info.js\n/open-web-fellows/fellows/ -> /src/pages/open-web-fellows/fellows.js\n```\n\n## Source Structure\n\nRunning `npm start` is the entry point to build our code and run our server. All our npm scripts and npm dependencies are stored in package.json.\n\nOur client side code is written in react and lives in `/src`. Our files are organized into two directories, `/src/components/` and `/src/pages/`. Components are shared UI elements, like header and footer used to build pages.\n\nWe have an npm script that uses [babel](https://babeljs.io/) to compile the contents of `/src` from [es6](https://en.wikipedia.org/wiki/ECMAScript#6th_Edition) to `/dist` as [es5](https://en.wikipedia.org/wiki/ECMAScript#5th_Edition). These are built to run on the server and client.\n\nWe use [webpack](https://webpack.github.io/) to package our client side files into `/public`, which is handled in `webpack.config.js`\n\nWe use [react-router](https://github.com/reactjs/react-router) to handle the site structure, which live in `/src/avocacy-main.js` and `/src/encrypt-main.js`.\n\nWe use [node](http://nodejs.org/) version 8 or higher to run our server, which is handled in `app.js`.\n\n\"Encrypt\" pages are handled a bit differently from the rest. The standard routes, and source, use `/src/advocacy-main.js` for pretty much all the generating and packaging of files, routes, and source. Encrypt uses `/src/encrypt-app.js` to package client side files, and uses `/src/encrypt-main.js` and `/src/lib/react-server-rooute.js` to handle server side rendering.\n"
 },
 {
  "repo": "panxzz/NN-blackout",
  "language": "JavaScript",
  "readme_contents": "# NN-blackout\nPurposefully slows loading of webpages to simulate what the internet would look like without Net Neutrality.\n\nThis script will only become active on the day of the blackout (if and when it's scheduled) and on that day it will make webpages look like they are loading really slowly and possibly \"break\" other things; then after a few seconds a modal will pop saying \"This is what the internet would look like without net neutrality... Tell the FCC that you support net neutrality\". Clicking anywhere on the screen when the modal is shown will close the message and allow normal operation of the site.\n\nThe idea is that a ton of individual website owners would be able to just pop this script on their site to \"opt-in\" to a massive blackout of the web on a specific day. If they ever wanted to \"opt-out\" (say after we've won the internet back, or if they are concerned that it would go down on important days) they would just need to remove the call to this script.\n\nTo see a live demo of the script in action visit http://novanetllc.org/example/\n\n<h3>Ideas for \"breaking websites\":</h3>\n<ul>\n<li>slow loading of all assets</li>\n<li>reduce image resolutions?</li>\n<li>break other parts??</li>\n</ul>\n\nhttps://www.reddit.com/r/technology/comments/6bytpx/sopa_pipa_cispa_acta_tpp_itu_cispa_again_tafta_we/dhqybzv/\n\n<h3>Installation</h3>\n\nSimply copy/paste this script into any page(s) of your website to \"opt-in\"\n<pre><script src=\"https://rawgit.com/panxzz/NN-blackout/master/blackout.js\"></script></pre>\nBy leaving this script on your site you are already covered, your site will \"break\" then display the modal message on the specified blackout date.\n\nTo test what your site will look like you can use this script:\n<pre><script src=\"https://rawgit.com/panxzz/NN-blackout/master/blackout-test.js\"></script></pre>\nThis script will not check for the blackout and act like the blackout is always active."
 },
 {
  "repo": "j2kao/fcc_nn_research",
  "language": "Jupyter Notebook",
  "readme_contents": "# fcc_nn_research\n\n(somewhat) cleaned-up versions of notebooks used in researching public comments for FCC Proceeding 17-108 (Net Neutrality Repeal). I am posting the notebook for Exploratory Data Analysis first, and will include others as they are cleaned up.\n\n## Where's the data?\n\nSee below in the prerequisites section.\n\n## Recent Updates (11-27-2017)\n\n4 more notebooks have been uploaded. Run in numerical order to reconstruct the data processing pipeline. Notebook 4 contains the charts. Data for the final couple notebooks is being uploaded and will be linked here tomorrow morning.\n\n## Background Information\n\nI did this project as a part of the coursework for [Metis](https://www.thisismetis.com/) and was shocked to see my analysis blow up online. Humbled by the attention but I'm sure experienced data scientists out there could glean even more insights from the work. Please share with the rest of us what else you find in the data! Tweet at me [@jeffykao](https://twitter.com/jeffykao). :-)\n\n## Getting Started\n\nThis is just a rough sketch of the instructions to the get project up and running on your local machine. Once you get Anaconda installed on your machine, the libraries should be easy to install and the notebooks should be fairly straightforward to run. Instructions to install each library should be easily googlable (sp?).\n\n### Prerequisites\n\n#### Data\n\nFirst set of data (text and duplicate counts only) [posted on kaggle](https://www.kaggle.com/jeffkao/proc_17_108_unique_comments_text_dupe_count). The README  on kaggle will contain links to other versions and subsets of the same dataset.\n\nI'm working hard to get non-text data up as well and will let you know the progress by tweet [@jeffykao](https://twitter.com/jeffykao).\n\n#### Python/Anaconda version\n\n- Python 3.6.1 (64-bit)\n- conda 4.3.29\n\n#### Libraries used\n\n- [NumPy](http://www.numpy.org)\n- [scikit-learn](http://scikit-learn.org/stable/)\n- [matplotlib](http://matplotlib.org)\n- [pandas](http://pandas.pydata.org)\n- [HDBSCAN](https://github.com/scikit-learn-contrib/hdbscan)\n- [spaCy](https://spacy.io/usage/)\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details\n\n## Acknowledgments\n\n* [@drob](https://twitter.com/drob) for putting the blog post on blast and giving me some great advice in the aftermath\n* [@leland_mcinnes](https://twitter.com/leland_mcinnes) for authoring HDBSCAN\n* [@bekcunning](https://twitter.com/bekcunning) for sending me the link that made me finally _write that g***** blog post!_\n* [@prb_data](https://twitter.com/prb_data) & Joe Eddy, my instructors at [Metis](https://www.thisismetis.com/)\n* [@AndrewDBS](https://twitter.com/AndrewDBS) who convinced me to get a twitter account\n* My amazing & creative wife/editor who read through & greatly improved my drafts\n* Sweat pants.\n"
 },
 {
  "repo": "berkmancenter/internet_monitor",
  "language": "HTML",
  "readme_contents": "Internet Monitor\n================\n\nThis is the website for the Internet Monitor home page: https://thenetmonitor.org\n\nSetup\n=====\n\nThis is a Ruby on Rails website running RefineryCMS.\n\nInstall dependencies:\n\n* `apt-get install ruby-dev rubygems`\n* `gem install bundler`\n\nTo get it up and running:\n\n1. clone this repository\n1. install the required gems with: `bundle install`\n1. copy config/database.yml.example to config/database.yml and edit it to match your setup\n1. create the initial database: `rake db:setup`\n1. start the rails server: `rails s`\n1. browse to http://localhost:3000\n\n\n\n"
 },
 {
  "repo": "ahmia/ahmia-site",
  "language": "Python",
  "readme_contents": "[![Build Status](https://travis-ci.org/ahmia/ahmia-site.svg?branch=master)](https://travis-ci.org/ahmia/ahmia-site)\n[![Code Health](https://landscape.io/github/ahmia/ahmia-site/master/landscape.svg?style=flat)](https://landscape.io/github/ahmia/ahmia-site/master)\n[![Requirements Status](https://requires.io/github/ahmia/ahmia-site/requirements.svg?branch=master)](https://requires.io/github/ahmia/ahmia-site/requirements/?branch=master)\n\n![https://ahmia.fi/](https://raw.githubusercontent.com/razorfinger/ahmia/ahmia-redesign/ahmia-logotype.png)\n\nAhmia is the search engine for `.onion` domains on the Tor anonymity\nnetwork. It is led by [Juha Nurmi](//github.com/juhanurmi) and is based\nin Finland. This repository contains ahmia.fi source code.\n\n# Compatibility\n\nThe newest version of Ahmia is built with Python 3.6, Django 1.11 and Elasticsearch 6.2 (5.6 is also compatible).\nYou will need to know these technologies to create a working Ahmia installation.\nAhmia crawls using [OnionBot](https://github.com/ahmia/ahmia-crawler).\n\n# Prerequisites\n[Ahmia-index](https://github.com/ahmia/ahmia-index) should be installed and running\n\n# Installation guide\n\n## Install dependencies:\n\n### Ubuntu 16.04\n```sh\n$ (sudo) apt-get install build-essential python3 python3-pip python3-dev python3-setuptools\npython3-virtualenv libxml2-dev libxslt1-dev python3-dev libpq-dev libffi-dev libssl-dev\n```\n\n### Fedora 23\n```sh\n$ (sudo) dnf install @development-tools redhat-rpm-config python3-pip python3-virtualenv\n$ (sudo) dnf install libxml-devel libxslt-devel python3-devel postgresql-devel libffi-devel openssl-devel\n```\n\n## Install requirements in a virtual environment\n\n```sh\n$ virtualenv /path/to/venv\n$ source /path/to/venv/bin/activate\n(venv)$ pip install -r requirements/dev.txt\n```\n\nOr globally, for instance, in production server:\n\n```sh\n$ pip3 install -r requirements/prod.txt\n```\n\n## Configuration\n\nThis is a common step, both for local (dev) and production environment.\n\n```\n$ cp ahmia/ahmia/settings/example.env ahmia/ahmia/settings/.env\n```\n\nPlease **modify the values** in `.env`, to fit your needs. You have to specify\nat least the postgresql credentials, if you are using the production settings.\n\n\n## Setup Website\n\n### Migrate db\n```sh\n$ python3 ahmia/manage.py makemigrations\n$ python3 ahmia/manage.py migrate\n```\n\n### Make the static files\n```sh\n$ python3 ahmia/manage.py collectstatic\n```\n\n# Run site in dev mode\n\n## Start development server\n\nDevelopment settings use sqlite as a database.\nDefault settings should work out of the box.\n\n```sh\n$ python3 ahmia/manage.py runserver\n```\n\n## Crontab\n\n* Rule to remove onions added by users weekly\n```sh\n0 0 */7 * * python3 /usr/local/lib/ahmia-site/ahmia/manage.py remove_onions --settings=ahmia.settings.prod\n```\n\n* Rule to update usage statistics hourly (could be once per day as well)\n```sh\n59 * * * * python3 /usr/local/lib/ahmia-site/ahmia/manage.py update_stats --settings=ahmia.settings.prod\n```\n\n* Rule to clean up some DB tables on the first day of each month\n```sh\n0 0 1 * * python3 /usr/local/lib/ahmia-site/ahmia/manage.py cleanup_db --settings=ahmia.settings.prod\n```\n\n* Rule to build PagePopularity Score Index every 10 days\n```sh\n0 0 */10 * * python3 /usr/local/lib/ahmia-site/ahmia/manage.py calc_page_pop --settings=ahmia.settings.prod\n```\n\n__NOTE__: If you are using virtualenv replace `python3` with the absolute path to your virtualenv's python executable, e.g `/path/to/venv/lib/python`\n\n__NOTE__: If your deployment directory isn't `/usr/local/lib/ahmia-site` replace accordingly\n# FAQ\n\n## How can I populate my index to do searches ?\nYou should use [OnionElasticBot](https://github.com/ahmia/ahmia-crawler/tree/master/ahmia) to populate your index.\n\n## Why can't my browser load django statics ?\nThe django settings.py is configured in a way that it only serves statics if DEBUG is True.\nPlease verify [here](https://github.com/ahmia/ahmia-site/blob/master/ahmia/ahmia/settings/dev.py#L6)\nif it's the case. You can change this behaviour\n[here](https://github.com/ahmia/ahmia-site/blob/master/ahmia/ahmia/urls.py#L71).\n\n## What should I use to host ahmia in a production environment ?\n\nWe suggest to deploy ahmia using Apache2 or Nginx with Gunicorn.\nConfig samples are in [config/](https://github.com/ahmia/ahmia-site/tree/master/conf).\n\n* Moreover you need to create a postgres database, and insert the database credentials in\n`ahmia/ahmia/settings/.env`.\n\n* Configure and run nginx:\n```sh\n(sudo) cp conf/nginx/django-ahmia /etc/nginx/sites-enabled/django-ahmia\n(sudo) service nginx start\n```\n\nEITHER:\n\n* Run gunicorn via bash scripts (work as daemons ~ edit files to change):\n```sh\nbash ./bin/run-ahmia.sh\nbash ./bin/run-ahmia-onion.sh\n```\n\nOR\n\n* Alternatively you can **configure and** run gunicorn as systemd daemon\n```sh\n(sudo) cp conf/gunicorn/*.service /etc/systemd/system/\n(sudo) service gunicorn (re)start\n(sudo) systemctl enable /etc/systemd/system/ahmia.service\n(sude) systemctl enable /etc/systemd/system/msydqstlz2kzerdg.service\n```\n\nIn that case it is **highly recommended** editing `/etc/systemd/system/gunicorn.service` to replace:\n-- `User` with the login user (eithewise gunicorn will be ran as **root**).\n-- `ExecStart` value, with your gunicorn path  (needed if gunicorn in virtualenv)\n\n## How to run the Django Dev Server using the Production Settings?\n\nIf you want to have a quick grasp of the production settings, using the development server:\n\n```sh\n$ python3 ahmia/manage.py runserver --settings=ahmia.settings.prod\n```\n\n__NOTE__: You can also append `--settings=ahmia.settings.prod` to any other `manage.py` command.\n\n# Support\n\nNo support is currently provided. It is up to you for now. This will change as Ahmia stabilizes.\n\n# License\n\nAhmia is licensed under the [3-clause BSD license](\nhttps://en.wikipedia.org/wiki/BSD_licenses#3-clause_license_.28.22Revised_BSD_License.22.2C_.22New_BSD_License.22.2C_or_.22Modified_BSD_License.22.29).\n"
 },
 {
  "repo": "apache/spark",
  "language": "Scala",
  "readme_contents": "# Apache Spark\n\nSpark is a unified analytics engine for large-scale data processing. It provides\nhigh-level APIs in Scala, Java, Python, and R, and an optimized engine that\nsupports general computation graphs for data analysis. It also supports a\nrich set of higher-level tools including Spark SQL for SQL and DataFrames,\nMLlib for machine learning, GraphX for graph processing,\nand Structured Streaming for stream processing.\n\n<https://spark.apache.org/>\n\n[![Jenkins Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3)\n[![AppVeyor Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)\n[![PySpark Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)\n\n\n## Online Documentation\n\nYou can find the latest Spark documentation, including a programming\nguide, on the [project web page](https://spark.apache.org/documentation.html).\nThis README file only contains basic setup instructions.\n\n## Building Spark\n\nSpark is built using [Apache Maven](https://maven.apache.org/).\nTo build Spark and its example programs, run:\n\n    ./build/mvn -DskipTests clean package\n\n(You do not need to do this if you downloaded a pre-built package.)\n\nMore detailed documentation is available from the project site, at\n[\"Building Spark\"](https://spark.apache.org/docs/latest/building-spark.html).\n\nFor general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](https://spark.apache.org/developer-tools.html).\n\n## Interactive Scala Shell\n\nThe easiest way to start using Spark is through the Scala shell:\n\n    ./bin/spark-shell\n\nTry the following command, which should return 1,000,000,000:\n\n    scala> spark.range(1000 * 1000 * 1000).count()\n\n## Interactive Python Shell\n\nAlternatively, if you prefer Python, you can use the Python shell:\n\n    ./bin/pyspark\n\nAnd run the following command, which should also return 1,000,000,000:\n\n    >>> spark.range(1000 * 1000 * 1000).count()\n\n## Example Programs\n\nSpark also comes with several sample programs in the `examples` directory.\nTo run one of them, use `./bin/run-example <class> [params]`. For example:\n\n    ./bin/run-example SparkPi\n\nwill run the Pi example locally.\n\nYou can set the MASTER environment variable when running examples to submit\nexamples to a cluster. This can be a mesos:// or spark:// URL,\n\"yarn\" to run on YARN, and \"local\" to run\nlocally with one thread, or \"local[N]\" to run locally with N threads. You\ncan also use an abbreviated class name if the class is in the `examples`\npackage. For instance:\n\n    MASTER=spark://host:7077 ./bin/run-example SparkPi\n\nMany of the example programs print usage help if no params are given.\n\n## Running Tests\n\nTesting first requires [building Spark](#building-spark). Once Spark is built, tests\ncan be run using:\n\n    ./dev/run-tests\n\nPlease see the guidance on how to\n[run tests for a module, or individual tests](https://spark.apache.org/developer-tools.html#individual-tests).\n\nThere is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md\n\n## A Note About Hadoop Versions\n\nSpark uses the Hadoop core library to talk to HDFS and other Hadoop-supported\nstorage systems. Because the protocols have changed in different versions of\nHadoop, you must build Spark against the same version that your cluster runs.\n\nPlease refer to the build documentation at\n[\"Specifying the Hadoop Version and Enabling YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)\nfor detailed guidance on building for a particular distribution of Hadoop, including\nbuilding for particular Hive and Hive Thriftserver distributions.\n\n## Configuration\n\nPlease refer to the [Configuration Guide](https://spark.apache.org/docs/latest/configuration.html)\nin the online documentation for an overview on how to configure Spark.\n\n## Contributing\n\nPlease review the [Contribution to Spark guide](https://spark.apache.org/contributing.html)\nfor information on how to get started contributing to the project.\n"
 },
 {
  "repo": "apache/hadoop",
  "language": "Java",
  "readme_contents": "For the latest information about Hadoop, please visit our website at:\n\n   http://hadoop.apache.org/\n\nand our wiki, at:\n\n   https://cwiki.apache.org/confluence/display/HADOOP/\n"
 },
 {
  "repo": "jbhuang0604/awesome-computer-vision",
  "language": null,
  "readme_contents": "\n# Awesome Computer Vision: [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\nA curated list of awesome computer vision resources, inspired by [awesome-php](https://github.com/ziadoz/awesome-php).\n\nFor a list people in computer vision listed with their academic genealogy, please visit [here](https://github.com/jbhuang0604/awesome-computer-vision/blob/master/people.md)\n\n## Contributing\nPlease feel free to send me [pull requests](https://github.com/jbhuang0604/awesome-computer-vision/pulls) or email (jbhuang1@illinois.edu) to add links.\n\n## Table of Contents\n\n - [Books](#books)\n - [Courses](#courses)\n - [Papers](#papers)\n - [Software](#software)\n - [Datasets](#datasets)\n - [Tutorials and Talks](#tutorials-and-talks)\n - [Resources for students](#resources-for-students)\n - [Blogs](#blogs)\n - [Links](#links)\n - [Songs](#songs)\n\n## Books\n\n#### Computer Vision\n* [Computer Vision:  Models, Learning, and Inference](http://www.computervisionmodels.com/) - Simon J. D. Prince 2012\n* [Computer Vision: Theory and Application](http://szeliski.org/Book/) - Rick Szeliski 2010\n* [Computer Vision: A Modern Approach (2nd edition)](http://www.amazon.com/Computer-Vision-Modern-Approach-2nd/dp/013608592X/ref=dp_ob_title_bk) - David Forsyth and Jean Ponce 2011\n* [Multiple View Geometry in Computer Vision](http://www.robots.ox.ac.uk/~vgg/hzbook/) - Richard Hartley and Andrew Zisserman 2004\n* [Computer Vision](http://www.amazon.com/Computer-Vision-Linda-G-Shapiro/dp/0130307963) - Linda G. Shapiro 2001\n* [Vision Science: Photons to Phenomenology](http://www.amazon.com/Vision-Science-Phenomenology-Stephen-Palmer/dp/0262161834/) - Stephen E. Palmer 1999\n* [Visual Object Recognition synthesis lecture](http://www.morganclaypool.com/doi/abs/10.2200/S00332ED1V01Y201103AIM011) - Kristen Grauman and Bastian Leibe 2011\n* [Computer Vision for Visual Effects](http://cvfxbook.com/) - Richard J. Radke, 2012\n* [High dynamic range imaging: acquisition, display, and image-based lighting](http://www.amazon.com/High-Dynamic-Range-Imaging-Second/dp/012374914X) - Reinhard, E., Heidrich, W., Debevec, P., Pattanaik, S., Ward, G., Myszkowski, K 2010\n* [Numerical Algorithms: Methods for Computer Vision, Machine Learning, and Graphics](https://people.csail.mit.edu/jsolomon/share/book/numerical_book.pdf) - Justin Solomon 2015\n\n#### OpenCV Programming\n* [Learning OpenCV: Computer Vision with the OpenCV Library](http://www.amazon.com/Learning-OpenCV-Computer-Vision-Library/dp/0596516134) - Gary Bradski and Adrian Kaehler\n* [Practical Python and OpenCV](https://www.pyimagesearch.com/practical-python-opencv/) - Adrian Rosebrock\n* [OpenCV Essentials](http://www.amazon.com/OpenCV-Essentials-Oscar-Deniz-Suarez/dp/1783984244/ref=sr_1_1?s=books&ie=UTF8&qid=1424594237&sr=1-1&keywords=opencv+essentials#) - Oscar Deniz Suarez, M\u00aa del Milagro Fernandez Carrobles, Noelia Vallez Enano, Gloria Bueno Garcia, Ismael Serrano Gracia\n\n#### Machine Learning\n* [Pattern Recognition and Machine Learning](http://research.microsoft.com/en-us/um/people/cmbishop/prml/index.htm) - Christopher M. Bishop 2007\n* [Neural Networks for Pattern Recognition](http://www.engineering.upm.ro/master-ie/sacpi/mat_did/info068/docum/Neural%20Networks%20for%20Pattern%20Recognition.pdf) - Christopher M. Bishop 1995\n* [Probabilistic Graphical Models: Principles and Techniques](http://pgm.stanford.edu/) - Daphne Koller and Nir Friedman 2009\n* [Pattern Classification](http://www.amazon.com/Pattern-Classification-2nd-Richard-Duda/dp/0471056693) - Peter E. Hart, David G. Stork, and Richard O. Duda 2000\n* [Machine Learning](http://www.amazon.com/Machine-Learning-Tom-M-Mitchell/dp/0070428077/) - Tom M. Mitchell 1997\n* [Gaussian processes for machine learning](http://www.gaussianprocess.org/gpml/) - Carl Edward Rasmussen and Christopher K. I. Williams 2005\n* [Learning From Data](https://work.caltech.edu/telecourse.html)- Yaser S. Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin 2012\n* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) - Michael Nielsen 2014\n* [Bayesian Reasoning and Machine Learning](http://www.cs.ucl.ac.uk/staff/d.barber/brml/) - David Barber, Cambridge University Press, 2012\n\n#### Fundamentals\n * [Linear Algebra and Its Applications](http://www.amazon.com/Linear-Algebra-Its-Applications-4th/dp/0030105676/ref=sr_1_4?ie=UTF8&qid=1421433773&sr=8-4&keywords=Linear+Algebra+and+Its+Applications) - Gilbert Strang 1995\n\n## Courses\n\n#### Computer Vision\n * [EENG 512 / CSCI 512 - Computer Vision](http://inside.mines.edu/~whoff/courses/EENG512/) - William Hoff (Colorado School of Mines)\n * [Visual Object and Activity Recognition](https://sites.google.com/site/ucbcs29443/) - Alexei A. Efros and Trevor Darrell (UC Berkeley)\n * [Computer Vision](http://courses.cs.washington.edu/courses/cse455/12wi/) - Steve Seitz (University of Washington)\n * Visual Recognition [Spring 2016](http://vision.cs.utexas.edu/381V-spring2016/), [Fall 2016](http://vision.cs.utexas.edu/381V-fall2016/) - Kristen Grauman (UT Austin)\n * [Language and Vision](http://www.tamaraberg.com/teaching/Spring_15/) - Tamara Berg (UNC Chapel Hill)\n * [Convolutional Neural Networks for Visual Recognition](http://vision.stanford.edu/teaching/cs231n/) - Fei-Fei Li and Andrej Karpathy (Stanford University)\n * [Computer Vision](http://cs.nyu.edu/~fergus/teaching/vision/index.html) - Rob Fergus (NYU)\n * [Computer Vision](https://courses.engr.illinois.edu/cs543/sp2015/) - Derek Hoiem (UIUC)\n * [Computer Vision: Foundations and Applications](http://vision.stanford.edu/teaching/cs131_fall1415/index.html) - Kalanit Grill-Spector and Fei-Fei Li (Stanford University)\n * [High-Level Vision: Behaviors, Neurons and Computational Models](http://vision.stanford.edu/teaching/cs431_spring1314/) - Fei-Fei Li (Stanford University)\n * [Advances in Computer Vision](http://6.869.csail.mit.edu/fa15/) - Antonio Torralba and Bill Freeman (MIT)\n * [Computer Vision](http://www.vision.rwth-aachen.de/course/11/) - Bastian Leibe (RWTH Aachen University)\n * [Computer Vision 2](http://www.vision.rwth-aachen.de/course/9/) - Bastian Leibe (RWTH Aachen University)\n * [Computer Vision](http://klewel.com/conferences/epfl-computer-vision/) Pascal Fua (EPFL):\n * [Computer Vision 1](http://cvlab-dresden.de/courses/computer-vision-1/) Carsten Rother (TU Dresden):\n * [Computer Vision 2](http://cvlab-dresden.de/courses/CV2/) Carsten Rother (TU Dresden):\n * [Multiple View Geometry](https://youtu.be/RDkwklFGMfo?list=PLTBdjV_4f-EJn6udZ34tht9EVIW7lbeo4) Daniel Cremers (TU Munich):\n\n\n\n\n#### Computational Photography\n* [Image Manipulation and Computational Photography](http://inst.eecs.berkeley.edu/~cs194-26/fa14/) - Alexei A. Efros (UC Berkeley)\n* [Computational Photography](http://graphics.cs.cmu.edu/courses/15-463/2012_fall/463.html) - Alexei A. Efros (CMU)\n* [Computational Photography](https://courses.engr.illinois.edu/cs498dh3/) - Derek Hoiem (UIUC)\n* [Computational Photography](http://cs.brown.edu/courses/csci1290/) - James Hays (Brown University)\n* [Digital & Computational Photography](http://stellar.mit.edu/S/course/6/sp12/6.815/) - Fredo Durand (MIT)\n* [Computational Camera and Photography](http://ocw.mit.edu/courses/media-arts-and-sciences/mas-531-computational-camera-and-photography-fall-2009/) - Ramesh Raskar (MIT Media Lab)\n* [Computational Photography](https://www.udacity.com/course/computational-photography--ud955) - Irfan Essa (Georgia Tech)\n* [Courses in Graphics](http://graphics.stanford.edu/courses/) - Stanford University\n* [Computational Photography](http://cs.nyu.edu/~fergus/teaching/comp_photo/index.html) - Rob Fergus (NYU)\n* [Introduction to Visual Computing](http://www.cs.toronto.edu/~kyros/courses/320/) - Kyros Kutulakos (University of Toronto)\n* [Computational Photography](http://www.cs.toronto.edu/~kyros/courses/2530/) - Kyros Kutulakos (University of Toronto)\n* [Computer Vision for Visual Effects](https://www.ecse.rpi.edu/~rjradke/cvfxcourse.html) - Rich Radke (Rensselaer Polytechnic Institute)\n* [Introduction to Image Processing](https://www.ecse.rpi.edu/~rjradke/improccourse.html) - Rich Radke (Rensselaer Polytechnic Institute)\n\n#### Machine Learning and Statistical Learning\n * [Machine Learning](https://www.coursera.org/learn/machine-learning) - Andrew Ng (Stanford University)\n * [Learning from Data](https://work.caltech.edu/telecourse.html) - Yaser S. Abu-Mostafa (Caltech)\n * [Statistical Learning](https://class.stanford.edu/courses/HumanitiesandScience/StatLearning/Winter2015/about) - Trevor Hastie and Rob Tibshirani (Stanford University)\n * [Statistical Learning Theory and Applications](http://www.mit.edu/~9.520/fall14/) - Tomaso Poggio, Lorenzo Rosasco, Carlo Ciliberto, Charlie Frogner, Georgios Evangelopoulos, Ben Deen (MIT)\n * [Statistical Learning](http://www.stat.rice.edu/~gallen/stat640.html) - Genevera Allen (Rice University)\n * [Practical Machine Learning](http://www.cs.berkeley.edu/~jordan/courses/294-fall09/) - Michael Jordan (UC Berkeley)\n * [Course on Information Theory, Pattern Recognition, and Neural Networks](http://videolectures.net/course_information_theory_pattern_recognition/) - David MacKay (University of Cambridge)\n * [Methods for Applied Statistics: Unsupervised Learning](http://web.stanford.edu/~lmackey/stats306b/) - Lester Mackey (Stanford)\n * [Machine Learning](http://www.robots.ox.ac.uk/~az/lectures/ml/index.html) - Andrew Zisserman (University of Oxford)\n * [Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120) - Sebastian Thrun (Stanford University)\n * [Machine Learning](https://www.udacity.com/course/machine-learning--ud262) - Charles Isbell, Michael Littman (Georgia Tech)\n * [(Convolutional) Neural Networks for Visual Recognition](https://cs231n.github.io/) - Fei-Fei Li, Andrej Karphaty, Justin Johnson (Stanford University)\n * [Machine Learning for Computer Vision](https://youtu.be/QZmZFeZxEKI?list=PLTBdjV_4f-EIiongKlS9OKrBEp8QR47Wl) - Rudolph Triebel (TU Munich)\n\n\n\n#### Optimization\n * [Convex Optimization I](http://stanford.edu/class/ee364a/) - Stephen Boyd (Stanford University)\n * [Convex Optimization II](http://stanford.edu/class/ee364b/) - Stephen Boyd (Stanford University)\n * [Convex Optimization](https://class.stanford.edu/courses/Engineering/CVX101/Winter2014/about) - Stephen Boyd (Stanford University)\n * [Optimization at MIT](http://optimization.mit.edu/classes.php) - (MIT)\n * [Convex Optimization](http://www.stat.cmu.edu/~ryantibs/convexopt/) - Ryan Tibshirani (CMU)\n\n## Papers\n\n#### Conference papers on the web\n * [CVPapers](http://www.cvpapers.com/) - Computer vision papers on the web\n * [SIGGRAPH Paper on the web](http://kesen.realtimerendering.com/) - Graphics papers on the web\n * [NIPS Proceedings](http://papers.nips.cc/) - NIPS papers on the web\n * [Computer Vision Foundation open access](http://www.cv-foundation.org/openaccess/menu.py)\n * [Annotated Computer Vision Bibliography](http://iris.usc.edu/Vision-Notes/bibliography/contents.html) - Keith Price (USC)\n * [Calendar of Computer Image Analysis, Computer Vision Conferences](http://iris.usc.edu/Information/Iris-Conferences.html) - (USC)\n\n#### Survey Papers\n * [Visionbib Survey Paper List](http://surveys.visionbib.com/index.html)\n * [Foundations and Trends\u00ae in Computer Graphics and Vision](http://www.nowpublishers.com/CGV)\n * [Computer Vision: A Reference Guide](http://link.springer.com/book/10.1007/978-0-387-31439-6)\n\n## Tutorials and talks\n\n#### Computer Vision\n * [Computer Vision Talks](http://www.computervisiontalks.com/) - Lectures, keynotes, panel discussions on computer vision\n * [The Three R's of Computer Vision](https://www.youtube.com/watch?v=Mqg6eorYRIQ) - Jitendra Malik (UC Berkeley) 2013\n * [Applications to Machine Vision](http://videolectures.net/epsrcws08_blake_amv/) - Andrew Blake (Microsoft Research) 2008\n * [The Future of Image Search](http://videolectures.net/kdd08_malik_fis/?q=image) - Jitendra Malik (UC Berkeley) 2008\n * [Should I do a PhD in Computer Vision?](https://www.youtube.com/watch?v=M17oGxh3Ny8) - Fatih Porikli (Australian National University)\n - [Graduate Summer School 2013: Computer Vision](http://www.ipam.ucla.edu/programs/summer-schools/graduate-summer-school-computer-vision/?tab=schedule) - IPAM, 2013\n\n#### Recent Conference Talks\n- [CVPR 2015](http://www.pamitc.org/cvpr15/) - Jun 2015\n- [ECCV 2014](http://videolectures.net/eccv2014_zurich/) - Sep 2014\n- [CVPR 2014](http://techtalks.tv/cvpr-2014-oral-talks/) - Jun 2014\n- [ICCV 2013](http://techtalks.tv/iccv2013/) - Dec 2013\n- [ICML 2013](http://techtalks.tv/icml/2013/) - Jul 2013\n- [CVPR 2013](http://techtalks.tv/cvpr2013/) - Jun 2013\n- [ECCV 2012](http://videolectures.net/eccv2012_firenze/) - Oct 2012\n- [ICML 2012](http://techtalks.tv/icml/2012/orals/) - Jun 2012\n- [CVPR 2012](http://techtalks.tv/cvpr2012webcast/) - Jun 2012\n\n#### 3D Computer Vision\n * [3D Computer Vision: Past, Present, and Future](https://www.youtube.com/watch?v=kyIzMr917Rc) - Steve Seitz (University of Washington) 2011\n * [Reconstructing the World from Photos on the Internet](https://www.youtube.com/watch?v=04Kgg3QEXFI) - Steve Seitz (University of Washington) 2013\n\n#### Internet Vision\n * [The Distributed Camera](http://www.technologyreview.com/video/426265/meet-2011-tr35-winner-noah-snavely/) - Noah Snavely (Cornell University) 2011\n * [Planet-Scale Visual Understanding](https://www.youtube.com/watch?v=UHkCa9-Z1Ps) - Noah Snavely (Cornell University) 2014\n * [A Trillion Photos](https://www.youtube.com/watch?v=6MWEfpKUfRc) - Steve Seitz (University of Washington) 2013\n\n#### Computational Photography\n * [Reflections on Image-Based Modeling and Rendering](https://www.youtube.com/watch?v=j90_0Ndk7XM) - Richard Szeliski (Microsoft Research) 2013\n * [Photographing Events over Time](https://www.youtube.com/watch?v=ZvPaHZZVPRk) - William T. Freeman (MIT) 2011\n * [Old and New algorithm for Blind Deconvolution](http://videolectures.net/nipsworkshops2011_weiss_deconvolution/) -  Yair Weiss (The Hebrew University of Jerusalem) 2011\n * [A Tour of Modern \"Image Processing\"](http://videolectures.net/nipsworkshops2010_milanfar_tmi/) -  Peyman Milanfar (UC Santa Cruz/Google) 2010\n * [Topics in image and video processing](http://videolectures.net/mlss07_blake_tiivp/) Andrew Blake (Microsoft Research) 2007\n * [Computational Photography](https://www.youtube.com/watch?v=HJVNI0mkmqk) - William T. Freeman (MIT) 2012\n * [Revealing the Invisible](https://www.youtube.com/watch?v=_BWnIQY_X98) - Fr\u00e9do Durand (MIT) 2012\n * [Overview of Computer Vision and Visual Effects](https://www.youtube.com/watch?v=rE-hVtytT-I) - Rich Radke (Rensselaer Polytechnic Institute) 2014\n\n#### Learning and Vision\n * [Where machine vision needs help from machine learning](http://videolectures.net/colt2011_freeman_help/?q=computer%20vision) - William T. Freeman (MIT) 2011\n * [Learning in Computer Vision](http://videolectures.net/mlss08au_lucey_linv/) - Simon Lucey (CMU) 2008\n * [Learning and Inference in Low-Level Vision](http://videolectures.net/nips09_weiss_lil/?q=computer%20vision) - Yair Weiss (The Hebrew University of Jerusalem) 2009\n\n#### Object Recognition\n * [Object Recognition](http://research.microsoft.com/apps/video/dl.aspx?id=231358) - Larry Zitnick (Microsoft Research)\n * [Generative Models for Visual Objects and Object Recognition via Bayesian Inference](http://videolectures.net/mlas06_li_gmvoo/?q=Fei-Fei%20Li) - Fei-Fei Li (Stanford University)\n\n#### Graphical Models\n * [Graphical Models for Computer Vision](http://videolectures.net/uai2012_felzenszwalb_computer_vision/?q=computer%20vision) - Pedro Felzenszwalb (Brown University) 2012\n * [Graphical Models](http://videolectures.net/mlss09uk_ghahramani_gm/) - Zoubin Ghahramani (University of Cambridge) 2009\n * [Machine Learning, Probability and Graphical Models](http://videolectures.net/mlss06tw_roweis_mlpgm/) - Sam Roweis (NYU) 2006\n * [Graphical Models and Applications](http://videolectures.net/mlss09us_weiss_gma/?q=Graphical%20Models) -  Yair Weiss (The Hebrew University of Jerusalem) 2009\n\n#### Machine Learning\n * [A Gentle Tutorial of the EM Algorithm](https://nikola-rt.ee.washington.edu/people/bulyko/papers/em.pdf) - Jeff A. Bilmes (UC Berkeley) 1998\n * [Introduction To Bayesian Inference](http://videolectures.net/mlss09uk_bishop_ibi/) - Christopher Bishop (Microsoft Research) 2009\n * [Support Vector Machines](http://videolectures.net/mlss06tw_lin_svm/) - Chih-Jen Lin (National Taiwan University) 2006\n * [Bayesian or Frequentist, Which Are You? ](http://videolectures.net/mlss09uk_jordan_bfway/) - Michael I. Jordan (UC Berkeley)\n\n#### Optimization\n * [Optimization Algorithms in Machine Learning](http://videolectures.net/nips2010_wright_oaml/) - Stephen J. Wright (University of Wisconsin-Madison)\n * [Convex Optimization](http://videolectures.net/mlss07_vandenberghe_copt/?q=convex%20optimization) - Lieven Vandenberghe (University of California, Los Angeles)\n * [Continuous Optimization in Computer Vision](https://www.youtube.com/watch?v=oZqoWozVDVg) - Andrew Fitzgibbon (Microsoft Research)\n * [Beyond stochastic gradient descent for large-scale machine learning](http://videolectures.net/sahd2014_bach_stochastic_gradient/) - Francis Bach (INRIA)\n * [Variational Methods for Computer Vision](https://www.youtube.com/playlist?list=PLTBdjV_4f-EJ7A2iIH5L5ztqqrWYjP2RI) - Daniel Cremers (Technische Universit\u00e4t M\u00fcnchen) ([lecture 18 missing from playlist](https://www.youtube.com/watch?v=GgcbVPNd3SI))\n\n#### Deep Learning\n * [A tutorial on Deep Learning](http://videolectures.net/jul09_hinton_deeplearn/) - Geoffrey E. Hinton (University of Toronto)\n * [Deep Learning](http://videolectures.net/kdd2014_salakhutdinov_deep_learning/?q=Hidden%20Markov%20model#) -  Ruslan Salakhutdinov (University of Toronto)\n * [Scaling up Deep Learning](http://videolectures.net/kdd2014_bengio_deep_learning/) - Yoshua Bengio (University of Montreal)\n * [ImageNet Classification with Deep Convolutional Neural Networks](http://videolectures.net/machine_krizhevsky_imagenet_classification/?q=deep%20learning) -  Alex Krizhevsky (University of Toronto)\n * [The Unreasonable Effectivness Of Deep Learning](http://videolectures.net/sahd2014_lecun_deep_learning/) Yann LeCun (NYU/Facebook Research) 2014\n * [Deep Learning for Computer Vision](https://www.youtube.com/watch?v=qgx57X0fBdA) - Rob Fergus (NYU/Facebook Research)\n * [High-dimensional learning with deep network contractions](http://videolectures.net/sahd2014_mallat_dimensional_learning/) - St\u00e9phane Mallat (Ecole Normale Superieure)\n * [Graduate Summer School 2012: Deep Learning, Feature Learning](http://www.ipam.ucla.edu/programs/summer-schools/graduate-summer-school-deep-learning-feature-learning/?tab=schedule) - IPAM, 2012\n * [Workshop on Big Data and Statistical Machine Learning](http://www.fields.utoronto.ca/programs/scientific/14-15/bigdata/machine/)\n * [Machine Learning Summer School](https://www.youtube.com/channel/UC3ywjSv5OsDiDAnOP8C1NiQ) - Reykjavik, Iceland 2014\n\t* [Deep Learning Session 1](https://www.youtube.com/watch?v=JuimBuvEWBg) - Yoshua Bengio (Universtiy of Montreal)\n\t* [Deep Learning Session 2](https://www.youtube.com/watch?v=Fl-W7_z3w3o) - Yoshua Bengio (University of Montreal)\n\t* [Deep Learning Session 3](https://www.youtube.com/watch?v=_cohR7LAgWA) - Yoshua Bengio (University of Montreal)\n\n## Software\n\n#### Annotation tools\n* [Comma Coloring](http://commacoloring.herokuapp.com/)\n* [Annotorious](https://annotorious.github.io/)\n* [LabelME](http://labelme.csail.mit.edu/Release3.0/)\n* [gtmaker](https://github.com/sanko-shoko/gtmaker)\n\n#### External Resource Links\n * [Computer Vision Resources](https://sites.google.com/site/jbhuang0604/resources/vision) - Jia-Bin Huang (UIUC)\n * [Computer Vision Algorithm Implementations](http://www.cvpapers.com/rr.html) - CVPapers\n * [Source Code Collection for Reproducible Research](http://www.csee.wvu.edu/~xinl/reproducible_research.html) - Xin Li (West Virginia University)\n * [CMU Computer Vision Page](http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/v-source.html)\n\n#### General Purpose Computer Vision Library\n* [Open CV](http://opencv.org/)\n* [mexopencv](http://kyamagu.github.io/mexopencv/)\n* [SimpleCV](http://simplecv.org/)\n* [Open source Python module for computer vision](https://github.com/jesolem/PCV)\n* [ccv: A Modern Computer Vision Library](https://github.com/liuliu/ccv)\n* [VLFeat](http://www.vlfeat.org/)\n* [Matlab Computer Vision System Toolbox](http://www.mathworks.com/products/computer-vision/)\n* [Piotr's Computer Vision Matlab Toolbox](http://vision.ucsd.edu/~pdollar/toolbox/doc/index.html)\n* [PCL: Point Cloud Library](http://pointclouds.org/)\n* [ImageUtilities](https://gitorious.org/imageutilities)\n\n#### Multiple-view Computer Vision\n* [MATLAB Functions for Multiple View Geometry](http://www.robots.ox.ac.uk/~vgg/hzbook/code/)\n* [Peter Kovesi's Matlab Functions for Computer Vision and Image Analysis](http://staffhome.ecm.uwa.edu.au/~00011811/Research/MatlabFns/index.html)\n* [OpenGV ](http://laurentkneip.github.io/opengv/) - geometric computer vision algorithms\n* [MinimalSolvers](http://cmp.felk.cvut.cz/mini/) - Minimal problems solver\n* [Multi-View Environment](http://www.gcc.tu-darmstadt.de/home/proj/mve/)\n* [Visual SFM](http://ccwu.me/vsfm/)\n* [Bundler SFM](http://www.cs.cornell.edu/~snavely/bundler/)\n* [openMVG: open Multiple View Geometry](http://imagine.enpc.fr/~moulonp/openMVG/) - Multiple View Geometry; Structure from Motion library & softwares\n* [Patch-based Multi-view Stereo V2](http://www.di.ens.fr/pmvs/)\n* [Clustering Views for Multi-view Stereo](http://www.di.ens.fr/cmvs/)\n* [Floating Scale Surface Reconstruction](http://www.gris.informatik.tu-darmstadt.de/projects/floating-scale-surface-recon/)\n* [Large-Scale Texturing of 3D Reconstructions](http://www.gcc.tu-darmstadt.de/home/proj/texrecon/)\n* [Awesome 3D reconstruction list](https://github.com/openMVG/awesome_3DReconstruction_list)\n\n\n#### Feature Detection and Extraction\n* [VLFeat](http://www.vlfeat.org/)\n* [SIFT](http://www.cs.ubc.ca/~lowe/keypoints/)\n  * David G. Lowe, \"Distinctive image features from scale-invariant keypoints,\" International Journal of Computer Vision, 60, 2 (2004), pp. 91-110.\n* [SIFT++](http://www.robots.ox.ac.uk/~vedaldi/code/siftpp.html)\n* [BRISK](http://www.asl.ethz.ch/people/lestefan/personal/BRISK)\n  * Stefan Leutenegger, Margarita Chli and Roland Siegwart, \"BRISK: Binary Robust Invariant Scalable Keypoints\", ICCV 2011\n* [SURF](http://www.vision.ee.ethz.ch/~surf/)\n  * Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \"SURF: Speeded Up Robust Features\", Computer Vision and Image Understanding (CVIU), Vol. 110, No. 3, pp. 346--359, 2008\n* [FREAK](http://www.ivpe.com/freak.htm)\n  * A. Alahi, R. Ortiz, and P. Vandergheynst, \"FREAK: Fast Retina Keypoint\", CVPR 2012\n* [AKAZE](http://www.robesafe.com/personal/pablo.alcantarilla/kaze.html)\n  * Pablo F. Alcantarilla, Adrien Bartoli and Andrew J. Davison, \"KAZE Features\", ECCV 2012\n* [Local Binary Patterns](https://github.com/nourani/LBP)\n\n#### High Dynamic Range Imaging\n* [HDR_Toolbox](https://github.com/banterle/HDR_Toolbox)\n\n#### Semantic Segmentation\n* [List of Semantic Segmentation algorithms](http://www.it-caesar.com/list-of-contemporary-semantic-segmentation-datasets/)\n\n#### Low-level Vision\n\n###### Stereo Vision\n * [Middlebury Stereo Vision](http://vision.middlebury.edu/stereo/)\n * [The KITTI Vision Benchmark Suite](http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stero)\n * [LIBELAS: Library for Efficient Large-scale Stereo Matching](http://www.cvlibs.net/software/libelas/)\n * [Ground Truth Stixel Dataset](http://www.6d-vision.com/ground-truth-stixel-dataset)\n\n###### Optical Flow\n * [Middlebury Optical Flow Evaluation](http://vision.middlebury.edu/flow/)\n * [MPI-Sintel Optical Flow Dataset and Evaluation](http://sintel.is.tue.mpg.de/)\n * [The KITTI Vision Benchmark Suite](http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow)\n * [HCI Challenge](http://hci.iwr.uni-heidelberg.de/Benchmarks/document/Challenging_Data_for_Stereo_and_Optical_Flow/)\n * [Coarse2Fine Optical Flow](http://people.csail.mit.edu/celiu/OpticalFlow/) - Ce Liu (MIT)\n * [Secrets of Optical Flow Estimation and Their Principles](http://cs.brown.edu/~dqsun/code/cvpr10_flow_code.zip)\n * [C++/MatLab Optical Flow by C. Liu (based on Brox et al. and Bruhn et al.)](http://people.csail.mit.edu/celiu/OpticalFlow/)\n * [Parallel Robust Optical Flow by S\u00e1nchez P\u00e9rez et al.](http://www.ctim.es/research_works/parallel_robust_optical_flow/)\n\n###### Image Denoising\nBM3D, KSVD,\n\n###### Super-resolution\n * [Multi-frame image super-resolution](http://www.robots.ox.ac.uk/~vgg/software/SR/)\n    * Pickup, L. C. Machine Learning in Multi-frame Image Super-resolution, PhD thesis 2008\n * [Markov Random Fields for Super-Resolution](http://people.csail.mit.edu/billf/project%20pages/sresCode/Markov%20Random%20Fields%20for%20Super-Resolution.html)\n    * W. T Freeman and C. Liu. Markov Random Fields for Super-resolution and Texture Synthesis. In A. Blake, P. Kohli, and C. Rother, eds., Advances in Markov Random Fields for Vision and Image Processing, Chapter 10. MIT Press, 2011\n * [Sparse regression and natural image prior](https://people.mpi-inf.mpg.de/~kkim/supres/supres.htm)\n    * K. I. Kim and Y. Kwon, \"Single-image super-resolution using sparse regression and natural image prior\", IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 32, no. 6, pp. 1127-1133, 2010.\n * [Single-Image Super Resolution via a Statistical Model](http://www.cs.technion.ac.il/~elad/Various/SingleImageSR_TIP14_Box.zip)\n    * T. Peleg and M. Elad, A Statistical Prediction Model Based on Sparse Representations for Single Image Super-Resolution, IEEE Transactions on Image Processing, Vol. 23, No. 6, Pages 2569-2582, June 2014\n * [Sparse Coding for Super-Resolution](http://www.cs.technion.ac.il/~elad/Various/Single_Image_SR.zip)\n    * R. Zeyde, M. Elad, and M. Protter On Single Image Scale-Up using Sparse-Representations, Curves & Surfaces, Avignon-France, June 24-30, 2010 (appears also in Lecture-Notes-on-Computer-Science - LNCS).\n * [Patch-wise Sparse Recovery](http://www.ifp.illinois.edu/~jyang29/ScSR.htm)\n    * Jianchao Yang, John Wright, Thomas Huang, and Yi Ma. Image super-resolution via sparse representation. IEEE Transactions on Image Processing (TIP), vol. 19, issue 11, 2010.\n * [Neighbor embedding](http://www.jdl.ac.cn/user/hchang/doc/code.rar)\n    * H. Chang, D.Y. Yeung, Y. Xiong. Super-resolution through neighbor embedding. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), vol.1, pp.275-282, Washington, DC, USA, 27 June - 2 July 2004.\n * [Deformable Patches](https://sites.google.com/site/yuzhushome/single-image-super-resolution-using-deformable-patches)\n    *  Yu Zhu, Yanning Zhang and Alan Yuille, Single Image Super-resolution using Deformable Patches, CVPR 2014\n * [SRCNN](http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html)\n    * Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Learning a Deep Convolutional Network for Image Super-Resolution, in ECCV 2014\n * [A+: Adjusted Anchored Neighborhood Regression](http://www.vision.ee.ethz.ch/~timofter/ACCV2014_ID820_SUPPLEMENTARY/index.html)\n    * R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted Anchored Neighborhood Regression for Fast Super-Resolution, ACCV 2014\n * [Transformed Self-Exemplars](https://sites.google.com/site/jbhuang0604/publications/struct_sr)\n    * Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja, Single Image Super-Resolution using Transformed Self-Exemplars, IEEE Conference on Computer Vision and Pattern Recognition, 2015\n\n###### Image Deblurring\n\nNon-blind deconvolution\n * [Spatially variant non-blind deconvolution](http://homes.cs.washington.edu/~shanqi/work/spvdeconv/)\n * [Handling Outliers in Non-blind Image Deconvolution](http://cg.postech.ac.kr/research/deconv_outliers/)\n * [Hyper-Laplacian Priors](http://cs.nyu.edu/~dilip/research/fast-deconvolution/)\n * [From Learning Models of Natural Image Patches to Whole Image Restoration](http://people.csail.mit.edu/danielzoran/epllcode.zip)\n * [Deep Convolutional Neural Network for Image Deconvolution](http://lxu.me/projects/dcnn/)\n * [Neural Deconvolution](http://webdav.is.mpg.de/pixel/neural_deconvolution/)\n\nBlind deconvolution\n * [Removing Camera Shake From A Single Photograph](http://www.cs.nyu.edu/~fergus/research/deblur.html)\n * [High-quality motion deblurring from a single image](http://www.cse.cuhk.edu.hk/leojia/projects/motion_deblurring/)\n * [Two-Phase Kernel Estimation for Robust Motion Deblurring](http://www.cse.cuhk.edu.hk/leojia/projects/robust_deblur/)\n * [Blur kernel estimation using the radon transform](http://people.csail.mit.edu/taegsang/Documents/RadonDeblurringCode.zip)\n * [Fast motion deblurring](http://cg.postech.ac.kr/research/fast_motion_deblurring/)\n * [Blind Deconvolution Using a Normalized Sparsity Measure](http://cs.nyu.edu//~dilip/research/blind-deconvolution/)\n * [Blur-kernel estimation from spectral irregularities](http://www.cs.huji.ac.il/~raananf/projects/deblur/)\n * [Efficient marginal likelihood optimization in blind deconvolution](http://www.wisdom.weizmann.ac.il/~levina/papers/LevinEtalCVPR2011Code.zip)\n * [Unnatural L0 Sparse Representation for Natural Image Deblurring](http://www.cse.cuhk.edu.hk/leojia/projects/l0deblur/)\n * [Edge-based Blur Kernel Estimation Using Patch Priors](http://cs.brown.edu/~lbsun/deblur2013/deblur2013iccp.html)\n * [Blind Deblurring Using Internal Patch Recurrence](http://www.wisdom.weizmann.ac.il/~vision/BlindDeblur.html)\n\nNon-uniform Deblurring\n * [Non-uniform Deblurring for Shaken Images](http://www.di.ens.fr/willow/research/deblurring/)\n * [Single Image Deblurring Using Motion Density Functions](http://grail.cs.washington.edu/projects/mdf_deblurring/)\n * [Image Deblurring using Inertial Measurement Sensors](http://research.microsoft.com/en-us/um/redmond/groups/ivm/imudeblurring/)\n * [Fast Removal of Non-uniform Camera Shake](http://webdav.is.mpg.de/pixel/fast_removal_of_camera_shake/)\n\n\n###### Image Completion\n * [GIMP Resynthesizer](http://registry.gimp.org/node/27986)\n * [Priority BP](http://lafarren.com/image-completer/)\n * [ImageMelding](http://www.ece.ucsb.edu/~psen/melding)\n * [PlanarStructureCompletion](https://sites.google.com/site/jbhuang0604/publications/struct_completion)\n\n###### Image Retargeting\n * [RetargetMe](http://people.csail.mit.edu/mrub/retargetme/)\n\n###### Alpha Matting\n * [Alpha Matting Evaluation](http://www.alphamatting.com/)\n * [Closed-form image matting](http://people.csail.mit.edu/alevin/matting.tar.gz)\n * [Spectral Matting](http://www.vision.huji.ac.il/SpectralMatting/)\n * [Learning-based Matting](http://www.mathworks.com/matlabcentral/fileexchange/31412-learning-based-digital-matting)\n * [Improving Image Matting using Comprehensive Sampling Sets](http://www.alphamatting.com/ImprovingMattingComprehensiveSamplingSets_CVPR2013.zip)\n\n###### Image Pyramid\n* [The Steerable Pyramid](http://www.cns.nyu.edu/~eero/steerpyr/)\n* [CurveLab](http://www.curvelet.org/)\n\n###### Edge-preserving image processing\n * [Fast Bilateral Filter](http://people.csail.mit.edu/sparis/bf/)\n * [O(1) Bilateral Filter](http://www.cs.cityu.edu.hk/~qiyang/publications/code/qx.cvpr09.ctbf.zip)\n * [Recursive Bilateral Filtering](http://www.cs.cityu.edu.hk/~qiyang/publications/eccv-12/)\n * [Rolling Guidance Filter](http://www.cse.cuhk.edu.hk/leojia/projects/rollguidance/)\n * [Relative Total Variation](http://www.cse.cuhk.edu.hk/leojia/projects/texturesep/index.html)\n * [L0 Gradient Optimization](http://www.cse.cuhk.edu.hk/leojia/projects/L0smoothing/index.html)\n * [Domain Transform](http://www.inf.ufrgs.br/~eslgastal/DomainTransform/)\n * [Adaptive Manifold](http://inf.ufrgs.br/~eslgastal/AdaptiveManifolds/)\n * [Guided image filtering](http://research.microsoft.com/en-us/um/people/kahe/eccv10/)\n\n#### Intrinsic Images\n\n* [Recovering Intrinsic Images with a global Sparsity Prior on Reflectance](http://people.tuebingen.mpg.de/mkiefel/projects/intrinsic/)\n* [Intrinsic Images by Clustering](http://giga.cps.unizar.es/~elenag/projects/EGSR2012_intrinsic/)\n\n#### Contour Detection and Image Segmentation\n * [Mean Shift Segmentation](http://coewww.rutgers.edu/riul/research/code/EDISON/)\n * [Graph-based Segmentation](http://cs.brown.edu/~pff/segment/)\n * [Normalized Cut](http://www.cis.upenn.edu/~jshi/software/)\n * [Grab Cut](http://grabcut.weebly.com/background--algorithm.html)\n * [Contour Detection and Image Segmentation](http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html)\n * [Structured Edge Detection](http://research.microsoft.com/en-us/downloads/389109f6-b4e8-404c-84bf-239f7cbf4e3d/)\n * [Pointwise Mutual Information](http://web.mit.edu/phillipi/pmi-boundaries/)\n * [SLIC Super-pixel](http://ivrl.epfl.ch/research/superpixels)\n * [QuickShift](http://www.vlfeat.org/overview/quickshift.html)\n * [TurboPixels](http://www.cs.toronto.edu/~babalex/research.html)\n * [Entropy Rate Superpixel](http://mingyuliu.net/)\n * [Contour Relaxed Superpixels](http://www.vsi.cs.uni-frankfurt.de/research/current-projects/research/superpixel-segmentation/)\n * [SEEDS](http://www.mvdblive.org/seeds/)\n * [SEEDS Revised](https://github.com/davidstutz/seeds-revised)\n * [Multiscale Combinatorial Grouping](http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/)\n * [Fast Edge Detection Using Structured Forests](https://github.com/pdollar/edges)\n\n#### Interactive Image Segmentation\n * [Random Walker](http://cns.bu.edu/~lgrady/software.html)\n * [Geodesic Segmentation](http://www.tc.umn.edu/~baixx015/)\n * [Lazy Snapping](http://research.microsoft.com/apps/pubs/default.aspx?id=69040)\n * [Power Watershed](http://powerwatershed.sourceforge.net/)\n * [Geodesic Graph Cut](http://www.adobe.com/technology/people/san-jose/brian-price.html)\n * [Segmentation by Transduction](http://www.cs.cmu.edu/~olivierd/)\n\n#### Video Segmentation\n * [Video Segmentation with Superpixels](http://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/image-and-video-segmentation/video-segmentation-with-superpixels/)\n * [Efficient hierarchical graph-based video segmentation](http://www.cc.gatech.edu/cpl/projects/videosegmentation/)\n * [Object segmentation in video](http://lmb.informatik.uni-freiburg.de/Publications/2011/OB11/)\n * [Streaming hierarchical video segmentation](http://www.cse.buffalo.edu/~jcorso/r/supervoxels/)\n\n#### Camera calibration\n * [Camera Calibration Toolbox for Matlab](http://www.vision.caltech.edu/bouguetj/calib_doc/)\n * [Camera calibration With OpenCV](http://docs.opencv.org/trunk/doc/tutorials/calib3d/camera_calibration/camera_calibration.html#)\n * [Multiple Camera Calibration Toolbox](https://sites.google.com/site/prclibo/toolbox)\n\n#### Simultaneous localization and mapping\n\n###### SLAM community:\n * [openSLAM](https://www.openslam.org/)\n * [Kitti Odometry: benchmark for outdoor visual odometry (codes may be available)](http://www.cvlibs.net/datasets/kitti/eval_odometry.php)\n\n###### Tracking/Odometry:\n * [LIBVISO2: C++ Library for Visual Odometry 2](http://www.cvlibs.net/software/libviso/)\n * [PTAM: Parallel tracking and mapping](http://www.robots.ox.ac.uk/~gk/PTAM/)\n * [KFusion: Implementation of KinectFusion](https://github.com/GerhardR/kfusion)\n * [kinfu_remake: Lightweight, reworked and optimized version of Kinfu.](https://github.com/Nerei/kinfu_remake)\n * [LVR-KinFu: kinfu_remake based Large Scale KinectFusion with online reconstruction](http://las-vegas.uni-osnabrueck.de/related-projects/lvr-kinfu/)\n * [InfiniTAM: Implementation of multi-platform large-scale depth tracking and fusion](http://www.robots.ox.ac.uk/~victor/infinitam/)\n * [VoxelHashing: Large-scale KinectFusion](https://github.com/nachtmar/VoxelHashing)\n * [SLAMBench: Multiple-implementation of KinectFusion](http://apt.cs.manchester.ac.uk/projects/PAMELA/tools/SLAMBench/)\n * [SVO: Semi-direct visual odometry](https://github.com/uzh-rpg/rpg_svo)\n * [DVO: dense visual odometry](https://github.com/tum-vision/dvo_slam)\n * [FOVIS: RGB-D visual odometry](https://code.google.com/p/fovis/)\n\n###### Graph Optimization:\n * [GTSAM: General smoothing and mapping library for Robotics and SFM](https://collab.cc.gatech.edu/borg/gtsam?destination=node%2F299) -- Georgia Institute of Technology\n * [G2O: General framework for graph optomization](https://github.com/RainerKuemmerle/g2o)\n\n###### Loop Closure:\n * [FabMap: appearance-based loop closure system](http://www.robots.ox.ac.uk/~mjc/Software.htm) - also available in [OpenCV2.4.11](http://docs.opencv.org/2.4/modules/contrib/doc/openfabmap.html)\n * [DBoW2: binary bag-of-words loop detection system](http://webdiis.unizar.es/~dorian/index.php?p=32)\n\n###### Localization & Mapping:\n * [RatSLAM](https://code.google.com/p/ratslam/)\n * [LSD-SLAM](https://github.com/tum-vision/lsd_slam)\n * [ORB-SLAM](https://github.com/raulmur/ORB_SLAM)\n\n#### Single-view Spatial Understanding\n * [Geometric Context](http://web.engr.illinois.edu/~dhoiem/projects/software.html) - Derek Hoiem (CMU)\n * [Recovering Spatial Layout](http://web.engr.illinois.edu/~dhoiem/software/counter.php?Down=varsha_spatialLayout.zip) - Varsha Hedau (UIUC)\n * [Geometric Reasoning](http://www.cs.cmu.edu/~./dclee/code/index.html) - David C. Lee (CMU)\n * [RGBD2Full3D](https://github.com/arron2003/rgbd2full3d) - Ruiqi Guo (UIUC)\n\n#### Object Detection\n * [INRIA Object Detection and Localization Toolkit](http://pascal.inrialpes.fr/soft/olt/)\n * [Discriminatively trained deformable part models](http://www.cs.berkeley.edu/~rbg/latent/)\n * [VOC-DPM](https://github.com/rbgirshick/voc-dpm)\n * [Histograms of Sparse Codes for Object Detection](http://www.ics.uci.edu/~dramanan/software/sparse/)\n * [R-CNN: Regions with Convolutional Neural Network Features](https://github.com/rbgirshick/rcnn)\n * [SPP-Net](https://github.com/ShaoqingRen/SPP_net)\n * [BING: Objectness Estimation](http://mmcheng.net/bing/comment-page-9/)\n * [Edge Boxes](https://github.com/pdollar/edges)\n * [ReInspect](https://github.com/Russell91/ReInspect)\n\n#### Nearest Neighbor Search\n\n###### General purpose nearest neighbor search\n * [ANN: A Library for Approximate Nearest Neighbor Searching](http://www.cs.umd.edu/~mount/ANN/)\n * [FLANN - Fast Library for Approximate Nearest Neighbors](http://www.cs.ubc.ca/research/flann/)\n * [Fast k nearest neighbor search using GPU](http://vincentfpgarcia.github.io/kNN-CUDA/)\n\n###### Nearest Neighbor Field Estimation\n * [PatchMatch](http://gfx.cs.princeton.edu/gfx/pubs/Barnes_2009_PAR/index.php)\n * [Generalized PatchMatch](http://gfx.cs.princeton.edu/pubs/Barnes_2010_TGP/index.php)\n * [Coherency Sensitive Hashing](http://www.eng.tau.ac.il/~simonk/CSH/)\n * [PMBP: PatchMatch Belief Propagation](https://github.com/fbesse/pmbp)\n * [TreeCANN](http://www.eng.tau.ac.il/~avidan/papers/TreeCANN_code_20121022.rar)\n\n#### Visual Tracking\n* [Visual Tracker Benchmark](https://sites.google.com/site/trackerbenchmark/benchmarks/v10)\n* [Visual Tracking Challenge](http://www.votchallenge.net/)\n* [Kanade-Lucas-Tomasi Feature Tracker](http://www.ces.clemson.edu/~stb/klt/)\n* [Extended Lucas-Kanade Tracking](http://www.eng.tau.ac.il/~oron/ELK/ELK.html)\n* [Online-boosting Tracking](http://www.vision.ee.ethz.ch/boostingTrackers/)\n* [Spatio-Temporal Context Learning](http://www4.comp.polyu.edu.hk/~cslzhang/STC/STC.htm)\n* [Locality Sensitive Histograms](http://www.shengfenghe.com/visual-tracking-via-locality-sensitive-histograms.html)\n* [Enhanced adaptive coupled-layer LGTracker++](http://www.cv-foundation.org/openaccess/content_iccv_workshops_2013/W03/papers/Xiao_An_Enhanced_Adaptive_2013_ICCV_paper.pdf)\n* [TLD: Tracking - Learning - Detection](http://personal.ee.surrey.ac.uk/Personal/Z.Kalal/tld.html)\n* [CMT: Clustering of Static-Adaptive Correspondences for Deformable Object Tracking](http://www.gnebehay.com/cmt/)\n* [Kernelized Correlation Filters](http://home.isr.uc.pt/~henriques/circulant/)\n* [Accurate Scale Estimation for Robust Visual Tracking](http://www.cvl.isy.liu.se/en/research/objrec/visualtracking/scalvistrack/index.html)\n* [Multiple Experts using Entropy Minimization](http://cs-people.bu.edu/jmzhang/MEEM/MEEM.html)\n* [TGPR](http://www.dabi.temple.edu/~hbling/code/TGPR.htm)\n* [CF2: Hierarchical Convolutional Features for Visual Tracking](https://sites.google.com/site/jbhuang0604/publications/cf2)\n* [Modular Tracking Framework](http://webdocs.cs.ualberta.ca/~vis/mtf/index.html)\n\n#### Saliency Detection\n\n#### Attributes\n\n#### Action Reconition\n\n#### Egocentric cameras\n\n#### Human-in-the-loop systems\n\n#### Image Captioning\n * [NeuralTalk](https://github.com/karpathy/neuraltalk\ufeff) -\n\n#### Optimization\n * [Ceres Solver](http://ceres-solver.org/) - Nonlinear least-square problem and unconstrained optimization solver\n * [NLopt](http://ab-initio.mit.edu/wiki/index.php/NLopt)- Nonlinear least-square problem and unconstrained optimization solver\n * [OpenGM](http://hci.iwr.uni-heidelberg.de/opengm2/) - Factor graph based discrete optimization and inference solver\n * [GTSAM](https://collab.cc.gatech.edu/borg/gtsam/) - Factor graph based lease-square optimization solver\n\n#### Deep Learning\n * [Awesome Deep Vision](https://github.com/kjw0612/awesome-deep-vision)\n\n#### Machine Learning\n * [Awesome Machine Learning](https://github.com/josephmisiti/awesome-machine-learning)\n * [Bob: a free signal processing and machine learning toolbox for researchers](http://idiap.github.io/bob/)\n * [LIBSVM -- A Library for Support Vector Machines](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)\n\n## Datasets\n\n#### External Dataset Link Collection\n * [CV Datasets on the web](http://www.cvpapers.com/datasets.html) - CVPapers\n * [Are we there yet?](http://rodrigob.github.io/are_we_there_yet/build/) - Which paper provides the best results on standard dataset X?\n * [Computer Vision Dataset on the web](http://www.cvpapers.com/datasets.html)\n * [Yet Another Computer Vision Index To Datasets](http://riemenschneider.hayko.at/vision/dataset/)\n * [ComputerVisionOnline Datasets](http://www.computervisiononline.com/datasets)\n * [CVOnline Dataset](http://homepages.inf.ed.ac.uk/cgi/rbf/CVONLINE/entries.pl?TAG363)\n * [CV datasets](http://clickdamage.com/sourcecode/cv_datasets.php)\n * [visionbib](http://datasets.visionbib.com/info-index.html)\n * [VisualData](http://www.visualdata.io/)\n\n#### Low-level Vision\n\n###### Stereo Vision\n * [Middlebury Stereo Vision](http://vision.middlebury.edu/stereo/)\n * [The KITTI Vision Benchmark Suite](http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stero)\n * [LIBELAS: Library for Efficient Large-scale Stereo Matching](http://www.cvlibs.net/software/libelas/)\n * [Ground Truth Stixel Dataset](http://www.6d-vision.com/ground-truth-stixel-dataset)\n\n###### Optical Flow\n * [Middlebury Optical Flow Evaluation](http://vision.middlebury.edu/flow/)\n * [MPI-Sintel Optical Flow Dataset and Evaluation](http://sintel.is.tue.mpg.de/)\n * [The KITTI Vision Benchmark Suite](http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow)\n * [HCI Challenge](http://hci.iwr.uni-heidelberg.de/Benchmarks/document/Challenging_Data_for_Stereo_and_Optical_Flow/)\n\n###### Video Object Segmentation\n * [DAVIS: Densely Annotated VIdeo Segmentation](http://davischallenge.org/)\n * [SegTrack v2](http://web.engr.oregonstate.edu/~lif/SegTrack2/dataset.html)\n\n\n###### Change Detection\n * [Labeled and Annotated Sequences for Integral Evaluation of SegmenTation Algorithms](http://www.gti.ssr.upm.es/data/LASIESTA)\n * [ChangeDetection.net](http://www.changedetection.net/)\n\n###### Image Super-resolutions\n * [Single-Image Super-Resolution: A Benchmark](https://eng.ucmerced.edu/people/cyang35/ECCV14/ECCV14.html)\n\n#### Intrinsic Images\n * [Ground-truth dataset and baseline evaluations for intrinsic image algorithms](http://www.mit.edu/~kimo/publications/intrinsic/)\n * [Intrinsic Images in the Wild](http://opensurfaces.cs.cornell.edu/intrinsic/)\n * [Intrinsic Image Evaluation on Synthetic Complex Scenes](http://www.cic.uab.cat/Datasets/synthetic_intrinsic_image_dataset/)\n\n#### Material Recognition\n * [OpenSurface](http://opensurfaces.cs.cornell.edu/)\n * [Flickr Material Database](http://people.csail.mit.edu/celiu/CVPR2010/)\n * [Materials in Context Dataset](http://opensurfaces.cs.cornell.edu/publications/minc/)\n\n#### Multi-view Reconsturction\n* [Multi-View Stereo Reconstruction](http://vision.middlebury.edu/mview/)\n\n#### Saliency Detection\n\n#### Visual Tracking\n * [Visual Tracker Benchmark](https://sites.google.com/site/trackerbenchmark/benchmarks/v10)\n * [Visual Tracker Benchmark v1.1](https://sites.google.com/site/benchmarkpami/)\n * [VOT Challenge](http://www.votchallenge.net/)\n * [Princeton Tracking Benchmark](http://tracking.cs.princeton.edu/)\n * [Tracking Manipulation Tasks (TMT)](http://webdocs.cs.ualberta.ca/~vis/trackDB/)\n\n#### Visual Surveillance\n * [VIRAT](http://www.viratdata.org/)\n * [CAM2](https://cam2.ecn.purdue.edu/)\n\n#### Saliency Detection\n\n#### Change detection\n * [ChangeDetection.net](http://changedetection.net/)\n\n#### Visual Recognition\n\n###### Image Classification\n * [The PASCAL Visual Object Classes](http://pascallin.ecs.soton.ac.uk/challenges/VOC/)\n * [ImageNet Large Scale Visual Recognition Challenge](http://www.image-net.org/challenges/LSVRC/2014/)\n\n###### Scene Recognition\n * [SUN Database](http://groups.csail.mit.edu/vision/SUN/)\n * [Place Dataset](http://places.csail.mit.edu/)\n\n###### Object Detection\n * [The PASCAL Visual Object Classes](http://pascallin.ecs.soton.ac.uk/challenges/VOC/)\n * [ImageNet Object Detection Challenge](http://www.image-net.org/challenges/LSVRC/2014/)\n * [Microsoft COCO](http://mscoco.org/)\n\n###### Semantic labeling\n * [Stanford background dataset](http://dags.stanford.edu/projects/scenedataset.html)\n * [CamVid](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/)\n * [Barcelona Dataset](http://www.cs.unc.edu/~jtighe/Papers/ECCV10/)\n * [SIFT Flow Dataset](http://www.cs.unc.edu/~jtighe/Papers/ECCV10/siftflow/SiftFlowDataset.zip)\n\n###### Multi-view Object Detection\n * [3D Object Dataset](http://cvgl.stanford.edu/resources.html)\n * [EPFL Car Dataset](http://cvlab.epfl.ch/data/pose)\n * [KTTI Dection Dataset](http://www.cvlibs.net/datasets/kitti/eval_object.php)\n * [SUN 3D Dataset](http://sun3d.cs.princeton.edu/)\n * [PASCAL 3D+](http://cvgl.stanford.edu/projects/pascal3d.html)\n * [NYU Car Dataset](http://nyc3d.cs.cornell.edu/)\n\n###### Fine-grained Visual Recognition\n * [Fine-grained Classification Challenge](https://sites.google.com/site/fgcomp2013/)\n * [Caltech-UCSD Birds 200](http://www.vision.caltech.edu/visipedia/CUB-200.html)\n\n###### Pedestrian Detection\n * [Caltech Pedestrian Detection Benchmark](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)\n * [ETHZ Pedestrian Detection](https://data.vision.ee.ethz.ch/cvl/aess/dataset/)\n\n#### Action Recognition\n\n###### Image-based\n\n###### Video-based\n * [HOLLYWOOD2 Dataset](http://www.di.ens.fr/~laptev/actions/hollywood2/)\n * [UCF Sports Action Data Set](http://crcv.ucf.edu/data/UCF_Sports_Action.php)\n\n###### Image Deblurring\n * [Sun dataset](http://cs.brown.edu/~lbsun/deblur2013/deblur2013iccp.html)\n * [Levin dataset](http://www.wisdom.weizmann.ac.il/~levina/papers/LevinEtalCVPR09Data.rar)\n\n#### Image Captioning\n * [Flickr 8K](http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/KCCA.html)\n * [Flickr 30K](http://shannon.cs.illinois.edu/DenotationGraph/)\n * [Microsoft COCO](http://mscoco.org/)\n\n#### Scene Understanding\n # [SUN RGB-D](http://rgbd.cs.princeton.edu/) - A RGB-D Scene Understanding Benchmark Suite\n # [NYU depth v2](http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html) - Indoor Segmentation and Support Inference from RGBD Images\n\n#### Aerial images\n # [Aerial Image Segmentation](https://zenodo.org/record/1154821#.WmN9kHWnHIp) - Learning Aerial Image Segmentation From Online Maps\n\n\n## Resources for students\n\n#### Resource link collection\n * [Resources for students](http://people.csail.mit.edu/fredo/student.html) - Fr\u00e9do Durand (MIT)\n * [Advice for Graduate Students](http://www.dgp.toronto.edu/~hertzman/advice/) - Aaron Hertzmann (Adobe Research)\n * [Graduate Skills Seminars](http://www.dgp.toronto.edu/~hertzman/courses/gradSkills/2010/) - Yashar Ganjali, Aaron Hertzmann (University of Toronto)\n * [Research Skills](http://research.microsoft.com/en-us/um/people/simonpj/papers/giving-a-talk/giving-a-talk.htm) - Simon Peyton Jones (Microsoft Research)\n * [Resource collection](http://web.engr.illinois.edu/~taoxie/advice.htm) - Tao Xie (UIUC) and Yuan Xie (UCSB)\n\n#### Writing\n * [Write Good Papers](http://people.csail.mit.edu/fredo/FredoGoodWriting.pdf) - Fr\u00e9do Durand (MIT)\n * [Notes on writing](http://people.csail.mit.edu/fredo/PUBLI/writing.pdf) - Fr\u00e9do Durand (MIT)\n * [How to Write a Bad Article](http://people.csail.mit.edu/fredo/FredoBadWriting.pdf) - Fr\u00e9do Durand (MIT)\n * [How to write a good CVPR submission](http://billf.mit.edu/sites/default/files/documents/cvprPapers.pdf) - William T. Freeman (MIT)\n * [How to write a great research paper](https://www.youtube.com/watch?v=g3dkRsTqdDA) - Simon Peyton Jones (Microsoft Research)\n * [How to write a SIGGRAPH paper](http://www.slideshare.net/jdily/how-to-write-a-siggraph-paper) - SIGGRAPH ASIA 2011 Course\n * [Writing Research Papers](http://www.dgp.toronto.edu/~hertzman/advice/writing-technical-papers.pdf) - Aaron Hertzmann (Adobe Research)\n * [How to Write a Paper for SIGGRAPH](http://www.computer.org/csdl/mags/cg/1987/12/mcg1987120062.pdf) - Jim Blinn\n * [How to Get Your SIGGRAPH Paper Rejected](http://www.siggraph.org/sites/default/files/kajiya.pdf) - Jim Kajiya (Microsoft Research)\n * [How to write a SIGGRAPH paper](www.liyiwei.org/courses/how-siga11/liyiwei.pptx) - Li-Yi Wei (The University of Hong Kong)\n * [How to Write a Great Paper](http://www-hagen.informatik.uni-kl.de/~bertram/talks/getpublished.pdf) - Martin Martin Hering Hering--Bertram (Hochschule Bremen University of Applied Sciences)\n * [How to have a paper get into SIGGRAPH?](http://www-ui.is.s.u-tokyo.ac.jp/~takeo/writings/siggraph.html) - Takeo Igarashi (The University of Tokyo)\n * [Good Writing](http://www.cs.cmu.edu/~pausch/Randy/Randy/raibert.htm) - Marc H. Raibert (Boston Dynamics, Inc.)\n * [How to Write a Computer Vision Paper](http://web.engr.illinois.edu/~dhoiem/presentations/How%20to%20Write%20a%20Computer%20Vison%20Paper.ppt) - Derek Hoiem (UIUC)\n * [Common mistakes in technical writing](http://www.cs.dartmouth.edu/~wjarosz/writing.html) - Wojciech Jarosz (Dartmouth College)\n\n\n#### Presentation\n * [Giving a Research Talk](http://people.csail.mit.edu/fredo/TalkAdvice.pdf) - Fr\u00e9do Durand (MIT)\n * [How to give a good talk](http://www.dgp.toronto.edu/~hertzman/courses/gradSkills/2010/GivingGoodTalks.pdf) - David Fleet (University of Toronto) and Aaron Hertzmann (Adobe Research)\n * [Designing conference posters](http://colinpurrington.com/tips/poster-design) - Colin Purrington\n\n#### Research\n * [How to do research](http://people.csail.mit.edu/billf/www/papers/doresearch.pdf) - William T. Freeman (MIT)\n * [You and Your Research](http://www.cs.virginia.edu/~robins/YouAndYourResearch.html) - Richard Hamming\n * [Warning Signs of Bogus Progress in Research in an Age of Rich Computation and Information](http://yima.csl.illinois.edu/psfile/bogus.pdf) - Yi Ma (UIUC)\n * [Seven Warning Signs of Bogus Science](http://www.quackwatch.com/01QuackeryRelatedTopics/signs.html) - Robert L. Park\n * [Five Principles for Choosing Research Problems in Computer Graphics](https://www.youtube.com/watch?v=v2Qaf8t8I6c) - Thomas Funkhouser (Cornell University)\n * [How To Do Research In the MIT AI Lab](http://www.cs.indiana.edu/mit.research.how.to.html) - David Chapman (MIT)\n * [Recent Advances in Computer Vision](http://www.slideshare.net/antiw/recent-advances-in-computer-vision) - Ming-Hsuan Yang (UC Merced)\n * [How to Come Up with Research Ideas in Computer Vision?](http://www.slideshare.net/jbhuang/how-to-come-up-with-new-research-ideas-4005840) - Jia-Bin Huang (UIUC)\n * [How to Read Academic Papers](http://www.slideshare.net/jbhuang/how-to-read-academic-papers) - Jia-Bin Huang (UIUC)\n\n#### Time Management\n * [Time Management](https://www.youtube.com/watch?v=oTugjssqOT0) - Randy Pausch (CMU)\n\n## Blogs\n * [Learn OpenCV](http://www.learnopencv.com/) - Satya Mallick\n * [Tombone's Computer Vision Blog](http://www.computervisionblog.com/) - Tomasz Malisiewicz\n * [Computer vision for dummies](http://www.visiondummy.com/) - Vincent Spruyt\n * [Andrej Karpathy blog](http://karpathy.github.io/) - Andrej Karpathy\n * [AI Shack](http://aishack.in/) - Utkarsh Sinha\n * [Computer Vision Talks](http://computer-vision-talks.com/) - Eugene Khvedchenya\n * [Computer Vision Basics with Python Keras and OpenCV](https://github.com/jrobchin/Computer-Vision-Basics-with-Python-Keras-and-OpenCV) - Jason Chin (University of Western Ontario)\n\n\n## Links\n* [The Computer Vision Industry](http://www.cs.ubc.ca/~lowe/vision.html) - David Lowe\n* [German Computer Vision Research Groups & Companies](http://hci.iwr.uni-heidelberg.de/Links/German_Vision/)\n* [awesome-deep-learning](https://github.com/ChristosChristofidis/awesome-deep-learning)\n* [awesome-machine-learning](https://github.com/josephmisiti/awesome-machine-learning)\n* [Cat Paper Collection](http://www.eecs.berkeley.edu/~junyanz/cat/cat_papers.html)\n* [Computer Vision News](http://www.rsipvision.com/computer-vision-news/)\n*\n## Songs\n* [The Fundamental Matrix Song](http://danielwedge.com/fmatrix/)\n* [The RANSAC Song](http://danielwedge.com/ransac/)\n* [Machine Learning A Cappella - Overfitting Thriller](https://www.youtube.com/watch?v=DQWI1kvmwRg)\n\n## Licenses\nLicense\n\n[![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)\n\nTo the extent possible under law, [Jia-Bin Huang](www.jiabinhuang.com) has waived all copyright and related or neighboring rights to this work.\n"
 },
 {
  "repo": "GSA/data",
  "language": "HTML",
  "readme_contents": "# GSA Data\n\nA home for miscellaneous data published by the [General Services Administration](http://gsa.gov). If you use any of this data for something, please do let us know by [opening an issue](https://github.com/gsa/data/issues) and telling us about it!\n\n## Datasets\n\n* [.gov domains](dotgov-domains/#readme) - official public list of .gov domains, updated every few weeks\n* [.gov federal websites](dotgov-websites/#readme) - various files of trivially discoverable Federal .gov hostnames\n* [GSA Enterprise Architecture](enterprise-architecture/)\n\n## Copyright information\n\nThis data is in the public domain under [17 U.S.C. 105](https://www.law.cornell.edu/uscode/text/17/105).\n"
 },
 {
  "repo": "GoogleTrends/data",
  "language": "JavaScript",
  "readme_contents": "This repo contains open-source datasets behind the graphics, interactives, and analyses at [Google Trends](https://www.google.com/trends). Every day we will add new datasets behind our graphics and charts. \n\n<h3>What is the data?</h3>\nThe data primarily comes from our analysis of Google Trends, but will on occasion include other Google tools such as YouTube, Play and Waze. It is primarily:<br>\n\u2022 Aggregated<br>\n\u2022 Anonymised<br>\n\u2022 Indexed<br>\n\u2022 Normalized\n\n<h3>What can you do with it?</h3>\nThe data is deliberately designed for you to play with, explore and create visualizations. We want to know what you do so we can share it on our social channels and inspire others to play with it too.\n\n<h3>Useful links:</h3>\n\u2022 [Google Trends](https://www.google.com/trends)<br>\n\u2022 [Google News Lab](https://www.google.com/newslab)<br>\n\u2022 [@GoogleTrends](https://www.twitter.com/googletrends)<br>\n\n<h3>Contact us</h3>\nnewslabtrends@google.com\n\n"
 },
 {
  "repo": "nationalparkservice/data",
  "language": "JavaScript",
  "readme_contents": "# NPMap Data\n\nThis repository contains data used in the NPMap team's projects. It is **not** the official source for any of the data it contains. You should visit the [NPS Data Store](https://irma.nps.gov/Portal/) to browse and download official datasets for the National Park Service.\n\n## License\n\nUnless otherwise noted, all data in this repository are available in the public domain.\n"
 },
 {
  "repo": "beamandrew/medical-data",
  "language": null,
  "readme_contents": "# Medical Data for Machine Learning\nThis is a curated list of medical data for machine learning.  \nThis list is provided for informational purposes only, please make sure you respect any and all usage restrictions for any of the data listed here.\n\n## 1. Medical Imaging Data\n\n__EchoNet-Dynamic__  \nA Large New Cardiac Motion Video Data Resource for Medical Machine Learning, from Stanford.\nOverview: https://echonet.github.io/dynamic/index.html\nAccess: https://echonet.github.io/dynamic/index.html#access\n\n***\n\n__The National Library of Medicine presents MedPix\u00ae__  \nDatabase of 53,000 medical images from 13,000 patients with annotations. __Requires registration__.  \nInformation: https://medpix.nlm.nih.gov/home  \n\n***\n\n__ABIDE: The Autism Brain Imaging Data Exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism.__  \nFunction MRI images for 539 individuals suffering from ASD and 573 typical controls. These 1112 datasets are composed of structural and resting state functional MRI data along with an extensive array of phenotypic information. __Requires registration__.    \nPaper: http://www.ncbi.nlm.nih.gov/pubmed/23774715  \nInformation: http://fcon_1000.projects.nitrc.org/indi/abide/  \nPreprocessed version: http://preprocessed-connectomes-project.org/abide/  \n\n***\n\n__Alzheimer's Disease Neuroimaging Initiative (ADNI)__  \nMRI database on Alzheimer's patients and healthy controls. Also has clinical, genomic, and biomaker data. __Requires registration__.  \nPaper: http://www.neurology.org/content/74/3/201.short  \nAccess: http://adni.loni.usc.edu/data-samples/access-data/\n\n***\n\n__CT Colongraphy for Colon Cancer (Cancer Imaging Archive)__\nCT scan for diagnosing of colon cancer. Includes data for patients without polyps, 6-9mm polyps, and greater than 10 mm polyps.\nAccess: https://wiki.cancerimagingarchive.net/display/Public/CT+COLONOGRAPHY#dc149b9170f54aa29e88f1119e25ba3e\n\n***\n\n__Digital Retinal Images for Vessel Extraction (DRIVE)__  \nThe DRIVE database is for comparative studies on segmentation of blood vessels in retinal images. It consists of 40 photographs out of which 7 showing signs of mild early diabetic retinopathy.  \nPaper: https://ieeexplore.ieee.org/document/1282003  \nAccess: http://www.isi.uu.nl/Research/Databases/DRIVE/download.php  \n\n***\n\n__AMRG Cardiac Atlas__\nThe AMRG Cardiac MRI Atlas is a complete labelled MRI image set of a normal patient's heart acquired with the Auckland MRI Research Group 's Siemens Avanto scanner. The atlas aims to provide university and school students, MR technologists, clinicians...\n\n\n__Congenital Heart Disease (CHD) Atlas__\nThe Congenital Heart Disease (CHD) Atlas represents MRI data sets, physiologic clinical data and computer models from adults and children with various congenital heart defects. The data have been acquired from several clinical centers including Rady...\n\n\n__DETERMINE__\nDefibrillators to Reduce Risk by Magnetic Resonance Imaging Evaluation, is a prospective, multicenter, randomized clinical trials in patients with coronary artery diseases and mild-to-moderate left ventricular dysfunction. The primary objective...\n\n\n__MESA__\nMulti-Ethnic Study of Atherosclerosis, is a large-scale cardiovascular population study (>6,500 participants) conducted in six centres in the USA. It aims to investigate the manifestation of subclinical to clinical cardiovascular disease before...\n\n***\n\n__OASIS__\nThe Open Access Series of Imaging Studies (OASIS) is a project aimed at making MRI data sets of the brain freely available to the scientific community. Two datasets are available: a cross-sectional and a longitudinal set.\n\n* Cross-sectional MRI Data in Young, Middle Aged, Nondemented and Demented Older Adults: This set consists of a cross-sectional collection of 416 subjects aged 18 to 96.  For each subject, 3 or 4 individual T1-weighted MRI scans obtained in single scan sessions are included.  The subjects are all right-handed and include both men and women.  100 of the included subjects over the age of 60 have been clinically diagnosed with very mild to moderate Alzheimer\u2019s disease (AD).  Additionally, a reliability data set is included containing 20 nondemented subjects imaged on a subsequent visit within 90 days of their initial session.\n* Longitudinal MRI Data in Nondemented and Demented Older Adults: This set consists of a longitudinal collection of 150 subjects aged 60 to 96. Each subject was scanned on two or more visits, separated by at least one year for a total of 373 imaging sessions. For each subject, 3 or 4 individual T1-weighted MRI scans obtained in single scan sessions are included. The subjects are all right-handed and include both men and women. 72 of the subjects were characterized as nondemented throughout the study. 64 of the included subjects were characterized as demented at the time of their initial visits and remained so for subsequent scans, including 51 individuals with mild to moderate Alzheimer\u2019s disease. Another 14 subjects were characterized as nondemented at the time of their initial visit and were subsequently characterized as demented at a later visit.\n\nAccess: http://www.oasis-brains.org/\n\n***\n\n__Isic Archive - Melanoma__\nThis archive contains 23k images of classified skin lesions. It contains both malignant and benign examples.  \n\nEach example contains the image of the lesion, meta data regarding the lesion (including clasisfication and segmentation) and meta data regarding the patient.  \n\nThe data can be viewed in this link: https://www.isic-archive.com (in the gallery section)  \nIt can be downloaded through the site or by using this repository:   \nhttps://github.com/GalAvineri/ISIC-Archive-Downloader  \n\n***\n\n__SCMR Consensus Data__\nThe SCMR Consensus Dataset is a set of 15 cardiac MRI studies of mixed pathologies (5 healthy, 6 myocardial infarction, 2 heart failure and 2 hypertrophy), which were acquired from different MR machines (4 GE, 5 Siemens, 6 Philips). The main objectives...\n\n\n__Sunnybrook Cardiac Data__\nThe Sunnybrook Cardiac Data (SCD), also known as the 2009 Cardiac MR Left Ventricle Segmentation Challenge data, consist of 45 cine-MRI images from a mixed of patients and pathologies: healthy, hypertrophy, heart failure with infarction and heart...\n\nAccess: http://www.cardiacatlas.org/studies/\n\n***\n\n__Lung Image Database Consortium (LIDC)__\n\n\nPreliminary clinical studies have shown that spiral CT scanning of the lungs can improve early detection of lung cancer in high-risk individuals. Image processing algorithms have the potential to assist in lesion detection on spiral CT studies, and to assess the stability or change in lesion size on serial CT studies. The use of such computer-assisted algorithms could significantly enhance the sensitivity and specificity of spiral CT lung screening, as well as lower costs by reducing physician time needed for interpretation.\n\nThe intent of the Lung Imaging Database Consortium (LIDC) initiative was to support a consortium of institutions to develop consensus guidelines for a spiral CT lung image resource and to construct a database of spiral CT lung images. The investigators funded under this initiative created a set of guidelines and metrics for database use and for developing a database as a test-bed and showcase for those methods. The database is available to researchers and users through the Internet and has wide utility as a research, teaching, and training resource.\n\nSpecifically, the LIDC initiative aims were to provide:\n\n*\ta reference database for the relative evaluation of image processing or CAD algorithms and\n*\ta flexible query system that will provide investigators the opportunity to evaluate a wide range of technical parameters and de-identified clinical information within this database that may be important for research applications.\n\nThis resource will stimulate further database development for image processing and CAD evaluation for applications that include cancer screening, diagnosis, and image guided intervention, and treatment. Therefore, the NCI encourages investigator-initiated grant applications that utilize the database in their research. NCI also encourages investigator-initiated grant applications that provide tools or methodology that may improve or complement the mission of the LIDC.\n\n\nAccess: https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI#\n***\n\n__TCIA Collections__\n\nCancer imaging data sets across various cancer types (e.g. carcinoma, lung cancer, myeloma) and various imaging modalities.\nThe image data in The Cancer Imaging Archive (TCIA) is organized into purpose-built collections of subjects. The subjects typically have a cancer type and/or anatomical site (lung, brain, etc.) in common. Each link in the table below contains information concerning the scientific value of a collection, information about how to obtain any supporting non-image data which may be available, and links to view or download the imaging data. To support reproducibility in scientific research, TCIA supports Digital Object Identifiers (DOIs) which allow users to share subsets of TCIA data referenced in a research manuscript.\n\nAccess: http://www.cancerimagingarchive.net/\n\n***\n\n__Belarus tuberculosis portal__\n\nTuberculosis (TB) is a major problem of Belarus Public Health. Recently situation has been complicated with emergence and development of MDR/XDR TB and HIV/TB which require long-term treatment. Many and the most severe cases usually disseminate across the country to different TB dispensaries. The ability of leading Belarus TB specialists to follow such patients will be greatly improved by using a common database containing patients\u2019 radiological images, lab work and clinical data. This will also significantly improve adherence to the treatment protocol and result in a better record of the treatment outcomes.\nCriteria for inclusion clinical cases in the database of the portal - patients admitted to the MDR-TB department of RSPC of Pulmonology and Tuberculosis with diagnosed or suspected of MDR-TB, which conducted CT \u2013 study (\u00b1 2 months from the date of registration)\nBelarus dataset have both chest X-rays and CT scans of the same patient.\n\nAccess: http://tuberculosis.by/\n\n***\n\n__DDSM: Digital Database for Screening Mammography__\n\n The Digital Database for Screening Mammography (DDSM) is a resource for use by the mammographic image analysis research community. Primary support for this project was a grant from the Breast Cancer Research Program of the U.S. Army Medical Research and Materiel Command. The DDSM project is a collaborative effort involving co-p.i.s at the Massachusetts General Hospital (D. Kopans, R. Moore), the University of South Florida (K. Bowyer), and Sandia National Laboratories (P. Kegelmeyer). Additional cases from Washington University School of Medicine were provided by Peter E. Shile, MD, Assistant Professor of Radiology and Internal Medicine. Additional collaborating institutions include Wake Forest University School of Medicine (Departments of Medical Engineering and Radiology), Sacred Heart Hospital and ISMD, Incorporated. The primary purpose of the database is to facilitate sound research in the development of computer algorithms to aid in screening. Secondary purposes of the database may include the development of algorithms to aid in the diagnosis and the development of teaching or training aids. The database contains approximately 2,500 studies. Each study includes two images of each breast, along with some associated patient information (age at time of study, ACR breast density rating, subtlety rating for abnormalities, ACR keyword description of abnormalities) and image information (scanner, spatial resolution, ...). Images containing suspicious areas have associated pixel-level \"ground truth\" information about the locations and types of suspicious regions. Also provided are software both for accessing the mammogram and truth images and for calculating performance figures for automated image analysis algorithms.\n\n\nAccess: http://marathon.csee.usf.edu/Mammography/Database.html\n\n***\n\n__INbreast: Database for Digital Mammography__\n\nThe INbreast database is a mammographic database, with images acquired at a Breast Centre, located in a University Hospital (Hospital de S\u00e3o Jo\u00e3o, Breast Centre, Porto, Portugal). INbreast has a total of 115 cases (410 images) of which 90 cases are from women with both breasts (4 images per case) and 25 cases are from mastectomy patients (2 images per case). Several types of lesions (masses, calcifications, asymmetries, and distortions) are included. Accurate contours made by specialists are also provided in XML format. \n\n\nAccess: http://medicalresearch.inescporto.pt/breastresearch/index.php/Get_INbreast_Database\n\n***\n\n__mini-MIAS: MIAS MiniMammographic Database__\n\nThe Mammographic Image Analysis Society (MIAS) is an organisation of UK research groups interested in the understanding of mammograms and has generated a database of digital mammograms. Films taken from the UK National Breast Screening Programme have been digitised to 50 micron pixel edge with a Joyce-Loebl scanning microdensitometer, a device linear in the optical density range 0-3.2 and representing each pixel with an 8-bit word. The database contains 322 digitised films and is available on 2.3GB 8mm (ExaByte) tape. It also includes radiologist's \"truth\"-markings on the locations of any abnormalities that may be present. The database has been reduced to a 200 micron pixel edge and padded/clipped so that all the images are 1024x1024. Mammographic images are available via the Pilot European Image Processing Archive (PEIPA) at the University of Essex. \n\n\nAccess: http://peipa.essex.ac.uk/info/mias.html\n\n*** \n\n\n__Prostate__\n\nProstate cancer (CaP) has been reported on a worldwide scale to be the second most frequently diagnosed cancer of men accounting for 13.6% (Ferlay et al. (2010)). Statistically, in 2008, the number of new diagnosed cases was estimated to be 899,000 with no less than 258,100 deaths (Ferlay et al. (2010)).\n\nMagnetic resonance imaging (MRI) provides imaging techniques allowing to diagnose and localize CaP. The I2CVB provides a multi-parametric MRI dataset to help at the development of computer-aided detection and diagnosis (CAD) system.\nAccess: http://i2cvb.github.io/\n\n***\nAccess: http://www.ehealthlab.cs.ucy.ac.cy/index.php/facilities/32-software/218-datasets\n\n- __MRI Lesion Segmentation in Multiple Sclerosis Database__\n\n- __Emergency Tele-Orthopedics X-ray Digital Library__\n\n- __IMT Segmentation__\n\n- __Needle EMG MUAP Time Domain Features__\n\n***\n\n__DICOM image sample sets__\nThese datasets are exclusively available for research and teaching. You are not authorized to redistribute or sell them, or use them for commercial purposes.\n\nAll these DICOM files are compressed in JPEG2000 transfer syntax.\n\n\nAccess: http://www.osirix-viewer.com/resources/dicom-image-library/\n\n***\n__SCR database: Segmentation in Chest Radiographs__\n\n\nThe automatic segmentation of anatomical structures in chest radiographs is of great importance for computer-aided diagnosis in these images. The SCR database has been established to facilitate comparative studies on segmentation of the lung fields, the heart and the clavicles in standard posterior-anterior chest radiographs.\n\nIn the spirit of cooperative scientific progress, we freely share the SCR database and are committed to maintaining a public repository of results of various algorithms on these segmentation tasks. On these pages, instructions can be found on downloading the database and uploading results, and benchmark results of various methods can be inspected.\n\nAccess: http://www.isi.uu.nl/Research/Databases/SCR/\n\n***\n__Medical Image Databases & Libraries__\n\n**Access: http://www.omnimedicalsearch.com/image_databases.html**\n\n **General Category**\n\n- e-Anatomy.org - Interactive Atlas of Anatomy - e-anatomy is an anatomy e-learning web site. More than 1500 slices from normal CT and MR exams were selected in order to cover the entire sectional anatomy of human body. Images were labeled using Terminologia Anatomica. A user-friendly interface allows to cine through multi-slice image series combined with interactive textual information, 3D models and anatomy drawings.\n\n- Medical Pictures and Definitions - Welcome to the largest database of medical pictures and definitions on the Internet. There are many sites sites that provide medical information but very few that provide medical pictures. As far as we know we are the only one that provides a medical picture database with basic information about each term pictured. Editor's Note: Nice website with free access & no pesky registration to 1200+ health and medical related images with definitions.\n\n- Nucleus Medical Art - Medical Illustrations, Medical Art. Includes 3D animations. \"Nucleus Medical Art, Inc. is a leading creator and distributor of medical illustrations, medical animations, and interactive multimedia for publishing, legal, healthcare, entertainment, pharmaceutical, medical device, academia and other markets, both in the U.S. and abroad. Editors Note: Great website.\n\n- Medical Image Databases on the Internet (UTHSCSA Library) - A directory of links to websites with topic specific medical related images.\n\n- Surgery Videos - A National Library of Medicine MedlinePlus collection of links to 100s and 100s of different surgical procedures. You must have RealPlayer media player on your computer to view these videos which are free of charge.\n\n- The ADAM Medical Encyclopedia with Illustrations. Perhaps one of the best illustrated medical works on the internet today, the ADAM Medical Encyclopedia includes over 4,000 articles about diseases, tests, symptoms, injuries, and surgeries. It also contains an extensive library of medical photographs and illustrations to back up those 4,000 articles. These illustrations and articles are free to the public.\n\n- Hardin MD - Medical and Disease Pictures, is a Free and established resource that has been offered by the University of Iowa for quite some time. The home page is in directory style where users will have to drill down to find the images they are looking for, many of which go offsite. Nevertheless, Hardin MD is an excellent gateway to 1,000s of detailed medical photos and illustrations.\n\n- Health Education Assets Library (HEAL) - Health on the Net Foundation Media Gallery Headquartered in Switzerland, (HON) is an international body that seeks to encourage ethical provision of online health information. \"HONmedia (the image gallery) is an unique repository of over 6'800 medical images and videos, pertaining to 1,700 topics and themes. This peerless database has been created manually by HON and new image links are constantly being added from the world-wide Web. HON encourages users to make their own image links available via the Submit an image link.\" Library includes anatomical images, visual affects of diseases and conditions and procedures.\n\n- Public Health Image Library (PHIL) Created by a Working Group at the Centers for Disease Control and Prevention (CDC), the PHIL offers an organized, universal electronic gateway to CDC's pictures. We welcome public health professionals, the media, laboratory scientists, educators, students, and the worldwide public to use this material for reference, teaching, presentation, and public health messages. The content is organized into hierarchical categories of people, places, and science, and is presented as single images, image sets, and multimedia files.\n\n- Images from the History of Medicine - This system provides access to the nearly 60,000 images in the prints and photograph collection of the History of Medicine Division (HMD) of the U.S. National Library of Medicine (NLM). The collection includes portraits, pictures of institutions, caricatures, genre scenes, and graphic art in a variety of media, illustrating the social and historical aspects of medicine.\n\n- Pozemedicale.org - Collection of medical images in Spanish, Italian, Portuguese and Italian.\n\n- Old Medical Pictures: Hundreds of fascinating and interesting old, but high quality photographs and images from the late 19th and early 20th century.\n\n**Subject Speciality Image Libraries and Collections**\n\n- Anatomy of the Human Body by Henry Gray - The Bartleby.com edition of Gray\u2019s Anatomy of the Human Body features 1,247 vibrant engravings\u2014many in color\u2014from the classic 1918 publication.\n\n- The Crookston Collection - A collection of medical slides taken by Dr. John H. Crookston that have been digitized and are available to the public and doctors.\n\n- DAVE Project - A searchable library of gastrointestinal endoscopic video clips covering a wide spectrum endoscopic imaging.\n\n- Dermnet - Browsable collection of over 8,000 high quality, dermatology images.\n\n- Interactive Dermatology Atlas - Image reference source for common and uncommon skin problems.\n\n- The Multi-Dimensional Human Embryo is a collaboration funded by the National Institute of Child Health and Human Development (NICHD) to produce and make available over the internet a three-dimensional image reference of the Human Embryo based on magnetic resonance imaging.\n\n- GastroLab Endoscopy Archives Was initiated in 1996 with the goal of maintaining an endoscopic image gallery free to use for all interested health care personals.\n\n- MedPix Is a Radiology and Medical Picture Databases resource tool. The home page interface is confusing and the entire website design is not user-friendly and has a mid 1990s feel to it. However, if you have the time (patience) it could prove to be an important resource for some.\n\n- OBGYN.net Image Library - This site is devoted entirely to providing access to images of interest to women's health. In addition to providing you with access to OBGYN.net images we also point to other women's health related images on the Internet. Because of the graphic nature of the material some individuals may prefer not to view these images.They are provided for educational purposes only.\n\n\n***\n__VIA Group Public Databases__\n\n Documented image databases are essential for the development of quantitative image analysis tools especially for tasks of computer-aided diagnosis (CAD). In collaboration with the I-ELCAP group we have established two public image databases that contain lung CT images in the DICOM format together with documentation of abnormalities by radiologists. Please access the links below for more details:\n\nAccess: http://www.via.cornell.edu/databases/\n\n***\n__CVonline: Image Databases__\nAccess: http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm\n\n***\n__The USC-SIPI Image Database__\n The USC-SIPI image database is a collection of digitized images. It is maintained primarily to support research in image processing, image analysis, and machine vision. The first edition of the USC-SIPI image database was distributed in 1977 and many new images have been added since then.\n\nThe database is divided into volumes based on the basic character of the pictures. Images in each volume are of various sizes such as 256x256 pixels, 512x512 pixels, or 1024x1024 pixels. All images are 8 bits/pixel for black and white images, 24 bits/pixel for color images. The following volumes are currently available:\n\n\tTextures \tBrodatz textures, texture mosaics, etc.\n\tAerials \tHigh altitude aerial images\n\tMiscellaneous \tLena, the mandrill, and other favorites\n\tSequences \tMoving head, fly-overs, moving vehicles\n\nAccess: http://sipi.usc.edu/database/\n\n***\n__Histology dataset: image registration of differently stain slices__\n\n The dataset consists of 2D histological microscopy tissue slices, stained with different stains, and landmarks denoting key-points in each slice. The task is image registration - align all slices in particular set of images (consecutive stain cuts) together, for instance to the initial image plane. The main challenges for these images are the following: very large image size, appearance differences, and lack of distinctive appearance objects. The dataset contains 108 image pairs and manually placed landmarks for registration quality evaluation.\n \nAccess: http://cmp.felk.cvut.cz/~borovji3/?page=dataset\n\n***\n\n## 2. Challenges/Contest Data\n\n__Visual Concept Extraction Challenge in Radiology__\n Manually annotated radiological data of several anatomical structures (e.g. kidney, lung, bladder, etc.) from several different imaging modalities (e.g. CT and MR). They also provide a cloud computing instance that anyone can use to develop and evaluate models against benchmarks.\n\nAccess: http://www.visceral.eu/\n\n***\n__Grand Challenges in Biomedical Image Analysis__\n\nA collection of biomedical imaging challenges in order to _facilitate better comparisons between new and existing solutions_, by standardizing evaluation criteria. You can create your own challenge as well. As of this writing, there are 92 challenges that provide downloadable data sets.\n\nAccess: http://www.grand-challenge.org/\n***\n\n__Dream Challenges__\n\nDREAM Challenges pose fundamental questions about systems biology and translational medicine. Designed and run by a community of researchers from a variety of organizations, our challenges invite participants to propose solutions \u2014 fostering collaboration and building communities in the process. Expertise and institutional support are provided by Sage Bionetworks, along with the infrastructure to host challenges via their Synapse platform. Together, we share a vision allowing individuals and groups to collaborate openly so that the  \u201cwisdom of the crowd\u201d provides the greatest impact on science and human health.\n\n- The Digital Mammography DREAM Challenge.\n- ICGC-TCGA DREAM Somatic Mutation Calling RNA Challenge (SMC-RNA)\n- DREAM Idea Challenge\n- These were the active challenges at the time of adding, many more past challenges and upcoming challenges are present!\n\nAccess: http://dreamchallenges.org/\n\n***\n__Kaggle diabetic retinopathy__\n\nHigh-resolution retinal images that are annotated on a 0\u20134 severity scale by clinicians, for the detection of diabetic retinopathy. This data set is part of a completed Kaggle competition, which is generally a great source for publicly available data sets.\n\nAccess: https://www.kaggle.com/c/diabetic-retinopathy-detection\n\n***\n__Cervical Cancer Screening__\n\nIn this kaggle competition, you will develop algorithms to correctly classify cervix types based on cervical images. These different types of cervix in our data set are all considered normal (not cancerous), but since the transformation zones aren't always visible, some of the patients require further testing while some don't.\n\nAccess: https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening/data\n\n***\n\n__Multiple sclerosis lesion segmentation__\n\n challenge 2008\\. A collection of brain MRI scans to detect MS lesions.\n\nAccess: http://www.ia.unc.edu/MSseg/\n\n***\n__Multimodal Brain Tumor Segmentation Challenge__\n\nLarge data set of brain tumor magnetic resonance scans. They\u2019ve been extending this data set and challenge each year since 2012.\n\nAccess: http://braintumorsegmentation.org/\n\n***\n__Coding4Cancer__\n\nA new initiative by the Foundation for the National Institutes of Health and Sage Bionetworks to host a series of challenges to improve cancer screening. The first is for digital mammography readings. The second is for lung cancer detection. The challenges are not yet launched.\n\nAccess: http://coding4cancer.org/\n***\n\n__EEG Challenge Datasets on Kaggle__\n\n- Melbourne University AES/MathWorks/NIH Seizure Prediction - Predict seizures in long-term human intracranial EEG recordings\n\nAccess: https://www.kaggle.com/c/melbourne-university-seizure-prediction\n\n- American Epilepsy Society Seizure Prediction Challenge - Predict seizures in intracranial EEG recordings\n\nAccess: https://www.kaggle.com/c/seizure-prediction\n\n- UPenn and Mayo Clinic's Seizure Detection Challenge - Detect seizures in intracranial EEG recordings\n\nAccess: https://www.kaggle.com/c/seizure-detection\n\n- Grasp-and-Lift EEG Detection - Identify hand motions from EEG recordings\n\nAccess: https://www.kaggle.com/c/grasp-and-lift-eeg-detection\n\n***\n__Challenges track in MICCAI Conference__\n\nThe Medical Image Computing and Computer Assisted Intervention. Most of the challenges would've been covered by websites like grand-challenges etc. You can still see all of them under the \"Satellite Events\" tab of the conference sites.\n\n- 2019 - https://www.miccai2019.org/programme/workshops-challenges-tutorials/#tablepress-10\n- 2018 - https://www.miccai2018.org/en/WORKSHOP---CHALLENGE---TUTORIAL.html\n- 2017 - http://www.miccai2017.org/satellite-events\n- 2016 - http://www.miccai2016.org/en/SATELLITE-EVENTS.html\n- 2015 - https://www.miccai2015.org/frontend/index.php?page_id=589\n\nAccess: http://www.miccai.org/ConferenceHistory\n\n***\n\n__International Symposium on Biomedical Imaging (ISBI)__\n\nThe IEEE International Symposium on Biomedical Imaging (ISBI) is a scientific conference dedicated to mathematical, algorithmic, and computational aspects of biomedical imaging, across all scales of observation. Most of these challenges will be listed in grand-challenges. You can still access it by visiting the \"Challenges\" tab under \"Program\" in each year's website.\n\n- 2019 - https://biomedicalimaging.org/2019/challenges/\n- 2018 - https://biomedicalimaging.org/2018/challenges/\n- 2017 - http://biomedicalimaging.org/2017/challenges/\n- 2016 - http://biomedicalimaging.org/2016/?page_id=416\n\nAccess: http://biomedicalimaging.org\n\n***\n\n__Continuous Registration Challenge (CRC)__\n\nContinuous Registration Challenge (CRC) is a challenge for registration of lung- and brain images inspired by modern software development practices. Participants implement their algorithm using the open source SuperElastix C++ API.\nThe challenge focuses on pairwise registration of lungs and brains, two problems frequently encountered in clinical settings. They have collected seven open-access data sets and one private data set (3+1 lung data sets, 4 brain data sets). The challenge results will be presented and discussed at the upcoming Workshop On Biomedical Image Registration (WBIR 2018).\n\nAccess: https://continuousregistration.grand-challenge.org/home/\n\n***\n\n__Automatic Non-rigid Histological Image Registration (ANHIR)__\n\nThis ANHIR challenge aims at the automatic nonlinear image registration of 2D whole slice imaging (WSI) microscopy images of histopathology tissue samples stained with different dyes. The task is difficult due to non-linear deformations affecting the tissue samples, different appearance of each stain, repetitive texture, and the large size of the whole slide images.\n\n* Challenge: https://anhir.grand-challenge.org/\n* Benchmark: http://borda.github.io/BIRL\n* Refernce: [BIRL: Benchmark on Image Registration methods with Landmark validation](https://www.researchgate.net/publication/338291737_BIRL_Benchmark_on_Image_Registration_methods_with_Landmark_validation)\n\n***\n\n__Bone X-Ray Deep Learning Competition using MURA__\n\nMURA (musculoskeletal radiographs) is a large dataset of bone X-rays. The Stanford ML Group and AIMI Center are hosting a competition where algorithms are tasked with determining whether an X-ray study is normal or abnormal. The algorithms are evaluated on a test set of 207 musculoskeletal studies, where each study was individually retrospectively labeled as normal or abnormal by 6 board-certified radiologists. Three of these radiologists were used to create a gold standard, defined as the majority vote of the labels of the radiologists, and the other three were used to obtain the best radiologist performance, defined as the maximum score of the three radiologists with the gold standard as groundtruth. The challenge leaderboard is hosted publicly and updated every two weeks.\n\nAccess: https://stanfordmlgroup.github.io/competitions/mura/\n\n***\n\n__2019 Kidney and Kidney Tumor Segmentation Challenge (KiTS19)__\n\nThe KiTS19 challenge is on the semantic segmentation of kidneys and kidney tumors in contrast-enhanced CT scans. The dataset consists of 300 patients with preoperative arterial-phase abdominal CTs annotated by experts. 210 (70%) of these were released as a training set and the remaining 90 (30%) were held out as a test set. This challenge was held in conjunction with MICCAI 2019.\n\nAccess: https://github.com/neheller/kits19/\n\n***\n\n## 3. Data derived from Electronic Health Records (EHRs)\n\n__Building the graph of medicine from millions of clinical narratives__  \nCo-occurence statistics for medical terms extracted from 14 million clinical notes and 260,000 patients.  \nPaper: http://www.nature.com/articles/sdata201432  \nData: http://datadryad.org/resource/doi:10.5061/dryad.jp917  \n\n***\n__Learning Low-Dimensional Representations of Medical Concept__  \nLow-dimensional embeddings of medical concepts constructed using claims data. Note that this paper utilizes data from _Building the graph of medicine from millions of clinical narratives_    \nPaper: http://cs.nyu.edu/~dsontag/papers/ChoiChiuSontag_AMIA_CRI16.pdf  \nData: https://github.com/clinicalml/embeddings  \n\n***\n__MIMIC-III, a freely accessible critical care database__  \nAnonymized critical care EHR database on 38,597 patients and 53,423 ICU admissions. __Requires registration__.  \nPaper: http://www.nature.com/articles/sdata201635  \nData: http://physionet.org/physiobank/database/mimic3cdb/  \n***\n\n__Clinical Concept Embeddings Learned from Massive Sources of Medical Data__  \nEmbeddings for 108,477 medical concepts learned from 60 million patients, 1.7 million journal articles, and clinical notes of 20 million patients  \nPaper: https://arxiv.org/abs/1804.01486  \nEmbeddings: \u00a0https://figshare.com/s/00d69861786cd0156d81  \nInteractive tool: http://cui2vec.dbmi.hms.harvard.edu  \n***\n\n__Evaluation of Embeddings of Laboratory Test Codes for Patients at a Cancer Center__  \n200 dimensional Word2Vec embeddings of 1098 laboratory test codes (LOINCs) trained from 8,280,238 lab orders for 79,081 patients at City of Hope National Medical Center (Los Angeles, CA).  \nPaper: https://arxiv.org/abs/1907.09600  \nEmbeddings and Code: https://github.com/elleros/DSHealth2019_loinc_embeddings  \n***\n\n\n## 4. National Healthcare Data  \n__Centers for Disease Control and Prevention (CDC)__  \nData from the CDC on many areas, including:  \n- Biomonitoring\n- Child Vaccinations\n- Flu Vaccinations\n- Health Statistics\n- Injury & Violence\n- MMWR\n- Motor Vehicle\n- NCHS\n- NNDSS\n- Pregnancy & Vaccination\n- STDs\n- Smoking & Tobacco Use\n- Teen Vaccinations\n- Traumatic Brain Injury\n- Vaccinations\n- Web Metrics\n\nLanding page: https://data.cdc.gov  \nData Catalog: https://data.cdc.gov/browse  \n\n***\n__Medicare Data__  \nData from the Centers for Medicare & Medicaid Services (CMS) on hospitals, nursing homes, physicians, home healthcare, dialysis, and device providers.  \nLanding page: https://data.medicare.gov  \nExplorer: https://data.medicare.gov/data  \n***\n__Texas Public Use Inpatient Data File__\nData on 11 Million inpatient visits with diagnosis, procedure codes and outcomes from Texas between 2006 & 2009.\n\nLink: https://www.dshs.texas.gov/thcic/hospitals/Inpatientpudf.shtm\n\n***\n__Dollars for Doctors__  \nPropublica investigation of money paid by pharmaceutical companies to doctors.  \nInformation: https://www.propublica.org/series/dollars-for-docs  \nSearch tool: https://projects.propublica.org/docdollars/  \nData request: https://projects.propublica.org/data-store/sets/health-d4d-national-2   \n\n***\n__DocGraph__\nPhysician interaction network obtained through a freedom of information act request. Covers nearly 1 million entities.  \nMain page: http://www.docgraph.com  \nInformation: http://thehealthcareblog.com/blog/2012/11/05/tracking-the-social-doctor-opening-up-physician-referral-data-and-much-more/  \nData: http://linea.docgraph.org  \n***\n\n## 5. UCI Datasets\n__Liver Disorders Data Set__  \nData on 345 patients with and without liver disease. Features are 5 blood biomarkers thought to be involved with liver disease.  \nData: https://archive.ics.uci.edu/ml/datasets/Liver+Disorders  \n\n__Thyroid Disease Data Set__  \nData: https://archive.ics.uci.edu/ml/datasets/Thyroid+Disease  \n\n__Breast Cancer Data Set__  \nData: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer  \n\n__Heart Disease Data Set__  \nData: https://archive.ics.uci.edu/ml/datasets/Heart+Disease  \n\n__Lymphography Data Set__  \nData: https://archive.ics.uci.edu/ml/datasets/Lymphography  \n\n__Parkinsons Data Set__  \nData: https://archive.ics.uci.edu/ml/datasets/parkinsons\n\n__Parkinsons Telemonitoring Data Set__  \nData: https://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring\n\n__Parkinson Speech Dataset with Multiple Types of Sound Recordings Data Set__  \nData: https://archive.ics.uci.edu/ml/datasets/Parkinson+Speech+Dataset+with++Multiple+Types+of+Sound+Recordings\n\n__Parkinson's Disease Classification Data Set__  \nData: https://archive.ics.uci.edu/ml/datasets/Parkinson%27s+Disease+Classification\n\n__Primary Tumor Dataset__\nData: https://archive.ics.uci.edu/ml/datasets/primary+tumor\n\n## 6. Biomedical Literature\n__PMC Open Access Subset__  \nCollection of all the full-text, open access articles in Pubmed central.  \nInformation: http://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/  \nArchived files: http://www.ncbi.nlm.nih.gov/pmc/tools/ftp/#Data_Mining  \n\n__PubMed 200k RCT__\n\nCollection of pubmed abstracts from randomized control trials (RCTs). Annotations for each sentence in the abstract are available.\n\nPaper: https://arxiv.org/abs/1710.06071\n\nData: https://github.com/Franck-Dernoncourt/pubmed-rct\n\n__Web API of PubMed Articles__\n\nNLM also provided Web API for accessing biomedical literatures in PubMed.\n\nInstructions for getting PubMed articles: https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PubMed/ (not full text, just title, abstract, etc.)\n\nFor articles in PubMed Central, instructions for getting the whole articles: https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PMC/\n\n__EBM NLP__\n\nCollection of pubmed abstracts from randomized control trials (RCTs). Annotation of Population, Intervention, and Outcomes (PICO elements) are available.\n\nPaper: https://arxiv.org/abs/1806.04185\n\nData: https://ebm-nlp.herokuapp.com/annotations\n\nWebsite: https://ebm-nlp.herokuapp.com/index\n\n__Evidence Inference__\n\nA dataset for inferring the results of randomized control trials (RCTs). A collection of pubmed RCTs from the open access subset. Annotations of (intervention, comparison intervention, outcome, significance finding, evidence span) are available. \n\nPaper: https://arxiv.org/abs/1904.01606\n\nData: https://github.com/jayded/evidence-inference/tree/master/annotations\n\nWebsite: http://evidence-inference.ebm-nlp.com/\n\n__PubMedQA__\n\nA dataset for biomedical research question answering. The task is to use yes/no/maybe to answer naturally occuring questions in PubMed titles.\n\nPaper: https://arxiv.org/abs/1909.06146\n\nData: https://github.com/pubmedqa/pubmedqa\n\nWebsite: https://pubmedqa.github.io/\n\n\n## 6. TREC Precision Medicine / Clinical Decision Support Track\nText REtrieval Conference (TREC) is running a track on Precision Medicine / Clinical Decision Support from 2014.\n\n__2014 Clinical Decision Support Track__  \nFocus: Retrieval of biomedical articles relevant for answering generic clinical questions about medical records.  \nInformation and Data: http://www.trec-cds.org/2014.html\n\n__2015 Clinical Decision Support Track__  \nFocus: Retrieval of biomedical articles relevant for answering generic clinical questions about medical records.  \nInformation and Data: http://www.trec-cds.org/2015.html\n\n__2016 Clinical Decision Support Track__  \nFocus: Retrieval of biomedical articles relevant for answering generic clinical questions about medical records. Actual electronic health record (EHR) patient records are be used instead of synthetic cases.   \nInformation and Data: http://www.trec-cds.org/2016.html\n\n__2017 Clinical Decision Support Track__  \nFocus: Retrieve useful precision medicine-related information to clinicians treating cancer patients.  \nInformation and Data: http://www.trec-cds.org/2017.html\n\n## 7. Medical Speech Data\n__The TORGO Database: Acoustic and articulatory speech from speakers with dysarthria__  \nThe TORGO database of dysarthric articulation consists of aligned acoustics and measured 3D articulatory features from speakers with either cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS), which are two of the most prevalent causes of speech disability (Kent and Rosen, 2004), and matched controls. This database, called TORGO, is the result of a collaboration between the departments of Computer Science and Speech-Language Pathology at the University of Toronto and the Holland-Bloorview Kids Rehab hospital in Toronto.\n\nInformation and data: http://www.cs.toronto.edu/~complingweb/data/TORGO/torgo.html  \n\nPaper: [link](https://www.researchgate.net/publication/225446742_The_TORGO_database_of_acoustic_and_articulatory_speech_from_speakers_with_dysarthria)\n***\n\n__NKI-CCRT Corpus: Speech Intelligibility Before and After Advanced Head and Neck Cancer Treated with Concomitant Chemoradiotherapy.__  \n NKI-CCRT corpus with individual listener judgements on the intelligibility of recordings of 55 speakers treated for cancer of the head and neck will be made available for restricted scientific use. The corpus contains recordings and perceptual evaluations of speech intelligibility over three evaluation moments: before treatment and after treatment (10-weeks and 12-months). Treatment was by means of chemoradiotherapy (CCRT).\n\nPaper: http://lrec.elra.info/proceedings/lrec2012/pdf/230_Paper.pdf\n\nAccess: Contact authors.\n\n\n***\n\n__Atypical Affect Interspeech Sub-Challenge__\n\n  Bj\u00f6rn Schuller, Simone Hantke, and colleagues are providing the EMOTASS Corpus. This unique corpus is the first to give access to recordings of affective speech from disabled individuals encompassing a broader variety of mental, neurological, and physical disabilities. It comprises recordings of 15 disabled adult individuals (ages range from 19 to 58 years with a mean age of 31.6 years). The task will be classification of five emotions from their speech facing atypical display. Recordings were made in their everyday working environment. Overall, around 11k utterances and around nine hours of speech are included.     \n\nPaper: http://emotion-research.net/sigs/speech-sig/is2018_compare.pdf\n\nLink: http://emotion-research.net/sigs/speech-sig/is18-compare.\n\n\n***\n\n__Autism Sub-Challenge__\n\nThe Autism Sub-Challenge is based upon the \u201cChild Pathological Speech Database\u201d (CPSD) . It provides speech as recorded in two university departments of child and adolescent psychiatry, located in Paris, France (Universite Pierre et Marie Curie/Pitie Salpetiere Hospital and Universite Rene Descartes/Necker Hospital). The dataset used in the Sub-Challenge contains 2.5 k instances of speech recordings from 99 children aged 6 to 18\n\nPaper: http://emotion-research.net/sigs/speech-sig/is2013_compare.pdf\n\nLink: http://emotion-research.net/sigs/speech-sig/is13-compare.\n"
 },
 {
  "repo": "src-d/awesome-machine-learning-on-source-code",
  "language": null,
  "readme_contents": "# Awesome Machine Learning On Source Code [![Awesome Machine Learning On Source Code](badges/awesome.svg)](https://github.com/src-d/awesome-machine-learning-on-source-code) [![CI Status](https://travis-ci.org/src-d/awesome-machine-learning-on-source-code.svg)](https://travis-ci.org/src-d/awesome-machine-learning-on-source-code)\n\n![Awesome Machine Learning On Source Code](img/awesome-machine-learning-artwork.png)\n\n**Notice: This repository is no longer actively maintained, and no further updates will be done, nor issues/PRs will be answered or attended.**\nAn alternative actively maintained can be found at [ml4code.github.io](https://ml4code.github.io/papers.html) [repository](https://github.com/ml4code/ml4code.github.io).\n\nA curated list of awesome research papers, datasets and software projects devoted to machine learning _and_ source code. [#MLonCode](https://twitter.com/hashtag/MLonCode)\n\n## Contents\n\n- [Digests](#digests)\n- [Conferences](#conferences)\n- [Competitions](#competitions)\n- [Papers](#papers)\n  - [Program Synthesis and Induction](#program-synthesis-and-induction)\n  - [Source Code Analysis and Language modeling](#source-code-analysis-and-language-modeling)\n  - [Neural Network Architectures and Algorithms](#neural-network-architectures-and-algorithms)\n  - [Embeddings in Software Engineering](#embeddings-in-software-engineering)\n  - [Program Translation](#program-translation)\n  - [Code Suggestion and Completion](#code-suggestion-and-completion)\n  - [Program Repair and Bug Detection](#program-repair-and-bug-detection)\n  - [APIs and Code Mining](#apis-and-code-mining)\n  - [Code Optimization](#code-optimization)\n  - [Topic Modeling](#topic-modeling)\n  - [Sentiment Analysis](#sentiment-analysis)\n  - [Code Summarization](#code-summarization)\n  - [Clone Detection](#clone-detection)\n  - [Differentiable Interpreters](#differentiable-interpreters)\n  - [Related research](#related-research)<details><summary>(links require \"Related research\" spoiler to be open)</summary>\n    - [AST Differencing](#ast-differencing)\n    - [Binary Data Modeling](#binary-data-modeling)\n    - [Soft Clustering Using T-mixture Models](#soft-clustering-using-t-mixture-models)\n    - [Natural Language Parsing and Comprehension](#natural-language-parsing-and-comprehension)\n      </details>\n- [Posts](#posts)\n- [Talks](#talks)\n- [Software](#software)\n  - [Machine Learning](#machine-learning)\n  - [Utilities](#utilities)\n- [Datasets](#datasets)\n- [Credits](#credits)\n- [Contributions](#contributions)\n- [License](#license)\n\n## Digests\n\n- [Learning from \"Big Code\"](http://learnbigcode.github.io) - Techniques, challenges, tools, datasets on \"Big Code\".\n- [A Survey of Machine Learning for Big Code and Naturalness](https://ml4code.github.io/) - Survey and literature review on Machine Learning on Source Code.\n\n## Conferences\n\n- <img src=\"badges/origin-academia-blue.svg\" alt=\"origin-academia\" align=\"top\"> [ACM International Conference on Software Engineering, ICSE](https://www.icse2018.org/)\n- <img src=\"badges/origin-academia-blue.svg\" alt=\"origin-academia\" align=\"top\"> [ACM International Conference on Automated Software Engineering, ASE](https://2019.aseconf.org)\n- <img src=\"badges/origin-academia-blue.svg\" alt=\"origin-academia\" align=\"top\"> [ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (FSE)](https://conf.researchr.org/home/fse-2018)\n- <img src=\"badges/origin-academia-blue.svg\" alt=\"origin-academia\" align=\"top\"> [2018 IEEE 25th International Conference on Software Analysis, Evolution, and Reengineering (SANER)](https://www.conference-publishing.com/list.php?Event=SANER18MAIN)\n- <img src=\"badges/origin-academia-blue.svg\" alt=\"origin-academia\" align=\"top\"> [Machine Learning for Programming](https://ml4p.org/)\n- <img src=\"badges/origin-academia-blue.svg\" alt=\"origin-academia\" align=\"top\"> [Workshop on NLP for Software Engineering](https://nl4se.github.io/)\n- <img src=\"badges/origin-industry-green.svg\" alt=\"origin-industry\" align=\"top\"> [SysML](http://www.sysml.cc/)\n  - [Talks](https://www.youtube.com/channel/UChutDKIa-AYyAmbT45s991g/)\n- <img src=\"badges/origin-academia-blue.svg\" alt=\"origin-academia\" align=\"top\"> [Mining Software Repositories](http://www.msrconf.org/)\n- <img src=\"badges/origin-industry-green.svg\" alt=\"origin-industry\" align=\"top\"> [AIFORSE](https://aiforse.org/)\n- <img src=\"badges/origin-industry-green.svg\" alt=\"origin-industry\" align=\"top\"> [source{d} tech talks](https://blog.sourced.tech/post/ml_talks_moscow/)\n  - [Talks](https://www.youtube.com/playlist?list=PL5Ld68ole7j3iQFUSB3fR9122dHCUWXsy)\n- <img src=\"badges/origin-academia-blue.svg\" alt=\"origin-academia\" align=\"top\"> [NIPS Neural Abstract Machines and Program Induction workshop](https://uclmr.github.io/nampi/)\n  - [Talks](https://www.youtube.com/playlist?list=PLzTDea_cM27LVPSTdK9RypSyqBHZWPywt)\n- <img src=\"badges/origin-academia-blue.svg\" alt=\"origin-academia\" align=\"top\"> [CamAIML](https://www.microsoft.com/en-us/research/event/artificial-intelligence-and-machine-learning-in-cambridge-2017/)\n  - [Learning to Code: Machine Learning for Program Induction](https://www.youtube.com/watch?v=vzDuVhFMB9Q) - Alexander Gaunt.\n- <img src=\"badges/origin-academia-blue.svg\" alt=\"origin-academia\" align=\"top\"> [MASES 2018](https://mases18.github.io/)\n\n## Competitions\n\n- [CodRep](https://github.com/KTH/CodRep-competition) - competition on automatic program repair: given a source line, find the insertion point.\n\n## Papers\n\n#### Program Synthesis and Induction\n\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Program Synthesis and Semantic Parsing with Learned Code Idioms](https://arxiv.org/abs/1906.10816v2) - Richard Shin, Miltiadis Allamanis, Marc Brockschmidt, Oleksandr Polozov, 2019.\n- <img src=\"badges/16-pages-gray.svg\" alt=\"16-pages\" align=\"top\"> [Synthetic Datasets for Neural Program Synthesis](https://openreview.net/forum?id=ryeOSnAqYm) - Richard Shin, Neel Kant, Kavi Gupta, Chris Bender, Brandon Trabucco, Rishabh Singh, Dawn Song, ICLR 2019.\n- <img src=\"badges/15-pages-gray.svg\" alt=\"15-pages\" align=\"top\"> [Execution-Guided Neural Program Synthesis](https://openreview.net/forum?id=H1gfOiAqYm) - Xinyun Chen, Chang Liu, Dawn Song, ICLR 2019.\n- <img src=\"badges/8-pages-gray.svg\" alt=\"8-pages\" align=\"top\"> [DeepFuzz: Automatic Generation of Syntax Valid C Programs for Fuzz Testing](https://faculty.ist.psu.edu/wu/papers/DeepFuzz.pdf) - Xiao Liu, Xiaoting Li, Rupesh Prajapati, Dinghao Wu, AAAI 2019.\n- <img src=\"badges/12-pages-beginner-brightgreen.svg\" alt=\"12-pages-beginner\" align=\"top\"> [NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System](https://arxiv.org/abs/1802.08979v2) - Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, Michael D. Ernst, LREC 2018.\n- <img src=\"badges/18-pages-gray.svg\" alt=\"18-pages\" align=\"top\"> [Recent Advances in Neural Program Synthesis](https://arxiv.org/abs/1802.02353v1) - Neel Kant, 2018.\n- <img src=\"badges/16-pages-gray.svg\" alt=\"16-pages\" align=\"top\"> [Neural Sketch Learning for Conditional Program Generation](https://arxiv.org/abs/1703.05698) - Vijayaraghavan Murali, Letao Qi, Swarat Chaudhuri, Chris Jermaine, ICLR 2018.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [Neural Program Search: Solving Programming Tasks from Description and Examples](https://arxiv.org/abs/1802.04335v1) - Illia Polosukhin, Alexander Skidanov, ICLR 2018.\n- <img src=\"badges/16-pages-gray.svg\" alt=\"16-pages\" align=\"top\"> [Neural Program Synthesis with Priority Queue Training](https://arxiv.org/abs/1801.03526v1) - Daniel A. Abolafia, Mohammad Norouzi, Quoc V. Le, 2018.\n- <img src=\"badges/31-pages-gray.svg\" alt=\"31-pages\" align=\"top\"> [Towards Synthesizing Complex Programs from Input-Output Examples](https://arxiv.org/abs/1706.01284v3) - Xinyun Chen, Chang Liu, Dawn Song, ICLR 2018.\n- <img src=\"badges/8-pages-gray.svg\" alt=\"8-pages\" align=\"top\"> [Glass-Box Program Synthesis: A Machine Learning Approach](https://arxiv.org/abs/1709.08669v1) - Konstantina Christakopoulou, Adam Tauman Kalai, AAAI 2018.\n- <img src=\"badges/14-pages-beginner-brightgreen.svg\" alt=\"14-pages\" align=\"top\"> [Synthesizing Benchmarks for Predictive Modeling](https://chriscummins.cc/pub/2017-cgo.pdf) - Chris Cummins, Pavlos Petoumenos, Zheng Wang, Hugh Leather, CGO 2017\n- <img src=\"badges/17-pages-beginner-brightgreen.svg\" alt=\"17-pages-beginner\" align=\"top\"> [Program Synthesis for Character Level Language Modeling](https://files.sri.inf.ethz.ch/website/papers/charmodel-iclr2017.pdf) - Pavol Bielik, Veselin Raychev, Martin Vechev, ICLR 2017.\n- <img src=\"badges/13-pages-beginner-brightgreen.svg\" alt=\"13-pages-beginner\" align=\"top\"> [SQLNet: Generating Structured Queries From Natural Language Without Reinforcement Learning](https://arxiv.org/abs/1711.04436v1) - Xiaojun Xu, Chang Liu, Dawn Song, 2017.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Learning to Select Examples for Program Synthesis](https://arxiv.org/abs/1711.03243v1) - Yewen Pu, Zachery Miranda, Armando Solar-Lezama, Leslie Pack Kaelbling, 2017.\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [Neural Program Meta-Induction](https://arxiv.org/abs/1710.04157v1) - Jacob Devlin, Rudy Bunel, Rishabh Singh, Matthew Hausknecht, Pushmeet Kohli, NIPS 2017.\n- <img src=\"badges/14-pages-beginner-brightgreen.svg\" alt=\"14-pages-beginner\" align=\"top\"> [Learning to Infer Graphics Programs from Hand-Drawn Images](https://arxiv.org/abs/1707.09627v4) - Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, Joshua B. Tenenbaum, 2017.\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [Neural Attribute Machines for Program Generation](https://arxiv.org/abs/1705.09231v2) - Matthew Amodio, Swarat Chaudhuri, Thomas Reps, 2017.\n- <img src=\"badges/11-pages-beginner-brightgreen.svg\" alt=\"11-pages-beginner\" align=\"top\"> [Abstract Syntax Networks for Code Generation and Semantic Parsing](https://arxiv.org/abs/1704.07535v1) - Maxim Rabinovich, Mitchell Stern, Dan Klein, ACL 2017.\n- <img src=\"badges/20-pages-gray.svg\" alt=\"20-pages\" align=\"top\"> [Making Neural Programming Architectures Generalize via Recursion](https://arxiv.org/pdf/1704.06611v1.pdf) - Jonathon Cai, Richard Shin, Dawn Song, ICLR 2017.\n- <img src=\"badges/14-pages-gray.svg\" alt=\"14-pages\" align=\"top\"> [A Syntactic Neural Model for General-Purpose Code Generation](https://arxiv.org/abs/1704.01696v1) - Pengcheng Yin, Graham Neubig, ACL 2017.\n- <img src=\"badges/12-pages-beginner-brightgreen.svg\" alt=\"12-pages-beginner\" align=\"top\"> [Program Synthesis from Natural Language Using Recurrent Neural Networks](https://homes.cs.washington.edu/~mernst/pubs/nl-command-tr170301.pdf) - Xi Victoria Lin, Chenglong Wang, Deric Pang, Kevin Vu, Luke Zettlemoyer, Michael Ernst, 2017.\n- <img src=\"badges/18-pages-beginner-brightgreen.svg\" alt=\"18-pages-beginner\" align=\"top\"> [RobustFill: Neural Program Learning under Noisy I/O](https://arxiv.org/abs/1703.07469v1) - Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, Pushmeet Kohli, ICML 2017.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [Lifelong Perceptual Programming By Example](https://openreview.net/pdf?id=HJStZKqel) - Gaunt, Alexander L., Marc Brockschmidt, Nate Kushman, and Daniel Tarlow, 2017.\n- <img src=\"badges/7-pages-gray.svg\" alt=\"7-pages\" align=\"top\"> [Neural Programming by Example](https://arxiv.org/abs/1703.04990v1) - Chengxun Shu, Hongyu Zhang, AAAI 2017.\n- <img src=\"badges/21-pages-gray.svg\" alt=\"21-pages\" align=\"top\"> [DeepCoder: Learning to Write Programs](https://arxiv.org/abs/1611.01989) - Balog Matej, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow, ICLR 2017.\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [A Differentiable Approach to Inductive Logic Programming](https://pdfs.semanticscholar.org/9698/409fc1603d28b6d51c38261f6243837c8bdd.pdf) - Yang Fan, Zhilin Yang, and William W. Cohen, 2017.\n- <img src=\"badges/12-pages-beginner-brightgreen.svg\" alt=\"12-pages-beginner\" align=\"top\"> [Latent Attention For If-Then Program Synthesis](https://arxiv.org/abs/1611.01867v1) - Xinyun Chen, Chang Liu, Richard Shin, Dawn Song, Mingcheng Chen, NIPS 2016.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\" id=\"card2code\"> [Latent Predictor Networks for Code Generation](https://arxiv.org/abs/1603.06744) - Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tom\u00e1\u0161 Ko\u010disk\u00fd, Andrew Senior, Fumin Wang, Phil Blunsom, ACL 2016.\n- <img src=\"badges/6-pages-gray.svg\" alt=\"6-pages\" align=\"top\"> [Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision (Short Version)](https://arxiv.org/abs/1612.01197) - Liang Chen, Jonathan Berant, Quoc Le, Kenneth D. Forbus, and Ni Lao, NIPS 2016.\n- <img src=\"badges/5-pages-gray.svg\" alt=\"5-pages\" align=\"top\"> [Programs as Black-Box Explanations](https://arxiv.org/abs/1611.07579) - Singh, Sameer, Marco Tulio Ribeiro, and Carlos Guestrin, NIPS 2016.\n- <img src=\"badges/15-pages-gray.svg\" alt=\"15-pages\" align=\"top\"> [Search-Based Generalization and Refinement of Code Templates](http://soft.vub.ac.be/Publications/2016/vub-soft-tr-16-06.pdf) - Tim Molderez, Coen De Roover, SSBSE 2016.\n- <img src=\"badges/14-pages-gray.svg\" alt=\"14-pages\" align=\"top\"> [Structured Generative Models of Natural Source Code](https://arxiv.org/abs/1401.0514) - Chris J. Maddison, Daniel Tarlow, ICML 2014.\n\n#### Source Code Analysis and Language modeling\n\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Modeling Vocabulary for Big Code Machine Learning](https://arxiv.org/abs/1904.01873v1) - Hlib Babii, Andrea Janes, Romain Robbes, 2019.\n- <img src=\"badges/24-pages-gray.svg\" alt=\"24-pages\" align=\"top\"> [Generative Code Modeling with Graphs](https://openreview.net/forum?id=Bke4KsA5FX) - Marc Brockschmidt, Miltiadis Allamanis, Alexander L. Gaunt, Oleksandr Polozov, ICLR 2019.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [NL2Type: Inferring JavaScript Function Types from Natural Language Information](http://software-lab.org/publications/icse2019_NL2Type.pdf) - Rabee Sohail Malik, Jibesh Patra, Michael Pradel, ICSE 2019.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [A Novel Neural Source Code Representation based on Abstract Syntax Tree](http://xuwang.tech/paper/astnn_icse2019.pdf) - Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, Xudong Liu, ICSE 2019.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [Deep Learning Type Inference](http://vhellendoorn.github.io/PDF/fse2018-j2t.pdf) - Vincent J. Hellendoorn, Christian Bird, Earl T. Barr and Miltiadis Allamanis, FSE 2018. [Code](https://github.com/DeepTyper/DeepTyper).\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Tree2Tree Neural Translation Model for Learning Source Code Changes](https://arxiv.org/pdf/1810.00314.pdf) - Saikat Chakraborty, Miltiadis Allamanis, Baishakhi Ray, 2018.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [code2seq: Generating Sequences from Structured Representations of Code](https://arxiv.org/abs/1808.01400) - Uri Alon, Omer Levy, Eran Yahav, 2018.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Syntax and Sensibility: Using language models to detect and correct syntax errors](http://softwareprocess.es/pubs/santos2018SANER-syntax.pdf) - Eddie Antonio Santos, Joshua Charles Campbell, Dhvani Patel, Abram Hindle, and Jos\u00e9 Nelson Amaral, SANER 2018.\n- <img src=\"badges/25-pages-gray.svg\" alt=\"25-pages\" align=\"top\"> [code2vec: Learning Distributed Representations of Code](https://arxiv.org/abs/1803.09473v2) - Uri Alon, Meital Zilberstein, Omer Levy, Eran Yahav, 2018.\n- <img src=\"badges/16-pages-gray.svg\" alt=\"16-pages\" align=\"top\"> [Learning to Represent Programs with Graphs](https://arxiv.org/abs/1711.00740v1) - Miltiadis Allamanis, Marc Brockschmidt, Mahmoud Khademi, ICLR 2018.\n- <img src=\"badges/36-pages-gray.svg\" alt=\"36-pages\" align=\"top\"> [A Survey of Machine Learning for Big Code and Naturalness](https://arxiv.org/abs/1709.06182v1) - Miltiadis Allamanis, Earl T. Barr, Premkumar Devanbu, Charles Sutton, 2017.\n- <img src=\"badges/36-pages-gray.svg\" alt=\"36-pages\" align=\"top\"> [Are Deep Neural Networks the Best Choice for Modeling Source Code?](http://web.cs.ucdavis.edu/~devanbu/isDLgood.pdf) - Vincent J. Hellendoorn, Premkumar Devanbu, FSE 2017.\n- <img src=\"badges/4-pages-gray.svg\" alt=\"4-pages\" align=\"top\"> [A deep language model for software code](https://arxiv.org/abs/1608.02715v1) - Hoa Khanh Dam, Truyen Tran, Trang Pham, 2016.\n- <img src=\"badges/8-pages-gray.svg\" alt=\"8-pages\" align=\"top\"> [Convolutional Neural Networks over Tree Structures for Programming Language Processing](https://arxiv.org/abs/1409.5718) - Lili Mou, Ge Li, Lu Zhang, Tao Wang, Zhi Jin, AAAI-16. [Code](https://github.com/crestonbunch/tbcnn).\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Suggesting Accurate Method and Class Names](http://homepages.inf.ed.ac.uk/csutton/publications/accurate-method-and-class.pdf) - Miltiadis Allamanis, Earl T. Barr, Christian Bird, Charles Sutton, FSE 2015.\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [Mining Source Code Repositories at Massive Scale using Language Modeling](http://homepages.inf.ed.ac.uk/csutton/publications/msr2013.pdf) - Miltiadis Allamanis, Charles Sutton, MSR 2013.\n\n#### Neural Network Architectures and Algorithms\n\n- <img src=\"badges/19-pages-gray.svg\" alt=\"19-pages\" align=\"top\"> [Learning Compositional Neural Programs with Recursive Tree Search and Planning](https://arxiv.org/abs/1905.12941v1) - Thomas Pierrot, Guillaume Ligner, Scott Reed, Olivier Sigaud, Nicolas Perrin, Alexandre Laterre, David Kas, Karim Beguir, Nando de Freitas, 2019.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [From Programs to Interpretable Deep Models and Back](https://link.springer.com/content/pdf/10.1007%2F978-3-319-96145-3_2.pdf) - Eran Yahav, ICCAV 2018.\n- <img src=\"badges/13-pages-gray.svg\" alt=\"13-pages\" align=\"top\"> [Neural Code Comprehension: A Learnable Representation of Code Semantics](https://arxiv.org/abs/1806.07336) - Tal Ben-Nun, Alice Shoshana Jakobovits, Torsten Hoefler, NIPS 2018.\n- <img src=\"badges/16-pages-gray.svg\" alt=\"16-pages\" align=\"top\"> [A General Path-Based Representation for Predicting Program Properties](https://arxiv.org/abs/1803.09544) - Uri Alon, Meital Zilberstein, Omer Levy, Eran Yahav, PLDI 2018.\n- <img src=\"badges/4-pages-gray.svg\" alt=\"4-pages\" align=\"top\"> [Cross-Language Learning for Program Classification using Bilateral Tree-Based Convolutional Neural Networks](https://arxiv.org/abs/1710.06159v2) - Nghi D. Q. Bui, Lingxiao Jiang, Yijun Yu, AAAI 2018.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Bilateral Dependency Neural Networks for Cross-Language Algorithm Classification](https://bdqnghi.github.io/files/SANER_2019_bilateral_dependency.pdf) - Nghi D. Q. Bui, Yijun Yu, Lingxiao Jiang, SANER 2018.\n- <img src=\"badges/17-pages-gray.svg\" alt=\"17-pages\" align=\"top\"> [Syntax-Directed Variational Autoencoder for Structured Data](https://openreview.net/pdf?id=SyqShMZRb) - Hanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, Le Song, ICLR 2018.\n- <img src=\"badges/19-pages-gray.svg\" alt=\"19-pages\" align=\"top\"> [Divide and Conquer with Neural Networks](https://arxiv.org/abs/1611.02401) - Nowak, Alex, and Joan Bruna, ICLR 2018.\n- <img src=\"badges/13-pages-gray.svg\" alt=\"13-pages\" align=\"top\"> [Hierarchical multiscale recurrent neural networks](https://arxiv.org/abs/1609.01704) - Chung Junyoung, Sungjin Ahn, and Yoshua Bengio, ICLR 2017.\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [Learning Efficient Algorithms with Hierarchical Attentive Memory](https://arxiv.org/abs/1602.03218) - Andrychowicz, Marcin, and Karol Kurach, 2016.\n- <img src=\"badges/6-pages-gray.svg\" alt=\"6-pages\" align=\"top\"> [Learning Operations on a Stack with Neural Turing Machines](https://arxiv.org/abs/1612.00827) - Deleu, Tristan, and Joseph Dureau, NIPS 2016.\n- <img src=\"badges/5-pages-gray.svg\" alt=\"5-pages\" align=\"top\"> [Probabilistic Neural Programs](https://arxiv.org/abs/1612.00712) - Murray, Kenton W., and Jayant Krishnamurthy, NIPS 2016.\n- <img src=\"badges/13-pages-gray.svg\" alt=\"13-pages\" align=\"top\"> [Neural Programmer-Interpreters](https://arxiv.org/abs/1511.06279) - Reed, Scott, and Nando de Freitas, ICLR 2016.\n- <img src=\"badges/9-pages-gray.svg\" alt=\"9-pages\" align=\"top\"> [Neural GPUs Learn Algorithms](https://arxiv.org/abs/1511.08228) - Kaiser, \u0141ukasz, and Ilya Sutskever, ICLR 2016.\n- <img src=\"badges/17-pages-gray.svg\" alt=\"17-pages\" align=\"top\"> [Neural Random-Access Machines](https://arxiv.org/abs/1511.06392v3) - Karol Kurach, Marcin Andrychowicz, Ilya Sutskever, ERCIM News 2016.\n- <img src=\"badges/18-pages-gray.svg\" alt=\"18-pages\" align=\"top\"> [Neural Programmer: Inducing Latent Programs with Gradient Descent](https://arxiv.org/abs/1511.04834) - Neelakantan, Arvind, Quoc V. Le, and Ilya Sutskever, ICLR 2015.\n- <img src=\"badges/25-pages-gray.svg\" alt=\"25-pages\" align=\"top\"> [Learning to Execute](https://arxiv.org/abs/1410.4615v3) - Wojciech Zaremba, Ilya Sutskever, 2015.\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets](https://arxiv.org/abs/1503.01007) - Joulin, Armand, and Tomas Mikolov, NIPS 2015.\n- <img src=\"badges/26-pages-gray.svg\" alt=\"26-pages\" align=\"top\"> [Neural Turing Machines](https://arxiv.org/abs/1410.5401) - Graves, Alex, Greg Wayne, and Ivo Danihelka, 2014.\n- <img src=\"badges/15-pages-gray.svg\" alt=\"15-pages\" align=\"top\"> [From Machine Learning to Machine Reasoning](https://arxiv.org/abs/1102.1808) - Bottou Leon, Journal of Machine Learning 2011.\n\n#### Embeddings in Software Engineering\n\n- <img src=\"badges/8-pages-gray.svg\" alt=\"8-pages\" align=\"top\"> [A Literature Study of Embeddings on Source Code](https://arxiv.org/abs/1904.03061) - Zimin Chen and Martin Monperrus, 2019.\n- <img src=\"badges/3-pages-gray.svg\" alt=\"3-pages\" align=\"top\"> [AST-Based Deep Learning for Detecting Malicious PowerShell](https://arxiv.org/pdf/1810.09230.pdf) - Gili Rusak, Abdullah Al-Dujaili, Una-May O'Reilly, 2018.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Deep Code Search](https://dl.acm.org/citation.cfm?id=3180167) - Xiaodong Gu, Hongyu Zhang, Sunghun Kim, ICSE 2018.\n- <img src=\"badges/4-pages-gray.svg\" alt=\"4-pages\" align=\"top\"> [Word Embeddings for the Software Engineering Domain](https://github.com/vefstathiou/SO_word2vec/blob/master/MSR18-w2v.pdf) - Vasiliki Efstathiou, Christos Chatzilenas, Diomidis Spinellis, MSR 2018.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=top> [\n  Code Vectors: Understanding Programs Through Embedded Abstracted Symbolic Traces](https://arxiv.org/abs/1803.06686) - Jordan Henkel, Shuvendu K. Lahiri, Ben Liblit, Thomas Reps, FSE 2018.\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [Document Distance Estimation via Code Graph Embedding](https://www.researchgate.net/publication/320074701_Document_Distance_Estimation_via_Code_Graph_Embedding) - Zeqi Lin, Junfeng Zhao, Yanzhen Zou, Bing Xie, Internetware 2017.\n- <img src=\"badges/3-pages-gray.svg\" alt=\"3-pages\" align=\"top\"> [Combining Word2Vec with revised vector space model for better code retrieval](https://www.researchgate.net/publication/318123700_Combining_Word2Vec_with_Revised_Vector_Space_Model_for_Better_Code_Retrieval) - Thanh Van Nguyen, Anh Tuan Nguyen, Hung Dang Phan, Trong Duc Nguyen, Tien N. Nguyen, ICSE 2017.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [From word embeddings to document similarities for improved information retrieval in software engineering](https://www.researchgate.net/publication/296526040_From_Word_Embeddings_To_Document_Similarities_for_Improved_Information_Retrieval_in_Software_Engineering) - Xin Ye, Hui Shen, Xiao Ma, Razvan Bunescu, Chang Liu, ICSE 2016.\n- <img src=\"badges/3-pages-gray.svg\" alt=\"3-pages\" align=\"top\"> [Mapping API Elements for Code Migration with Vector Representation](https://dl.acm.org/citation.cfm?id=2892661) - Trong Duc Nguyen, Anh Tuan Nguyen, Tien N. Nguyen, ICSE 2016.\n\n#### Program Translation\n\n- <img src=\"badges/18-pages-gray.svg\" alt=\"18-pages\" align=\"top\"> [Towards Neural Decompilation](https://arxiv.org/abs/1905.08325v1) - Omer Katz, Yuval Olshaker, Yoav Goldberg, Eran Yahav, 2019.\n- <img src=\"badges/14-pages-gray.svg\" alt=\"14-pages\" align=\"top\"> [Tree-to-tree Neural Networks for Program Translation](https://arxiv.org/abs/1802.03691v1) - Xinyun Chen, Chang Liu, Dawn Song, ICLR 2018.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Code Attention: Translating Code to Comments by Exploiting Domain Features](https://arxiv.org/abs/1709.07642v2) - Wenhao Zheng, Hong-Yu Zhou, Ming Li, Jianxin Wu, 2017.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Automatically Generating Commit Messages from Diffs using Neural Machine Translation](https://arxiv.org/abs/1708.09492v1) - Siyuan Jiang, Ameer Armaly, Collin McMillan, ASE 2017.\n- <img src=\"badges/5-pages-gray.svg\" alt=\"5-pages\" align=\"top\"> [A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation](https://arxiv.org/abs/1707.02275v1) - Antonio Valerio Miceli Barone, Rico Sennrich, ICNLP 2017.\n- <img src=\"badges/6-pages-gray.svg\" alt=\"6-pages\" align=\"top\"> [A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes](https://arxiv.org/abs/1704.04856v1) - Pablo Loyola, Edison Marrese-Taylor, Yutaka Matsuo, ACL 2017.\n\n#### Code Suggestion and Completion\n\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Aroma: Code Recommendation via Structural Code Search](https://arxiv.org/abs/1812.01158) - Sifei Luan, Di Yang, Koushik Sen and Satish Chandra, 2019.\n- <img src=\"badges/9-pages-gray.svg\" alt=\"9-pages\" align=\"top\"> [Intelligent Code Reviews Using Deep Learning](https://www.kdd.org/kdd2018/files/deep-learning-day/DLDay18_paper_40.pdf) - Anshul Gupta, Neel Sundaresan, KDD DL Day 2018.\n- <img src=\"badges/8-pages-gray.svg\" alt=\"8-pages\" align=\"top\"> [Code Completion with Neural Attention and Pointer Networks](https://arxiv.org/abs/1711.09573v1) - Jian Li, Yue Wang, Irwin King, Michael R. Lyu, 2017.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [Learning Python Code Suggestion with a Sparse Pointer Network](https://arxiv.org/abs/1611.08307) - Avishkar Bhoopchand, Tim Rockt\u00e4schel, Earl Barr, Sebastian Riedel, 2016.\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [Code Completion with Statistical Language Models](http://www.cs.technion.ac.il/~yahave/papers/pldi14-statistical.pdf) - Veselin Raychev, Martin Vechev, Eran Yahav, PLDI 2014.\n\n#### Program Repair and Bug Detection\n\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [SampleFix: Learning to Correct Programs by Sampling Diverse Fixes](https://arxiv.org/abs/1906.10502) - Hossein Hajipour, Apratim Bhattacharya, Mario Fritz, 2019.\n- <img src=\"badges/15-pages-gray.svg\" alt=\"15-pages\" align=\"top\"> [Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection](https://openreview.net/forum?id=ByloIiCqYQ) - Tue Le, Tuan Nguyen, Trung Le, Dinh Phung, Paul Montague, Olivier De Vel, Lizhen Qu, ICLR 2019.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Neural Program Repair by Jointly Learning to Localize and Repair](https://openreview.net/forum?id=ByloJ20qtm) - Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, Rishabh Singh, ICLR 2019.\n- <img src=\"badges/11-pages-beginner-brightgreen.svg\" alt=\"11-pages\" align=\"top\"> [Compiler Fuzzing through Deep Learning](https://chriscummins.cc/pub/2018-issta.pdf) - Chris Cummins, Pavlos Petoumenos, Alastair Murray, Hugh Leather, ISSTA 2018\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [Automatically assessing vulnerabilities discovered by compositional analysis](https://dl.acm.org/citation.cfm?id=3243130) - Saahil Ognawala, Ricardo Nales Amato, Alexander Pretschner and Pooja Kulkarni, MASES 2018.\n- <img src=\"badges/6-pages-gray.svg\" alt=\"6-pages\" align=\"top\"> [An Empirical Investigation into Learning Bug-Fixing Patches in the Wild via Neural Machine Translation](http://www.cs.wm.edu/~denys/pubs/ASE%2718-Learning-Bug-Fixes-NMT.pdf) - Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, Denys Poshyvanyk, ASE 2018.\n- <img src=\"badges/23-pages-gray.svg\" alt=\"23-pages\" align=\"top\"> [DeepBugs: A Learning Approach to Name-based Bug Detection](https://arxiv.org/pdf/1805.11683.pdf) - Michael Pradel, Koushik Sen, 2018.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Learning How to Mutate Source Code from Bug-Fixes](https://arxiv.org/abs/1812.10772) - Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, Denys Poshyvanyk, 2018.\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [A deep tree-based model for software defect prediction](https://arxiv.org/abs/1802.00921) - HK Dam, T Pham, SW Ng, [T Tran](https://truyentran.github.io), J Grundy, A Ghose, T Kim, CJ Kim, 2018.\n- <img src=\"badges/7-pages-gray.svg\" alt=\"7-pages\" align=\"top\"> [Automated Vulnerability Detection in Source Code Using Deep Representation Learning](https://arxiv.org/abs/1807.04320) - Rebecca L. Russell, Louis Kim, Lei H. Hamilton, Tomo Lazovich, Jacob A. Harer, Onur Ozdemir, Paul M. Ellingwood, Marc W. McConley, 2018.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Shaping Program Repair Space with Existing Patches and Similar Code](https://xiongyingfei.github.io/papers/ISSTA18a.pdf) - Jiajun Jiang, Yingfei Xiong, Hongyu Zhang, Qing Gao, Xiangqun Chen, 2018. ([code](https://github.com/xgdsmileboy/SimFix)).\n- <img src=\"badges/15-pages-gray.svg\" alt=\"15-pages\" align=\"top\"> [Learning to Repair Software Vulnerabilities with Generative Adversarial Networks](https://arxiv.org/abs/1805.07475) - Jacob A. Harer, Onur Ozdemir, Tomo Lazovich, Christopher P. Reale, Rebecca L. Russell, Louis Y. Kim, Peter Chin, 2018.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [Dynamic Neural Program Embedding for Program Repair](https://arxiv.org/abs/1711.07163v2) - Ke Wang, Rishabh Singh, Zhendong Su, ICLR 2018.\n- <img src=\"badges/8-pages-gray.svg\" alt=\"8-pages\" align=\"top\"> [Estimating defectiveness of source code: A predictive model using GitHub content](https://arxiv.org/abs/1803.07764) - Ritu Kapur, Balwinder Sodhi, 2018\n- <img src=\"badges/8-pages-gray.svg\" alt=\"8-pages\" align=\"top\"> [Automated software vulnerability detection with machine learning](https://arxiv.org/abs/1803.04497) - Jacob A. Harer, Louis Y. Kim, Rebecca L. Russell, Onur Ozdemir, Leonard R. Kosta, Akshay Rangamani, Lei H. Hamilton, Gabriel I. Centeno, Jonathan R. Key, Paul M. Ellingwood, Marc W. McConley, Jeffrey M. Opper, Peter Chin, Tomo Lazovich, IWSPA 2018.\n- <img src=\"badges/34-pages-gray.svg\" alt=\"34-pages\" align=\"top\"> [Learning a Static Analyzer from Data](https://arxiv.org/abs/1611.01752) - Pavol Bielik, Veselin Raychev, Martin Vechev, CAV 2017. [video](https://www.youtube.com/watch?v=bkieI3jLxVY).\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [To Type or Not to Type: Quantifying Detectable Bugs in JavaScript](http://earlbarr.com/publications/typestudy.pdf) - Zheng Gao, Christian Bird, Earl Barr, ICSE 2017.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Sorting and Transforming Program Repair Ingredients via Deep Learning Code Similarities](https://arxiv.org/abs/1707.04742) - Martin White, Michele Tufano, Mat\u00edas Mart\u00ednez, Martin Monperrus, Denys Poshyvanyk, 2017.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [Semantic Code Repair using Neuro-Symbolic Transformation Networks](https://arxiv.org/abs/1710.11054v1) - Jacob Devlin, Jonathan Uesato, Rishabh Singh, Pushmeet Kohli, 2017.\n- <img src=\"badges/6-pages-gray.svg\" alt=\"6-pages\" align=\"top\"> [Automated Identification of Security Issues from Commit Messages and Bug Reports](http://asankhaya.github.io/pdf/automated-identification-of-security-issues-from-commit-messages-and-bug-reports.pdf) - Yaqin Zhou and Asankhaya Sharma, FSE 2017.\n- <img src=\"badges/31-pages-gray.svg\" alt=\"31-pages\" align=\"top\"> [SmartPaste: Learning to Adapt Source Code](https://arxiv.org/abs/1705.07867) - Miltiadis Allamanis, Marc Brockschmidt, 2017.\n- <img src=\"badges/7-pages-gray.svg\" alt=\"7-pages\" align=\"top\"> [End-to-End Prediction of Buffer Overruns from Raw Source Code via Neural Memory Networks](https://arxiv.org/abs/1703.02458v1) - Min-je Choi, Sehun Jeong, Hakjoo Oh, Jaegul Choo, IJCAI 2017.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [Tailored Mutants Fit Bugs Better](https://arxiv.org/abs/1611.02516) - Miltiadis Allamanis, Earl T. Barr, Ren\u00e9 Just, Charles Sutton, 2016.\n\n#### APIs and Code Mining\n\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [SAR: Learning Cross-Language API Mappings with Little Knowledge](https://bdqnghi.github.io/files/FSE_2019.pdf) - Nghi D. Q. Bui, Yijun Yu, Lingxiao Jiang, FSE 2019.\n- <img src=\"badges/4-pages-gray.svg\" alt=\"4-pages\" align=\"top\"> [Hierarchical Learning of Cross-Language Mappings through Distributed Vector Representations for Code](https://arxiv.org/abs/1803.04715) - Nghi D. Q. Bui, Lingxiao Jiang, ICSE 2018.\n- <img src=\"badges/7-pages-gray.svg\" alt=\"7-pages\" align=\"top\"> [DeepAM: Migrate APIs with Multi-modal Sequence to Sequence Learning](https://arxiv.org/abs/1704.07734v1) - Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, Sunghun Kim, IJCAI 2017.\n- <img src=\"badges/9-pages-gray.svg\" alt=\"9-pages\" align=\"top\"> [Mining Change Histories for Unknown Systematic Edits](http://soft.vub.ac.be/Publications/2017/vub-soft-tr-17-04.pdf) - Tim Molderez, Reinout Stevens, Coen De Roover, MSR 2017.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Deep API Learning](https://arxiv.org/abs/1605.08535v3) - Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, Sunghun Kim, FSE 2016.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [Exploring API Embedding for API Usages and Applications](http://home.eng.iastate.edu/~trong/projects/jv2cs/) - Nguyen, Nguyen, Phan and Nguyen, Journal of Systems and Software 2017.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [API usage pattern recommendation for software development](http://www.sciencedirect.com/science/article/pii/S0164121216301200) - Haoran Niu, Iman Keivanloo, Ying Zou, 2017.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Parameter-Free Probabilistic API Mining across GitHub](http://homepages.inf.ed.ac.uk/csutton/publications/fse2016.pdf) - Jaroslav Fowkes, Charles Sutton, FSE 2016.\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [A Subsequence Interleaving Model for Sequential Pattern Mining](http://homepages.inf.ed.ac.uk/csutton/publications/kdd2016-subsequence-interleaving.pdf) - Jaroslav Fowkes, Charles Sutton, KDD 2016.\n- <img src=\"badges/4-pages-gray.svg\" alt=\"4-pages\" align=\"top\"> [Lean GHTorrent: GitHub data on demand](https://bvasiles.github.io/papers/lean-ghtorrent.pdf) - Georgios Gousios, Bogdan Vasilescu, Alexander Serebrenik, Andy Zaidman, MSR 2014.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Mining idioms from source code](http://homepages.inf.ed.ac.uk/csutton/publications/idioms.pdf) - Miltiadis Allamanis, Charles Sutton, FSE 2014.\n- <img src=\"badges/4-pages-gray.svg\" alt=\"4-pages\" align=\"top\"> [The GHTorent Dataset and Tool Suite](http://www.gousios.gr/pub/ghtorrent-dataset-toolsuite.pdf) - Georgios Gousios, MSR 2013.\n\n#### Code Optimization\n\n- <img src=\"badges/27-pages-gray.svg\" alt=\"27-pages\" align=\"top\"> [The Case for Learned Index Structures](https://arxiv.org/abs/1712.01208v2) - Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, Neoklis Polyzotis, SIGMOD 2018.\n- <img src=\"badges/14-pages-gray.svg\" alt=\"14-pages\" align=\"top\"> [End-to-end Deep Learning of Optimization Heuristics](https://chriscummins.cc/pub/2017-pact.pdf) - Chris Cummins, Pavlos Petoumenos, Zheng Wang, Hugh Leather, PACT 2017\n- <img src=\"badges/14-pages-gray.svg\" alt=\"14-pages\" align=\"top\"> [Learning to superoptimize programs](https://arxiv.org/abs/1611.01787v3) - Rudy Bunel, Alban Desmaison, M. Pawan Kumar, Philip H.S. Torr, Pushmeet Kohlim ICLR 2017.\n- <img src=\"badges/18-pages-gray.svg\" alt=\"18-pages\" align=\"top\"> [Neural Nets Can Learn Function Type Signatures From Binaries](https://www.usenix.org/system/files/conference/usenixsecurity17/sec17-chua.pdf) - Zheng Leong Chua, Shiqi Shen, Prateek Saxena, and Zhenkai Liang, USENIX Security Symposium 2017.\n- <img src=\"badges/25-pages-gray.svg\" alt=\"25-pages\" align=\"top\"> [Adaptive Neural Compilation](https://arxiv.org/abs/1605.07969v2) - Rudy Bunel, Alban Desmaison, Pushmeet Kohli, Philip H.S. Torr, M. Pawan Kumar, NIPS 2016.\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [Learning to Superoptimize Programs - Workshop Version](https://arxiv.org/abs/1612.01094) - Bunel, Rudy, Alban Desmaison, M. Pawan Kumar, Philip H. S. Torr, and Pushmeet Kohli, NIPS 2016.\n\n#### Topic Modeling\n\n- <img src=\"badges/9-pages-gray.svg\" alt=\"9-pages\" align=\"top\"> [A Language-Agnostic Model for Semantic Source Code Labeling](https://dl.acm.org/citation.cfm?id=3243132) - Ben Gelman, Bryan Hoyle, Jessica Moore, Joshua Saxe and David Slater, MASES 2018.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [Topic modeling of public repositories at scale using names in source code](https://arxiv.org/abs/1704.00135) - Vadim Markovtsev, Eiso Kant, 2017.\n- <img src=\"badges/4-pages-gray.svg\" alt=\"4-pages\" align=\"top\"> [Why, When, and What: Analyzing Stack Overflow Questions by Topic, Type, and Code](http://homepages.inf.ed.ac.uk/csutton/publications/msrCh2013.pdf) - Miltiadis Allamanis, Charles Sutton, MSR 2013.\n- <img src=\"badges/30-pages-gray.svg\" alt=\"30-pages\" align=\"top\"> [Semantic clustering: Identifying topics in source code](http://scg.unibe.ch/archive/drafts/Kuhn06bSemanticClustering.pdf) - Adrian Kuhn, St\u00e9phane Ducasse, Tudor Girba, Information & Software Technology 2007.\n\n#### Sentiment Analysis\n\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [A Benchmark Study on Sentiment Analysis for Software Engineering Research](https://arxiv.org/abs/1803.06525) - Nicole Novielli, Daniela Girardi, Filippo Lanubile, MSR 2018.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [Sentiment Analysis for Software Engineering: How Far Can We Go?](http://www.inf.usi.ch/phd/lin/downloads/Lin2018a.pdf) - Bin Lin, Fiorella Zampetti, Gabriele Bavota, Massimiliano Di Penta, Michele Lanza, Rocco Oliveto, ICSE 2018.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Leveraging Automated Sentiment Analysis in Software Engineering](http://cs.uno.edu/~zibran/resources/MyPapers/SentiStrengthSE_2017.pdf) - Md Rakibul Islam, Minhaz F. Zibran, MSR 2017.\n- <img src=\"badges/27-pages-gray.svg\" alt=\"27-pages\" align=\"top\"> [Sentiment Polarity Detection for Software Development](https://arxiv.org/pdf/1709.02984.pdf) - Fabio Calefato, Filippo Lanubile, Federico Maiorano, Nicole Novielli, Empirical Software Engineering 2017.\n- <img src=\"badges/6-pages-gray.svg\" alt=\"6-pages\" align=\"top\"> [SentiCR: A Customized Sentiment Analysis Tool for Code Review Interactions](https://drive.google.com/file/d/0Byog0ILN8S1haGxpT3hvSzZxdms/view) - Toufique Ahmed, Amiangshu Bosu, Anindya Iqbal, Shahram Rahimi, ASE 2017.\n\n#### Code Summarization\n\n- <img src=\"badges/7-pages-gray.svg\" alt=\"7-pages\" align=\"top\"> [Summarizing Source Code with Transferred API Knowledge](https://xin-xia.github.io/publication/ijcai18.pdf) - Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, Zhi Jin, IJCAI 2018.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [Deep Code Comment Generation](https://xin-xia.github.io/publication/icpc182.pdf) - Xing Hu, Ge Li, Xin Xia, David Lo, Zhi Jin, ICPC 2018.\n- <img src=\"badges/6-pages-gray.svg\" alt=\"6-pages\" align=\"top\"> [A Neural Framework for Retrieval and Summarization of Source Code](https://dl.acm.org/citation.cfm?id=3240471) - Qingying Chen, Minghui Zhou, ASE 2018.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Improving Automatic Source Code Summarization via Deep Reinforcement Learning](https://arxiv.org/abs/1811.07234) - Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu and Philip S. Yu, ASE 2018.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [A Convolutional Attention Network for Extreme Summarization of Source Code](https://arxiv.org/abs/1602.03001) - Miltiadis Allamanis, Hao Peng, Charles Sutton, ICML 2016.\n- <img src=\"badges/4-pages-gray.svg\" alt=\"4-pages\" align=\"top\"> [TASSAL: Autofolding for Source Code Summarization](http://homepages.inf.ed.ac.uk/csutton/publications/icse2016-demo.pdf) - Jaroslav Fowkes, Pankajan Chanthirasegaran, Razvan Ranca, Miltiadis Allamanis, Mirella Lapata, Charles Sutton, ICSE 2016.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [Summarizing Source Code using a Neural Attention Model](https://github.com/sriniiyer/codenn/blob/master/summarizing_source_code.pdf) - Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer, ACL 2016.\n- <img src=\"badges/13-pages-gray.svg\" alt=\"13-pages\" align=\"top\"> [Automatic Generation of Pull Request Descriptions](https://arxiv.org/abs/1909.06987) - Zhongxin Liu, Xin Xia, Christoph Treude, David Lo, Shanping Li, ASE 2019.\n\n#### Clone Detection\n\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [Learning-Based Recursive Aggregation of Abstract Syntax Trees for Code Clone Detection](https://pvs.ifi.uni-heidelberg.de/fileadmin/papers/2019/Buech-Andrzejak-SANER2019.pdf) - Lutz B\u00fcch and Artur Andrzejak, SANER 2019.\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [Oreo: detection of clones in the twilight zone](https://dl.acm.org/citation.cfm?id=3236026) - Vaibhav Saini, Farima Farmahinifarahani, Yadong Lu, Pierre Baldi, and Cristina V. Lopes, FSE 2018.\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [A Deep Learning Approach to Program Similarity](https://dl.acm.org/citation.cfm?id=3243131) - Niccol\u00f2 Marastoni, Roberto Giacobazzi and Mila Dalla Preda, MASES 2018.\n- <img src=\"badges/6-pages-gray.svg\" alt=\"6-pages\" align=\"top\"> [Recurrent Neural Network for Code Clone Detection](https://seim-conf.org/media/materials/2018/proceedings/SEIM-2018_Short_Papers.pdf#page=48) - Arseny Zorin and Vladimir Itsykson, SEIM 2018.\n- <img src=\"badges/8-pages-gray.svg\" alt=\"8-pages\" align=\"top\"> [The Adverse Effects of Code Duplication in Machine Learning Models of Code](https://arxiv.org/abs/1812.06469) - Miltiadis Allamanis, 2018.\n- <img src=\"badges/28-pages-gray.svg\" alt=\"28-pages\" align=\"top\"> [D\u00e9j\u00e0Vu: a map of code duplicates on GitHub](http://janvitek.org/pubs/oopsla17b.pdf) - Cristina V. Lopes, Petr Maj, Pedro Martins, Vaibhav Saini, Di Yang, Jakub Zitny, Hitesh Sajnani, Jan Vitek, Programming Languages OOPSLA 2017.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [Some from Here, Some from There: Cross-project Code Reuse in GitHub](http://web.cs.ucdavis.edu/~filkov/papers/clones.pdf) - Mohammad Gharehyazie, Baishakhi Ray, Vladimir Filkov, MSR 2017.\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [Deep Learning Code Fragments for Code Clone Detection](http://www.cs.wm.edu/~denys/pubs/ASE%2716-DeepLearningClones.pdf) - Martin White, Michele Tufano, Christopher Vendome, and Denys Poshyvanyk, ASE 2016.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [A study of repetitiveness of code changes in software evolution](https://lib.dr.iastate.edu/cgi/viewcontent.cgi?referer=https://scholar.google.com/&httpsredir=1&article=1016&context=cs_conf) - HA Nguyen, AT Nguyen, TT Nguyen, TN Nguyen, H Rajan, ASE 2013.\n\n#### Differentiable Interpreters\n\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer](https://arxiv.org/abs/1803.11361v1) - Joseph Suarez, Justin Johnson, Fei-Fei Li, 2018.\n- <img src=\"badges/16-pages-gray.svg\" alt=\"16-pages\" align=\"top\"> [Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction](https://arxiv.org/abs/1802.02696v1) - Da Xiao, Jo-Yu Liao, Xingyuan Yuan, ICLR 2018.\n- <img src=\"badges/10-pages-gray.svg\" alt=\"10-pages\" align=\"top\"> [Differentiable Programs with Neural Libraries](https://arxiv.org/abs/1611.02109v2) - Alexander L. Gaunt, Marc Brockschmidt, Nate Kushman, Daniel Tarlow, ICML 2017.\n- <img src=\"badges/15-pages-gray.svg\" alt=\"15-pages\" align=\"top\"> [Differentiable Functional Program Interpreters](https://arxiv.org/abs/1611.01988v2) - John K. Feser, Marc Brockschmidt, Alexander L. Gaunt, Daniel Tarlow, 2017.\n- <img src=\"badges/18-pages-gray.svg\" alt=\"18-pages\" align=\"top\"> [Programming with a Differentiable Forth Interpreter](https://arxiv.org/abs/1605.06640) - Bo\u0161njak, Matko, Tim Rockt\u00e4schel, Jason Naradowsky, and Sebastian Riedel, ICML 2017.\n- <img src=\"badges/15-pages-gray.svg\" alt=\"15-pages\" align=\"top\"> [Neural Functional Programming](https://arxiv.org/abs/1611.01988v1) - Feser John K., Marc Brockschmidt, Alexander L. Gaunt, and Daniel Tarlow, ICLR 2017.\n- <img src=\"badges/7-pages-gray.svg\" alt=\"7-pages\" align=\"top\"> [TerpreT: A Probabilistic Programming Language for Program Induction](https://arxiv.org/abs/1612.00817) - Gaunt, Alexander L., Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan Taylor, and Daniel Tarlow, NIPS 2016.\n\n<a name=\"related-research\"></a>\n\n<details>\n<summary>Related research</summary>\n\n#### AST Differencing\n\n- <img src=\"badges/12-pages-gray.svg\" alt=\"12-pages\" align=\"top\"> [ClDiff: Generating Concise Linked Code Differences](https://chenbihuan.github.io/paper/ase18-huang-cldiff.pdf) - Kaifeng Huang, Bihuan Chen, Xin Peng, Daihong Zhou, Ying Wang, Yang Liu, Wenyun Zhao, ASE 2018. [Code](https://github.com/FudanSELab/CLDIFF).\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [Generating Accurate and Compact Edit Scripts Using Tree Differencing](http://www.xifiggam.eu/wp-content/uploads/2018/08/GeneratingAccurateandCompactEditScriptsusingTreeDifferencing.pdf) - Veit Frick, Thomas Grassauer, Fabian Beck, Martin Pinzger, ICSME 2018.\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [Fine-grained and Accurate Source Code Differencing](https://hal.archives-ouvertes.fr/hal-01054552/document) - Jean-R\u00e9my Falleri, Flor\u00e9al Morandat, Xavier Blanc, Matias Martinez, Martin Monperrus, ASE 2014.\n\n#### Binary Data Modeling\n\n- [Clustering Binary Data with Bernoulli Mixture Models](https://nsgrantham.com/documents/clustering-binary-data.pdf) - Neal S. Grantham.\n- [A Family of Blockwise One-Factor Distributions for Modelling High-Dimensional Binary Data](https://arxiv.org/pdf/1511.01343.pdf) - Matthieu Marbac and Mohammed Sedki, Computational Statistics & Data Analysis 2017.\n- [BayesBinMix: an R Package for Model Based Clustering of Multivariate Binary Data](https://arxiv.org/pdf/1609.06960.pdf) - Panagiotis Papastamoulis and Magnus Rattray, R Journal 2016.\n\n#### Soft Clustering Using T-mixture Models\n\n- [Robust mixture modelling using the t distribution](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.218.7334&rep=rep1&type=pdf) - D. Peel and G. J. McLachlan, Statistics and Computing 2000.\n- [Robust mixture modeling using the skew t distribution](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1030.9865&rep=rep1&type=pdf) - Tsung I. Lin, Jack C. Lee and Wan J. Hsieh, Statistics and Computing 2010.\n\n#### Natural Language Parsing and Comprehension\n\n- <img src=\"badges/11-pages-gray.svg\" alt=\"11-pages\" align=\"top\"> [A Fast Unified Model for Parsing and Sentence Understanding](https://arxiv.org/abs/1603.06021) - Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D. Manning, Christopher Potts, ACL 2016.\n\n</details>\n\n## Posts\n\n- [Semantic Code Search](https://towardsdatascience.com/semantic-code-search-3cd6d244a39c)\n- [Learning from Source Code](https://www.microsoft.com/en-us/research/blog/learning-source-code/)\n- [Training a Model to Summarize Github Issues](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)\n- [Sequence Intent Classification Using Hierarchical Attention Networks](https://www.microsoft.com/developerblog/2018/03/06/sequence-intent-classification/)\n- [Syntax-Directed Variational Autoencoder for Structured Data](https://mlatgt.blog/2018/02/08/syntax-directed-variational-autoencoder-for-structured-data/)\n- [Weighted MinHash on GPU helps to find duplicate GitHub repositories.](https://blog.sourced.tech//post/minhashcuda/)\n- [Source Code Identifier Embeddings](https://blog.sourced.tech/post/id2vec/)\n- [Using recurrent neural networks to predict next tokens in the java solutions](https://codeforces.com/blog/entry/52327)\n- [The half-life of code & the ship of Theseus](https://erikbern.com/2016/12/05/the-half-life-of-code.html)\n- [The eigenvector of \"Why we moved from language X to language Y\"](https://erikbern.com/2017/03/15/the-eigenvector-of-why-we-moved-from-language-x-to-language-y.html)\n- [Analyzing Github, How Developers Change Programming Languages Over Time](https://blog.sourced.tech/post/language_migrations/)\n- [Topic Modeling of GitHub Repositories](https://blog.sourced.tech//post/github_topic_modeling/)\n- [Aroma: Using machine learning for code recommendation](https://ai.facebook.com/blog/aroma-ml-for-code-recommendation/)\n\n## Talks\n\n- [Machine Learning on Source Code](http://vmarkovtsev.github.io/pydays-2018-vienna/)\n- [Similarity of GitHub Repositories by Source Code Identifiers](http://vmarkovtsev.github.io/techtalks-2017-moscow/)\n- [Using deep RNN to model source code](http://vmarkovtsev.github.io/re-work-2016-london/)\n- [Source code abstracts classification using CNN (1)](http://vmarkovtsev.github.io/re-work-2016-berlin/)\n- [Source code abstracts classification using CNN (2)](http://vmarkovtsev.github.io/data-natives-2016/)\n- [Source code abstracts classification using CNN (3)](http://vmarkovtsev.github.io/slush-2016/)\n- [Embedding the GitHub contribution graph](https://egorbu.github.io/techtalks-2017-moscow)\n- [Measuring code sentiment in a Git repository](http://vmarkovtsev.github.io/gophercon-2018-moscow/)\n\n## Software\n\n#### Machine Learning\n\n- [Differentiable Neural Computer (DNC)](https://github.com/deepmind/dnc) - TensorFlow implementation of the Differentiable Neural Computer.\n- [sourced.ml](https://github.com/src-d/ml) - Abstracts feature extraction from source code syntax trees and working with ML models.\n- [vecino](https://github.com/src-d/vecino) - Finds similar Git repositories.\n- [apollo](https://github.com/src-d/apollo) - Source code deduplication as scale, research.\n- [gemini](https://github.com/src-d/gemini) - Source code deduplication as scale, production.\n- [enry](https://github.com/src-d/enry) - Insanely fast file based programming language detector.\n- [hercules](https://github.com/src-d/hercules) - Git repository mining framework with batteries on top of go-git.\n- [DeepCS](https://github.com/guxd/deep-code-search) - Keras and Pytorch implementations of DeepCS (Deep Code Search).\n- [Code Neuron](https://github.com/vmarkovtsev/codeneuron) - Recurrent neural network to detect code blocks in natural language text.\n- [Naturalize](https://github.com/mast-group/naturalize) - Language agnostic framework for learning coding conventions from a codebase and then expoiting this information for suggesting better identifier names and formatting changes in the code.\n- [Extreme Source Code Summarization](https://github.com/mast-group/convolutional-attention) - Convolutional attention neural network that learns to summarize source code into a short method name-like summary by just looking at the source code tokens.\n- [Summarizing Source Code using a Neural Attention Model](https://github.com/sriniiyer/codenn) - CODE-NN, uses LSTM networks with attention to produce sentences that describe C# code snippets and SQL queries from StackOverflow. Torch over C#/SQL\n- [Probabilistic API Miner](https://github.com/mast-group/api-mining) - Near parameter-free probabilistic algorithm for mining the most interesting API patterns from a list of API call sequences.\n- [Interesting Sequence Miner](https://github.com/mast-group/sequence-mining) - Novel algorithm that mines the most interesting sequences under a probabilistic model. It is able to efficiently infer interesting sequences directly from the database.\n- [TASSAL](https://github.com/mast-group/tassal) - Tool for the automatic summarization of source code using autofolding. Autofolding automatically creates a summary of a source code file by folding non-essential code and comment blocks.\n- [JNice2Predict](http://www.nice2predict.org/) - Efficient and scalable open-source framework for structured prediction, enabling one to build new statistical engines more quickly.\n- [Clone Digger](http://clonedigger.sourceforge.net/download.html) - clone detection for Python and Java.\n- [Sensibility](https://github.com/naturalness/sensibility) - Uses LSTMs to detect and correct syntax errors in Java source code.\n- [DeepBugs](https://github.com/michaelpradel/DeepBugs) - Framework for learning bug detectors from an existing code corpus.\n- [DeepSim](https://github.com/parasol-aser/deepsim) - a deep learning-based approach to measure code functional similarity.\n- [rnn-autocomplete](https://github.com/ZeRoGerc/rnn-autocomplete) - Neural code autocompletion with RNN (bachelor's thesis).\n- [MindsDB](https://github.com/mindsdb/mindsdb) - MindsDB is an Explainable AutoML framework for developers. With MindsDB you can build, train and use state of the art ML models in as simple as one line of code.\n\n#### Utilities\n\n- [go-git](https://github.com/src-d/go-git) - Highly extensible Git implementation in pure Go which is friendly to data mining.\n- [bblfsh](https://github.com/bblfsh) - Self-hosted server for source code parsing.\n- [engine](https://github.com/src-d/engine) - Scalable and distributed data retrieval pipeline for source code.\n- [minhashcuda](https://github.com/src-d/minhashcuda) - Weighted MinHash implementation on CUDA to efficiently find duplicates.\n- [kmcuda](https://github.com/src-d/kmcuda) - k-means on CUDA to cluster and to search for nearest neighbors in dense space.\n- [wmd-relax](https://github.com/src-d/wmd-relax) - Python package which finds nearest neighbors at Word Mover's Distance.\n- [Tregex, Tsurgeon and Semgrex](https://nlp.stanford.edu/software/tregex.shtml) - Tregex is a utility for matching patterns in trees, based on tree relationships and regular expression matches on nodes (the name is short for \"tree regular expressions\").\n- [source{d} models](https://github.com/src-d/models) - Machine Learning models for MLonCode trained using the source{d} stack.\n\n#### Datasets\n\n- [Neural-Code-Search-Evaluation-Dataset](https://github.com/facebookresearch/Neural-Code-Search-Evaluation-Dataset) - dataset contains links to 4.7M methods from 24k+ repositories with 287 StackOverflow questions and code snippet answers.\n- [CodeSearchNet](https://github.com/github/CodeSearchNet) -  collection of datasets and benchmarks for code retrieval using natural language. Contains 2M pairs of (`comment`, `code`).\n- [Public Git Archive](https://github.com/src-d/datasets/tree/master/PublicGitArchive) - 6 TB of Git repositories from GitHub.\n- [StackOverflow Question-Code Dataset](https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset) - ~148K Python and ~120K SQL question-code pairs mined from StackOverflow.\n- [GitHub Issue Titles and Descriptions for NLP Analysis](https://www.kaggle.com/davidshinn/github-issues/) - ~8 million GitHub issue titles and descriptions from 2017.\n- [GitHub repositories - languages distribution](https://data.world/source-d/github-repositories-languages-distribution) - Programming languages distribution in 14,000,000 repositories on GitHub (October 2016).\n- [452M commits on GitHub](https://data.world/vmarkovtsev/452-m-commits-on-github) - \u2248 452M commits' metadata from 16M repositories on GitHub (October 2016).\n- [GitHub readme files](https://data.world/vmarkovtsev/github-readme-files) - Readme files of all GitHub repositories (16M) (October 2016).\n- [from language X to Y](https://data.world/vmarkovtsev/from-language-x-to-y) - Cache file Erik Bernhardsson collected for his awesome blog post.\n- [GitHub word2vec 120k](https://data.world/vmarkovtsev/github-word-2-vec-120-k) - Sequences of identifiers extracted from top starred 120,000 GitHub repositories.\n- [GitHub Source Code Names](https://data.world/vmarkovtsev/github-source-code-names) - Names in source code extracted from 13M GitHub repositories, not people.\n- [GitHub duplicate repositories](https://data.world/vmarkovtsev/github-duplicate-repositories) - GitHub repositories not marked as forks but very similar to each other.\n- [GitHub lng keyword frequencies](https://data.world/vmarkovtsev/github-lng-keyword-frequencies) - Programming language keyword frequency extracted from 16M GitHub repositories.\n- [GitHub Java Corpus](http://groups.inf.ed.ac.uk/cup/javaGithub/) - GitHub Java corpus is a set of Java projects collected from GitHub that we have used in a number of our publications. The corpus consists of 14,785 projects and 352,312,696 LOC.\n- [150k Python Dataset](https://www.sri.inf.ethz.ch/py150) - Dataset consisting of 150,000 Python ASTs.\n- [150k JavaScript Dataset](https://www.sri.inf.ethz.ch/js150) - Dataset consisting of 150,000 JavaScript files and their parsed ASTs.\n- [card2code](https://github.com/deepmind/card2code) - This dataset contains the language to code datasets described in the paper [Latent Predictor Networks for Code Generation](#card2code).\n- [NL2Bash](https://github.com/TellinaTool/nl2bash) - This dataset contains a set of ~10,000 bash one-liners collected from websites such as StackOverflow and their English descriptions written by Bash programmers, as described in the [paper](https://arxiv.org/abs/1802.08979).\n- [GitHub JavaScript Dump October 2016](https://archive.org/details/javascript-sources-oct2016.sqlite3) - Dataset consisting of 494,352 syntactically-valid JavaScript files obtained from the top ~10000 starred JavaScript repositories on GitHub, with licenses, and parsed ASTs.\n- [BigCloneBench](https://jeffsvajlenko.weebly.com/bigcloneeval.html) - Clone detection benchmark of 8 million function clone pairs in the IJaDataset.\n\n## Credits\n\n- A lot of references and articles were taken from [mast-group](https://mast-group.github.io/).\n- Inspired by [Awesome Machine Learning](https://github.com/josephmisiti/awesome-machine-learning).\n\n## Contributions\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md). TL;DR: create a [pull request](https://github.com/src-d/awesome-machine-learning-on-source-code/pulls) which is [signed off](https://github.com/src-d/awesome-machine-learning-on-source-code/blob/master/CONTRIBUTING.md#certificate-of-origin).\n\n## License\n\n[![License: CC BY-SA 4.0](badges/License-CC-BY--SA-4.0-lightgrey.svg)](https://creativecommons.org/licenses/by-sa/4.0/)\n"
 },
 {
  "repo": "igrigorik/decisiontree",
  "language": "Ruby",
  "readme_contents": "# Decision Tree\n\nA Ruby library which implements [ID3 (information gain)](https://en.wikipedia.org/wiki/ID3_algorithm) algorithm for decision tree learning. Currently, continuous and discrete datasets can be learned.\n\n- Discrete model assumes unique labels & can be graphed and converted into a png for visual analysis\n- Continuous looks at all possible values for a variable and iteratively chooses the best threshold between all possible assignments. This results in a binary tree which is partitioned by the threshold at every step. (e.g. temperate > 20C)\n\n## Features\n- ID3 algorithms for continuous and discrete cases, with support for inconsistent datasets.\n- [Graphviz component](http://rockit.sourceforge.net/subprojects/graphr/) to visualize the learned tree \n- Support for multiple, and symbolic outputs and graphing of continuous trees.\n- Returns default value when no branches are suitable for input\n\n## Implementation\n\n- Ruleset is a class that trains an ID3Tree with 2/3 of the training data, converts it into set of rules and prunes the rules with the remaining 1/3 of the training data (in a [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm) way).\n- Bagging is a bagging-based trainer (quite obvious), which trains 10 Ruleset trainers and when predicting chooses the best output based on voting.\n\n[Blog post with explanation & examples](http://www.igvita.com/2007/04/16/decision-tree-learning-in-ruby/)\n\n## Example\n\n```ruby\nrequire 'decisiontree'\n\nattributes = ['Temperature']\ntraining = [\n  [36.6, 'healthy'],\n  [37, 'sick'],\n  [38, 'sick'],\n  [36.7, 'healthy'],\n  [40, 'sick'],\n  [50, 'really sick'],\n]\n\n# Instantiate the tree, and train it based on the data (set default to '1')\ndec_tree = DecisionTree::ID3Tree.new(attributes, training, 'sick', :continuous)\ndec_tree.train\n\ntest = [37, 'sick']\ndecision = dec_tree.predict(test)\nputs \"Predicted: #{decision} ... True decision: #{test.last}\"\n\n# => Predicted: sick ... True decision: sick\n\n# Specify type (\"discrete\" or \"continuous\") in the training data\nlabels = [\"hunger\", \"color\"]\ntraining = [\n        [8, \"red\", \"angry\"],\n        [6, \"red\", \"angry\"],\n        [7, \"red\", \"angry\"],\n        [7, \"blue\", \"not angry\"],\n        [2, \"red\", \"not angry\"],\n        [3, \"blue\", \"not angry\"],\n        [2, \"blue\", \"not angry\"],\n        [1, \"red\", \"not angry\"]\n]\n\ndec_tree = DecisionTree::ID3Tree.new(labels, training, \"not angry\", color: :discrete, hunger: :continuous)\ndec_tree.train\n\ntest = [7, \"red\", \"angry\"]\ndecision = dec_tree.predict(test)\nputs \"Predicted: #{decision} ... True decision: #{test.last}\"\n\n# => Predicted: angry ... True decision: angry\n```\n\n## License\n\nThe [MIT License](https://opensource.org/licenses/MIT) - Copyright (c) 2006 Ilya Grigorik\n"
 },
 {
  "repo": "keon/awesome-nlp",
  "language": null,
  "readme_contents": "# \u4ee4\u4eba\u8b9a\u5606\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406 [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\n> \u5c08\u9580\u7528\u65bc\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684\u7cbe\u9078\u8cc7\u6e90\u5217\u8868\n\n![Awesome NLP Logo](/images/logo.jpg)\n\n> * \u539f\u6587\u5730\u5740\uff1a[\u4ee4\u4eba\u8b9a\u5606\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406](https://github.com/keon/awesome-nlp)\n> * \u539f\u6587\u4f5c\u8005\uff1a[Keon](https://github.com/keon), [Martin](https://github.com/outpark), [Nirant](https://github.com/NirantK), [Dhruv](https://github.com/the-ethan-hunt)\n> * \u7ffb\u8b6f\uff1a[NeroCube](https://github.com/NeroCube)\n\n_\u8acb\u5728\u63d0\u4ea4\u4e4b\u524d\u95b1\u8b80 [\u8ca2\u737b\u6307\u5357](contributing.md) \u3002\u8acb\u96a8\u6642\u5275\u5efa [\u62c9\u53d6\u8acb\u6c42](https://github.com/keonkim/awesome-nlp/pulls)._\n\n## \u5167\u5bb9\n\n* [\u7814\u7a76\u6458\u8981\u548c\u8da8\u52e2](#\u7814\u7a76\u6458\u8981\u548c\u8da8\u52e2)\n* [\u6559\u5b78](#\u6559\u5b78)\n  * [\u95b1\u8b80\u5167\u5bb9](#\u95b1\u8b80\u5167\u5bb9)\n  * [\u5f71\u7247\u548c\u8ab2\u7a0b](#\u5f71\u7247\u548c\u8ab2\u7a0b)\n  * [\u66f8\u7c4d](#\u66f8\u7c4d)\n* [\u51fd\u5f0f\u5eab](#\u51fd\u5f0f\u5eab)\n  * [Node.js](#user-content-node-js)\n  * [Python](#user-content-python)\n  * [C++](#user-content-c++)\n  * [Java](#user-content-java)\n  * [Kotlin](#user-content-kotlin)\n  * [Scala](#user-content-scala)\n  * [R](#user-content-r)\n  * [Clojure](#user-content-clojure)\n  * [Ruby](#user-content-ruby)\n  * [Rust](#user-content-rust)\n* [\u670d\u52d9](#\u670d\u52d9)\n* [\u8a3b\u91cb\u5de5\u5177](#\u8a3b\u91cb\u5de5\u5177)\n* [\u8cc7\u6599\u96c6](#\u8cc7\u6599\u96c6)\n* [\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u97d3\u6587](#\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u97d3\u6587)\n* [\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u963f\u62c9\u4f2f\u8a9e](#\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u963f\u62c9\u4f2f\u8a9e)\n* [\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u4e2d\u6587](#\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u4e2d\u6587)\n* [\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u5fb7\u6587](#\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u5fb7\u6587)\n* [\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u897f\u73ed\u7259\u8a9e](#\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u897f\u73ed\u7259\u8a9e)\n* [\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u5370\u5ea6\u8a9e](#\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u5370\u5ea6\u8a9e)\n* [\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u6cf0\u8a9e](#\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u6cf0\u8a9e)\n* [\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u4e39\u9ea5\u8a9e](#\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u4e39\u9ea5\u8a9e)\n* [\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u8d8a\u5357\u8a9e](#\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u8d8a\u5357\u8a9e)\n* [\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u5370\u5ea6\u5c3c\u897f\u4e9e](#\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u5370\u5ea6\u5c3c\u897f\u4e9e)\n* [\u5176\u4ed6\u8a9e\u8a00](#\u5176\u4ed6\u8a9e\u8a00)\n* [\u8ca2\u737b](#\u8ca2\u737b)\n\n## \u7814\u7a76\u6458\u8981\u548c\u8da8\u52e2\n\n* [\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u6982\u8ff0](https://nlpoverview.com/) \u662f\u61c9\u7528\u65bc\u81ea\u7136\u8a9e\u8a00\u6df1\u5ea6\u5b78\u7fd2\u6280\u8853\u7684\u6700\u65b0\u6982\u8ff0\uff0c\u5305\u62ec\u7406\u8ad6\uff0c\u5be6\u73fe\uff0c\u61c9\u7528\u548c\u6700\u5148\u9032\u7684\u7d50\u679c\u3002\u5c0d\u65bc\u7814\u7a76\u4eba\u54e1\u4f86\u8aaa\uff0c\u9019\u662f\u4e00\u500b\u5049\u5927\u7684Deep NLP\u7c21\u4ecb\u3002 \n* [\u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u9032\u5c55](https://nlpprogress.com/) \u8ffd\u96a8\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684\u9032\u5c55\uff0c\u5305\u62ec\u8cc7\u6599\u96c6\u548c\u5e38\u898b\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u7684\u7576\u524d\u6700\u65b0\u6280\u8853\u3002\n* [\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684 ImageNet \u6642\u523b\u5df2\u7d93\u5230\u4f86](https://thegradient.pub/nlp-imagenet/)\n* [ACL 2018 \u4eae\u9ede: \u5728\u66f4\u5177\u6311\u6230\u6027\u7684\u8a2d\u7f6e\u4e2d\u7406\u89e3\u8868\u793a\u548c\u8a55\u4f30](http://ruder.io/acl-2018-highlights/)\n* [ACL 2017 \u7684\u56db\u500b\u6df1\u5ea6\u5b78\u7fd2\u8da8\u52e2\u3002\u7b2c\u4e00\u90e8\u5206\uff1a\u8a9e\u8a00\u7d50\u69cb\u548c\u8a5e\u8a9e\u5d4c\u5165](https://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-1.html)\n* [ACL 2017 \u7684\u56db\u500b\u6df1\u5ea6\u5b78\u7fd2\u8da8\u52e2\u3002\u7b2c\u4e8c\u90e8\u5206\uff1a\u53ef\u89e3\u91cb\u6027\u548c\u6ce8\u610f\u529b](https://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-2.html)\n* [2017 \u5e74 EMNLP \u7684\u4eae\u9ede\uff1a\u6fc0\u52d5\u4eba\u5fc3\u7684\u8cc7\u6599\u96c6\uff0c\u96c6\u7fa4\u7684\u56de\u6b78\u8207\u5176\u4ed6\u66f4\u591a\uff01](http://blog.aylien.com/highlights-emnlp-2017-exciting-datasets-return-clusters/)\n* [\u6df1\u5ea6\u5b78\u7fd2\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP): \u9032\u5c55\u8207\u8da8\u52e2](https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=The%20Wild%20Week%20in%20AI)\n* [\u81ea\u7136\u8a9e\u8a00\u751f\u6210\u7684\u73fe\u72c0\u8abf\u67e5](https://arxiv.org/abs/1703.09902)\n\n## \u6559\u5b78\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n### \u95b1\u8b80\u5167\u5bb9\n\n\u901a\u7528\u6a5f\u5668\u5b78\u7fd2\n\n* \u4f86\u81ea Google \u9ad8\u7d1a\u5275\u610f\u5de5\u7a0b\u5e2b Jason \u7684[\u6a5f\u5668\u5b78\u7fd2 101](https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/edit?usp=sharing) \uff0c\u70ba\u5de5\u7a0b\u5e2b\u548c\u7ba1\u7406\u968e\u5c64\u89e3\u91cb\u6a5f\u5668\u5b78\u7fd2\u3002\n* a16z [AI \u5287\u672c](https://aiplaybook.a16z.com/) \u662f\u4e00\u500b\u5f88\u597d\u7684\u93c8\u63a5\uff0c\u53ef\u4ee5\u8f49\u767c\u7d66\u60a8\u7684\u7d93\u7406\u6216\u6f14\u793a\u5167\u5bb9\u3002\n* [\u7e7c\u5668\u5b78\u7fd2\u90e8\u843d\u683c](https://bmcfee.github.io/#home) by Brian McFee\n* [Ruder's \u90e8\u843d\u683c](http://ruder.io/#open) \u7531 [Sebastian Ruder](https://twitter.com/seb_ruder) \u9032\u884c\u8a55\u8ad6\u5f97\u6700\u597d\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7814\u7a76\u3002\n\n\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4ecb\u7d39\u8207\u6307\u5357\n\n* [\u7406\u89e3\u548c\u5be6\u65bd\u81ea\u7136\u8a9e\u8a00\u8655\u7406](https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/) \u7684\u7d42\u6975\u6307\u5357\u3002\n* [Hackernoon \u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7c21\u4ecb](https://hackernoon.com/learning-ai-if-you-suck-at-math-p7-the-magic-of-natural-language-processing-f3819a689386) \u9069\u7528\u65bc\u90a3\u4e9b\u8cde\u5473\u6578\u5b78\u7684\u4eba-\u7528\u4ed6\u5011\u81ea\u5df1\u7684\u8a71\u4f86\u8aaa\u3002\n* [Vik Paruchari \u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u6559\u5b78](http://www.vikparuchuri.com/blog/natural-language-processing-tutorial/)\n* [\u81ea\u7136\u8a9e\u8a00\u8655\u7406: \u4e00\u4efd\u7c21\u4ecb](https://academic.oup.com/jamia/article/18/5/544/829676) \u4f86\u81ea\u725b\u6d25\u5927\u5b78\u3002\n* [\u4f7f\u7528 Pytorch \u9032\u884c\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684\u6df1\u5ea6\u5b78\u7fd2](https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html)\n* [\u52d5\u624b\u505a NLTK \u6559\u5b78](https://github.com/hb20007/hands-on-nltk-tutorial) - \u4ee5  \n Jupyter \u7b46\u8a18\u672c\u5f62\u5f0f\u7684\u5be6\u8e10 NLTK \u6559\u5b78\u3002\n\n\u90e8\u843d\u683c\u8207\u7c21\u5831\n\n* \u90e8\u843d\u683c: [\u6df1\u5ea6\u5b78\u7fd2, \u81ea\u7136\u8a9e\u8a00\u8655\u7406, \u8207\u5448\u73fe\u6cd5](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\n* \u90e8\u843d\u683c: [\u5716\u89e3 BERT, ELMo, \u8207 co. (\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u662f\u5982\u4f55\u7834\u89e3\u9077\u79fb\u5b78\u7fd2\u7684)](https://jalammar.github.io/illustrated-bert/) \u8207 [\u5716\u89e3\u8f49\u63db\u5668](https://jalammar.github.io/illustrated-transformer/)\n* \u90e8\u843d\u683c: Hal Daum\u00e9 III \u7684[\u81ea\u7136\u8a9e\u8a00\u8655\u7406](https://nlpers.blogspot.com/)\n* [Radim \u0158eh\u016f\u0159ek \u7684\u6559\u5b78](https://radimrehurek.com/gensim/tutorial.html) \u4f7f\u7528 Python \u8207 [gensim](https://radimrehurek.com/gensim/index.html) \u8655\u7406\u8a9e\u8a00\u8a9e\u6599\u5eab\u3002\n* [arXiv: \u81ea\u7136\u8a9e\u8a00\u8655\u7406 (\u5927\u90e8\u5206) \u4f86\u81ea Scratch](https://arxiv.org/pdf/1103.0398.pdf)\n* [Karpathy \u7684\u905e\u6b78\u795e\u7d93\u7db2\u7d61\u7684\u4e0d\u5408\u7406\u6709\u6548\u6027](https://karpathy.github.io/2015/05/21/rnn-effectiveness)\n\n### \u5f71\u7247\u548c\u8ab2\u7a0b\n\n#### \u6df1\u5ea6\u5b78\u7fd2\u8207\u81ea\u7136\u8a9e\u8a00\u8655\u7406\n\n\u7528\u65bc\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684\u8a5e\u5d4c\u5165, \u905e\u6b78\u795e\u7d93\u7db2\u7d61, \u9577\u77ed\u671f\u8a18\u61b6\u795e\u7d93\u7db2\u7d61\u8207\u5377\u7a4d\u795e\u7d93\u7db2\u8def | [\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n* Udacity \u7684[\u4eba\u5de5\u667a\u6167\u5165\u9580](https://www.udacity.com/course/intro-to-artificial-intelligence--cs271) \u8ab2\u7a0b\u6d89\u53ca\u5230\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u3002\n* Udacity \u7684[\u6df1\u5ea6\u5b78\u7fd2](https://udacity.com/course/deep-learning--ud730) \u4f7f\u7528Tensorflow \u4f7f\u7528\u6df1\u5ea6\u5b78\u7fd2\u7684 NLP \u4efb\u52d9\u7684\u90e8\u5206\uff08\u5305\u62ec Word2Vec\uff0cRNN\u7684 \u548c LSTMs\uff09\u3002\n* \u725b\u6d25\u5927\u5b78\u7684[\u6df1\u5ea6\u81ea\u7136\u8a9e\u8a00\u8655\u7406](https://github.com/oxford-cs-deepnlp-2017/lectures)\u6709\u5f71\u7247\uff0c\u6f14\u8b1b\u6295\u5f71\u7247\u548c\u95b1\u8b80\u7d20\u6750\u3002\n* \u65af\u5766\u798f\u5927\u5b78\u7684[\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u6df1\u5ea6\u5b78\u7fd2 (cs224-n)](https://web.stanford.edu/class/cs224n/) \u7531 Richard Socher \u548c Christopher Manning \u5b8c\u6210\u3002\n* Coursera \u7684[\u81ea\u7136\u8a9e\u8a00\u8655\u7406](https://www.coursera.org/learn/language-processing) \u7531\u570b\u7acb\u7814\u7a76\u5927\u5b78\u9ad8\u7b49\u7d93\u6fdf\u5b78\u9662\u5b8c\u6210\u3002\n* \u5361\u5167\u57fa\u6885\u9686\u5927\u5b78\u7684\u8a9e\u8a00\u6280\u8853\u7814\u7a76\u6240[\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684\u795e\u7d93\u7db2\u8def](http://phontron.com/class/nn4nlp2017/)\u3002\n\n#### \u7d93\u5178\u81ea\u7136\u8a9e\u8a00\u8655\u7406\n\n\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684\u8c9d\u8449\u65af\uff0c\u7d71\u8a08\u548c\u8a9e\u8a00\u5b78\u65b9\u6cd5| | [\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n* [\u7d71\u8a08\u6a5f\u5668\u7ffb\u8b6f](http://mt-class.org) - \u6a5f\u5668\u7ffb\u8b6f\u8ab2\u7a0b\uff0c\u5177\u6709\u5f88\u68d2\u7684\u4f5c\u696d\u548c\u6295\u5f71\u7247\u3002\n* [\u4f7f\u7528 Python 3 \u9032\u884c NLTK \u81ea\u7136\u8a9e\u8a00\u8655\u7406](https://www.youtube.com/playlist?list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL) \u7531 Harrison Kinsley(sentdex) \u4f7f\u7528 NLTK \u7a0b\u5f0f\u78bc\u5be6\u73fe\u7684\u597d\u6559\u5b78\u3002\n* \u7531 Jordan Boyd-Graber \u5728\u99ac\u91cc\u862d\u5927\u5b78\u7684[\u8a08\u7b97\u8a9e\u8a00\u5b78 I](https://www.youtube.com/playlist?list=PLegWUnz91WfuPebLI97-WueAP90JO-15i)\u8b1b\u5ea7\u3002\n* \u7531 Yandex \u6578\u64da\u5b78\u9662\u7684[\u6df1\u5ea6\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u8ab2\u7a0b](https://github.com/yandexdataschool/nlp_course)\u6db5\u84cb\u5f9e\u6587\u672c\u5d4c\u5165\u5230\u6a5f\u5668\u7ffb\u8b6f\u7684\u91cd\u8981\u601d\u60f3\uff0c\u5305\u62ec\u5e8f\u5217\u5efa\u6a21\uff0c\u8a9e\u8a00\u6a21\u578b\u7b49\u3002\n\n### \u66f8\u7c4d\n\n* Dan Jurafsy \u6559\u6388\u7684[\u8a9e\u97f3\u548c\u8a9e\u8a00\u8655\u7406](https://web.stanford.edu/~jurafsky/slp3/)\n* [R \u4e2d\u7684\u6587\u5b57\u63a2\u52d8](https://www.tidytextmining.com)\n* [Python \u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406](https://www.nltk.org/book/)\n\n## \u51fd\u5f0f\u5eab\n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n* <a id=\"node-js\">**Node.js and Javascript** - \u7528\u65bc\u81ea\u7136\u8a9e\u8a00\u7684 Node.js \u51fd\u5f0f\u5eab</a> | [\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n  * [Twitter-text](https://github.com/twitter/twitter-text) - \u4f7f\u7528 JavaScript \u5be6\u73fe\u7684 Twitter \u6587\u672c\u8655\u7406\u5eab\u3002\n  * [Knwl.js](https://github.com/benhmoore/Knwl.js) - JS\u4e2d\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u5668\u3002\n  * [Retext](https://github.com/retextjs/retext) - \u7528\u65bc\u5206\u6790\u548c\u64cd\u7e31\u81ea\u7136\u8a9e\u8a00\u7684\u53ef\u200b\u200b\u64f4\u5c55\u7cfb\u7d71\u3002\n  * [NLP Compromise](https://github.com/spencermountain/compromise) - \u700f\u89bd\u5668\u4e2d\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u3002\n  * [Natural](https://github.com/NaturalNode/natural) - \u7bc0\u9ede\u7684\u4e00\u822c\u81ea\u7136\u8a9e\u8a00\u8a2d\u65bd\u3002\n  - [Poplar](https://github.com/synyi/poplar) - \u4e00\u7a2e\u57fa\u65bc Web \u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u8a3b\u91cb\u5de5\u5177\uff08NLP\uff09\u3002\n\n* <a id=\"python\"> **Python** - \u7528\u65bc\u81ea\u7136\u8a9e\u8a00\u7684 Python \u51fd\u5f0f\u5eab</a> | [\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n  * [TextBlob](http://textblob.readthedocs.org/) - \u70ba\u5c08\u7814\u5e38\u898b\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\uff08NLP\uff09\u4efb\u52d9\u63d0\u4f9b\u4e00\u81f4\u7684 API\u3002 \u7ad9\u5728[\u81ea\u7136\u8a9e\u8a00\u5de5\u5177\u5305 (NLTK)](https://www.nltk.org/) \u548c [\u6a21\u5f0f](https://github.com/clips/pattern)\u8180\u4e0a\uff0c\u4e26\u8207\u5169\u8005\u5f88\u597d\u5730\u914d\u5408 :+1:\n  * [spaCy](https://github.com/explosion/spaCy) - \u4f7f\u7528 Python \u8207 Cython \u7522\u696d\u5f37\u5ea6\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406  :+1:\n    * [textacy](https://github.com/chartbeat-labs/textacy) -  \u5728spaCy\u4e0a\u69cb\u5efa\u7684\u66f4\u9ad8\u7d1a\u5225\u7684\u81ea\u7136\u8207\u513c\u8655\u7406\u3002\n  * [gensim](https://radimrehurek.com/gensim/index.html) - \u7528\u65bc\u5f9e\u7d14\u6587\u672c\u9032\u884c\u7121\u76e3\u7763\u8a9e\u7fa9\u5efa\u6a21\u7684 \u51fd\u5f0f\u5eab :+1:\n  * [scattertext](https://github.com/JasonKessler/scattertext) - \u7528\u65bc\u751f\u6210\u8a9e\u6599\u5eab\u4e4b\u9593\u8a9e\u8a00\u5dee\u7570\u7684 d3 \u53ef\u8996\u5316\u7684 Python \u51fd\u5f0f\u5eab\u3002\n  * [AllenNLP](https://github.com/allenai/allennlp) - \u4e00\u500b\u67b6\u69cb\u5728 PyTorch \u4e0a\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u51fd\u5f0f\u5eab\uff0c\u7528\u65bc\u958b\u767c\u5404\u7a2e\u8a9e\u8a00\u4efb\u52d9\u6700\u5148\u9032\u7684\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\u3002\n  * [PyTorch-NLP](https://github.com/PetrochukM/PyTorch-NLP) - \u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7814\u7a76\u5de5\u5177\u5305\u8a2d\u8a08\u4f86\u652f\u63f4\u5feb\u901f\u5efa\u7acb\u66f4\u597d\u7684\u6578\u64da\u52a0\u8f09\u5668\uff0c\u8a5e\u5411\u91cf\u52a0\u8f09\u5668\uff0c\u795e\u7d93\u7db2\u8def\u5c64\u8868\u793a\uff0c\u5e38\u898b\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u6307\u6a19\uff08\u5982BLEU\uff09\u539f\u578b\u3002\n  * [Rosetta](https://github.com/columbia-applied-data-science/rosetta) - \u6587\u672c\u8655\u7406\u5de5\u5177\u548c\u5305\u88dd (\u4f8b\u5982\uff1a Vowpal Wabbit)\n  * [PyNLPl](https://github.com/proycon/pynlpl) - Python \u81ea\u7136\u8a9e\u8a00\u8655\u7406\u51fd\u5f0f\u5eab. \u9069\u7528\u65bc Python \u7684\u901a\u7528\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u51fd\u5f0f\u5eab\u3002 \u9084\u5305\u542b\u4e00\u4e9b\u7528\u65bc\u89e3\u6790\u5e38\u898b\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u683c\u5f0f\u7684\u7279\u5b9a\u6a21\u584a, \u6700\u5e38\u898b\u7684\u662f\u7528\u65bc [FoLiA](https://proycon.github.io/folia/)\uff0c\u9084\u5305\u62ec ARPA \u8a9e\u8a00\u6a21\u578b\uff0cMoses \u77ed\u8a9e\u8868\uff0cGIZA ++\u5c0d\u9f4a\u3002\n\n  * [jPTDP](https://github.com/datquocnguyen/jPTDP) - \u7528\u65bc\u806f\u5408\u8a5e\u6027\uff08POS\uff09\u6a19\u8a18\u548c\u4f9d\u8cf4\u6027\u89e3\u6790\u7684\u5de5\u5177\u5305\u3002jPTDP \u63d0\u4f9b40\u591a\u7a2e\u8a9e\u8a00\u7684\u9810\u8a13\u7df4\u6a21\u578b\u3002\n  * [BigARTM](https://github.com/bigartm/bigartm) - \u4e00\u500b\u7528\u65bc\u4e3b\u984c\u5efa\u6a21\u7684\u5feb\u901f\u51fd\u5f0f\u5eab\u3002\n  * [Snips NLU](https://github.com/snipsco/snips-nlu) - \u7528\u65bc\u610f\u5716\u89e3\u6790\u7684\u7522\u54c1\u5c31\u7dd2\u51fd\u5f0f\u5eab\u3002\n  * [Chazutsu](https://github.com/chakki-works/chazutsu) - \u7528\u65bc\u4e0b\u8f09\u548c\u89e3\u6790\u6a19\u6e96\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7814\u7a76\u6578\u64da\u96c6\u7684\u51fd\u5f0f\u5eab\u3002\n  * [Word Forms](https://github.com/gutfeeling/word_forms) - Word forms \u53ef\u4ee5\u6e96\u78ba\u751f\u6210\u6240\u6709\u53ef\u80fd\u7684\u82f1\u8a9e\u55ae\u8a5e\u5f62\u5f0f\u3002\n  * [Multilingual Latent Dirichlet Allocation (LDA)](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA) - \u4e00\u7a2e\u591a\u8a9e\u8a00\u548c\u53ef\u64f4\u5c55\u7684\u6587\u6a94\u805a\u985e\u7ba1\u9053\u3002\n  * [NLP Architect](https://github.com/NervanaSystems/nlp-architect) - \u7528\u65bc\u63a2\u7d22 NLP \u548c NLU \u6700\u5148\u9032\u7684\u6df1\u5ea6\u5b78\u7fd2\u62d3\u64b2\u548c\u6280\u8853\u7684\u51fd\u5f0f\u5eab\u3002\n  * [Flair](https://github.com/zalandoresearch/flair) - \u4e00\u500b\u975e\u5e38\u7c21\u55ae\u7684\u6846\u67b6\uff0c\u7528\u65bc\u5728 PyTorch \u4e0a\u69cb\u5efa\u6700\u5148\u9032\u7684\u591a\u8a9e\u8a00 NLP\u3002\u5305\u62ec BERT\uff0cELMo \u548c Flair \u5d4c\u5165\u3002\n  * [Kashgari](https://github.com/BrikerMan/Kashgari) - \u7c21\u55ae\u7684\uff0c\u57fa\u65bc Keras \u7684\u591a\u8a9e\u8a00\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u6846\u67b6\uff0c\u5141\u8a31\u60a8\u57285\u5206\u9418\u5167\u69cb\u5efa\u6a21\u578b\uff0c\u7528\u65bc\u547d\u540d\u5be6\u9ad4\u8b58\u5225\uff08NER\uff09\uff0c\u8a5e\u6027\u6a19\u8a3b\uff08PoS\uff09\u548c\u6587\u672c\u5206\u985e\u4efb\u52d9\u3002 \u5305\u62ec BERT \u548c word2vec \u5d4c\u5165\u3002\n\n\n* <a id=\"c++\">**C++** - C++ \u51fd\u5f0f\u5eab</a> | [\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n  * [MIT \u8cc7\u8a0a\u63d0\u53d6\u5de5\u5177\u5305 ](https://github.com/mit-nlp/MITIE) - \u7528\u65bc\u547d\u540d\u5be6\u9ad4\u8b58\u5225\u548c\u95dc\u4fc2\u63d0\u53d6\u7684 C\uff0cC++ \u548cPython \u5de5\u5177\u3002\n  * [CRF++](https://taku910.github.io/crfpp/) - \u689d\u4ef6\u96a8\u6a5f\u5834\uff08CRF\uff09\u7684\u958b\u6e90\u5c08\u6848\uff0c\u7528\u65bc\u5be6\u73fe\u5206\u5272/\u6a19\u8a18\u9806\u5e8f\u6578\u64da\u548c\u5176\u4ed6\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u3002\n  * [CRFsuite](http://www.chokkan.org/software/crfsuite/) - CRFsuite \u5be6\u73fe\u7528\u65bc\u6a19\u8a18\u9806\u5e8f\u6578\u64da\u7684\u689d\u4ef6\u96a8\u6a5f\u5b57\u6bb5\uff08CRF\uff09\u3002\n  * [BLLIP Parser](https://github.com/BLLIP/bllip-parser) - BLLIP \u81ea\u7136\u8a9e\u8a00\u89e3\u6790\u5668\uff08\u4e5f\u7a31\u70ba Charniak-Johnson \u89e3\u6790\u5668\uff09\n  * [colibri-core](https://github.com/proycon/colibri-core) - C++ \u51fd\u5f0f\u5eab\uff0c\u547d\u4ee4\u884c\u5de5\u5177\u548c Python \u7d81\u5b9a\u7528\u65bc\u5feb\u901f\u4e14\u5167\u5b58\u6709\u6548\u7684\u65b9\u5f0f\u63d0\u53d6\u548c\u4f7f\u7528\u57fa\u672c\u8a9e\u8a00\u7d50\u69cb\uff0c\u5982 n-gram \u548c skipgrams\u3002\n  * [ucto](https://github.com/LanguageMachines/ucto) - \u9069\u7528\u65bc\u5404\u7a2e\u8a9e\u8a00\u7684\u57fa\u65bc Unicode \u7684\u5e38\u898f\u8868\u9054\u5f0f\u6a19\u8a18\u751f\u6210\u5668\u3002\u5de5\u5177\u548c C++\u51fd\u5f0f\u5eab\u3002\u652f\u6301 FoLiA \u683c\u5f0f\u3002\n  * [libfolia](https://github.com/LanguageMachines/libfolia) - \u7528\u65bc [FoLiA \u683c\u5f0f](https://proycon.github.io/folia/)\u7684 C++ \u51fd\u5f0f\u5eab\u3002\n  * [frog](https://github.com/LanguageMachines/frog) - \u70ba\u8377\u862d\u8a9e\u958b\u767c\u7684\u57fa\u65bc\u5167\u5b58\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u5957\u4ef6\uff1aPoS \u6a19\u8a18\u5668\uff0clemmatiser\uff0c\u4f9d\u8cf4\u89e3\u6790\u5668\uff0cNER\uff0c\u6dfa\u5c64\u89e3\u6790\u5668\uff0c\u5f62\u614b\u5206\u6790\u5668\u3002\n  * [MeTA](https://github.com/meta-toolkit/meta) - [MeTA : ModErn Text Analysis](https://meta-toolkit.org/) \u662f\u4e00\u500b C++ \u6578\u64da\u79d1\u5b78\u5de5\u5177\u5305\uff0c\u53ef\u4ee5\u5e6b\u52a9\u6316\u6398\u5927\u6587\u672c\u6578\u64da\u3002\n  * [Mecab (\u65e5\u6587)](https://taku910.github.io/mecab/)\n  * [Moses](http://statmt.org/moses/)\n  * [StarSpace](https://github.com/facebookresearch/StarSpace) - \u4e00\u500b\u4f86\u81ea Facebook \u7684\u51fd\u5f0f\u5eab\u7528\u65bc\u5275\u5efa\u55ae\u8a5e\u7d1a\uff0c\u6bb5\u7d1a\uff0c\u6587\u6a94\u7d1a\u548c\u6587\u672c\u5206\u985e\u7684\u5d4c\u5165\n\n* <a id=\"java\">**Java** - Java \u81ea\u7136\u8a9e\u8a00\u8655\u7406\u51fd\u5f0f\u5eab</a> | [\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n  * [\u65af\u5766\u798f\u5927\u5b78 NLP](https://nlp.stanford.edu/software/index.shtml)\n  * [OpenNLP](https://opennlp.apache.org/)\n  * [NLP4J](https://emorynlp.github.io/nlp4j/)\n  * [Java \u4e2d\u7684 Word2vec](https://deeplearning4j.org/docs/latest/deeplearning4j-nlp-word2vec)\n  * [ReVerb](https://github.com/knowitall/reverb/) Web-Scale \u958b\u653e\u4fe1\u606f\u63d0\u53d6\u3002\n  * [OpenRegex](https://github.com/knowitall/openregex) \u4e00\u7a2e\u9ad8\u6548\u9748\u6d3b\u7684\u57fa\u65bc token \u7684\u6b63\u5247\u8868\u9054\u5f0f\u8a9e\u8a00\u548c\u5f15\u64ce\u3002\n  * [CogcompNLP](https://github.com/CogComp/cogcomp-nlp) - \u5728\u4f0a\u5229\u8afe\u4f0a\u5927\u5b78\u7684\u8a8d\u77e5\u8a08\u7b97\u7d44\u958b\u767c\u7684\u6838\u5fc3\u51fd\u5f0f\u5eab\u3002\n  * [MALLET](http://mallet.cs.umass.edu/) - \u7528\u65bc LanguagE Toolkit \u7684\u6a5f\u5668\u5b78\u7fd2 - \u7528\u65bc\u7d71\u8a08\u81ea\u7136\u8a9e\u8a00\u8655\u7406\uff0c\u6587\u6a94\u5206\u985e\uff0c\u805a\u985e\uff0c\u4e3b\u984c\u5efa\u6a21\uff0c\u8cc7\u8a0a\u63d0\u53d6\u548c\u5176\u4ed6\u6a5f\u5668\u5b78\u7fd2\u61c9\u7528\u7a0b\u5e8f\u7684\u6587\u672c\u5305\u3002\n  * [RDRPOSTagger](https://github.com/datquocnguyen/RDRPOSTagger) - \u4e00\u500b\u7a69\u5065\u7684 POS \u6a19\u8a18\u5de5\u5177\u5305\uff08\u5305\u62ec Java \u548c Python\uff09\u4ee5\u53ca40\u591a\u7a2e\u8a9e\u8a00\u7684\u9810\u8a13\u7df4\u6a21\u578b\u3002\n  \n* <a id=\"kotlin\">**Kotlin** - Kotlin \u81ea\u7136\u8a9e\u8a00\u8655\u7406\u51fd\u5f0f\u5eab</a> | [\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n  * [Lingua](https://github.com/pemistahl/lingua/) \u9069\u7528\u65bc Kotlin \u548c Java \u7684\u8a9e\u8a00\u6aa2\u6e2c\u51fd\u5f0f\u5eab\uff0c\u9069\u7528\u65bc\u9577\u6587\u672c\u548c\u77ed\u6587\u672c\u3002\n  * [Kotidgy](https://github.com/meiblorn/kotidgy) \u2014 \u4e00\u7a2e\u7528 Kotlin \u7de8\u5beb\u57fa\u65bc\u7d22\u5f15\u7684\u6587\u672c\u6578\u64da\u751f\u6210\u5668\u3002\n  \n* <a id=\"scala\">**Scala** - Scala \u81ea\u7136\u8a9e\u8a00\u8655\u7406\u51fd\u5f0f\u5eab</a> | [\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n  * [Saul](https://github.com/CogComp/saul) - \u7528\u65bc\u958b\u767c\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7cfb\u7d71\u7684\u51fd\u5f0f\u5eab\uff0c\u5305\u62ec\u5167\u7f6e\u6a21\u584a\uff0c\u5982 SRL\uff0cPOS \u7b49\u3002\n  * [ATR4S](https://github.com/ispras/atr4s) - \u5177\u6709\u6700\u5148\u9032\u7684[\u81ea\u52d5\u8853\u8a9e\u8b58\u5225](https://en.wikipedia.org/wiki/Terminology_extraction)\u65b9\u6cd5\u7684\u5de5\u5177\u5305\u3002\n  * [tm](https://github.com/ispras/tm) - \u57fa\u65bc\u6b63\u5247\u5316\u591a\u8a9e\u8a00 [PLSA](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis) \u7684\u4e3b\u984c\u5efa\u6a21\u5be6\u73fe\u3002 \n  * [word2vec-scala](https://github.com/Refefer/word2vec-scala) - word2vec \u6a21\u578b\u7684 Scala \u63a5\u53e3; \u5305\u62ec\u5c0d\u8a5e\u8ddd\u96e2\u548c\u8a5e\u985e\u6bd4\u7b49\u5411\u91cf\u7684\u64cd\u4f5c\u3002\n  * [Epic](https://github.com/dlwh/epic) - Epic \u662f\u4e00\u500b\u7528 Scala \u7de8\u5beb\u7684\u9ad8\u6027\u80fd\u7d71\u8a08\u89e3\u6790\u5668\uff0c\u4ee5\u53ca\u7528\u65bc\u69cb\u5efa\u8907\u96dc\u7d50\u69cb\u5316\u9810\u6e2c\u6a21\u578b\u7684\u6846\u67b6\u3002\n\n* <a id=\"R\">**R** - R \u81ea\u7136\u8a9e\u8a00\u8655\u7406\u51fd\u5f0f\u5eab</a> | [\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n  * [text2vec](https://github.com/dselivanov/text2vec) -  R \u4e2d\u7684\u5feb\u901f\u77e2\u91cf\u5316\uff0c\u4e3b\u984c\u5efa\u6a21\uff0c\u8ddd\u96e2\u548c GloVe \u5b57\u5d4c\u5165\u3002\n  * [wordVectors](https://github.com/bmschmidt/wordVectors) - \u7528\u65bc\u5275\u5efa\u548c\u63a2\u7d22 word2vec \u548c\u5176\u4ed6\u55ae\u8a5e\u5d4c\u5165\u6a21\u578b\u7684 R \u5305\u3002\n  * [RMallet](https://github.com/mimno/RMallet) - \u8207 Java \u6a5f\u5668\u5b78\u7fd2\u5de5\u5177 MALLET \u63a5\u53e3\u7684 R \u5305\u3002\n  * [dfr-browser](https://github.com/agoldst/dfr-browser) -  \u5275\u5efa\u7528\u65bc\u5728 Web \u700f\u89bd\u5668\u4e2d\u700f\u89bd\u6587\u672c\u4e3b\u984c\u6a21\u578b\u7684 d3 \u53ef\u8996\u5316\u3002\n  * [dfrtopics](https://github.com/agoldst/dfrtopics) - \u7528\u65bc\u63a2\u7d22\u6587\u672c\u4e3b\u984c\u6a21\u578b\u7684 R \u5305\u3002\n  * [sentiment_classifier](https://github.com/kevincobain2000/sentiment_classifier) - \u4f7f\u7528Word Sense Disambiguation \u548c WordNet Reader \u7684\u60c5\u611f\u5206\u985e\u3002\n  * [jProcessing](https://github.com/kevincobain2000/jProcessing) - \u65e5\u672c\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u5eab\uff0c\u5177\u6709\u65e5\u8a9e\u60c5\u611f\u5206\u985e\u3002\n\n* <a id=\"clojure\">**Clojure**</a> | [\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n  * [Clojure-openNLP](https://github.com/dakrone/clojure-opennlp) - Clojure \u4e2d\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\uff08opennlp\uff09\u3002\n  * [Infections-clj](https://github.com/r0man/inflections-clj) - \u7528\u65bc Clojure \u548c ClojureScript \u7684\u985e\u4f3c Rails \u7684\u8b8a\u5f62\u51fd\u5f0f\u5eab\u3002\n  * [postagga](https://github.com/fekr/postagga) - \u7528\u65bc\u89e3\u6790 Clojure \u548c ClojureScript \u4e2d\u7684\u81ea\u7136\u8a9e\u8a00\u7684\u51fd\u5f0f\u5eab\u3002\n\n* <a id=\"ruby\">**Ruby**</a> | [\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n  * Kevin Dias \u7684 [\u81ea\u7136\u8a9e\u8a00\u8655\u7406\uff08NLP\uff09Ruby \u51fd\u5f0f\u5eab\uff0c\u5de5\u5177\u548c\u8edf\u4ef6\u7684\u96c6\u5408](https://github.com/diasks2/ruby-nlp)\n  * [Ruby \u4e2d\u5be6\u7528\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406](https://github.com/arbox/nlp-with-ruby)\n\n* <a id=\"rust\">**Rust**</a> | [\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n  * [whatlang](https://github.com/greyblake/whatlang-rs) \u2014 \u57fa\u65bc\u4e09\u5143\u7d44\u7684\u81ea\u7136\u8a9e\u8a00\u8b58\u5225\u51fd\u5f0f\u5eab\u3002\n  - [snips-nlu-rs](https://github.com/snipsco/snips-nlu-rs) - \u7528\u65bc\u610f\u5716\u89e3\u6790\u7684\u751f\u7522\u5c31\u7dd2\u7b49\u7d1a\u51fd\u793a\u5eab\u3002\n\n### \u670d\u52d9\n\n\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4f5c\u70ba\u5177\u6709\u66f4\u9ad8\u7d1a\u529f\u80fd\u7684 API\uff0c\u4f8b\u5982 NER\uff0c\u4e3b\u984c\u6a19\u8a18\u7b49 | [\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n- [Wit-ai](https://github.com/wit-ai/wit) - \u61c9\u7528\u7a0b\u5e8f\u548c\u8a2d\u5099\u7684\u81ea\u7136\u8a9e\u8a00\u754c\u9762\u3002\n- [IBM Watson \u7684\u81ea\u7136\u8a9e\u610f\u7406\u89e3](https://github.com/watson-developer-cloud/natural-language-understanding-nodejs) - API \u548c Github \u6f14\u793a\u3002\n- [Amazon \u7406\u89e3](https://aws.amazon.com/comprehend/) - NLP \u548c ML \u5957\u4ef6\u6db5\u84cb\u4e86\u6700\u5e38\u898b\u7684\u4efb\u52d9\uff0c\u5982 NER\uff0c\u6a19\u8a18\u548c\u60c5\u611f\u5206\u6790\u3002\n- [Google \u96f2\u7aef\u81ea\u7136\u8a9e\u8a00 API](https://cloud.google.com/natural-language/) - \u81f3\u5c119\u7a2e\u8a9e\u8a00\u7684\u8a9e\u6cd5\u5206\u6790\uff0cNER\uff0c\u60c5\u611f\u5206\u6790\u548c\u5167\u5bb9\u6a19\u8a18\u5305\u62ec\u82f1\u8a9e\u548c\u4e2d\u6587\uff08\u7c21\u9ad4\u548c\u7e41\u9ad4\uff09\u3002\n- [ParallelDots](https://www.paralleldots.com/text-analysis-apis) - \u9ad8\u5c64\u6b21\u6587\u672c\u5206\u6790 API \u670d\u52d9\uff0c\u5f9e\u60c5\u611f\u5206\u6790\u5230\u610f\u5716\u5206\u6790\u3002\n- [Microsoft \u8a8d\u77e5\u670d\u52d9](https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics/)\n- [TextRazor](https://www.textrazor.com/)\n- [Rosette](https://www.rosette.com/)\n- [Textalytic](https://www.textalytic.com) - \u700f\u89bd\u5668\u4e2d\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\uff0c\u5305\u62ec\u60c5\u611f\u5206\u6790\uff0c\u547d\u540d\u5be6\u9ad4\u63d0\u53d6\uff0cPOS\u6a19\u8a18\uff0c\u8a5e\u983b\uff0c\u4e3b\u984c\u5efa\u6a21\uff0c\u6587\u5b57\u96f2\u7b49\u3002\n\n### \u8a3b\u91cb\u5de5\u5177\n\n- [GATE](https://gate.ac.uk/overview.html) - \u901a\u7528\u67b6\u69cb\u548c\u6587\u672c\u5de5\u7a0b\u5df2\u670915\u5e74\u6b77\u53f2\uff0c\u514d\u8cbb\u958b\u6e90\u3002\n- [Anafora](https://github.com/weitechen/anafora) \u662f\u514d\u8cbb\u7684\u958b\u6e90\uff0c\u57fa\u65bc Web \u7684\u539f\u59cb\u6587\u672c\u8a3b\u91cb\u5de5\u5177\u3002\n- [brat](https://brat.nlplab.org/) - brat \u5feb\u901f\u8a3b\u89e3\u5de5\u5177\u662f\u4e00\u500b\u7528\u65bc\u5354\u4f5c\u6587\u672c\u8a3b\u91cb\u7684\u5728\u7dda\u74b0\u5883\u3002\n- [tagtog](https://www.tagtog.net/), \u9700\u82b1 $\u3002\n- [prodigy](https://prodi.gy/) \u662f\u4e00\u500b\u7531\u4e3b\u52d5\u5b78\u7fd2\u9a45\u52d5\u7684\u8a3b\u91cb\u5de5\u5177\uff0c\u9700\u82b1 $\u3002\n- [LightTag](https://lighttag.io) - \u70ba\u5718\u968a\u63d0\u4f9b\u8a17\u7ba1\u548c\u7ba1\u7406\u7684\u6587\u672c\u8a3b\u91cb\u5de5\u5177\uff0c\u9700\u82b1 $\u3002\n\n## \u6280\u8853\n\n### \u6587\u672c\u5d4c\u5165\n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n\u6587\u672c\u5d4c\u5165\u5141\u8a31\u6df1\u5ea6\u5b78\u7fd2\u5728\u8f03\u5c0f\u7684\u6578\u64da\u96c6\u4e0a\u6709\u6548\u3002\u9019\u4e9b\u901a\u5e38\u662f\u6df1\u5165\u5b78\u7fd2\u7684\u7b2c\u4e00\u6b65\u8f38\u5165\u548c\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4e2d\u6700\u6d41\u884c\u7684\u9077\u79fb\u5b78\u7fd2\u65b9\u5f0f\u3002\u5d4c\u5165\u53ea\u662f\u7c21\u55ae\u7684\u5411\u91cf\uff0c\u6bd4\u5be6\u969b\u503c\u7684\u5b57\u7b26\u4e32\u8868\u793a\u66f4\u70ba\u901a\u7528\u7684\u65b9\u5f0f\u3002Word\u5d4c\u5165\u88ab\u8a8d\u70ba\u662f\u5927\u591a\u6578\u6df1\u5ea6NLP\u4efb\u52d9\u7684\u4e00\u500b\u5f88\u597d\u7684\u8d77\u9ede\u3002\n\n\u55ae\u8a5e\u5d4c\u5165\u4e2d\u6700\u6d41\u884c\u7684\u540d\u5b57\u662f Google\uff08Mikolov\uff09\u7684 word2vec \u548c\u53f2\u4e39\u4f5b\u7684 PenVe\uff08Pennington\uff0cSocher \u548cManning\uff09\u3002fastText \u4f3c\u4e4e\u662f\u4e00\u7a2e\u975e\u5e38\u6d41\u884c\u7684\u591a\u8a9e\u8a00\u5b50\u8a5e\u5d4c\u5165\u3002\n\n#### \u8a5e\u5d4c\u5165\n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n|\u5d4c\u5165 |\u8ad6\u6587| \u7d44\u7e54| gensim - \u57f9\u8a13\u652f\u63f4 |\u90e8\u843d\u683c|\n|---|---|---|---|---|\n|word2vec|[\u5b98\u65b9\u5be6\u4f5c](https://code.google.com/archive/p/word2vec/), T.Mikolove et al. 2013. \u5206\u6563\u5f0f\u8a5e\u8a9e\u8868\u9054\u53ca\u5176\u7d44\u5408\u6027\u3002[pdf](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) |Google|\u662f :heavy_check_mark:|  colah \u5728[\u6df1\u5ea6\u5b78\u7fd2\uff0c\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u548c\u9673\u8ff0](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\u4e2d\u7684\u8996\u89ba\u6703\u89e3\u91cb; gensim \u7684[\u7406\u89e3 word2vec](https://rare-technologies.com/making-sense-of-word2vec) |\n|GloVe|Jeffrey Pennington, Richard Socher \u8207 Christopher D. Manning. 2014. GloVe: \u5168\u5c40\u5411\u91cf\u7684\u5b57\u8a5e\u8868\u793a [pdf](https://nlp.stanford.edu/pubs/glove.pdf)|\u53f2\u4e39\u4f5b|\u5426 :negative_squared_cross_mark:|acoyler \u7684 [GloVe \u65e9\u5831](https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/) |\n|fastText|[\u5b98\u65b9\u5be6\u4f5c](https://github.com/facebookresearch/fastText), T. Mikolov et al. 2017. \u4f7f\u7528\u5b50\u8a5e\u8cc7\u8a0a\u8c50\u5bcc\u55ae\u8a5e\u5411\u91cf\u3002 [pdf](https://arxiv.org/abs/1607.04606)|Facebook|\u662f :heavy_check_mark:|[Fasttext: \u6df1\u5165\u89e3\u6790](https://towardsdatascience.com/fasttext-under-the-hood-11efc57b2b3)|\n\n\u7d66\u521d\u5b78\u8005\u7684\u7b46\u8a18:\n\n- \u7d93\u9a57\u6cd5\u5247: **fastText >> GloVe > word2vec**\n- \u4f60\u53ef\u4ee5\u627e\u5230\u8a31\u591a\u8a9e\u8a00[\u9810\u8a13\u7df4 fasttext \u5411\u91cf](https://fasttext.cc/docs/en/pretrained-vectors.html)\u3002\n- \u5982\u679c\u4f60\u5c0d word2vec \u548c GloVe \u80cc\u5f8c\u7684\u908f\u8f2f\u548c\u76f4\u89ba\u611f\u8208\u8da3: [\u8a5e\u5411\u91cf\u7684\u9a5a\u4eba\u529b\u91cf](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)\u4e26\u5f88\u597d\u5730\u4ecb\u7d39\u9019\u4e9b\u4e3b\u984c\u3002\n- [arXiv: \u9ad8\u6548\u6587\u672c\u5206\u985e\u7684\u9326\u56ca\u5999\u65b9](https://arxiv.org/abs/1607.01759), \u8207 [arXiv: FastText.zip: \u58d3\u7e2e\u6587\u672c\u5206\u985e\u6a21\u578b](https://arxiv.org/abs/1612.03651) \u4f5c\u70ba fasttext \u7684\u4e00\u90e8\u5206\u767c\u5e03\u3002\n\n\n#### \u57fa\u65bc\u53e5\u5b50\u548c\u8a9e\u8a00\u6a21\u578b\u7684\u8a5e\u5d4c\u5165\n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n- _ElMo_ \u5f9e[\u6df1\u5ea6\u60c5\u5883\u8a5e\u8868\u793a](https://arxiv.org/abs/1802.05365) - [PyTorch \u5be6\u4f5c](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md) - [TF \u5be6\u4f5c](https://github.com/allenai/bilm-tf)\n- _ULimFit_ Jeremy Howard \u8207 Sebastian Ruder \u7684[\u901a\u7528\u8a9e\u8a00\u6a21\u578b\u9032\u884c\u6587\u672c\u5206\u985e\u5fae\u8abf](https://arxiv.org/abs/1801.06146)\n- _InferSent_ facebook \u7684 [\u81ea\u7136\u8a9e\u8a00\u63a8\u8ad6\u8cc7\u6599\u7684\u901a\u7528\u8a9e\u53e5\u8868\u793a\u76e3\u7763\u662f\u5b78\u7fd2](https://arxiv.org/abs/1705.02364)\n- _CoVe_ from [\u5728\u7ffb\u8b6f\u4e2d\u5b78\u7fd2: \u60c5\u5883\u8a5e\u76f8\u91cf](https://arxiv.org/abs/1708.00107)\n- _\u4f86\u81ea[\u6587\u4ef6\u8207\u53e5\u5b50\u7684\u5206\u6563\u5f0f\u8868\u9054](https://cs.stanford.edu/~quocle/paragraph_vector.pdf). \u53c3\u95b1 [gensim \u7684 doc2vec \u6559\u5b78](https://rare-technologies.com/doc2vec-tutorial/)\n- [sense2vec](https://arxiv.org/abs/1511.06388) - \u95dc\u65bc\u8a5e\u7fa9\u6d88\u6b67\u3002\n- [\u8df3\u904e\u601d\u8003\u8c61\u91cf](https://arxiv.org/abs/1506.06726) - \u55ae\u8a5e\u8868\u793a\u65b9\u6cd5\u3002\n- [\u81ea\u9069\u61c9 skip-gram](https://arxiv.org/abs/1502.07257) - \u985e\u4f3c\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u81ea\u9069\u61c9\u5c6c\u6027\u3002\n- [\u5e8f\u5217\u5230\u5e8f\u5217\u5b78\u7fd2](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) - \u6a5f\u5668\u7ffb\u8b6f\u7684\u8a5e\u5411\u91cf\u3002\n\n### \u56de\u7b54\u554f\u984c\u8207\u77e5\u8b58\u63d0\u53d6\n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n- Facebook \u900f\u904e\u7dad\u57fa\u767e\u79d1 [DrQA: \u6253\u958b\u9818\u57df\u70ba\u984c\u89e3\u7b54](https://github.com/facebookresearch/DrQA) \n- DocQA: AllenAI \u7684[\u7c21\u55ae\u800c\u6709\u6548\u7684\u591a\u6bb5\u95b1\u8b80\u7406\u89e3](https://github.com/allenai/document-qa)\n- [\u7528\u65bc\u81ea\u7136\u8a9e\u8a00\u554f\u7b54\u7684\u99ac\u723e\u53ef\u592b\u908f\u8f2f\u7db2\u7d61](https://arxiv.org/pdf/1507.03045v1.pdf)\n- [\u57fa\u65bc\u6a21\u677f\u7684\u8cc7\u8a0a\u63d0\u53d6\u6c92\u6709\u7528\u5230\u6a21\u677f](https://www.usna.edu/Users/cs/nchamber/pubs/acl2011-chambers-templates.pdf)\n- [\u77e9\u9663\u5206\u89e3\u8207\u901a\u7528\u6a21\u5f0f\u7684\u95dc\u4fc2\u63d0\u53d6](https://www.anthology.aclweb.org/N/N13/N13-1008.pdf)\n- [Privee\uff1a\u81ea\u52d5\u5206\u6790Web\u96b1\u79c1\u7b56\u7565\u7684\u9ad4\u7cfb\u7d50\u69cb](https://www.sebastianzimmeck.de/zimmeckAndBellovin2014Privee.pdf)\n- [\u6559\u5b78\u6a5f\u5668\u95b1\u8b80\u548c\u7406\u89e3](https://arxiv.org/abs/1506.03340) - DeepMind paper\n- [\u8d70\u5411\u5f62\u5f0f\u5206\u4f48\u8a9e\u7fa9\uff1a\u7528\u5f35\u91cf\u6a21\u64ec\u908f\u8f2f\u6f14\u7b97](https://www.aclweb.org/anthology/S13-1001)\n- [MLN \u6559\u5b78\u7684\u6f14\u793a\u6295\u5f71\u7247](https://github.com/clulab/nlp-reading-group/blob/master/fall-2015-resources/mln-summary-20150918.ppt)\n- [MLNs \u7684 QA \u61c9\u7528\u6f14\u793a\u6295\u5f71\u7247](https://github.com/clulab/nlp-reading-group/blob/master/fall-2015-resources/Markov%20Logic%20Networks%20for%20Natural%20Language%20Question%20Answering.pdf)\n- [\u6f14\u793a\u6295\u5f71\u7247](https://github.com/clulab/nlp-reading-group/blob/master/fall-2015-resources/poon-paper.pdf)\n\n## \u8cc7\u6599\u96c6\n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n- [nlp-datasets](https://github.com/niderhoff/nlp-datasets) \u5f88\u597d\u7684\u81ea\u7136\u8a9e\u8a00\u8cc7\u6599\u96c6\u96c6\u5408\n\n## \u591a\u8a9e\u8a00\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u6846\u67b6\n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n- [UDPipe](https://github.com/ufal/udpipe) \u662f\u4e00\u500b\u53ef\u8a13\u7df4\u7684\u7ba1\u9053\uff0c\u7528\u65bc\u6a19\u8a18\uff0c\u6a19\u8a18\uff0c\u89e3\u91cb\u548c\u89e3\u6790\u901a\u7528\u6a39\u5eab\u548c\u5176\u4ed6 CoNLL-U \u6587\u4ef6\u3002\u4e3b\u8981\u7528 C++ \u7de8\u5beb\uff0c\u70ba\u591a\u8a9e\u8a00NLP\u8655\u7406\u63d0\u4f9b\u5feb\u901f\u53ef\u9760\u7684\u89e3\u6c7a\u65b9\u6848\u3002\n- [NLP-Cube](https://github.com/adobe/NLP-Cube) : \u81ea\u7136\u8a9e\u8a00\u8655\u7406\u6d41\u6c34\u7dda - \u53e5\u5b50\u5206\u88c2\uff0c\u6a19\u8a18\u5316\uff0c\u8a5e\u5f62\u9084\u539f\uff0c\u8a5e\u6027\u6a19\u8a3b\u548c\u4f9d\u8cf4\u6027\u5206\u6790\u3002\u7528 Dynet 2.0 \u7528 Python \u7de8\u5beb\u7684\u65b0\u5e73\u53f0\u3002\u63d0\u4f9b\u7368\u7acb\uff08CLI / Python \u7d81\u5b9a\uff09\u548c\u670d\u52d9\u5668\u529f\u80fd\uff08REST API\uff09\u3002\n\n## \u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u97d3\u6587\n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n### \u51fd\u5f0f\u5eab\n\n- [KoNLPy](http://konlpy.org) - \u7528\u65bc\u97d3\u8a9e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684Python\u5305\u3002\n- [Mecab (Korean)](https://eunjeon.blogspot.com/) - \u97d3\u6587\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406 C++ \u51fd\u5f0f\u5eab\n- [KoalaNLP](https://koalanlp.github.io/koalanlp/) - \u97d3\u570b\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684 Scala \u51fd\u5f0f\u5eab\u3002\n- [KoNLP](https://cran.r-project.org/package=KoNLP) - \u97d3\u6587\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406 R \u5305\u3002\n\n### \u90e8\u843d\u683c\u8207\u6559\u5b78\n\n- [dsindex \u7684\u90e8\u843d\u683c](https://dsindex.github.io/)\n- [\u97d3\u570b\u6c5f\u539f\u5927\u5b78\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u8ab2\u7a0b](http://cs.kangwon.ac.kr/~leeck/NLP/)\n\n### \u8cc7\u6599\u96c6\n\n- [KAIST \u8a9e\u6599\u5eab](http://semanticweb.kaist.ac.kr/home/index.php/KAIST_Corpus) - \u97d3\u570b\u9ad8\u7b49\u79d1\u5b78\u6280\u8853\u7814\u7a76\u6240\u7684\u8a9e\u6599\u5eab\u3002\n- [\u97d3\u570b Naver \u60c5\u611f\u96fb\u5f71\u8a9e\u6599\u5eab](https://github.com/e9t/nsmc/)\n- [\u671d\u9bae\u65e5\u5831\u6a94\u6848\u9928](http://srchdb1.chosun.com/pdf/i_archive/) - \u4f86\u81ea\u97d3\u570b\u4e3b\u8981\u5831\u7d19\u4e4b\u4e00\u7684\u671d\u9bae\u65e5\u5831\u7684\u97d3\u6587\u6578\u64da\u96c6\u3002\n\n## \u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u963f\u62c9\u4f2f\u8a9e\n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n### \u51fd\u5f0f\u5eab\n\n- [goarabic](https://github.com/01walid/goarabic) - Go\u5305\u7528\u65bc\u963f\u62c9\u4f2f\u8a9e\u6587\u672c\u8655\u7406\u3002\n- [jsastem](https://github.com/ejtaal/jsastem) - \u7528\u65bc\u963f\u62c9\u4f2f\u8a5e\u5e79\u7684Javascript\u3002\n- [PyArabic](https://pypi.org/project/PyArabic/) - \u963f\u62c9\u4f2f\u8a9e\u7684 Python \u51fd\u5f0f\u5eab\u3002\n\n### \u8cc7\u6599\u96c6\n\n- [\u591a\u57df\u6578\u64da\u96c6](https://github.com/hadyelsahar/large-arabic-sentiment-analysis-resouces) - \u963f\u62c9\u4f2f\u8a9e\u60c5\u611f\u5206\u6790\u7684\u6700\u5927\u53ef\u7528\u591a\u57df\u8cc7\u6e90\u3002\n- [LABR](https://github.com/mohamedadaly/labr) - LArge\u963f\u62c9\u4f2f\u66f8\u7c4d\u8a55\u8ad6\u6578\u64da\u96c6\u3002\n- [Arabic \u505c\u7528\u8a5e](https://github.com/mohataher/arabic-stop-words) - \u4f86\u81ea\u5404\u7a2e\u8cc7\u6e90\u7684\u963f\u62c9\u4f2f\u8a9e\u505c\u7528\u8a5e\u5217\u8868\u3002\n\n## \u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u4e2d\u6587\n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n### \u51fd\u5f0f\u5eab\n\n- [jieba](https://github.com/fxsjy/jieba#jieba-1) - \u4e2d\u6587\u8a5e\u5f59\u5206\u5272\u5be6\u7528\u7a0b\u5e8f\u7684 Python \u5305\u3002\n- [SnowNLP](https://github.com/isnowfy/snownlp) - \u4e2d\u6587\u81ea\u7136\u8a9e\u8a00\u8655\u7406 Python \u5305\u3002\n- [FudanNLP](https://github.com/FudanNLP/fnlp) - \u7528\u65bc\u4e2d\u6587\u6587\u672c\u8655\u7406\u7684 Java \u51fd\u5f0f\u5eab\u3002\n\n## \u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u5fb7\u6587\n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n- [\u5fb7\u6587-\u81ea\u7136\u8a9e\u8a00\u8655\u7406](https://github.com/adbar/German-NLP) - \u958b\u767c\u7684\u958b\u653e\u5f0f\u8a2a\u554f/\u958b\u6e90/\u73fe\u6210\u8cc7\u6e90\u548c\u5de5\u5177\u5217\u8868\uff0c\u7279\u5225\u95dc\u6ce8\u5fb7\u8a9e\u3002\n \n## \u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u897f\u73ed\u7259\u8a9e\n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n### \u8cc7\u6599\n\n- [\u54e5\u502b\u6bd4\u4e9e\u653f\u6cbb\u6f14\u8aaa](https://github.com/dav009/LatinamericanTextResources)\n- [\u54e5\u672c\u54c8\u6839\u6a39\u5eab](https://mbkromann.github.io/copenhagen-dependency-treebank/)\n- [\u897f\u73ed\u7259\u8a9e\u5341\u5104\u5b57\u8a9e\u6599\u5eab\u8207 Word2Vec \u5d4c\u5165](https://github.com/crscardellino/sbwce)\n\n## \u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u5370\u5ea6\u8a9e\n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n### \u5370\u5730\u8a9e\n\n### \u8cc7\u6599, \u6587\u96c6\u8207\u6a39\u5eab\n\n- [\u5370\u5730\u8a9e\u4f9d\u8cf4\u6a39\u5eab](https://ltrc.iiit.ac.in/treebank_H2014/) - \u5370\u5730\u8a9e\u548c\u70cf\u723e\u90fd\u8a9e\u7684\u591a\u4ee3\u8868\u6027\u591a\u5c64\u6a39\u5eab\u3002\n- [\u5728\u5370\u5730\u8a9e\u7684\u666e\u904d\u4f9d\u8cf4\u6027\u6a39\u5eab](https://universaldependencies.org/treebanks/hi_hdtb/index.html)\n  - [\u4e26\u884c\u901a\u7528\u4f9d\u8cf4\u6a39\u5eab\u5370\u5730\u8a9e](http://universaldependencies.org/treebanks/hi_pud/index.html) - \u4e0a\u8ff0\u6a39\u5eab\u7684\u4e00\u5c0f\u90e8\u5206\u3002\n\n## \u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u6cf0\u8a9e\n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n### \u51fd\u5f0f\u5eab\n\n- [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp) - Python \u5305\u4e2d\u7684\u6cf0\u8a9e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u3002\n- [JTCC](https://github.com/wittawatj/jtcc) - Java \u4e2d\u7684\u5b57\u7b26\u96c6\u7fa4\u5eab\u3002\n- [CutKum](https://github.com/pucktada/cutkum) - \u5728 TensorFlow \u4e2d\u4f7f\u7528\u6df1\u5ea6\u5b78\u7fd2\u9032\u884c\u5206\u8a5e\u3002\n- [\u6cf0\u8a9e\u5de5\u5177\u5305](https://pypi.python.org/pypi/tltk/) - \u57fa\u65bc Wirote Aroonmanakun \u65bc2002\u5e74\u64b0\u5beb\u7684\u4e00\u7bc7\u8ad6\u6587\uff0c\u5176\u4e2d\u5305\u62ec\u6578\u64da\u96c6\u3002\n- [SynThai](https://github.com/KenjiroAI/SynThai) - \u5728 Python \u4e2d\u4f7f\u7528\u6df1\u5ea6\u5b78\u7fd2\u9032\u884c\u5206\u8a5e\u548c POS \u6a19\u8a18\u3002\n\n### \u8cc7\u6599\n\n- [Inter-BEST](https://www.nectec.or.th/corpus/index.php?league=pm) - \u5177\u6709500\u842c\u500b\u55ae\u8a5e\u5206\u8a5e\u7684\u6587\u672c\u8a9e\u6599\u5eab\u3002\n- [Prime Minister 29](https://github.com/PyThaiNLP/lexicon-thai/tree/master/thai-corpus/Prime%20Minister%2029) - \u6578\u64da\u96c6\u5305\u542b\u73fe\u4efb\u6cf0\u570b\u7e3d\u7406\u7684\u6f14\u8b1b\u3002\n\n\n## \u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u4e39\u9ea5\u8a9e \n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n- [\u4e39\u9ea5\u7684\u547d\u540d\u5be6\u9ad4\u8b58\u5225](https://github.com/ITUnlp/daner)\n\n## \u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u8d8a\u5357\u8a9e\n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n### \u51fd\u5f0f\u5eab\n\n- [underthesea](https://github.com/undertheseanlp/underthesea) - \u8d8a\u5357\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u5de5\u5177\u5305\u3002\n- [vn.vitk](https://github.com/phuonglh/vn.vitk) - \u8d8a\u5357\u6587\u672c\u8655\u7406\u5de5\u5177\u5305\u3002\n- [VnCoreNLP](https://github.com/vncorenlp/VnCoreNLP) - \u8d8a\u5357\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u5de5\u5177\u5305\u3002\n\n### \u8cc7\u6599\n\n- [\u8d8a\u5357\u6a39\u5eab](https://vlsp.hpda.vn/demo/?page=resources&lang=en) - \u9078\u5340\u89e3\u6790\u4efb\u52d9\u768410,000\u500b\u53e5\u5b50\u3002\n- [BKTreeBank](https://arxiv.org/pdf/1710.05519.pdf) -  \u8d8a\u5357\u4f9d\u8cf4\u6a39\u5eab\u3002\n- [UD_Vietnamese](https://github.com/UniversalDependencies/UD_Vietnamese-VTB) - \u8d8a\u5357\u901a\u7528\u4f9d\u8cf4\u6a39\u5eab\u3002\n- [VIVOS](https://ailab.hcmus.edu.vn/vivos/) - \u4e00\u500b\u514d\u8cbb\u7684\u8d8a\u5357\u8a9e\u8a00\u8a9e\u6599\u5eab\uff0c\u7531 AILab \u768415\u5c0f\u6642\u9304\u97f3\u8b1b\u8a71\u7d44\u6210\u3002\n- [VNTQcorpus(big).txt](http://viet.jnlp.org/download-du-lieu-tu-vung-corpus) - \u65b0\u805e\u4e2d\u7684175\u842c\u53e5\u8a71\u3002\n\n## \u81ea\u7136\u8a9e\u8a00\u8655\u7406-\u5370\u5ea6\u5c3c\u897f\u4e9e\n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n### \u8cc7\u6599\u96c6\n- [ILPS](http://ilps.science.uva.nl/resources/bahasa/) \u7684Kompas \u548c Tempo \u7cfb\u5217\u3002\n- [\u7528\u65bcPoS\u6a19\u8a18\u7684PANL10N](http://www.panl10n.net/english/outputs/Indonesia/UI/0802/UI-1M-tagged.zip): 39K\u53e5\u5b50\u548c900K\u5b57\u6a19\u8a18\u3002\n- [\u7528\u65bcPoS\u6a19\u8a18\u7684IDN](https://github.com/famrashel/idn-tagged-corpus): \u8a72\u8a9e\u6599\u5eab\u5305\u542b10K\u500b\u53e5\u5b50\u548c250K\u500b\u55ae\u8a5e\u6a19\u8a18\u3002\n- [\u5370\u5ea6\u5c3c\u897f\u4e9e\u6a39\u5eab](https://github.com/famrashel/idn-treebank)\u548c [\u666e\u904d\u4f9d\u8cf4 - \u5370\u5ea6\u5c3c\u897f\u4e9e\u8a9e](https://github.com/UniversalDependencies/UD_Indonesian-GSD)\n- [IndoSum](https://github.com/kata-ai/indosum) \u7528\u65bc\u6587\u672c\u6458\u8981\u548c\u5206\u985e\u3002\n- [Wordnet-Bahasa](http://wn-msa.sourceforge.net/) - \u5927\u578b\uff0c\u514d\u8cbb\u7684\u8a9e\u7fa9\u8a5e\u5178\u3002\n\n### \u51fd\u5f0f\u5eab\u8207\u5d4c\u5165\n- \u81ea\u7136\u8a9e\u8a00\u5de5\u5177\u5305 [bahasa](https://github.com/kangfend/bahasa)\n- [\u5370\u5c3c\u8a9e\u5d4c\u5165](https://github.com/galuhsahid/indonesian-word-embedding)\n- \u9810\u8a13\u7df4\u7684\u8a13\u7df4 [\u5370\u5c3c fastText \u6587\u672c\u5d4c\u5165](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.id.zip) \u7684\u7dad\u57fa\u767e\u79d1\u3002\n\n## \u5176\u4ed6\u8a9e\u8a00 \n\n[\u8fd4\u56de\u9802\u90e8](#\u5167\u5bb9)\n\n- \u4fc4\u8a9e: [pymorphy2](https://github.com/kmike/pymorphy2) - - \u4fc4\u8a9e\u597d\u7684\u8a5e\u6027\u6a19\u8a18\u3002\n- \u4e9e\u6d32\u8a9e\u8a00: ElasticSearch \u4e2d\u7684\u6cf0\u8a9e\uff0c\u8001\u64be\u8a9e\uff0c\u4e2d\u6587\uff0c\u65e5\u8a9e\u548c\u97d3\u8a9e [ICU Tokenizer](https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-icu-tokenizer.html) \u5be6\u73fe\u3002\n- \u53e4\u4ee3\u8a9e\u8a00: [CLTK](https://github.com/cltk/cltk): \u53e4\u5178\u8a9e\u8a00\u5de5\u5177\u5305\u662f\u4e00\u500b Python \u51fd\u5f0f\u5eab\u548c\u7528\u65bc\u5728\u53e4\u4ee3\u8a9e\u8a00\u4e2d\u9032\u884c\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684\u6587\u672c\u96c6\u5408\u3002\n- Dutch: [python-frog](https://github.com/proycon/python-frog) - Python \u7d81\u5b9a\u5230 Frog\uff0c\u4e00\u500b\u8377\u862d\u8a9e\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u5957\u4ef6\u3002\uff08pos \u6a19\u8a18\uff0c\u8a5e\u5f62\u9084\u539f\uff0c\u4f9d\u8cf4\u89e3\u6790\uff0cNER\n- \u5e0c\u4f2f\u4f86\u8a9e: [NLPH_Resources](https://github.com/NLPH/NLPH_Resources) - \u5e0c\u4f2f\u4f86\u8a9e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684\u8ad6\u6587\uff0c\u8a9e\u6599\u5eab\u548c\u8a9e\u8a00\u8cc7\u6e90\u7684\u96c6\u5408\u3002\n\n## \u8ca2\u737b\n\n\u521d\u59cb\u7b56\u5c55\u4eba\u548c\u4f86\u6e90\u7684[\u8ca2\u737b](./CREDITS.md)\u3002\n"
 },
 {
  "repo": "openai/gym",
  "language": "Python",
  "readme_contents": "**Status:** Maintenance (expect bug fixes and minor updates)\n\nOpenAI Gym\n**********\n\n**OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms.** This is the ``gym`` open-source library, which gives you access to a standardized set of environments.\n\n.. image:: https://travis-ci.org/openai/gym.svg?branch=master\n    :target: https://travis-ci.org/openai/gym\n\n`See What's New section below <#what-s-new>`_\n\n``gym`` makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano. You can use it from Python code, and soon from other languages.\n\nIf you're not sure where to start, we recommend beginning with the\n`docs <https://gym.openai.com/docs>`_ on our site. See also the `FAQ <https://github.com/openai/gym/wiki/FAQ>`_.\n\nA whitepaper for OpenAI Gym is available at http://arxiv.org/abs/1606.01540, and here's a BibTeX entry that you can use to cite it in a publication::\n\n  @misc{1606.01540,\n    Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},\n    Title = {OpenAI Gym},\n    Year = {2016},\n    Eprint = {arXiv:1606.01540},\n  }\n\n.. contents:: **Contents of this document**\n   :depth: 2\n\nBasics\n======\n\nThere are two basic concepts in reinforcement learning: the\nenvironment (namely, the outside world) and the agent (namely, the\nalgorithm you are writing). The agent sends `actions` to the\nenvironment, and the environment replies with `observations` and\n`rewards` (that is, a score).\n\nThe core `gym` interface is `Env <https://github.com/openai/gym/blob/master/gym/core.py>`_, which is\nthe *unified environment interface*. There is no interface for agents;\nthat part is left to you. The following are the ``Env`` methods you\nshould know:\n\n- `reset(self)`: Reset the environment's state. Returns `observation`.\n- `step(self, action)`: Step the environment by one timestep. Returns `observation`, `reward`, `done`, `info`.\n- `render(self, mode='human')`: Render one frame of the environment. The default mode will do something human friendly, such as pop up a window. \n\nSupported systems\n-----------------\n\nWe currently support Linux and OS X running Python 3.5 -- 3.8\nWindows support is experimental - algorithmic, toy_text, classic_control and atari *should* work on Windows (see next section for installation instructions); nevertheless, proceed at your own risk.\n\nInstallation\n============\n\nYou can perform a minimal install of ``gym`` with:\n\n.. code:: shell\n\n    git clone https://github.com/openai/gym.git\n    cd gym\n    pip install -e .\n\nIf you prefer, you can do a minimal install of the packaged version directly from PyPI:\n\n.. code:: shell\n\n    pip install gym\n\nYou'll be able to run a few environments right away:\n\n- algorithmic\n- toy_text\n- classic_control (you'll need ``pyglet`` to render though)\n\nWe recommend playing with those environments at first, and then later\ninstalling the dependencies for the remaining environments.\n\nYou can also `run gym on gitpod.io <https://gitpod.io/#https://github.com/openai/gym/blob/master/examples/agents/cem.py>`_ to play with the examples online.  \nIn the preview window you can click on the mp4 file you want to view. If you want to view another mp4 file, just press the back button and click on another mp4 file. \n\nInstalling everything\n---------------------\n\nTo install the full set of environments, you'll need to have some system\npackages installed. We'll build out the list here over time; please let us know\nwhat you end up installing on your platform. Also, take a look at the docker files (py.Dockerfile) to\nsee the composition of our CI-tested images.\n\nOn Ubuntu 16.04 and 18.04:\n\n.. code:: shell\n    \n    apt-get install -y libglu1-mesa-dev libgl1-mesa-dev libosmesa6-dev xvfb ffmpeg curl patchelf libglfw3 libglfw3-dev cmake zlib1g zlib1g-dev swig\n\nMuJoCo has a proprietary dependency we can't set up for you. Follow\nthe\n`instructions <https://github.com/openai/mujoco-py#obtaining-the-binaries-and-license-key>`_\nin the ``mujoco-py`` package for help. Note that we currently do not support MuJoCo 2.0 and above, so you will need to install a version of mujoco-py which is built\nfor a lower version of MuJoCo like MuJoCo 1.5 (example - ``mujoco-py-1.50.1.0``).\nAs an alternative to ``mujoco-py``, consider `PyBullet <https://github.com/openai/gym/blob/master/docs/environments.md#pybullet-robotics-environments>`_ which uses the open source Bullet physics engine and has no license requirement.\n\nOnce you're ready to install everything, run ``pip install -e '.[all]'`` (or ``pip install 'gym[all]'``).\n\nPip version\n-----------\n\nTo run ``pip install -e '.[all]'``, you'll need a semi-recent pip.\nPlease make sure your pip is at least at version ``1.5.0``. You can\nupgrade using the following: ``pip install --ignore-installed\npip``. Alternatively, you can open `setup.py\n<https://github.com/openai/gym/blob/master/setup.py>`_ and\ninstall the dependencies by hand.\n\nRendering on a server\n---------------------\n\nIf you're trying to render video on a server, you'll need to connect a\nfake display. The easiest way to do this is by running under\n``xvfb-run`` (on Ubuntu, install the ``xvfb`` package):\n\n.. code:: shell\n\n     xvfb-run -s \"-screen 0 1400x900x24\" bash\n\nInstalling dependencies for specific environments\n-------------------------------------------------\n\nIf you'd like to install the dependencies for only specific\nenvironments, see `setup.py\n<https://github.com/openai/gym/blob/master/setup.py>`_. We\nmaintain the lists of dependencies on a per-environment group basis.\n\nEnvironments\n============\n\nSee `List of Environments <docs/environments.md>`_ and the `gym site <http://gym.openai.com/envs/>`_.\n\nFor information on creating your own environments, see `Creating your own Environments <docs/creating-environments.md>`_.\n\nExamples\n========\n\nSee the ``examples`` directory.\n\n- Run `examples/agents/random_agent.py <https://github.com/openai/gym/blob/master/examples/agents/random_agent.py>`_ to run a simple random agent.\n- Run `examples/agents/cem.py <https://github.com/openai/gym/blob/master/examples/agents/cem.py>`_ to run an actual learning agent (using the cross-entropy method).\n- Run `examples/scripts/list_envs <https://github.com/openai/gym/blob/master/examples/scripts/list_envs>`_ to generate a list of all environments.\n\nTesting\n=======\n\nWe are using `pytest <http://doc.pytest.org>`_ for tests. You can run them via:\n\n.. code:: shell\n\n    pytest\n\n\n.. _See What's New section below:\n\nResources\n=========\n\n-  `OpenAI.com`_\n-  `Gym.OpenAI.com`_\n-  `Gym Docs`_\n-  `Gym Environments`_\n-  `OpenAI Twitter`_\n-  `OpenAI YouTube`_\n\n.. _OpenAI.com: https://openai.com/\n.. _Gym.OpenAI.com: http://gym.openai.com/\n.. _Gym Docs: http://gym.openai.com/docs/\n.. _Gym Environments: http://gym.openai.com/envs/\n.. _OpenAI Twitter: https://twitter.com/openai\n.. _OpenAI YouTube: https://www.youtube.com/channel/UCXZCJLdBC09xxGZ6gcdrc6A\n\nWhat's new\n==========\n- 2020-09-29 (v 0.17.3)\n   + Allow custom spaces in VectorEnv (thanks @tristandeleu!)\n   + CarRacing performance improvements (thanks @leocus!)\n   + Dict spaces are now iterable (thanks @NotNANtoN!)\n\n- 2020-05-08 (v 0.17.2)\n   - remove unnecessary precision warning when creating Box with scalar bounds - thanks @johannespitz!\n   - remove six from the dependencies\n   + FetchEnv sample goal range can be specified through kwargs - thanks @YangRui2015!\n\n- 2020-03-05 (v 0.17.1)\n   + update cloudpickle dependency to be >=1.2.0,<1.4.0\n\n- 2020-02-21 (v 0.17.0)\n   - Drop python 2 support\n   + Add python 3.8 build\n\n- 2020-02-09 (v 0.16.0)\n   + EnvSpec API change - remove tags field (retro-active version bump, the changes are actually already in the codebase since 0.15.5 - thanks @wookayin for keeping us in check!)\n\n- 2020-02-03 (v0.15.6)\n   + pyglet 1.4 compatibility (this time for real :))\n   + Fixed the bug in BipedalWalker and BipedalWalkerHardcore, bumped version to 3 (thanks @chozabu!)\n\n- 2020-01-24 (v0.15.5)\n    + pyglet 1.4 compatibility\n    - remove python-opencv from the requirements\n   \n- 2019-11-08 (v0.15.4)\n    + Added multiple env wrappers (thanks @zuoxingdong and @hartikainen!)\n    - Removed mujoco >= 2.0 support due to lack of tests\n\n- 2019-10-09 (v0.15.3)\n    + VectorEnv modifications - unified the VectorEnv api (added reset_async, reset_wait, step_async, step_wait methods to SyncVectorEnv); more flexibility in AsyncVectorEnv workers\n\n- 2019-08-23 (v0.15.2)\n    + More Wrappers - AtariPreprocessing, FrameStack, GrayScaleObservation, FilterObservation,  FlattenDictObservationsWrapper, PixelObservationWrapper, TransformReward (thanks @zuoxingdong, @hartikainen)\n    + Remove rgb_rendering_tracking logic from mujoco environments (default behavior stays the same for the -v3 environments, rgb rendering returns a view from tracking camera)\n    + Velocity goal constraint for MountainCar (thanks @abhinavsagar)\n    + Taxi-v2 -> Taxi-v3 (add missing wall in the map to replicate env as describe in the original paper, thanks @kobotics)\n    \n- 2019-07-26 (v0.14.0)\n    + Wrapper cleanup\n    + Spec-related bug fixes\n    + VectorEnv fixes\n\n- 2019-06-21 (v0.13.1)\n    + Bug fix for ALE 0.6 difficulty modes\n    + Use narrow range for pyglet versions\n\n- 2019-06-21 (v0.13.0)\n    + Upgrade to ALE 0.6 (atari-py 0.2.0) (thanks @JesseFarebro!)\n\n- 2019-06-21 (v0.12.6)\n    + Added vectorized environments (thanks @tristandeleu!). Vectorized environment runs multiple copies of an environment in parallel. To create a vectorized version of an environment, use `gym.vector.make(env_id, num_envs, **kwargs)`, for instance, `gym.vector.make('Pong-v4',16)`.\n\n- 2019-05-28 (v0.12.5)\n    + fixed Fetch-slide environment to be solvable.\n\n- 2019-05-24 (v0.12.4)\n    + remove pyopengl dependency and use more narrow atari-py and box2d-py versions\n\n- 2019-03-25 (v0.12.1)\n    + rgb rendering in MuJoCo locomotion `-v3` environments now comes from tracking camera (so that agent does not run away from the field of view). The old behaviour can be restored by passing rgb_rendering_tracking=False kwarg. Also, a potentially breaking change!!! Wrapper class now forwards methods and attributes to wrapped env.\n\n- 2019-02-26 (v0.12.0)\n    + release mujoco environments v3 with support for gym.make kwargs such as `xml_file`, `ctrl_cost_weight`, `reset_noise_scale` etc\n\n- 2019-02-06 (v0.11.0)\n    + remove gym.spaces.np_random common PRNG; use per-instance PRNG instead.\n    + support for kwargs in gym.make\n    + lots of bugfixes\n\n- 2018-02-28: Release of a set of new robotics environments.\n- 2018-01-25: Made some aesthetic improvements and removed unmaintained parts of gym. This may seem like a downgrade in functionality, but it is actually a long-needed cleanup in preparation for some great new things that will be released in the next month.\n\n    + Now your `Env` and `Wrapper` subclasses should define `step`, `reset`, `render`, `close`, `seed` rather than underscored method names.\n    + Removed the `board_game`, `debugging`, `safety`, `parameter_tuning` environments since they're not being maintained by us at OpenAI. We encourage authors and users to create new repositories for these environments.\n    + Changed `MultiDiscrete` action space to range from `[0, ..., n-1]` rather than `[a, ..., b-1]`.\n    + No more `render(close=True)`, use env-specific methods to close the rendering.\n    + Removed `scoreboard` directory, since site doesn't exist anymore.\n    + Moved `gym/monitoring` to `gym/wrappers/monitoring`\n    + Add `dtype` to `Space`.\n    + Not using python's built-in module anymore, using `gym.logger`\n\n- 2018-01-24: All continuous control environments now use mujoco_py >= 1.50.\n  Versions have been updated accordingly to -v2, e.g. HalfCheetah-v2. Performance\n  should be similar (see https://github.com/openai/gym/pull/834) but there are likely\n  some differences due to changes in MuJoCo.\n- 2017-06-16: Make env.spec into a property to fix a bug that occurs\n  when you try to print out an unregistered Env.\n- 2017-05-13: BACKWARDS INCOMPATIBILITY: The Atari environments are now at\n  *v4*. To keep using the old v3 environments, keep gym <= 0.8.2 and atari-py\n  <= 0.0.21. Note that the v4 environments will not give identical results to\n  existing v3 results, although differences are minor. The v4 environments\n  incorporate the latest Arcade Learning Environment (ALE), including several\n  ROM fixes, and now handle loading and saving of the emulator state. While\n  seeds still ensure determinism, the effect of any given seed is not preserved\n  across this upgrade because the random number generator in ALE has changed.\n  The `*NoFrameSkip-v4` environments should be considered the canonical Atari\n  environments from now on.\n- 2017-03-05: BACKWARDS INCOMPATIBILITY: The `configure` method has been removed\n  from `Env`. `configure` was not used by `gym`, but was used by some dependent\n  libraries including `universe`. These libraries will migrate away from the\n  configure method by using wrappers instead. This change is on master and will be released with 0.8.0.\n- 2016-12-27: BACKWARDS INCOMPATIBILITY: The gym monitor is now a\n  wrapper. Rather than starting monitoring as\n  `env.monitor.start(directory)`, envs are now wrapped as follows:\n  `env = wrappers.Monitor(env, directory)`. This change is on master\n  and will be released with 0.7.0.\n- 2016-11-1: Several experimental changes to how a running monitor interacts\n  with environments. The monitor will now raise an error if reset() is called\n  when the env has not returned done=True. The monitor will only record complete\n  episodes where done=True. Finally, the monitor no longer calls seed() on the\n  underlying env, nor does it record or upload seed information.\n- 2016-10-31: We're experimentally expanding the environment ID format\n  to include an optional username.\n- 2016-09-21: Switch the Gym automated logger setup to configure the\n  root logger rather than just the 'gym' logger.\n- 2016-08-17: Calling `close` on an env will also close the monitor\n  and any rendering windows.\n- 2016-08-17: The monitor will no longer write manifest files in\n  real-time, unless `write_upon_reset=True` is passed.\n- 2016-05-28: For controlled reproducibility, envs now support seeding\n  (cf #91 and #135). The monitor records which seeds are used. We will\n  soon add seed information to the display on the scoreboard.\n"
 },
 {
  "repo": "aikorea/awesome-rl",
  "language": null,
  "readme_contents": "# Awesome Reinforcement Learning  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\nA curated list of resources dedicated to reinforcement learning.\n\nWe have pages for other topics: [awesome-rnn](https://github.com/kjw0612/awesome-rnn), [awesome-deep-vision](https://github.com/kjw0612/awesome-deep-vision), [awesome-random-forest](https://github.com/kjw0612/awesome-random-forest)\n\nMaintainers: [Hyunsoo Kim](http://sites.duke.edu/hyunsookim/), [Jiwon Kim](http://github.com/kjw0612)\n\nWe are looking for more contributors and maintainers!\n\n\n## Contributing\nPlease feel free to [pull requests](https://github.com/aikorea/awesome-rl/pulls)\n\n## Table of Contents\n\n - [Theory](#theory)\n   - [Lectures](#lectures)\n   - [Books](#books)\n   - [Surveys](#surveys)\n   - [Papers / Thesis](#papers--thesis)\n - [Applications](#applications)\n   - [Game Playing](#game-playing)\n   - [Robotics](#robotics)\n   - [Control](#control)\n   - [Operations Research](#operations-research)\n   - [Human Computer Interaction](#human-computer-interaction)\n - [Codes](#codes)\n - [Tutorials / Websites](#tutorials--websites)\n - [Online Demos](#online-demos)\n - [Open Source Reinforcement Learning Platforms](#open-source-reinforcement-learning-platforms)\n\n## Codes\n - Codes for examples and exercises in Richard Sutton and Andrew Barto's Reinforcement Learning: An Introduction\n    - [Python Code](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction)\n    - [MATLAB Code (BROKEN LINK)](http://waxworksmath.com/Authors/N_Z/Sutton/sutton.html)\n    - [C/Lisp Code](http://incompleteideas.net/book/code/code2nd.html)\n    - [Julia Code](https://github.com/Ju-jl/ReinforcementLearningAnIntroduction.jl)\n    - [Book](http://incompleteideas.net/book/RLbook2018.pdf)\n - Simulation code for Reinforcement Learning Control Problems\n    - [Pole-Cart Problem](http://pages.cs.wisc.edu/~finton/poledriver.html)\n    - [Q-learning Controller](http://pages.cs.wisc.edu/~finton/qcontroller.html)\n - [MATLAB Environment and GUI for Reinforcement Learning](http://www.cs.colostate.edu/~anderson/res/rl/matlabpaper/rl.html)\n - [Reinforcement Learning Repository - University of Massachusetts, Amherst](http://www-anw.cs.umass.edu/rlr/)\n - [Brown-UMBC Reinforcement Learning and Planning Library (Java)](http://burlap.cs.brown.edu/)\n - [Reinforcement Learning in R (MDP, Value Iteration)](http://www.moneyscience.com/pg/blog/StatAlgo/read/635759/reinforcement-learning-in-r-markov-decision-process-mdp-and-value-iteration)\n - [Reinforcement Learning Environment in Python and MATLAB](https://jamh-web.appspot.com/download.htm)\n - [RL-Glue](http://glue.rl-community.org/wiki/Main_Page) (standard interface for RL) and [RL-Glue Library](http://library.rl-community.org/wiki/Main_Page)\n - [PyBrain Library](http://www.pybrain.org/) - Python-Based Reinforcement learning, Artificial intelligence, and Neural network\n - [RLPy Framework](http://rlpy.readthedocs.org/en/latest/) -  Value-Function-Based Reinforcement Learning Framework for Education and Research\n - [Maja](http://mmlf.sourceforge.net/) - Machine learning framework for problems in Reinforcement Learning in python\n - [TeachingBox](http://servicerobotik.hs-weingarten.de/en/teachingbox.php) - Java based Reinforcement Learning framework\n - [Policy Gradient Reinforcement Learning Toolbox for MATLAB](http://www.ias.informatik.tu-darmstadt.de/Research/PolicyGradientToolbox)\n - [PIQLE](http://sourceforge.net/projects/piqle/) - Platform Implementing Q-Learning and other RL algorithms\n - [BeliefBox](https://code.google.com/p/beliefbox/) - Bayesian reinforcement learning library and toolkit\n - [Deep Q-Learning with TensorFlow](https://github.com/nivwusquorum/tensorflow-deepq) - A deep Q learning demonstration using Google Tensorflow\n - [Atari](https://github.com/Kaixhin/Atari) - Deep Q-networks and asynchronous agents in Torch\n - [AgentNet](https://github.com/yandexdataschool/AgentNet) - A python library for deep reinforcement learning and custom recurrent networks using Theano+Lasagne.\n - [Reinforcement Learning Examples by RLCode](https://github.com/rlcode/reinforcement-learning) - A Collection of minimal and clean reinforcement learning examples\n - [OpenAI Baselines](https://github.com/openai/baselines) - Well tested implementations ([and results](https://github.com/openai/baselines-results)) of reinforcement learning algorithms from OpenAI \n - [PyTorch Deep RL](https://github.com/ShangtongZhang/DeepRL) - Popular deep RL algorithm implementations with PyTorch\n - [ChainerRL](https://github.com/chainer/chainerrl) - Popular deep RL algorithm implementations with Chainer\n - [Black-DROPS](https://github.com/resibots/blackdrops) - Modular and generic code for the model-based policy search Black-DROPS algorithm (IROS 2017 paper) and easy integration with the [DART](http://dartsim.github.io/) simulator\n\n## Theory\n\n### Lectures\n - [UCL] [COMPM050/COMPGI13 Reinforcement Learning](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) by David Silver\n - [UC Berkeley] CS188 Artificial Intelligence by Pieter Abbeel\n   - [Lecture 8: Markov Decision Processes 1](https://www.youtube.com/watch?v=i0o-ui1N35U)\n   - [Lecture 9: Markov Decision Processes 2](https://www.youtube.com/watch?v=Csiiv6WGzKM)\n   - [Lecture 10: Reinforcement Learning 1](https://www.youtube.com/watch?v=ifma8G7LegE)\n   - [Lecture 11: Reinforcement Learning 2](https://www.youtube.com/watch?v=Si1_YTw960c)\n - [Udacity (Georgia Tech.)] [CS7642 Reinforcement Learning](https://classroom.udacity.com/courses/ud600)\n - [Stanford] [CS229 Machine Learning - Lecture 16: Reinforcement Learning](https://www.youtube.com/watch?v=RtxI449ZjSc&feature=relmfu) by Andrew Ng\n - [UC Berkeley] [Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures)\n - [UC Berkeley] [CS294 Deep Reinforcement Learning](http://rll.berkeley.edu/deeprlcourse/) by John Schulman and Pieter Abbeel\n - [CMU] [10703: Deep Reinforcement Learning and Control, Spring 2017](https://katefvision.github.io/)\n - [MIT] [6.S094: Deep Learning for Self-Driving Cars](http://selfdrivingcars.mit.edu/)\n   - [Lecture 2: Deep Reinforcement Learning for Motion Planning](https://www.youtube.com/watch?v=QDzM8r3WgBw&list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf)\n - [Siraj Raval]: Introduction to AI for Video Games (Reinforcement Learning Video Series)\n   - [Introduction to AI for video games](https://youtu.be/i_McNBDP9Qs)\n   - [Monte Carlo Prediction](https://youtu.be/-YpalutQCKw)\n   - [Q learning explained](https://youtu.be/aCEvtRtNO-M)\n   - [Solving the basic game of Pong](https://youtu.be/pN7ETkOizGM)\n   - [Actor Critic Algorithms](https://youtu.be/w_3mmm0P0j8)\n   - [War Robots](https://youtu.be/tm5kQmjfZN8)\n    \n\n### Books\n - Richard Sutton and Andrew Barto, Reinforcement Learning: An Introduction (1st Edition, 1998) [[Book]](http://incompleteideas.net/book/ebook/the-book.html) [[Code]](http://incompleteideas.net/book/code/code.html)\n - Richard Sutton and Andrew Barto, Reinforcement Learning: An Introduction (2nd Edition, in progress, 2018) [[Book]](http://incompleteideas.net/book/RLbook2018.pdf) [[Code]](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction)\n - Csaba Szepesvari, Algorithms for Reinforcement Learning [[Book]](http://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf)\n - David Poole and Alan Mackworth, Artificial Intelligence: Foundations of Computational Agents [[Book Chapter]](http://artint.info/html/ArtInt_262.html)\n - Dimitri P. Bertsekas and John N. Tsitsiklis, Neuro-Dynamic Programming [[Book (Amazon)]](http://www.amazon.com/Neuro-Dynamic-Programming-Optimization-Neural-Computation/dp/1886529108/ref=sr_1_3?s=books&ie=UTF8&qid=1442461075&sr=1-3&refinements=p_27%3AJohn+N.+Tsitsiklis+Dimitri+P.+Bertsekas) [[Summary]](http://www.mit.edu/~dimitrib/NDP_Encycl.pdf)\n - Mykel J. Kochenderfer, Decision Making Under Uncertainty: Theory and Application [[Book (Amazon)]](http://www.amazon.com/Decision-Making-Under-Uncertainty-Application/dp/0262029251/ref=sr_1_1?ie=UTF8&qid=1441126550&sr=8-1&keywords=kochenderfer&pebp=1441126551594&perid=1Y6RG2EGRD26659CJHH9)\n - Deep Reinforcement Learning in Action [[Book(Manning)]](https://www.manning.com/books/deep-reinforcement-learning-in-action)\n\n\n### Surveys\n - Leslie Pack Kaelbling, Michael L. Littman, Andrew W. Moore, Reinforcement Learning: A Survey (JAIR 1996) [[Paper]](https://www.jair.org/index.php/jair/article/download/10166/24110/)\n - S. S. Keerthi and B. Ravindran, A Tutorial Survey of Reinforcement Learning (Sadhana 1994) [[Paper]](http://www.cse.iitm.ac.in/~ravi/papers/keerthi.rl-survey.pdf)\n - Matthew E. Taylor, Peter Stone, Transfer Learning for Reinforcement Learning Domains: A Survey (JMLR 2009) [[Paper]](http://www.jmlr.org/papers/volume10/taylor09a/taylor09a.pdf)\n - Jens Kober, J. Andrew Bagnell, Jan Peters, Reinforcement Learning in Robotics, A Survey (IJRR 2013) [[Paper]](http://www.ias.tu-darmstadt.de/uploads/Publications/Kober_IJRR_2013.pdf)\n - Michael L. Littman, Reinforcement learning improves behaviour from evaluative feedback (Nature 2015) [[Paper]](http://www.nature.com/nature/journal/v521/n7553/full/nature14540.html)\n - Marc P. Deisenroth, Gerhard Neumann, Jan Peter, A Survey on Policy Search for Robotics, Foundations and Trends in Robotics (2014) [[Book]](https://spiral.imperial.ac.uk:8443/bitstream/10044/1/12051/7/fnt_corrected_2014-8-22.pdf)\n - Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath, A Brief Survey of Deep Reinforcement Learning (IEEE Signal Processing Magazine 2017) [[DOI]](https://dx.doi.org/10.1109/MSP.2017.2743240) [[Paper]](https://arxiv.org/abs/1708.05866)\n - Benjamin Recht, A Tour of Reinforcement Learning: The View from Continuous Control (Annu. Rev. Control Robot. Auton. Syst. 2019) [[DOI]](https://dx.doi.org/10.1146/annurev-control-053018-023825)\n\n### Papers / Thesis\nFoundational Papers\n - Marvin Minsky, Steps toward Artificial Intelligence, Proceedings of the IRE, 1961. [[DOI]](https://dx.doi.org/10.1109/JRPROC.1961.287775) [[Paper]](http://staffweb.worc.ac.uk/DrC/Courses%202010-11/Comp%203104/Tutor%20Inputs/Session%209%20Prep/Reading%20material/Minsky60steps.pdf) (discusses issues in RL such as the \"credit assignment problem\")\n - Ian H. Witten, An Adaptive Optimal Controller for Discrete-Time Markov Environments, Information and Control, 1977. [[DOI]](https://doi.org/10.1016/S0019-9958(77)90354-0) [[Paper]](http://www.cs.waikato.ac.nz/~ihw/papers/77-IHW-AdaptiveController.pdf) (earliest publication on temporal-difference (TD) learning rule)\n  \nMethods\n - Dynamic Programming (DP):\n   - Christopher J. C. H. Watkins, Learning from Delayed Rewards, Ph.D. Thesis, Cambridge University, 1989. [[Thesis]](https://www.cs.rhul.ac.uk/home/chrisw/new_thesis.pdf)\n - Monte Carlo:\n   - Andrew Barto, Michael Duff, Monte Carlo Inversion and Reinforcement Learning, NIPS, 1994. [[Paper]](http://papers.nips.cc/paper/865-monte-carlo-matrix-inversion-and-reinforcement-learning.pdf)\n   - Satinder P. Singh, Richard S. Sutton, Reinforcement Learning with Replacing Eligibility Traces, Machine Learning, 1996. [[Paper]](http://www-all.cs.umass.edu/pubs/1995_96/singh_s_ML96.pdf)\n - Temporal-Difference:\n   - Richard S. Sutton, Learning to predict by the methods of temporal differences. Machine Learning 3: 9-44, 1988. [[Paper]](http://webdocs.cs.ualberta.ca/~sutton/papers/sutton-88-with-erratum.pdf)\n - Q-Learning (Off-policy TD algorithm):\n   - Chris Watkins, Learning from Delayed Rewards, Cambridge, 1989. [[Thesis]](http://www.cs.rhul.ac.uk/home/chrisw/thesis.html)\n - Sarsa (On-policy TD algorithm):\n   - G.A. Rummery, M. Niranjan, On-line Q-learning using connectionist systems, Technical Report, Cambridge Univ., 1994. [[Report]](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&ved=0CDIQFjACahUKEwj2lMm5wZDIAhUHkg0KHa6kAVM&url=ftp%3A%2F%2Fmi.eng.cam.ac.uk%2Fpub%2Freports%2Fauto-pdf%2Frummery_tr166.pdf&usg=AFQjCNHz6IrgcaaO5lzC7t8oEIBY9epozg&sig2=sa-emPme1m5Jav7YmaXsNQ&cad=rja)\n   - Richard S. Sutton, Generalization in Reinforcement Learning: Successful examples using sparse coding, NIPS, 1996. [[Paper]](http://webdocs.cs.ualberta.ca/~sutton/papers/sutton-96.pdf)\n - R-Learning (learning of relative values)\n   - Andrew Schwartz, A Reinforcement Learning Method for Maximizing Undiscounted Rewards, ICML, 1993. [[Paper-Google Scholar]](https://scholar.google.com/scholar?q=reinforcement+learning+method+for+maximizing+undiscounted+rewards&hl=en&as_sdt=0&as_vis=1&oi=scholart&sa=X&ved=0CBsQgQMwAGoVChMIho6p_MOQyAIVwh0eCh3XWAwM)\n - Function Approximation methods (Least-Square Temporal Difference, Least-Square Policy Iteration)\n   - Steven J. Bradtke, Andrew G. Barto, Linear Least-Squares Algorithms for Temporal Difference Learning, Machine Learning, 1996. [[Paper]](http://www-anw.cs.umass.edu/pubs/1995_96/bradtke_b_ML96.pdf)\n   - Michail G. Lagoudakis, Ronald Parr, Model-Free Least Squares Policy Iteration, NIPS, 2001. [[Paper]](http://www.cs.duke.edu/research/AI/LSPI/nips01.pdf) [[Code]](http://www.cs.duke.edu/research/AI/LSPI/)\n - Policy Search / Policy Gradient\n   - Richard Sutton, David McAllester, Satinder Singh, Yishay Mansour, Policy Gradient Methods for Reinforcement Learning with Function Approximation, NIPS, 1999. [[Paper]](http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)\n   - Jan Peters, Sethu Vijayakumar, Stefan Schaal, Natural Actor-Critic, ECML, 2005. [[Paper]](https://homes.cs.washington.edu/~todorov/courses/amath579/reading/NaturalActorCritic.pdf)\n   - Jens Kober, Jan Peters, Policy Search for Motor Primitives in Robotics, NIPS, 2009. [[Paper]](http://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics.pdf)\n   - Jan Peters, Katharina Mulling, Yasemin Altun, Relative Entropy Policy Search, AAAI, 2010. [[Paper]](http://www.kyb.tue.mpg.de/fileadmin/user_upload/files/publications/attachments/AAAI-2010-Peters_6439%5b0%5d.pdf)\n   - Freek Stulp, Olivier Sigaud, Path Integral Policy Improvement with Covariance Matrix Adaptation, ICML, 2012. [[Paper]](http://arxiv.org/pdf/1206.4621v1.pdf)\n   - Nate Kohl, Peter Stone, Policy Gradient Reinforcement Learning for Fast Quadrupedal Locomotion, ICRA, 2004. [[Paper]](http://www.cs.utexas.edu/~pstone/Papers/bib2html-links/icra04.pdf)\n   - Marc Deisenroth, Carl Rasmussen, PILCO: A Model-Based and Data-Efficient Approach to Policy Search, ICML, 2011. [[Paper]](http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf)\n   - Scott Kuindersma, Roderic Grupen, Andrew Barto, Learning Dynamic Arm Motions for Postural Recovery, Humanoids, 2011. [[Paper]](http://www-all.cs.umass.edu/pubs/2011/kuindersma_g_b_11.pdf)\n   - Konstantinos Chatzilygeroudis, Roberto Rama, Rituraj Kaushik, Dorian Goepp, Vassilis Vassiliades, Jean-Baptiste Mouret, Black-Box Data-efficient Policy Search for Robotics, IROS, 2017. [[Paper](https://arxiv.org/abs/1703.07261)]\n - Hierarchical RL\n   - Richard Sutton, Doina Precup, Satinder Singh, Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning, Artificial Intelligence, 1999. [[Paper]](https://webdocs.cs.ualberta.ca/~sutton/papers/SPS-aij.pdf)\n   - George Konidaris, Andrew Barto, Building Portable Options: Skill Transfer in Reinforcement Learning, IJCAI, 2007. [[Paper]](http://www-anw.cs.umass.edu/pubs/2007/konidaris_b_IJCAI07.pdf)\n - Deep Learning + Reinforcement Learning (A sample of recent works on DL+RL)\n   - V. Mnih, et. al., Human-level Control through Deep Reinforcement Learning, Nature, 2015. [[Paper]](http://www.readcube.com/articles/10.1038%2Fnature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D)\n   - Xiaoxiao Guo, Satinder Singh, Honglak Lee, Richard Lewis, Xiaoshi Wang, Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning, NIPS, 2014. [[Paper]](http://papers.nips.cc/paper/5421-deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning.pdf)\n   - Sergey Levine, Chelsea Finn, Trevor Darrel, Pieter Abbeel, End-to-End Training of Deep Visuomotor Policies. ArXiv, 16 Oct 2015. [[ArXiv]](http://arxiv.org/pdf/1504.00702v3.pdf)\n   - Tom Schaul, John Quan, Ioannis Antonoglou, David Silver, Prioritized Experience Replay, ArXiv, 18 Nov 2015. [[ArXiv]](http://arxiv.org/pdf/1511.05952v2.pdf)\n   - Hado van Hasselt, Arthur Guez, David Silver, Deep Reinforcement Learning with Double Q-Learning, ArXiv, 22 Sep 2015. [[ArXiv]](http://arxiv.org/abs/1509.06461)\n   - Volodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, Asynchronous Methods for Deep Reinforcement Learning, ArXiv, 4 Feb 2016. [[ArXiv]](https://arxiv.org/abs/1602.01783)\n    \n\n## Applications\n### Game Playing\nTraditional Games\n  - Backgammon - Gerald Tesauro, \"TD-Gammon\" game play using TD(\u03bb) (ACM 1995) [[Paper]](http://www.bkgm.com/articles/tesauro/tdl.html)\n  - Chess - Jonathan Baxter, Andrew Tridgell and Lex Weaver, \"KnightCap\" program using TD(\u03bb) (1999) [[arXiv]](http://arxiv.org/pdf/cs/9901002v1.pdf)\n  - Chess - Matthew Lai, Giraffe: Using deep reinforcement learning to play chess (2015) [[arXiv]](http://arxiv.org/pdf/1509.01549v2.pdf)\n\nComputer Games\n  - Atari 2600 Games - Volodymyr Mnih, Koray Kavukcuoglu, David Silver et al., Human-level Control through Deep Reinforcement Learning (Nature 2015) [[DOI]](https://dx.doi.org/doi:10.1038/nature14236) [[Paper]](http://www.readcube.com/articles/10.1038%2Fnature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D) [[Code]](https://sites.google.com/a/deepmind.com/dqn/) [[Video]](https://www.youtube.com/watch?v=iqXKQf2BOSE)\n  - Flappy Bird - Sarvagya Vaish, [Flappy Bird Reinforcement Learning](https://github.com/SarvagyaVaish/FlappyBirdRL) [[Video]](https://www.youtube.com/watch?v=xM62SpKAZHU)\n  - Mario - Kenneth O. Stanley and Risto Miikkulainen, MarI/O - learning to play Mario with evolutionary reinforcement learning using artificial neural networks (Evolutionary Computation 2002) [[Paper]](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf) [[Video]](https://www.youtube.com/watch?v=qv6UVOQ0F44)\n  - StarCraft II - Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki et al., Grandmaster level in StarCraft II using multi-agent reinforcement learning (Nature 2019) [[DOI]](https://doi.org/10.1038/s41586-019-1724-z) [[Paper]](https://www.nature.com/articles/s41586-019-1724-z.epdf) [[Video]](https://deepmind.com/research/open-source/alphastar-resources)\n\n### Robotics\n  - Nate Kohl and Peter Stone, Policy Gradient Reinforcement Learning for Fast Quadrupedal Locomotion (ICRA 2004) [[Paper]](http://www.cs.utexas.edu/~pstone/Papers/bib2html-links/icra04.pdf)\n  - Petar Kormushev, Sylvain Calinon and Darwin G. Caldwel, Robot Motor SKill Coordination with EM-based Reinforcement Learning (IROS 2010) [[Paper]](http://kormushev.com/papers/Kormushev-IROS2010.pdf) [[Video]](https://www.youtube.com/watch?v=W_gxLKSsSIE)\n  - Todd Hester, Michael Quinlan, and Peter Stone, Generalized Model Learning for Reinforcement Learning on a Humanoid Robot (ICRA 2010) [[Paper]](https://ccc.inaoep.mx/~mdprl/documentos/Hester_2010.pdf) [[Video]](https://www.youtube.com/watch?v=mRpX9DFCdwI&list=PL5nBAYUyJTrM48dViibyi68urttMlUv7e&index=12)\n  - George Konidaris, Scott Kuindersma, Roderic Grupen and Andrew Barto, Autonomous Skill Acquisition on a Mobile Manipulator (AAAI 2011) [[Paper]](http://lis.csail.mit.edu/pubs/konidaris-aaai11b.pdf) [[Video]](https://www.youtube.com/watch?v=yUICAkSQTZY)\n  - Marc Peter Deisenroth and Carl Edward Rasmussen,PILCO: A Model-Based and Data-Efficient Approach to Policy Search (ICML 2011) [[Paper]](http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf)\n  - Scott Niekum, Sachin Chitta, Bhaskara Marthi, et al., Incremental Semantically Grounded Learning from Demonstration (RSS 2013) [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.310.87&rep=rep1&type=pdf)\n  - Mark Cutler and Jonathan P. How, Efficient Reinforcement Learning for Robots using Informative Simulated Priors (ICRA 2015) [[Paper]](http://markjcutler.com/papers/Cutler15_ICRA.pdf) [[Video]](https://www.youtube.com/watch?v=kKClFx6l1HY)\n  - Antoine Cully, Jeff Clune, Danesh Tarapore and Jean-Baptiste Mouret, Robots that can adapt like animals (Nature 2015) [[ArXiv](https://arxiv.org/abs/1407.3501)] [[Video](https://www.youtube.com/watch?v=T-c17RKh3uE)] [[Code](https://github.com/resibots/cully_2015_nature)]\n  - Konstantinos Chatzilygeroudis, Roberto Rama, Rituraj Kaushik et al, Black-Box Data-efficient Policy Search for Robotics (IROS 2017) [[ArXiv](https://arxiv.org/abs/1703.07261)] [[Video](https://www.youtube.com/watch?v=kTEyYiIFGPM)] [[Code](https://github.com/resibots/blackdrops)]\n  - P. Travis Jardine, Michael Kogan, Sidney N. Givigi and Shahram Yousefi, Adaptive predictive control of a differential drive robot tuned with reinforcement learning (Int J Adapt Control Signal Process 2019) [[DOI]](https://dx.doi.org/10.1002/acs.2882)\n\n\n\n### Control\n  - Pieter Abbeel, Adam Coates, et al., An Application of Reinforcement Learning to Aerobatic Helicopter Flight (NIPS 2006) [[Paper]](http://heli.stanford.edu/papers/nips06-aerobatichelicopter.pdf) [[Video]](https://www.youtube.com/watch?v=VCdxqn0fcnE)\n  - J. Andrew Bagnell and Jeff G. Schneider, Autonomous helicopter control using Reinforcement Learning Policy Search Methods (ICRA 2001) [[Paper]](https://kilthub.cmu.edu/articles/Autonomous_Helicopter_Control_Using_Reinforcement_Learning_Policy_Search_Methods/6552119/files/12033380.pdf)\n\n### Operations Research\n  - Scott Proper and Prasad Tadepalli, Scaling Average-reward Reinforcement Learning for Product Delivery (AAAI 2004) [[Paper]](https://s3.amazonaws.com/academia.edu.documents/44453946/Scaling_Average-reward_Reinforcement_Lea20160405-20758-1wxkm8y.pdf)\n  - Naoki Abe, Naval Verma et al., Cross Channel Optimized Marketing by Reinforcement Learning (KDD 2004) [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.375.151&rep=rep1&type=pdf)\n  - Bernd Waschneck, Andre Reichstaller, Lenz Belzner et al., Deep reinforcement learning for semiconductor production scheduling (ASMC 2018) [[DOI]](https://dx.doi.org/10.1109/ASMC.2018.8373191) [[Paper]](https://www.researchgate.net/profile/Lenz_Belzner/publication/325713164_Deep_reinforcement_learning_for_semiconductor_production_scheduling/links/5be537caa6fdcc3a8dc89fb3/Deep-reinforcement-learning-for-semiconductor-production-scheduling.pdf)\n\n### Human Computer Interaction\n  - Satinder Singh, Diane Litman et al., Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System (JAIR 2002) [[Paper]](http://web.eecs.umich.edu/~baveja/Papers/RLDSjair.pdf)\n\n\n\n## Codes\n - Codes for examples and exercises in Richard Sutton and Andrew Barto's [Book](#books) Reinforcement Learning: An Introduction\n    - [Python Code](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction) (2nd Edition)\n    - [MATLAB Code](https://waxworksmath.com/Authors/N_Z/Sutton/RLAI_1st_Edition/sutton.html) (1st Edition)\n - Simulation code for Reinforcement Learning Control Problems\n    - [Pole-Cart Problem](http://pages.cs.wisc.edu/~finton/poledriver.html)\n    - [Q-learning Controller](http://pages.cs.wisc.edu/~finton/qcontroller.html)\n - [MATLAB Environment and GUI for Reinforcement Learning](http://www.cs.colostate.edu/~anderson/res/rl/matlabpaper/rl.html)\n - [Reinforcement Learning Repository - University of Massachusetts, Amherst](http://www-anw.cs.umass.edu/rlr/)\n - [Brown-UMBC Reinforcement Learning and Planning Library (Java)](http://burlap.cs.brown.edu/)\n - [Reinforcement Learning in R (MDP, Value Iteration)](http://www.moneyscience.com/pg/blog/StatAlgo/read/635759/reinforcement-learning-in-r-markov-decision-process-mdp-and-value-iteration)\n - [Reinforcement Learning Environment in Python and MATLAB](https://jamh-web.appspot.com/download.htm)\n - [RL-Glue](http://glue.rl-community.org/wiki/Main_Page) (standard interface for RL) and [RL-Glue Library](http://library.rl-community.org/wiki/Main_Page)\n - [PyBrain Library](http://www.pybrain.org/) - Python-Based Reinforcement learning, Artificial intelligence, and Neural network\n - [RLPy Framework](http://rlpy.readthedocs.org/en/latest/) -  Value-Function-Based Reinforcement Learning Framework for Education and Research\n - [Maja](http://mmlf.sourceforge.net/) - Machine learning framework for problems in Reinforcement Learning in python\n - [TeachingBox](http://servicerobotik.hs-weingarten.de/en/teachingbox.php) - Java based Reinforcement Learning framework\n - [Policy Gradient Reinforcement Learning Toolbox for MATLAB](http://www.ias.informatik.tu-darmstadt.de/Research/PolicyGradientToolbox)\n - [PIQLE](http://sourceforge.net/projects/piqle/) - Platform Implementing Q-Learning and other RL algorithms\n - [BeliefBox](https://code.google.com/p/beliefbox/) - Bayesian reinforcement learning library and toolkit\n - [Deep Q-Learning with TensorFlow](https://github.com/nivwusquorum/tensorflow-deepq) - A deep Q learning demonstration using Google Tensorflow\n - [Atari](https://github.com/Kaixhin/Atari) - Deep Q-networks and asynchronous agents in Torch\n - [AgentNet](https://github.com/yandexdataschool/AgentNet) - A python library for deep reinforcement learning and custom recurrent networks using Theano+Lasagne.\n - [Reinforcement Learning Examples by RLCode](https://github.com/rlcode/reinforcement-learning) - A Collection of minimal and clean reinforcement learning examples\n - [OpenAI Baselines](https://github.com/openai/baselines) - Well tested implementations ([and results](https://github.com/openai/baselines-results)) of reinforcement learning algorithms from OpenAI \n - [PyTorch Deep RL](https://github.com/ShangtongZhang/DeepRL) - Popular deep RL algorithm implementations with PyTorch\n - [ChainerRL](https://github.com/chainer/chainerrl) - Popular deep RL algorithm implementations with Chainer\n - [Black-DROPS](https://github.com/resibots/blackdrops) - Modular and generic code for the model-based policy search Black-DROPS algorithm (IROS 2017 paper) and easy integration with the [DART](http://dartsim.github.io/) simulator\n \n \n \n ## Tutorials / Websites\n  - Mance Harmon and Stephanie Harmon, [Reinforcement Learning: A Tutorial](http://old.nbu.bg/cogs/events/2000/Readings/Petrov/rltutorial.pdf)\n  - C. Igel, M.A. Riedmiller, et al., Reinforcement Learning in a Nutshell, ESANN, 2007. [[Paper]](http://image.diku.dk/igel/paper/RLiaN.pdf)\n  - UNSW - [Reinforcement Learning](http://www.cse.unsw.edu.au/~cs9417ml/RL1/index.html)\n    - [Introduction](http://www.cse.unsw.edu.au/~cs9417ml/RL1/introduction.html)\n    - [TD-Learning](http://www.cse.unsw.edu.au/~cs9417ml/RL1/tdlearning.html)\n    - [Q-Learning and SARSA](http://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html)\n    - [Applet for \"Cat and Mouse\" Game](http://www.cse.unsw.edu.au/~cs9417ml/RL1/applet.html)\n  - [ROS Reinforcement Learning Tutorial](http://wiki.ros.org/reinforcement_learning/Tutorials/Reinforcement%20Learning%20Tutorial)\n  - [POMDP for Dummies](http://cs.brown.edu/research/ai/pomdp/tutorial/index.html)\n  - Scholarpedia articles on:\n    - [Reinforcement Learning](http://www.scholarpedia.org/article/Reinforcement_learning)\n    - [Temporal Difference Learning](http://www.scholarpedia.org/article/Temporal_difference_learning)\n  - Repository with useful [MATLAB Software, presentations, and demo videos](http://busoniu.net/repository.php)\n  - [Bibliography on Reinforcement Learning](http://liinwww.ira.uka.de/bibliography/Neural/reinforcement.learning.html)\n  - UC Berkeley - CS 294: Deep Reinforcement Learning, Fall 2015 (John Schulman, Pieter Abbeel) [[Class Website]](http://rll.berkeley.edu/deeprlcourse/)\n  - [Blog posts on Reinforcement Learning, Parts 1-4](https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/) by Travis DeWolf\n  - [The Arcade Learning Environment](http://www.arcadelearningenvironment.org/) - Atari 2600 games environment for developing AI agents\n  - [Deep Reinforcement Learning: Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/) by Andrej Karpathy\n  - [Demystifying Deep Reinforcement Learning](https://www.nervanasys.com/demystifying-deep-reinforcement-learning/) \n  - [Let\u2019s make a DQN](https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/) \n  - [Simple Reinforcement Learning with Tensorflow, Parts 0-8](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.78km20i8r) by Arthur Juliani\n  - [Practical_RL](https://github.com/yandexdataschool/Practical_RL) - github-based course in reinforcement learning in the wild (lectures, coding labs, projects)\n  - [RLenv.directory: Explore and find new reinforcement learning environments.](https://rlenv.directory/)\n  - Katja Hofmann's talk at NeurIPS '19 - [RL: Past, Present and Future Perspectives](https://slideslive.com/38922817/reinforcement-learning-past-present-and-future-perspectives)\n\n\n\n## Online Demos\n - [Real-world demonstrations of Reinforcement Learning](http://www.dcsc.tudelft.nl/~robotics/media.html)\n - [Deep Q-Learning Demo](http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html) - A deep Q learning demonstration using ConvNetJS\n - [Deep Q-Learning with Tensor Flow](https://github.com/nivwusquorum/tensorflow-deepq) - A deep Q learning demonstration using Google Tensorflow\n - [Reinforcement Learning Demo](http://cs.stanford.edu/people/karpathy/reinforcejs/) - A reinforcement learning demo using reinforcejs by Andrej Karpathy\n\n\n## Open Source Reinforcement Learning Platforms\n- [OpenAI gym](https://github.com/openai/gym) - A toolkit for developing and comparing reinforcement learning algorithms\n- [OpenAI universe](https://github.com/openai/universe) - A software platform for measuring and training an AI's general intelligence across the world's supply of games, websites and other applications\n- [DeepMind Lab](https://github.com/deepmind/lab) - A customisable 3D platform for agent-based AI research\n- [Project Malmo](https://github.com/Microsoft/malmo) - A platform for Artificial Intelligence experimentation and research built on top of Minecraft by Microsoft\n- [ViZDoom](https://github.com/Marqt/ViZDoom) - Doom-based AI research platform for reinforcement learning from raw visual information\n- [Retro Learning Environment](https://github.com/nadavbh12/Retro-Learning-Environment) - An AI platform for reinforcement learning based on video game emulators. Currently supports SNES and Sega Genesis. Compatible with OpenAI gym.\n- [torch-twrl](https://github.com/twitter/torch-twrl) - A package that enables reinforcement learning in Torch by Twitter\n- [UETorch](https://github.com/facebook/UETorch) - A Torch plugin for Unreal Engine 4 by Facebook\n- [TorchCraft](https://github.com/TorchCraft/TorchCraft) - Connecting Torch to StarCraft\n- [garage](https://github.com/rlworkgroup/garage) - A framework for reproducible reinformcement learning research, fully compatible with OpenAI Gym and DeepMind Control Suite (successor to rllab)\n- [TensorForce](https://github.com/reinforceio/tensorforce) - Practical deep reinforcement learning on TensorFlow with Gitter support and OpenAI Gym/Universe/DeepMind Lab integration.\n- [tf-TRFL](https://github.com/deepmind/trfl/) - A library built on top of TensorFlow that exposes several useful building blocks for implementing Reinforcement Learning agents.\n- [OpenAI lab](https://github.com/kengz/openai_lab) - An experimentation system for Reinforcement Learning using OpenAI Gym, Tensorflow, and Keras.\n- [keras-rl](https://github.com/matthiasplappert/keras-rl) - State-of-the art deep reinforcement learning algorithms in Keras designed for compatibility with OpenAI.\n- [BURLAP](http://burlap.cs.brown.edu) - Brown-UMBC Reinforcement Learning and Planning, a library written in Java\n- [MAgent](https://github.com/geek-ai/MAgent) - A Platform for Many-agent Reinforcement Learning. \n- [Ray RLlib](http://ray.readthedocs.io/en/latest/rllib.html) - Ray RLlib is a reinforcement learning library that aims to provide both performance and composability.\n- [SLM Lab](https://github.com/kengz/SLM-Lab) - A research framework for Deep Reinforcement Learning using Unity, OpenAI Gym, PyTorch, Tensorflow.\n- [Unity ML Agents](https://github.com/Unity-Technologies/ml-agents) - Create reinforcement learning environments using the Unity Editor\n- [Intel Coach](https://github.com/NervanaSystems/coach) - Coach is a python reinforcement learning research framework containing implementation of many state-of-the-art algorithms.\n- [Microsoft AirSim](https://microsoft.github.io/AirSim/docs/reinforcement_learning/) - Open source simulator based on Unreal Engine for autonomous vehicles from Microsoft AI & Research.\n"
 },
 {
  "repo": "umutisik/Eigentechno",
  "language": "Jupyter Notebook",
  "readme_contents": "## Eigentechno\n\nCode for applying Principal Component Analysis to ~ 50 hours of electronic music loops. \n\nSee post:\nhttp://www.math.uci.edu/~isik/posts/Eigentechno.html\n\n"
 },
 {
  "repo": "jpmckinney/tf-idf-similarity",
  "language": "Ruby",
  "readme_contents": "# Ruby Vector Space Model (VSM) with tf\\*idf weights\n\n[![Gem Version](https://badge.fury.io/rb/tf-idf-similarity.svg)](https://badge.fury.io/rb/tf-idf-similarity)\n[![Build Status](https://secure.travis-ci.org/jpmckinney/tf-idf-similarity.png)](https://travis-ci.org/jpmckinney/tf-idf-similarity)\n[![Coverage Status](https://coveralls.io/repos/jpmckinney/tf-idf-similarity/badge.png)](https://coveralls.io/r/jpmckinney/tf-idf-similarity)\n[![Code Climate](https://codeclimate.com/github/jpmckinney/tf-idf-similarity.png)](https://codeclimate.com/github/jpmckinney/tf-idf-similarity)\n\nCalculates the similarity between texts using a [bag-of-words](https://en.wikipedia.org/wiki/Bag_of_words_model) [Vector Space Model](https://en.wikipedia.org/wiki/Vector_space_model) with [Term Frequency-Inverse Document Frequency (tf\\*idf)](https://en.wikipedia.org/wiki/Tf\u2013idf) weights. If your use case demands performance, use [Lucene](http://lucene.apache.org/core/) (see below).\n\n## Usage\n\n```ruby\nrequire 'matrix'\nrequire 'tf-idf-similarity'\n```\n\nCreate a set of documents:\n\n```ruby\ndocument1 = TfIdfSimilarity::Document.new(\"Lorem ipsum dolor sit amet...\")\ndocument2 = TfIdfSimilarity::Document.new(\"Pellentesque sed ipsum dui...\")\ndocument3 = TfIdfSimilarity::Document.new(\"Nam scelerisque dui sed leo...\")\ncorpus = [document1, document2, document3]\n```\n\nCreate a document-term matrix using [Term Frequency-Inverse Document Frequency function](https://en.wikipedia.org/wiki/Tf\u2013idf):\n\n```ruby\nmodel = TfIdfSimilarity::TfIdfModel.new(corpus)\n```\n\nOr, create a document-term matrix using the [Okapi BM25 ranking function](https://en.wikipedia.org/wiki/Okapi_BM25):\n\n```ruby\nmodel = TfIdfSimilarity::BM25Model.new(corpus)\n```\n\nCreate a similarity matrix:\n\n```ruby\nmatrix = model.similarity_matrix\n```\n\nFind the similarity of two documents in the matrix:\n\n```ruby\nmatrix[model.document_index(document1), model.document_index(document2)]\n```\n\nPrint the tf\\*idf values for terms in a document:\n\n```ruby\ntfidf_by_term = {}\ndocument1.terms.each do |term|\n  tfidf_by_term[term] = model.tfidf(document1, term)\nend\nputs tfidf_by_term.sort_by{|_,tfidf| -tfidf}\n```\n\nTokenize a document yourself, for example by excluding stop words:\n\n```ruby\nrequire 'unicode_utils'\ntext = \"Lorem ipsum dolor sit amet...\"\ntokens = UnicodeUtils.each_word(text).to_a - ['and', 'the', 'to']\ndocument1 = TfIdfSimilarity::Document.new(text, :tokens => tokens)\n```\n\nProvide, by yourself, the number of times each term appears and the number of tokens in the document:\n\n```ruby\nrequire 'unicode_utils'\ntext = \"Lorem ipsum dolor sit amet...\"\ntokens = UnicodeUtils.each_word(text).to_a - ['and', 'the', 'to']\nterm_counts = Hash.new(0)\nsize = 0\ntokens.each do |token|\n  # Unless the token is numeric.\n  unless token[/\\A\\d+\\z/]\n    # Remove all punctuation from tokens.\n    term_counts[token.gsub(/\\p{Punct}/, '')] += 1\n    size += 1\n  end\nend\ndocument1 = TfIdfSimilarity::Document.new(text, :term_counts => term_counts, :size => size)\n```\n\nOr, use your own classes for the tokenizer and tokens, like in [this example](https://gist.github.com/satoryu/0183a4eba365cc67e28988a09f3035b3).\n\n[Read the documentation at RubyDoc.info.](http://rubydoc.info/gems/tf-idf-similarity)\n\n## Troubleshooting\n\n```\nNoMethodError: undefined method `[]' for Matrix:Module\n```\n\nThe `matrix` gem conflicts with Ruby's internal `Matrix` module. Don't use the `matrix` gem.\n\n## Speed\n\nInstead of using the Ruby Standard Library's [Matrix](http://www.ruby-doc.org/stdlib-2.0/libdoc/matrix/rdoc/Matrix.html) class, you can use one of the [GNU Scientific Library (GSL)](http://www.gnu.org/software/gsl/), [NArray](http://narray.rubyforge.org/) or [NMatrix](https://github.com/SciRuby/nmatrix) (0.0.9 or greater) gems for faster matrix operations. For example:\n\n    require 'narray'\n    model = TfIdfSimilarity::TfIdfModel.new(corpus, :library => :narray)\n\nNArray seems to have the best performance of the three libraries.\n\nThe NMatrix gem gives access to [Automatically Tuned Linear Algebra Software (ATLAS)](http://math-atlas.sourceforge.net/), which you may know of through [Linear Algebra PACKage (LAPACK)](http://www.netlib.org/lapack/) or [Basic Linear Algebra Subprograms (BLAS)](http://www.netlib.org/blas/). Follow [these instructions](https://github.com/SciRuby/nmatrix#installation) to install the NMatrix gem.\n\n## Extras\n\nYou can access more term frequency, document frequency, and normalization formulas with:\n\n    require 'tf-idf-similarity/extras/document'\n    require 'tf-idf-similarity/extras/tf_idf_model'\n\nThe default tf\\*idf formula follows the [Lucene Conceptual Scoring Formula](http://lucene.apache.org/core/4_0_0/core/org/apache/lucene/search/similarities/TFIDFSimilarity.html).\n\n## Why?\n\nAt the time of writing, no other Ruby gem implemented the tf\\*idf formula used by Lucene, Sphinx and Ferret.\n\n* [rsemantic](https://github.com/josephwilk/rsemantic) now uses the same [term frequency](https://github.com/josephwilk/rsemantic/blob/master/lib/semantic/transform/tf_idf_transform.rb#L14) and [document frequency](https://github.com/josephwilk/rsemantic/blob/master/lib/semantic/transform/tf_idf_transform.rb#L13) formulas as Lucene.\n* [treat](https://github.com/louismullie/treat) offers many term frequency formulas, [one of which](https://github.com/louismullie/treat/blob/master/lib/treat/workers/extractors/tf_idf/native.rb#L13) is the same as Lucene.\n* [similarity](https://github.com/bbcrd/Similarity) uses [cosine normalization](https://github.com/bbcrd/Similarity/blob/master/lib/similarity/term_document_matrix.rb#L23), which corresponds roughly to Lucene.\n\n### Term frequencies\n\n* The [vss](https://github.com/mkdynamic/vss) gem does not normalize the frequency of a term in a document; this occurs frequently in the academic literature, but only to demonstrate why normalization is important.\n* The [tf_idf](https://github.com/reddavis/TF-IDF) and similarity gems normalize the frequency of a term in a document to the number of terms in that document, which never occurs in the literature.\n* The [tf-idf](https://github.com/mchung/tf-idf) gem normalizes the frequency of a term in a document to the number of *unique* terms in that document, which never occurs in the literature.\n\n### Document frequencies\n\n* The vss gem does not normalize the inverse document frequency.\n* The treat, tf_idf, tf-idf and similarity gems use variants of the typical inverse document frequency formula.\n\n### Normalization\n\n* The treat, tf_idf, tf-idf, rsemantic and vss gems have no normalization component.\n\n## Additional adapters\n\nAdapters for the following projects were also considered:\n\n* [Ruby-LAPACK](http://ruby.gfd-dennou.org/products/ruby-lapack/) is a very thin wrapper around LAPACK, which has an opaque Fortran-style naming scheme.\n* [Linalg](https://github.com/quix/linalg) and [RNum](http://rnum.rubyforge.org/) give access to LAPACK from Ruby but are old and unavailable as gems.\n\n## Reference\n\n* [G. Salton and C. Buckley. \"Term Weighting Approaches in Automatic Text Retrieval.\"\" Technical Report. Cornell University, Ithaca, NY, USA. 1987.](http://www.cs.odu.edu/~jbollen/IR04/readings/article1-29-03.pdf)\n* [E. Chisholm and T. G. Kolda. \"New term weighting formulas for the vector space method in information retrieval.\" Technical Report Number ORNL-TM-13756. Oak Ridge National Laboratory, Oak Ridge, TN, USA. 1999.](http://www.sandia.gov/~tgkolda/pubs/bibtgkfiles/ornl-tm-13756.pdf)\n\n## Further Reading\n\nLucene implements many more [similarity functions](http://lucene.apache.org/core/4_0_0/core/org/apache/lucene/search/similarities/Similarity.html), such as:\n\n* a [divergence from randomness (DFR) framework](http://lucene.apache.org/core/4_0_0/core/org/apache/lucene/search/similarities/DFRSimilarity.html)\n* a [framework for the family of information-based models](http://lucene.apache.org/core/4_0_0/core/org/apache/lucene/search/similarities/IBSimilarity.html)\n* a [language model with Bayesian smoothing using Dirichlet priors](http://lucene.apache.org/core/4_0_0/core/org/apache/lucene/search/similarities/LMDirichletSimilarity.html)\n* a [language model with Jelinek-Mercer smoothing](http://lucene.apache.org/core/4_0_0/core/org/apache/lucene/search/similarities/LMJelinekMercerSimilarity.html)\n\nLucene can even [combine similarity measures](http://lucene.apache.org/core/4_0_0/core/org/apache/lucene/search/similarities/MultiSimilarity.html).\n\nCopyright (c) 2012 James McKinney, released under the MIT license\n"
 },
 {
  "repo": "scikit-learn-contrib/lightning",
  "language": "Python",
  "readme_contents": ".. -*- mode: rst -*-\n\n.. image:: https://travis-ci.org/scikit-learn-contrib/lightning.svg?branch=master\n    :target: https://travis-ci.org/scikit-learn-contrib/lightning\n\n.. image:: https://ci.appveyor.com/api/projects/status/mmm0llccmvn5iooq?svg=true\n    :target: https://ci.appveyor.com/project/fabianp/lightning-bpc6r/branch/master\n\n.. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.200504.svg\n   :target: https://doi.org/10.5281/zenodo.200504\n\nlightning\n==========\n\nlightning is a library for large-scale linear classification, regression and\nranking in Python.\n\nHighlights:\n\n- follows the `scikit-learn <http://scikit-learn.org>`_ API conventions\n- supports natively both dense and sparse data representations\n- computationally demanding parts implemented in `Cython <http://cython.org>`_\n\nSolvers supported:\n\n- primal coordinate descent\n- dual coordinate descent (SDCA, Prox-SDCA)\n- SGD, AdaGrad, SAG, SAGA, SVRG\n- FISTA\n\nExample\n-------\n\nExample that shows how to learn a multiclass classifier with group lasso\npenalty on the News20 dataset (c.f., `Blondel et al. 2013\n<http://www.mblondel.org/publications/mblondel-mlj2013.pdf>`_):\n\n.. code-block:: python\n\n    from sklearn.datasets import fetch_20newsgroups_vectorized\n    from lightning.classification import CDClassifier\n\n    # Load News20 dataset from scikit-learn.\n    bunch = fetch_20newsgroups_vectorized(subset=\"all\")\n    X = bunch.data\n    y = bunch.target\n\n    # Set classifier options.\n    clf = CDClassifier(penalty=\"l1/l2\",\n                       loss=\"squared_hinge\",\n                       multiclass=True,\n                       max_iter=20,\n                       alpha=1e-4,\n                       C=1.0 / X.shape[0],\n                       tol=1e-3)\n\n    # Train the model.\n    clf.fit(X, y)\n\n    # Accuracy\n    print(clf.score(X, y))\n\n    # Percentage of selected features\n    print(clf.n_nonzero(percentage=True))\n\nDependencies\n------------\n\nlightning requires Python >= 2.7, setuptools, Numpy >= 1.3, SciPy >= 0.7 and\nscikit-learn >= 0.15. Building from source also requires Cython and a working C/C++ compiler. To run the tests you will also need nose >= 0.10.\n\nInstallation\n------------\n\nPrecompiled binaries for the stable version of lightning are available for the main platforms and can be installed using pip::\n\n    pip install sklearn-contrib-lightning\n\nor conda::\n\n    conda install -c conda-forge sklearn-contrib-lightning\n\n\nThe development version of lightning can be installed from its git repository. In this case it is assumed that you have the git version control system, a working C++ compiler, Cython and the numpy development libraries. In order to install the development version, type::\n\n  git clone https://github.com/scikit-learn-contrib/lightning.git\n  cd lightning\n  python setup.py build\n  sudo python setup.py install\n\nDocumentation\n-------------\n\nhttp://contrib.scikit-learn.org/lightning/\n\nOn Github\n---------\n\nhttps://github.com/scikit-learn-contrib/lightning\n\n\nCiting\n------\n\nIf you use this software, please cite it. Here is a BibTex snippet that you can use:\n\n\n.. code-block::\n\n  @misc{lightning_2016,\n    author       = {Blondel, Mathieu and\n                    Pedregosa, Fabian},\n    title        = {{Lightning: large-scale linear classification,\n                   regression and ranking in Python}},\n    year         = 2016,\n    doi          = {10.5281/zenodo.200504},\n    url          = {https://doi.org/10.5281/zenodo.200504}\n  }\n\n\nOther citing formats are available in `its Zenodo entry <https://doi.org/10.5281/zenodo.200504>`_ .\n\nAuthors\n-------\n\n- Mathieu Blondel, 2012-present\n- Manoj Kumar, 2015-present\n- Arnaud Rachez, 2016-present\n- Fabian Pedregosa, 2016-present\n"
 },
 {
  "repo": "gwding/draw_convnet",
  "language": "Python",
  "readme_contents": "# draw_convnet\n\nPython script for illustrating Convolutional Neural Network (ConvNet)\n\n## Example image\nWith `flag_omit=False`\n![](https://raw.githubusercontent.com/gwding/draw_convnet/master/convnet_fig.png)\n\nWith `flag_omit=True`\n![](https://raw.githubusercontent.com/gwding/draw_convnet/ccaa14e2f8e41580bd29b97a501f7a4218779356/convnet_fig_with_omitted_channel.png)\n\n\n## Known issues\nThe issue with matplotlib 2.0.x has been resolved, please let me know if you encounter problems.\n\n~~`Line2D` doesn't seem to work well under `python3 + matplotlib 2.0.0` as pointed out by @ahoereth~~\n\n## Using the code\nIt is NOT required to cite anything to use the code.\n\nIf you are not facing space limitation and it does not break the flow of the paper, you might consider adding something like \"This figure is generated by adapting the code from https://github.com/gwding/draw_convnet\" (maybe in the footnote).\n\nFYI, originally I used the code to generate the convnet figure in this paper \"Automatic moth detection from trap images for pest management\".\n"
 },
 {
  "repo": "scikit-learn/scikit-learn",
  "language": "Python",
  "readme_contents": ".. -*- mode: rst -*-\n\n|Azure|_ |Travis|_ |Codecov|_ |CircleCI|_ |Nightly wheels|_ |PythonVersion|_ |PyPi|_ |DOI|_\n\n.. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=master\n.. _Azure: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=master\n\n.. |Travis| image:: https://api.travis-ci.com/scikit-learn/scikit-learn.svg?branch=master\n.. _Travis: https://travis-ci.com/scikit-learn/scikit-learn\n\n.. |Codecov| image:: https://codecov.io/github/scikit-learn/scikit-learn/badge.svg?branch=master&service=github\n.. _Codecov: https://codecov.io/github/scikit-learn/scikit-learn?branch=master\n\n.. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/master.svg?style=shield&circle-token=:circle-token\n.. _CircleCI: https://circleci.com/gh/scikit-learn/scikit-learn\n\n.. |Nightly wheels| image:: https://github.com/scikit-learn/scikit-learn/workflows/Wheel%20builder/badge.svg?event=schedule\n.. _`Nightly wheels`: https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule\n\n.. |PythonVersion| image:: https://img.shields.io/badge/python-3.6%20%7C%203.7%20%7C%203.8-blue\n.. _PythonVersion: https://img.shields.io/badge/python-3.6%20%7C%203.7%20%7C%203.8-blue\n\n.. |PyPi| image:: https://badge.fury.io/py/scikit-learn.svg\n.. _PyPi: https://badge.fury.io/py/scikit-learn\n\n.. |DOI| image:: https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg\n.. _DOI: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn\n\n.. |PythonMinVersion| replace:: 3.6\n.. |NumPyMinVersion| replace:: 1.13.3\n.. |SciPyMinVersion| replace:: 0.19.1\n.. |JoblibMinVersion| replace:: 0.11\n.. |ThreadpoolctlMinVersion| replace:: 2.0.0\n.. |MatplotlibMinVersion| replace:: 2.1.1\n.. |Scikit-ImageMinVersion| replace:: 0.13\n.. |PandasMinVersion| replace:: 0.25.0\n.. |SeabornMinVersion| replace:: 0.9.0\n.. |PytestMinVersion| replace:: 5.0.1\n\n.. image:: doc/logos/scikit-learn-logo.png\n  :target: https://scikit-learn.org/\n\n**scikit-learn** is a Python module for machine learning built on top of\nSciPy and is distributed under the 3-Clause BSD license.\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https://scikit-learn.org/dev/about.html#authors>`__ page\nfor a list of core contributors.\n\nIt is currently maintained by a team of volunteers.\n\nWebsite: https://scikit-learn.org\n\nInstallation\n------------\n\nDependencies\n~~~~~~~~~~~~\n\nscikit-learn requires:\n\n- Python (>= |PythonMinVersion|)\n- NumPy (>= |NumPyMinVersion|)\n- SciPy (>= |SciPyMinVersion|)\n- joblib (>= |JoblibMinVersion|)\n- threadpoolctl (>= |ThreadpoolctlMinVersion|)\n\n=======\n\n**Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.**\nscikit-learn 0.23 and later require Python 3.6 or newer.\n\nScikit-learn plotting capabilities (i.e., functions start with ``plot_`` and\nclasses end with \"Display\") require Matplotlib (>= |MatplotlibMinVersion|).\nFor running the examples Matplotlib >= |MatplotlibMinVersion| is required.\nA few examples require scikit-image >= |Scikit-ImageMinVersion|, a few examples\nrequire pandas >= |PandasMinVersion|, some examples require seaborn >=\n|SeabornMinVersion|.\n\nUser installation\n~~~~~~~~~~~~~~~~~\n\nIf you already have a working installation of numpy and scipy,\nthe easiest way to install scikit-learn is using ``pip``   ::\n\n    pip install -U scikit-learn\n\nor ``conda``::\n\n    conda install -c conda-forge scikit-learn\n\nThe documentation includes more detailed `installation instructions <https://scikit-learn.org/stable/install.html>`_.\n\n\nChangelog\n---------\n\nSee the `changelog <https://scikit-learn.org/dev/whats_new.html>`__\nfor a history of notable changes to scikit-learn.\n\nDevelopment\n-----------\n\nWe welcome new contributors of all experience levels. The scikit-learn\ncommunity goals are to be helpful, welcoming, and effective. The\n`Development Guide <https://scikit-learn.org/stable/developers/index.html>`_\nhas detailed information about contributing code, documentation, tests, and\nmore. We've included some basic information in this README.\n\nImportant links\n~~~~~~~~~~~~~~~\n\n- Official source code repo: https://github.com/scikit-learn/scikit-learn\n- Download releases: https://pypi.org/project/scikit-learn/\n- Issue tracker: https://github.com/scikit-learn/scikit-learn/issues\n\nSource code\n~~~~~~~~~~~\n\nYou can check the latest sources with the command::\n\n    git clone https://github.com/scikit-learn/scikit-learn.git\n\nContributing\n~~~~~~~~~~~~\n\nTo learn more about making a contribution to scikit-learn, please see our\n`Contributing guide\n<https://scikit-learn.org/dev/developers/contributing.html>`_.\n\nTesting\n~~~~~~~\n\nAfter installation, you can launch the test suite from outside the source\ndirectory (you will need to have ``pytest`` >= |PyTestMinVersion| installed)::\n\n    pytest sklearn\n\nSee the web page https://scikit-learn.org/dev/developers/advanced_installation.html#testing\nfor more information.\n\n    Random number generation can be controlled during testing by setting\n    the ``SKLEARN_SEED`` environment variable.\n\nSubmitting a Pull Request\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBefore opening a Pull Request, have a look at the\nfull Contributing page to make sure your code complies\nwith our guidelines: https://scikit-learn.org/stable/developers/index.html\n\nProject History\n---------------\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https://scikit-learn.org/dev/about.html#authors>`__ page\nfor a list of core contributors.\n\nThe project is currently maintained by a team of volunteers.\n\n**Note**: `scikit-learn` was previously referred to as `scikits.learn`.\n\nHelp and Support\n----------------\n\nDocumentation\n~~~~~~~~~~~~~\n\n- HTML documentation (stable release): https://scikit-learn.org\n- HTML documentation (development version): https://scikit-learn.org/dev/\n- FAQ: https://scikit-learn.org/stable/faq.html\n\nCommunication\n~~~~~~~~~~~~~\n\n- Mailing list: https://mail.python.org/mailman/listinfo/scikit-learn\n- IRC channel: ``#scikit-learn`` at ``webchat.freenode.net``\n- Gitter: https://gitter.im/scikit-learn/scikit-learn\n- Twitter: https://twitter.com/scikit_learn\n- Stack Overflow: https://stackoverflow.com/questions/tagged/scikit-learn\n- Website: https://scikit-learn.org\n\nCitation\n~~~~~~~~\n\nIf you use scikit-learn in a scientific publication, we would appreciate citations: https://scikit-learn.org/stable/about.html#citing-scikit-learn\n"
 },
 {
  "repo": "tensorflow/tensorflow",
  "language": "C++",
  "readme_contents": "<div align=\"center\">\n  <img src=\"https://www.tensorflow.org/images/tf_logo_social.png\">\n</div>\n\n[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic)](https://badge.fury.io/py/tensorflow)\n[![PyPI](https://badge.fury.io/py/tensorflow.svg)](https://badge.fury.io/py/tensorflow)\n\n**`Documentation`** |\n------------------- |\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/) |\n\n[TensorFlow](https://www.tensorflow.org/) is an end-to-end open source platform\nfor machine learning. It has a comprehensive, flexible ecosystem of\n[tools](https://www.tensorflow.org/resources/tools),\n[libraries](https://www.tensorflow.org/resources/libraries-extensions), and\n[community](https://www.tensorflow.org/community) resources that lets\nresearchers push the state-of-the-art in ML and developers easily build and\ndeploy ML-powered applications.\n\nTensorFlow was originally developed by researchers and engineers working on the\nGoogle Brain team within Google's Machine Intelligence Research organization to\nconduct machine learning and deep neural networks research. The system is\ngeneral enough to be applicable in a wide variety of other domains, as well.\n\nTensorFlow provides stable [Python](https://www.tensorflow.org/api_docs/python)\nand [C++](https://www.tensorflow.org/api_docs/cc) APIs, as well as\nnon-guaranteed backward compatible API for\n[other languages](https://www.tensorflow.org/api_docs).\n\nKeep up-to-date with release announcements and security updates by subscribing\nto\n[announce@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/announce).\nSee all the [mailing lists](https://www.tensorflow.org/community/forums).\n\n## Install\n\nSee the [TensorFlow install guide](https://www.tensorflow.org/install) for the\n[pip package](https://www.tensorflow.org/install/pip), to\n[enable GPU support](https://www.tensorflow.org/install/gpu), use a\n[Docker container](https://www.tensorflow.org/install/docker), and\n[build from source](https://www.tensorflow.org/install/source).\n\nTo install the current release, which includes support for\n[CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and\nWindows)*:\n\n```\n$ pip install tensorflow\n```\n\nA smaller CPU-only package is also available:\n\n```\n$ pip install tensorflow-cpu\n```\n\nTo update TensorFlow to the latest version, add `--upgrade` flag to the above\ncommands.\n\n*Nightly binaries are available for testing using the\n[tf-nightly](https://pypi.python.org/pypi/tf-nightly) and\n[tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPi.*\n\n#### *Try your first TensorFlow program*\n\n```shell\n$ python\n```\n\n```python\n>>> import tensorflow as tf\n>>> tf.add(1, 2).numpy()\n3\n>>> hello = tf.constant('Hello, TensorFlow!')\n>>> hello.numpy()\nb'Hello, TensorFlow!'\n```\n\nFor more examples, see the\n[TensorFlow tutorials](https://www.tensorflow.org/tutorials/).\n\n## Contribution guidelines\n\n**If you want to contribute to TensorFlow, be sure to review the\n[contribution guidelines](CONTRIBUTING.md). This project adheres to TensorFlow's\n[code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.**\n\n**We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for\ntracking requests and bugs, please see\n[TensorFlow Discuss](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss)\nfor general questions and discussion, and please direct specific questions to\n[Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).**\n\nThe TensorFlow project strives to abide by generally accepted best practices in\nopen-source software development:\n\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1486/badge)](https://bestpractices.coreinfrastructure.org/projects/1486)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)\n\n## Continuous build status\n\n### Official Builds\n\nBuild Type                    | Status                                                                                                                                                                                             | Artifacts\n----------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------\n**Linux CPU**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html)                             | [PyPI](https://pypi.org/project/tf-nightly/)\n**Linux GPU**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html)                   | [PyPI](https://pypi.org/project/tf-nightly-gpu/)\n**Linux XLA**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html)                           | TBA\n**macOS**                     | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html)                       | [PyPI](https://pypi.org/project/tf-nightly/)\n**Windows CPU**               | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html)                         | [PyPI](https://pypi.org/project/tf-nightly/)\n**Windows GPU**               | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html)                         | [PyPI](https://pypi.org/project/tf-nightly-gpu/)\n**Android**                   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html)                                 | [![Download](https://api.bintray.com/packages/google/tensorflow/tensorflow/images/download.svg)](https://bintray.com/google/tensorflow/tensorflow/_latestVersion)\n**Raspberry Pi 0 and 1**      | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html)                             | [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl)\n**Raspberry Pi 2 and 3**      | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html)                             | [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl)\n**Libtensorflow MacOS CPU**   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/libtensorflow-mac-cpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/libtensorflow-mac-cpu.html)     | [Nightly GCS](https://storage.googleapis.com/libtensorflow-nightly) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Linux CPU**   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/libtensorflow-linux-cpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/libtensorflow-linux-cpu.html) | [Nightly GCS](https://storage.googleapis.com/libtensorflow-nightly) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Linux GPU**   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/libtensorflow-linux-gpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/libtensorflow-linux-gpu.html) | [Nightly GCS](https://storage.googleapis.com/libtensorflow-nightly) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Windows CPU** | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/libtensorflow-win-cpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/libtensorflow-win-cpu.html)     | [Nightly GCS](https://storage.googleapis.com/libtensorflow-nightly) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Windows GPU** | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/libtensorflow-win-gpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/libtensorflow-win-gpu.html)     | [Nightly GCS](https://storage.googleapis.com/libtensorflow-nightly) [Official GCS](https://storage.googleapis.com/tensorflow/)\n\n### Community Supported Builds\n\nBuild Type                                                                          | Status                                                                                                                                                                                                                                                                                                                                                                                              | Artifacts\n----------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------\n**Linux AMD ROCm GPU** Nightly                                                      | [![Build Status](http://ml-ci.amd.com:21096/job/tensorflow-rocm-nightly/badge/icon)](http://ml-ci.amd.com:21096/job/tensorflow-rocm-nightly)                                                                                                                                                                                                                                                        | [Nightly](http://ml-ci.amd.com:21096/job/tensorflow-rocm-nightly/lastSuccessfulBuild/)\n**Linux AMD ROCm GPU** Stable Release                                               | [![Build Status](http://ml-ci.amd.com:21096/job/tensorflow-rocm-release/badge/icon)](http://ml-ci.amd.com:21096/job/tensorflow-rocm-release/)                                                                                                                                                                                                                                                       | Release [1.15](http://ml-ci.amd.com:21096/job/tensorflow-rocm-release/lastSuccessfulBuild/) / [2.x](http://ml-ci.amd.com:21096/job/tensorflow-rocm-v2-release/lastSuccessfulBuild/)\n**Linux s390x** Nightly                                                             | [![Build Status](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/badge/icon)](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/)                                                                                                                                                                                                                                                                   | [Nightly](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/)\n**Linux s390x CPU** Stable Release                                                  | [![Build Status](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/badge/icon)](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/)                                                                                                                                                                                                                                            | [Release](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/)\n**Linux ppc64le CPU** Nightly                                                       | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/)                                                                                                                                                                                                                                             | [Nightly](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Nightly_Artifact/)\n**Linux ppc64le CPU** Stable Release                                                | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/)                                                                                                                                                                                                                             | Release [1.15](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/) / [2.x](https://powerci.osuosl.org/job/TensorFlow2_PPC64LE_CPU_Release_Build/)\n**Linux ppc64le GPU** Nightly                                                       | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Build/)                                                                                                                                                                                                                                             | [Nightly](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Nightly_Artifact/)\n**Linux ppc64le GPU** Stable Release                                                | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/)                                                                                                                                                                                                                             | Release [1.15](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/) / [2.x](https://powerci.osuosl.org/job/TensorFlow2_PPC64LE_GPU_Release_Build/)\n**Linux aarch64 CPU** Nightly (Linaro)<br> Python 3.8                               | [![Build Status](https://ci.linaro.org/jenkins/buildStatus/icon?job=ldcg-hpc-tensorflow)](https://ci.linaro.org/jenkins/job/ldcg-hpc-tensorflow/)                                                                                                                                                                                                                                                   | [Nightly](http://snapshots.linaro.org/hpc/python/tensorflow/latest/)\n**Linux aarch64 CPU** Stable Release (Linaro)                                       | [![Build Status](https://ci.linaro.org/jenkins/buildStatus/icon?job=ldcg-hpc-tensorflow)](https://ci.linaro.org/jenkins/job/ldcg-hpc-tensorflow/)                                                                                                                                                                                                                                                   | Release [1.x & 2.x](http://snapshots.linaro.org/hpc/python/tensorflow/latest/)\n**Linux aarch64 CPU** Nightly (OpenLab)<br> Python 3.6                              | [![Build Status](http://openlabtesting.org:15000/badge?project=tensorflow%2Ftensorflow)](https://status.openlabtesting.org/builds/builds?project=tensorflow%2Ftensorflow&job_name=tensorflow-arm64-build-daily-master)                                                                                                                                                                              | [Nightly](https://status.openlabtesting.org/builds/builds?project=tensorflow%2Ftensorflow&job_name=tensorflow-arm64-build-daily-master)\n**Linux aarch64 CPU** Stable Release (OpenLab)                                      | [![Build Status](http://openlabtesting.org:15000/badge?project=tensorflow%2Ftensorflow&job_name=tensorflow-v1.15.3-cpu-arm64-release-build-show&job_name=tensorflow-v2.1.0-cpu-arm64-release-build-show)](http://status.openlabtesting.org/builds?project=tensorflow%2Ftensorflow&job_name=tensorflow-v2.1.0-cpu-arm64-release-build-show&job_name=tensorflow-v1.15.3-cpu-arm64-release-build-show) | Release [1.15](http://status.openlabtesting.org/builds?project=tensorflow%2Ftensorflow&job_name=tensorflow-v1.15.3-cpu-arm64-release-build-show) / [2.x](http://status.openlabtesting.org/builds?project=tensorflow%2Ftensorflow&job_name=tensorflow-v2.1.0-cpu-arm64-release-build-show)\n**Linux CPU with Intel oneAPI Deep Neural Network Library (oneDNN)** Nightly        | [![Build Status](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/badge/icon)](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/)                                                                                                                                                                                                                           | [Nightly](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/)\n**Linux CPU with Intel oneAPI Deep Neural Network Library (oneDNN)** Stable Release | ![Build Status](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-release-whl/badge/icon)                                                                                                                                                                                                                                                                                                    | Release [1.15](https://pypi.org/project/intel-tensorflow/1.15.0/) / [2.x](https://pypi.org/project/intel-tensorflow/)\n**Red Hat\u00ae Enterprise Linux\u00ae 7.6 CPU & GPU** <br> Python 2.7, 3.6                   | [![Build Status](https://jenkins-tensorflow.apps.ci.centos.org/buildStatus/icon?job=tensorflow-rhel7-3.6&build=2)](https://jenkins-tensorflow.apps.ci.centos.org/job/tensorflow-rhel7-3.6/2/)                                                                                                                                                                                                       | [1.13.1 PyPI](https://tensorflow.pypi.thoth-station.ninja/index/)\n\n### Community Supported Containers\n\nContainer Type                                                    | Status | Artifacts\n----------------------------------------------------------------- | ------ | ---------\n**TensorFlow aarch64 Neoverse-N1 CPU** Stable (Linaro)<br> Debian | Static | Release [2.3](https://hub.docker.com/r/linaro/tensorflow-arm-neoverse-n1)\n\n## Resources\n\n*   [TensorFlow.org](https://www.tensorflow.org)\n*   [TensorFlow Tutorials](https://www.tensorflow.org/tutorials/)\n*   [TensorFlow Official Models](https://github.com/tensorflow/models/tree/master/official)\n*   [TensorFlow Examples](https://github.com/tensorflow/examples)\n*   [DeepLearning.AI TensorFlow Developer Professional Certificate](https://www.coursera.org/specializations/tensorflow-in-practice)\n*   [TensorFlow: Data and Deployment from Coursera](https://www.coursera.org/specializations/tensorflow-data-and-deployment)\n*   [Getting Started with TensorFlow 2 from Coursera](https://www.coursera.org/learn/getting-started-with-tensor-flow2)\n*   [Intro to TensorFlow for Deep Learning from Udacity](https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187)\n*   [Introduction to TensorFlow Lite from Udacity](https://www.udacity.com/course/intro-to-tensorflow-lite--ud190)\n*   [Machine Learning with TensorFlow on GCP](https://www.coursera.org/specializations/machine-learning-tensorflow-gcp)\n*   [TensorFlow Codelabs](https://codelabs.developers.google.com/?cat=TensorFlow)\n*   [TensorFlow Blog](https://blog.tensorflow.org)\n*   [Learn ML with TensorFlow](https://www.tensorflow.org/resources/learn-ml)\n*   [TensorFlow Twitter](https://twitter.com/tensorflow)\n*   [TensorFlow YouTube](https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ)\n*   [TensorFlow Roadmap](https://www.tensorflow.org/model_optimization/guide/roadmap)\n*   [TensorFlow White Papers](https://www.tensorflow.org/about/bib)\n*   [TensorBoard Visualization Toolkit](https://github.com/tensorflow/tensorboard)\n\nLearn more about the\n[TensorFlow community](https://www.tensorflow.org/community) and how to\n[contribute](https://www.tensorflow.org/community/contribute).\n\n## License\n\n[Apache License 2.0](LICENSE)\n"
 },
 {
  "repo": "Theano/Theano",
  "language": "Python",
  "readme_contents": "============================================================================================================\n`MILA will stop developing Theano <https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ>`_.\n\nThe PyMC developers are continuing Theano development in a `fork <https://github.com/pymc-devs/theano-pymc>`_.\n============================================================================================================\n\n\nTo install the package, see this page:\n   http://deeplearning.net/software/theano/install.html\n\nFor the documentation, see the project website:\n   http://deeplearning.net/software/theano/\n\nRelated Projects:\n   https://github.com/Theano/Theano/wiki/Related-projects\n\nIt is recommended that you look at the documentation on the website, as it will be more current than the documentation included with the package.\n\nIn order to build the documentation yourself, you will need sphinx. Issue the following command:\n\n::\n\n   python ./doc/scripts/docgen.py\n\nDocumentation is built into ``html/``\n\nThe PDF of the documentation can be found at ``html/theano.pdf``\n\n================\nDIRECTORY LAYOUT\n================\n\n``Theano`` (current directory) is the distribution directory.\n\n* ``Theano/theano`` contains the package\n* ``Theano/theano`` has several submodules:\n \n  * ``gof`` + ``compile`` are the core\n  * ``scalar`` depends upon core\n  * ``tensor`` depends upon ``scalar``\n  * ``sparse`` depends upon ``tensor``\n  * ``sandbox`` can depend on everything else\n\n* ``Theano/examples`` are copies of the example found on the wiki\n* ``Theano/benchmark`` and ``Theano/examples`` are in the distribution, but not in\n  the Python package\n* ``Theano/bin`` contains executable scripts that are copied to the bin folder\n  when the Python package is installed\n* Tests are distributed and are part of the package, i.e. fall in\n  the appropriate submodules\n* ``Theano/doc`` contains files and scripts used to generate the documentation\n* ``Theano/html`` is where the documentation will be generated\n"
 },
 {
  "repo": "shogun-toolbox/shogun",
  "language": "C++",
  "readme_contents": "# The SHOGUN machine learning toolbox\n-------------------------------------\n\nUnified and efficient Machine Learning since 1999.\n\nLatest release:\n\n[![Release](https://img.shields.io/github/release/shogun-toolbox/shogun.svg)](https://github.com/shogun-toolbox/shogun/releases/latest)\n\nCite Shogun:\n\n[![DOI](https://zenodo.org/badge/1555094.svg)](https://zenodo.org/badge/latestdoi/1555094)\n\nDevelop branch build status:\n\n[![Build status](https://dev.azure.com/shogunml/shogun/_apis/build/status/shogun-CI)](https://dev.azure.com/shogunml/shogun/_build/latest?definitionId=-1)\n[![codecov](https://codecov.io/gh/shogun-toolbox/shogun/branch/develop/graph/badge.svg)](https://codecov.io/gh/shogun-toolbox/shogun)\n\nDonate to Shogun via NumFocus:\n\n[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](http://numfocus.org)\n\n\nBuildbot: https://buildbot.shogun.ml.\n\n * See [doc/readme/ABOUT.md](doc/readme/ABOUT.md) for a project description.\n * See [doc/readme/INSTALL.md](doc/readme/INSTALL.md) for installation instructions.\n * See [doc/readme/INTERFACES.md](doc/readme/INTERFACES.md) for calling Shogun from its interfaces.\n * See [doc/readme/EXAMPLES.md](doc/readme/EXAMPLES.md) for details on creating API examples.\n * See [doc/readme/DEVELOPING.md](doc/readme/DEVELOPING.md) for how to hack Shogun.\n \n * See [API examples](http://shogun.ml/examples) for all interfaces.\n * See [the wiki](https://github.com/shogun-toolbox/shogun/wiki/) for extended developer information.\n\n## Interfaces\n-------------\n\nShogun is implemented in C++ and offers automatically generated, unified interfaces to Python, Octave, Java / Scala, Ruby, C#, R, Lua. We are currently working on adding more languages including JavaScript, D, and Matlab.\n\n|    Interface     |     Status                                                |\n|:----------------:|-----------------------------------------------------------|\n|Python            | *mature* (no known problems)                              |\n|Octave            | *mature* (no known problems)                              |\n|Java/Scala        | *stable* (no known problems)                              |\n|Ruby              | *stable* (no known problems)                              |\n|C#                | *stable* (no known problems)                              |\n|R                 | *beta*   (most examples work, static calls unavailable)   |\n|Perl              | *pre-alpha* (work in progress quality)                    |\n|JS                | *pre-alpha* (work in progress quality)                    |\n\nSee [our website](http://shogun.ml/examples) for examples in all languages.\n\n## Platforms\n------------\n\nShogun is supported under GNU/Linux, MacOSX, FreeBSD, and Windows.\n\n## Directory Contents\n---------------------\n\nThe following directories are found in the source distribution.\nNote that some folders are submodules that can be checked out with\n`git submodule update --init`.\n\n- *src* - source code, separated into C++ source and interfaces\n- *doc* - readmes (doc/readme, submodule), Jupyter notebooks, cookbook (API examples), licenses\n- *examples* - example files for all interfaces\n- *data* - data sets (submodule, required for examples)\n- *tests* - unit tests and continuous integration of interface examples\n- *applications* - applications of SHOGUN (outdated)\n- *benchmarks* - speed benchmarks\n- *cmake* - cmake build scripts\n\n## License\n----------\nShogun is distributed under [BSD 3-clause license](doc/license/LICENSE.md), with\noptional GPL3 components.\nSee [doc/licenses](doc/license) for details.\n"
 },
 {
  "repo": "davisking/dlib",
  "language": "C++",
  "readme_contents": "# dlib C++ library [![Travis Status](https://travis-ci.org/davisking/dlib.svg?branch=master)](https://travis-ci.org/davisking/dlib)\n\nDlib is a modern C++ toolkit containing machine learning algorithms and tools for creating complex software in C++ to solve real world problems. See [http://dlib.net](http://dlib.net) for the main project documentation and API reference.\n\n\n\n## Compiling dlib C++ example programs\n\nGo into the examples folder and type:\n\n```bash\nmkdir build; cd build; cmake .. ; cmake --build .\n```\n\nThat will build all the examples.\nIf you have a CPU that supports AVX instructions then turn them on like this:\n\n```bash\nmkdir build; cd build; cmake .. -DUSE_AVX_INSTRUCTIONS=1; cmake --build .\n```\n\nDoing so will make some things run faster.\n\nFinally, Visual Studio users should usually do everything in 64bit mode.  By default Visual Studio is 32bit, both in its outputs and its own execution, so you have to explicitly tell it to use 64bits.  Since it's not the 1990s anymore you probably want to use 64bits.  Do that with a cmake invocation like this:\n```bash\ncmake .. -G \"Visual Studio 14 2015 Win64\" -T host=x64 \n```\n\n## Compiling your own C++ programs that use dlib\n\nThe examples folder has a [CMake tutorial](https://github.com/davisking/dlib/blob/master/examples/CMakeLists.txt) that tells you what to do.  There are also additional instructions on the [dlib web site](http://dlib.net/compile.html).\n\nAlternatively, if you are using the [vcpkg](https://github.com/Microsoft/vcpkg/) dependency manager you can download and install dlib with CMake integration in a single command:\n```bash\nvcpkg install dlib\n```\n\n## Compiling dlib Python API\n\nBefore you can run the Python example programs you must compile dlib. Type:\n\n```bash\npython setup.py install\n```\n\n\n## Running the unit test suite\n\nType the following to compile and run the dlib unit test suite:\n\n```bash\ncd dlib/test\nmkdir build\ncd build\ncmake ..\ncmake --build . --config Release\n./dtest --runall\n```\n\nNote that on windows your compiler might put the test executable in a subfolder called `Release`. If that's the case then you have to go to that folder before running the test.\n\nThis library is licensed under the Boost Software License, which can be found in [dlib/LICENSE.txt](https://github.com/davisking/dlib/blob/master/dlib/LICENSE.txt).  The long and short of the license is that you can use dlib however you like, even in closed source commercial software.\n\n## dlib sponsors\n\nThis research is based in part upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA) under contract number 2014-14071600010. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ODNI, IARPA, or the U.S. Government.\n\n"
 },
 {
  "repo": "apache/predictionio",
  "language": "Scala",
  "readme_contents": "<!--\nLicensed to the Apache Software Foundation (ASF) under one or more\ncontributor license agreements.  See the NOTICE file distributed with\nthis work for additional information regarding copyright ownership.\nThe ASF licenses this file to You under the Apache License, Version 2.0\n(the \"License\"); you may not use this file except in compliance with\nthe License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# [Apache PredictionIO](http://predictionio.apache.org)\n\n[![Build\nStatus](https://api.travis-ci.org/apache/predictionio.svg?branch=develop)](https://travis-ci.org/apache/predictionio)\n\nApache PredictionIO is an open source machine learning framework\nfor developers, data scientists, and end users. It supports event collection,\ndeployment of algorithms, evaluation, querying predictive results via REST APIs.\nIt is based on scalable open source services like Hadoop, HBase (and other DBs),\nElasticsearch, Spark and implements what is called a Lambda Architecture.\n\nTo get started, check out http://predictionio.apache.org!\n\n\n## Table of contents\n- [Installation](#installation)\n- [Quick Start](#quick-start)\n- [Bugs and Feature Requests](#bugs-and-feature-requests)\n- [Documentation](#documentation)\n- [Contributing](#contributing)\n- [Community](#community)\n\n\n## Installation\n\nA few installation options available.\n\n*   [Installing Apache PredictionIO from\n    Binary/Source](http://predictionio.apache.org/install/install-sourcecode/)\n*   [Installing Apache PredictionIO with\n    Docker](http://predictionio.apache.org/install/install-docker/)\n\n## Quick Start\n\n*   [Recommendation Engine Template Quick\n    Start](http://predictionio.apache.org/templates/recommendation/quickstart/)\n    Guide\n*   [Similiar Product Engine Template Quick\n    Start](http://predictionio.apache.org/templates/similarproduct/quickstart/)\n    Guide\n*   [Classification Engine Template Quick\n    Start](http://predictionio.apache.org/templates/classification/quickstart/)\n    Guide\n\n\n## Bugs and Feature Requests\n\nUse [Apache JIRA](https://issues.apache.org/jira/browse/PIO) to report bugs or request new features.\n\n## Documentation\n\nDocumentation, included in this repo in the `docs/manual` directory, is built\nwith [Middleman](http://middlemanapp.com/) and publicly hosted at\n[predictionio.apache.org](http://predictionio.apache.org/).\n\nInterested in helping with our documentation? Read [Contributing\nDocumentation](http://predictionio.apache.org/community/contribute-documentation/).\n\n\n## Community\n\nKeep track of development and community news.\n\n*   Subscribe to the user mailing list <mailto:user-subscribe@predictionio.apache.org>\n    and the dev mailing list <mailto:dev-subscribe@predictionio.apache.org>\n*   Follow [@predictionio](https://twitter.com/predictionio) on Twitter.\n\n\n## Contributing\n\nRead the [Contribute Code](http://predictionio.apache.org/community/contribute-code/) page.\n\nYou can also list your projects on the [Community Project\npage](http://predictionio.apache.org//community/projects/).\n\n\n## License\n\nApache PredictionIO is under [Apache 2\nlicense](http://www.apache.org/licenses/LICENSE-2.0.html).\n"
 },
 {
  "repo": "deepmind/pysc2",
  "language": "Python",
  "readme_contents": "<div align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=-fKUyT14G-8\"\n     target=\"_blank\">\n    <img src=\"http://img.youtube.com/vi/-fKUyT14G-8/0.jpg\"\n         alt=\"DeepMind open source PySC2 toolset for Starcraft II\"\n         width=\"240\" height=\"180\" border=\"10\" />\n  </a>\n  <a href=\"https://www.youtube.com/watch?v=6L448yg0Sm0\"\n     target=\"_blank\">\n    <img src=\"http://img.youtube.com/vi/6L448yg0Sm0/0.jpg\"\n         alt=\"StarCraft II 'mini games' for AI research\"\n         width=\"240\" height=\"180\" border=\"10\" />\n  </a>\n  <a href=\"https://www.youtube.com/watch?v=WEOzide5XFc\"\n     target=\"_blank\">\n    <img src=\"http://img.youtube.com/vi/WEOzide5XFc/0.jpg\"\n         alt=\"Trained and untrained agents play StarCraft II 'mini-game'\"\n         width=\"240\" height=\"180\" border=\"10\" />\n  </a>\n</div>\n\n# PySC2 - StarCraft II Learning Environment\n\n[PySC2](https://github.com/deepmind/pysc2) is [DeepMind](http://deepmind.com)'s\nPython component of the StarCraft II Learning Environment (SC2LE). It exposes\n[Blizzard Entertainment](http://blizzard.com)'s [StarCraft II Machine Learning\nAPI](https://github.com/Blizzard/s2client-proto) as a Python RL Environment.\nThis is a collaboration between DeepMind and Blizzard to develop StarCraft II\ninto a rich environment for RL research. PySC2 provides an interface for RL\nagents to interact with StarCraft 2, getting observations and sending actions.\n\n\nWe have published an accompanying\n[blogpost](https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/)\nand [paper](https://arxiv.org/abs/1708.04782), which outlines our\nmotivation for using StarCraft II for DeepRL research, and some initial research\nresults using the environment.\n\n## About\n\nDisclaimer: This is not an official Google product.\n\nIf you use the StarCraft II Machine Learning API and/or PySC2 in your research,\nplease cite the [StarCraft II Paper](https://arxiv.org/abs/1708.04782)\n\nYou can reach us at [pysc2@deepmind.com](mailto:pysc2@deepmind.com).\n\n\n# Quick Start Guide\n\n## Get PySC2\n\n### PyPI\n\nThe easiest way to get PySC2 is to use pip:\n\n```shell\n$ pip install pysc2\n```\n\nThat will install the `pysc2` package along with all the required dependencies.\n[virtualenv](https://pypi.python.org/pypi/virtualenv) can help manage your\ndependencies. You may also need to upgrade pip: `pip install --upgrade pip`\nfor the `pysc2` install to work. If you're running on an older system you may\nneed to install `libsdl` libraries for the `pygame` dependency.\n\nPip will install a few of the  binaries to your bin directory. `pysc2_play` can\nbe used as a shortcut to `python -m pysc2.bin.play`.\n\n### From Source\n\nAlternatively you can install latest PySC2 codebase from git master branch:\n\n```shell\n$ pip install --upgrade https://github.com/deepmind/pysc2/archive/master.zip\n```\n\nor from a local clone of the git repo:\n\n```shell\n$ git clone https://github.com/deepmind/pysc2.git\n$ pip install --upgrade pysc2/\n```\n\n## Get StarCraft II\n\nPySC2 depends on the full StarCraft II game and only works with versions that\ninclude the API, which is 3.16.1 and above.\n\n### Linux\n\nFollow Blizzard's [documentation](https://github.com/Blizzard/s2client-proto#downloads) to\nget the linux version. By default, PySC2 expects the game to live in\n`~/StarCraftII/`. You can override this path by setting the `SC2PATH`\nenvironment variable or creating your own run_config.\n\n### Windows/MacOS\n\nInstall of the game as normal from [Battle.net](https://battle.net). Even the\n[Starter Edition](http://battle.net/sc2/en/legacy-of-the-void/) will work.\nIf you used the default install location PySC2 should find the latest binary.\nIf you changed the install location, you might need to set the `SC2PATH`\nenvironment variable with the correct location.\n\nPySC2 should work on MacOS and Windows systems running Python 2.7+ or 3.4+,\nbut has only been thoroughly tested on Linux. We welcome suggestions and patches\nfor better compatibility with other systems.\n\n## Get the maps\n\nPySC2 has many maps pre-configured, but they need to be downloaded into the SC2\n`Maps` directory before they can be played.\n\nDownload the [ladder maps](https://github.com/Blizzard/s2client-proto#downloads)\nand the [mini games](https://github.com/deepmind/pysc2/releases/download/v1.2/mini_games.zip)\nand extract them to your `StarcraftII/Maps/` directory.\n\n## Run an agent\n\nYou can run an agent to test the environment. The UI shows you the actions of\nthe agent and is helpful for debugging and visualization purposes.\n\n```shell\n$ python -m pysc2.bin.agent --map Simple64\n```\n\nIt runs a random agent by default, but you can specify others if you'd like,\nincluding your own.\n\n```shell\n$ python -m pysc2.bin.agent --map CollectMineralShards --agent pysc2.agents.scripted_agent.CollectMineralShards\n```\n\nYou can also run two agents against each other.\n\n```shell\n$ python -m pysc2.bin.agent --map Simple64 --agent2 pysc2.agents.random_agent.RandomAgent\n```\n\nTo specify the agent's race, the opponent's difficulty, and more, you can pass\nadditional flags. Run with `--help` to see what you can change.\n\n## Play the game as a human\n\nThere is a human agent interface which is mainly used for debugging, but it can\nalso be used to play the game. The UI is fairly simple and incomplete, but it's\nenough to understand the basics of the game. Also, it runs on Linux.\n\n```shell\n$ python -m pysc2.bin.play --map Simple64\n```\n\nIn the UI, hit `?` for a list of the hotkeys. The most basic ones are: `F4` to\nquit, `F5` to restart, `F9` to save a replay, and `Pgup`/`Pgdn` to control the\nspeed of the game. Otherwise use the mouse for selection and keyboard for\ncommands listed on the left.\n\nThe left side is a basic rendering. The right side is the feature layers that\nthe agent receives, with some coloring to make it more useful to us. You can\nenable or disable RGB or feature layer rendering and their resolutions with\ncommand-line flags.\n\n## Watch a replay\n\nRunning an agent and playing as a human save a replay by default. You can watch\nthat replay by running:\n\n```shell\n$ python -m pysc2.bin.play --replay <path-to-replay>\n```\n\nThis works for any replay as long as the map can be found by the game.\n\nThe same controls work as for playing the game, so `F4` to exit, `pgup`/`pgdn`\nto control the speed, etc.\n\nYou can save a video of the replay with the `--video` flag.\n\n## List the maps\n\n[Maps](docs/maps.md) need to be configured before they're known to the\nenvironment. You can see the list of known maps by running:\n\n```shell\n$ python -m pysc2.bin.map_list\n```\n\n## Run the tests\n\nIf you want to submit a pull request, please make sure the tests pass on both\npython 2 and 3.\n\n```shell\n$ python -m pysc2.bin.run_tests\n```\n\n# Environment Details\n\nFor a full description of the specifics of how the environment is configured,\nthe observations and action spaces work read the\n[environment documentation](docs/environment.md).\n\n# Mini-game maps\n\nThe mini-game map files referenced in the paper are stored under `pysc2/maps/`\nbut must be installed in `$SC2PATH/Maps`. Make sure to follow the download\ninstructions above.\n\nMaps are configured in the Python files in `pysc2/maps/`. The configs can set\nplayer and time limits, whether to use the game outcome or curriculum score, and\na handful of other things. For more information about the maps, and how to\nconfigure your own, read the [maps documentation](docs/maps.md).\n\n# Replays\n\nA replay lets you review what happened during a game. You can see the actions\nand observations that each player made as they played.\n\nBlizzard is releasing a large number of anonymized 1v1 replays played on the\nladder. You can find instructions for how to get the\n[replay files](https://github.com/Blizzard/s2client-proto#downloads) on their\nsite. You can also review your own replays.\n\nReplays can be played back to get the observations and actions made during that\ngame. The observations are rendered at the resolution you request, so may differ\nfrom what the human actually saw. Similarly the actions specify a point, which\ncould reflect a different pixel on the human's screen, so may not have an exact\nmatch in our observations, though they should be fairly similar.\n\nReplays are version dependent, so a 3.16 replay will fail in a 3.16.1 or 3.17\nbinary.\n\nYou can visualize the replays with the full game, or with `pysc2.bin.play`.\nAlternatively you can run `pysc2.bin.replay_actions` to process many replays\nin parallel.\n"
 },
 {
  "repo": "gokceneraslan/awesome-deepbio",
  "language": null,
  "readme_contents": "# Awesome DeepBio [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/gokceneraslan/awesome-deepbio)\n\nA curated list of awesome deep learning applications in the field of computational biology\n\n- **2007-08** | Fast model-based protein homology detection without alignment | *Sepp Hochreiter, Martin Heusel, and Klaus Obermayer* | [Bioinformatics](https://doi.org/10.1093/bioinformatics/btm247)\n\n- **2012-07** | Deep architectures for protein contact map prediction | *Pietro Di Lena, Ken Nagata and Pierre Baldi* [Bioinformatics](https://doi.org/10.1093/bioinformatics/bts475)\n\n- **2012-10** | Predicting protein residue\u2013residue contacts using deep networks and boosting | *Jesse Eickholt and Jianlin Cheng* | [Bioinformatics](https://doi.org/10.1093/bioinformatics/bts598)\n\n- **2013-03** | DNdisorder: predicting protein disorder using boosting and deep networks | *Jesse Eickholt and Jianlin Cheng* | [BMC Bioinformatics](https://doi.org/10.1186/1471-2105-14-88)\n\n- **2014-06** | Deep learning of the tissue-regulated splicing code | *Michael K. K. Leung, Hui Yuan Xiong, Leo J. Lee and Brendan J. Frey* | [Bioinformatics](https://doi.org/10.1093/bioinformatics/btu277)\n\n- **2014-10** | DANN: a deep learning approach for annotating the pathogenicity of genetic variants  | *Daniel Quang, Yifei Chen and Xiaohui Xie* | [Bioinformatics](https://doi.org/10.1093/bioinformatics/btu703)\n\n- **2014-11** | Pairwise input neural network for target-ligand interaction prediction | *Caihua Wang, Juan Liu, Fei Luo, Yafang Tan, Zixin Deng, Qian-Nan Hu* | [2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)](https://doi.org/10.1109/BIBM.2014.6999129)\n\n- **2014-12** | Deep learning as an opportunity in virtual screening | *Thomas Unterthiner, Andreas Mayr, G\u00fcnter Klambauer, Marvin Steijaert, J\u00f6rg K. Wegner, Hugo Ceulemans, & Sepp Hochreiter* | [In Proceedings of the Deep Learning Workshop at NIPS](http://www.datascienceassn.org/sites/default/files/Deep%20Learning%20as%20an%20Opportunity%20in%20Virtual%20Screening.pdf).\n\n- **2015-01** | Unsupervised feature construction and knowledge extraction from genome-wide assays of breast cancer with denoising autoencoders. | *Jie Tan, Matt Ung, Chao Cheng, Casey Greene* | [Pacific Symposium on Biocomputing (PSB)](https://doi.org/10.1142/9789814644730_0014) | [Models & Data](http://discovery.dartmouth.edu/~cgreene/da-psb2015/)\n\n- **2015-01** | The human splicing code reveals new insights into the genetic determinants of disease  | *Hui Y. Xiong, Babak Alipanahi, Leo J. Lee, Hannes Bretschneider, Daniele Merico, Ryan K. C. Yuen, Yimin Hua, Serge Gueroussov, Hamed S. Najafabadi, Timothy R. Hughes, Quaid Morris, Yoseph Barash, Adrian R. Krainer, Nebojsa Jojic, Stephen W. Scherer, Benjamin J. Blencowe, Brendan J. Frey* | [Science](https://doi.org/10.1126/science.1254806)\n\n- **2015-03** | Deep Feature Selection: Theory and Application to Identify Enhancers and Promoters | *Yifeng Li, Chih-Yu Chen, and Wyeth W. Wasserman* | [19th Annual International Conference, RECOMB 2015, Warsaw, Proceedings](https://doi.org/10.1007/978-3-319-16706-0_20)\n\n- **2015-05** | Trans-species learning of cellular signaling systems with bimodal deep belief networks | *Lujia Chen, Chunhui Cai, Vicky Chen and Xinghua Lu* | [Bioinformatics](https://doi.org/10.1093/bioinformatics/btv315)\n\n- **2015-05** | Deep convolutional neural networks for annotating gene expression patterns in the mouse brain | *Tao Zeng, Rongjian Li, Ravi Mukkamala, Jieping Ye and Shuiwang Ji* | [BMC Bioinformatics](https://doi.org/10.1186/s12859-015-0553-9)\n\n- **2015-07** | DeepBind: Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning | *Babak Alipanahi,\t Andrew Delong,\tMatthew T. Weirauch & Brendan J. Frey* | [Nature Biotechnology](https://doi.org/10.1038/nbt.3300)\n\n- **2015-08** | Deep learning for regulatory genomics | *Yongjin Park & Manolis Kellis* | [Nature Biotechnology](https://doi.org/10.1038/nbt.3313)\n\n- **2015-08** | DeepSEA: Predicting effects of noncoding variants with deep learning\u2013based sequence model | *Jian Zhou & Olga G. Troyanskaya* | [Nature Methods: Short intro](https://doi.org/10.1038/nmeth.3604) & [Nature Methods](https://doi.org/10.1038/nmeth.3547)\n\n- **2015-08** | Integrative Data Analysis of Multi-Platform Cancer Data with a Multimodal Deep Learning Approach | *Muxuan Liang, Zhizhong Li, Ting Chen, Jianyang Zeng* | [IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB)](https://doi.org/10.1109/TCBB.2014.2377729)\n\n- **2015-10** | A deep learning framework for modeling structural features of RNA-binding protein targets | *Sai Zhang, Jingtian Zhou, Hailin Hu, Haipeng Gong, Ligong Chen, Chao Cheng, and Jianyang Zeng* | [NAR](https://doi.org/10.1093/nar/gkv1025)\n\n- **2015-10** | Basset: Learning the regulatory code of the accessible genome with deep convolutional neural networks | *David R. Kelley, Jasper Snoek, John Rinn* | [Biorxiv](https://doi.org/10.1101/028399) | [code](https://github.com/davek44/Basset)\n\n- **2015-10** | Deep Learning for Drug-Induced Liver Injury | *Youjun Xu, Ziwei Dai, Fangjin Chen, Shuaishi Gao, Jianfeng Pei, and Luhua Lai* | [ASC Journal of Chemical Information and Modeling](https://doi.org/10.1021/acs.jcim.5b00238)\n\n- **2016-01** | ADAGE-Based Integration of Publicly Available Pseudomonas aeruginosa Gene Expression Data with Denoising Autoencoders Illuminates Microbe-Host Interactions | [mSystems](https://dx.doi.org/10.1128/mSystems.00025-15) | [code](https://github.com/greenelab/adage)\n\n- **2015-11** | De novo identification of replication-timing domains in the human genome by deep learning | *Feng Liu, Chao Ren, Hao Li, Pingkun Zhou, Xiaochen Bo and Wenjie Shu* | [Bioinformatics](https://doi.org/10.1093/bioinformatics/btv643)\n\n- **2015-11** | Recurrent Neural Network Based Hybrid Model of Gene Regulatory Network | *Khalid Raza, Mansaf Alam* | [Arxiv](https://arxiv.org/abs/1408.5405v2)\n\n- **2015-11** | Continuous Distributed Representation of Biological Sequences for Deep Proteomics and Genomics | *Ehsaneddin Asgari, Mohammad R. K. Mofrad* | [PloS one](http://dx.doi.org/10.1371/journal.pone.0141287)\n\n- **2016-01** | Learning a hierarchical representation of the yeast transcriptomic machinery using an autoencoder model | *Lujia Chen, Chunhui Cai, Vicky Chen and Xinghua Lu* | [BMC Bioinformatics](https://doi.org/10.1186/s12859-015-0852-1)\n\n- **2016-01** | PEDLA: predicting enhancers with a deep learning-based algorithmic framework | *Feng Liu, Hao Li, Chao Ren, Xiaochen Bo, Wenjie Shu* | [Biorxiv](https://doi.org/10.1101/036129)\n\n- **2016-01** | TensorFlow: Biology\u2019s Gateway to Deep Learning? | *Ladislav Rampasek, Anna Goldenberg* | [Cell Systems](https://doi.org/10.1016/j.cels.2016.01.009)\n\n- **2016-01** | ADAGE-Based Integration of Publicly Available Pseudomonas aeruginosa Gene Expression Data with Denoising Autoencoders Illuminates Microbe-Host Interactions | [mSystems](https://doi.org/10.1128/mSystems.00025-15) | [code](https://github.com/greenelab/adage)\n\n- **2016-01** | Deep Learning in Drug Discovery | *Erik Gawehn, Jan A. Hiss and Gisbert Schneider* | [Molecular Informatics](https://doi.org/10.1002/minf.201501008)\n\n- **2016-02** | DeepTox: toxicity prediction using deep learning | *Andreas Mayr, G\u00fcnter Klambauer, Thomas Unterthiner, and Sepp Hochreiter* | [Frontiers in Environmental Science](http://journal.frontiersin.org/article/10.3389/fenvs.2015.00080/full) \n\n- **2016-02** | Gene expression inference with deep learning | *Yifei Chen, Yi Li, Rajiv Narayan, Aravind Subramanian, Xiaohui Xie* | [Bioinformatics](https://doi.org/10.1093/bioinformatics/btw074)\n\n- **2016-02** | Semi-Supervised Learning of the Electronic Health Record for Phenotype Stratification | *Brett Beaulieu-Jones, Casey Greene* | [bioRxiv](https://doi.org/10.1101/039800)\n\n- **2016-03** | Genome-Wide Prediction of cis-Regulatory Regions Using Supervised Deep Learning Methods | *Yifeng Li, Wenqiang Shi, Wyeth W Wasserman* | [Biorxiv](https://doi.org/10.1101/041616)\n\n- **2016-03** | Applications of deep learning in biomedicine | *Polina Mamoshina, Armando Vieira, Evgeny Putin, and Alex Zhavoronkov* | [ACS Molecular Pharmaceutics](https://dx.doi.org/10.1021/acs.molpharmaceut.5b00982)\n\n- **2016-03** | Deep Learning in Bioinformatics | *Seonwoo Min, Byunghan Lee, Sungroh Yoon* | [Arxiv](http://arxiv.org/abs/1603.06430)\n\n- **2016-03** | DeepNano: Deep Recurrent Neural Networks for Base Calling in MinION Nanopore Reads | *Vladim\u00edr Bo\u017ea, Bro\u0148a Brejov\u00e1, Tom\u00e1\u0161 Vina\u0159* | [Arxiv](http://arxiv.org/abs/1603.09195) | [code](https://bitbucket.org/vboza/deepnano)\n\n- **2016-03** | deepTarget: End-to-end Learning Framework for microRNA Target Prediction using Deep Recurrent Neural Networks | *Byunghan Lee, Junghwan Baek, Seunghyun Park, Sungroh Yoon* | [Arxiv](http://arxiv.org/abs/1603.09123)\n\n- **2016-03** | Deep Learning in Label-free Cell Classification | *Claire Lifan Chen, Ata Mahjoubfar, Li-Chia Tai, Ian K. Blaby, Allen Huang, Kayvan Reza Niazi & Bahram Jalali* | [Nature Scientific Reports](https://doi.org/10.1038/srep21471)\n\n- **2016-04** | Accurate classification of protein subcellular localization from high throughput microscopy images using deep learning | *Tanel P\u00e4rnamaa, Leopold Parts* | [bioRxiv](http://dx.doi.org/10.1101/050757)\n\n- **2016-04** | DanQ: a hybrid convolutional and recurrent deep neural network for quantifying the function of DNA sequences | *Daniel Quang & Xiaohui Xie* | [Nucleic Acids Research](https://doi.org/10.1093/nar/gkw226) | [code](https://github.com/uci-cbcl/DanQ)\n\n- **2016-04** | deepMiRGene: Deep Neural Network based Precursor microRNA Prediction | *Seunghyun Park, Seonwoo Min, Hyun-soo Choi, and Sungroh Yoon* | [Arxiv](http://arxiv.org/abs/1605.00017)\n\n- **2016-04** | Microscopy cell counting and detection with fully convolutional regression networks | *Weidi Xie, J. Alison Noble and Andrew Zisserman* | [Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization](https://doi.org/10.1080/21681163.2016.1149104)\n\n- **2016-04** | Protein Secondary Structure Prediction Using Cascaded Convolutional and Recurrent Neural Networks | *Zhen Li and Yizhou Yu* | [Arxiv](https://arxiv.org/abs/1604.07176)\n\n- **2016-05** | Denoising genome-wide histone ChIP-seq with convolutional neural networks | *Pang Wei Koh, Emma Pierson, Anshul Kundaje* | [Biorxiv](https://doi.org/10.1101/052118)\n\n- **2016-05** | Deep Motif: Visualizing Genomic Sequence Classifications | *Jack Lanchantin, Ritambhara Singh, Zeming Lin, Yanjun Qi* | [Arxiv](http://arxiv.org/abs/1605.01133)\n\n- **2016-05** | Not Just a Black Box: Learning Important Features Through Propagating Activation Differences | *Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, Anshul Kundaje* | [Arxiv](https://arxiv.org/abs/1605.01713)\n\n- **2016-05** | Deep biomarkers of human aging: Application of deep neural networks to biomarker development | *Evgeny Putin, Polina Mamoshina, Alexander Aliper, Mikhail Korzinkin, Alexey Moskalev, Alexey Kolosov, Alexander Ostrovskiy, Charles Cantor, Jan Vijg, and Alex Zhavoronkov* | [Aging](https://doi.org/10.18632/aging.100968)\n\n- **2016-05** | Deep learning applications for predicting pharmacological properties of drugs and drug repurposing using transcriptomic data | *Alexander Aliper, Sergey Plis, Artem Artemov, Alvaro Ulloa, Polina Mamoshina, and Alex Zhavoronkov* | [ACS Molecular Pharmaceutics](https://doi.org/10.1021/acs.molpharmaceut.6b00248)\n\n- **2016-05** | Deep Machine Learning provides state-of-the-art performance in image-based plant phenotyping | *Michael P. Pound, Alexandra J. Burgess, Michael H. Wilson, Jonathan A. Atkinson, Marcus Griffiths, Aaron S. Jackson, Adrian Bulat, Yorgos Tzimiropoulos, Darren M. Wells, Erik H. Murchie, Tony P. Pridmore, Andrew P. French* | [Biorxiv](https://doi.org/10.1101/053033)\n\n- **2016-05** | Genetic Architect: Discovering Genomic Structure with Learned Neural Architectures | *Laura Deming, Sasha Targ, Nate Sauder, Diogo Almeida, Chun Jimmie Ye* | [Arxiv](https://arxiv.org/abs/1605.07156v1)\n\n- **2016-05** | DeepCyTOF: Automated Cell Classification of Mass Cytometry Data by Deep Learning and Domain Adaptation | *Huamin Li, Uri Shaham, Yi Yao, Ruth Montgomery, Yuval Kluger* | [Biorxiv](https://doi.org/10.1101/054411)\n\n- **2016-06** | Classifying and segmenting microscopy images with deep multiple instance learning | *Oren Z. Kraus, Jimmy Lei Ba and Brendan J. Frey* | [Bioinformatics](https://doi.org/10.1093/bioinformatics/btw252)\n\n- **2016-06** | Convolutional neural network architectures for predicting DNA\u2013protein binding | *Haoyang Zeng, Matthew D. Edwards, Ge Liu and David K. Gifford*  | [Bioinformatics](https://doi.org/10.1093/bioinformatics/btw255) | [code](http://cnn.csail.mit.edu)\n\n- **2016-06** | DeepLNC, a long non-coding RNA prediction tool using deep neural network | *Rashmi Tripathi, Sunil Patel, Vandana Kumari, Pavan Chakraborty, Pritish Kumar Varadwaj* | [Network Modeling Analysis in Health Informatics and Bioinformatics](https://doi.org/10.1007/s13721-016-0129-2)\n\n- **2016-06** | Virtual Screening: A Challenge for Deep Learning | *Javier P\u00e9rez-Sianes, Horacio P\u00e9rez-S\u00e1nchez, Fernando D\u00edaz* | [10th International Conference on Practical Applications of Computational Biology & Bioinformatics](https://doi.org/10.1007/978-3-319-40126-3_2)\n\n- **2016-07** | Deep learning for computational biology | *Christof Angermueller, Tanel P\u00e4rnamaa, Leopold Parts, Oliver Stegle* | [Molecular Systems Biology](https://doi.org/10.15252/msb.20156651)\n\n- **2016-07** | Deep Learning in Bioinformatics | *Seonwoo Min, Byunghan Lee, Sungroh Yoon* | [Briefings in Bioinformatics](https://doi.org/10.1093/bib/bbw068)\n\n- **2016-08** | DeepChrome: deep-learning for predicting gene expression from histone modifications | *Ritambhara Singh, Jack Lanchantin,  Gabriel Robins,  Yanjun Qi* | [Bioinformatics](https://doi.org/10.1093/bioinformatics/btw427)\n\n- **2016-08** | Deep Artificial Neural Networks and Neuromorphic Chips for Big Data Analysis: Pharmaceutical and Bioinformatics Applications | *Lucas Ant\u00f3n Pastur-Romay, Francisco Cedr\u00f3n, Alejandro Pazos and Ana Bel\u00e9n Porto-Pazos* | [International Journal of Molecular Sciences](https://doi.org/10.3390/ijms17081313)\n\n- **2016-08** | Deep GDashboard: Visualizing and Understanding Genomic Sequences Using Deep Neural Networks | *Jack Lanchantin, Ritambhara Singh, Beilun Wang, Yanjun Qi* | [Arxiv](https://arxiv.org/abs/1608.03644v2)\n\n- **2016-08** | Modeling translation elongation dynamics by deep learning reveals new insights into the landscape of ribosome stalling | *Sai Zhang, Hailin Hu, Jingtian Zhou, Xuan He and Jianyang Zeng* | [bioRxiv](http://dx.doi.org/10.1101/067108)\n\n- **2016-08** | DeepWAS: Directly integrating regulatory information into GWAS using deep learning supports master regulator MEF2C as risk factor for major depressive disorder | *G\u00f6kcen Eraslan, Janine Arloth, Jade Martins, Stella Iurato, Darina Czamara, Elisabeth B. Binder, Fabian J. Theis, Nikola S. Mueller* | [bioRxiv](https://dx.doi.org/10.1101/069096)\n\n- **2016-09** | The Next Era: Deep Learning in Pharmaceutical Research | *Sean Ekins* | [Pharmaceutical Research](https://dx.doi.org/10.1007/s11095-016-2029-7)\n\n- **2016-10** | Automatic chemical design using a data-driven continuous representation of molecules | *Rafael G\u00f3mez-Bombarelli, David Duvenaud, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams, Al\u00e1n Aspuru-Guzik* | [Arxiv](https://arxiv.org/abs/1610.02415)\n\n- **2016-10** | FIDDLE: An integrative deep learning framework for functional genomic data inference | *Umut Eser, L. Stirling Churchman* | [bioRxiv](http://dx.doi.org/10.1101/081380)\n\n- **2016-10** | Deep Learning for Imaging Flow Cytometry: Cell Cycle Analysis of Jurkat Cells | *Philipp Eulenberg, Niklas Koehler, Thomas Blasi, Andrew Filby, Anne E. Carpenter, Paul Rees, Fabian J. Theis, F. Alexander Wolf* | [bioRxiv](http://dx.doi.org/10.1101/081364)\n\n- **2016-10** | Leveraging uncertainty information from deep neural networks for disease detection | *Christian Leibig, Vaneeda Allken, Philipp Berens, Siegfried Wahl* | [bioRxiv](http://dx.doi.org/10.1101/084210)\n\n- **2016-11** | Predicting Enhancer-Promoter Interaction from Genomic Sequence with Deep Neural Networks | *Shashank Singh, Yang Yang, Barnabas Poczos, Jian Ma* | [bioRxiv](https://doi.org/10.1101/085241)\n\n- **2016-11** | RNA-protein binding motifs mining with a new hybrid deep learning based cross-domain knowledge integration approach | *Xiaoyong Pan, Hong-Bin Shen* | [bioRxiv](http://dx.doi.org/10.1101/085191)\n\n- **2016-11** | Low Data Drug Discovery with One-shot Learning | *Han Altae-Tran, Bharath Ramsundar, Aneesh S. Pappu, Vijay Pande* | [Arxiv](https://arxiv.org/abs/1611.03199)\n\n- **2016-11** | Diet Networks: Thin Parameters for Fat Genomic | *Adriana Romero, Pierre Luc Carrier, Akram Erraqabi, Tristan Sylvain, Alex Auvolat, Etienne Dejoie, Marc-Andr\u00e9 Legault, Marie-Pierre Dub\u00e9, Julie G. Hussin, Yoshua Bengio* | [Arxiv](https://arxiv.org/abs/1611.09340)\n\n- **2016-11** | DeeperBind: Enhancing Prediction of Sequence Specificities of DNA Binding Proteins | *Hamid Reza Hassanzadeh, May D. Wang* | [Arxiv](https://arxiv.org/abs/1611.05777)\n\n- **2016-11** | Deep learning with feature embedding for compound-protein interaction prediction | *Fangping Wan, Jianyang Zeng* | [bioRxiv](https://doi.org/10.1101/086033)\n\n- **2016-11** | Deep Learning Automates the Quantitative Analysis of Individual Cells in Live-Cell Imaging Experiments | *David A. Van Valen, Takamasa Kudo, Keara M. Lane, Derek N. Macklin, Nicolas T. Quach, Mialy M. DeFelice, Inbal Maayan, Yu Tanouchi, Euan A. Ashley, Markus W. Covert* | [PLoS Computational Biology](https://doi.org/10.1371/journal.pcbi.1005177)\n\n- **2016-12** | Creating a universal SNP and small indel variant caller with deep neural networks | *Ryan Poplin, Dan Newburger, Jojo Dijamco, Nam Nguyen, Dion Loy, Sam S. Gross, Cory Y. McLean, Mark A. DePristo* | [bioRxiv](https://doi.org/10.1101/092890)\n\n- **2016-12** | DeepCancer: Detecting Cancer through Gene Expressions via Deep Generative Learning | *Rajendra Rana Bhat, Vivek Viswanath, Xiaolin Li* | [Arxiv](http://arxiv.org/abs/1612.03211)\n\n- **2016-12** | Cox-nnet: an artificial neural network Cox regression for prognosis prediction | *Travers Ching, Xun Zhu, Lana Garmire* | [bioRxiv](https://doi.org/10.1101/093021)\n\n- **2016-12** | Deep learning is effective for the classification of OCT images of normal versus Age-related Macular Degeneration | *Cecilia S Lee, Doug M Baughman, Aaron Y Lee* | [bioRxiv](https://doi.org/10.1101/094276)\n\n- **2016-12** | Partitioned learning of deep Boltzmann machines for SNP data | *Moritz Hess, Stefan Lenz, Tamara Blaette, Lars Bullinger, Harald Binder* | [bioRxiv](https://doi.org/10.1101/095638)\n\n- **2016-12** | DeepAD: Alzheimer\u2032s Disease Classification via Deep Convolutional Neural Networks using MRI and fMRI | *Saman Sarraf, John Anderson, Ghassem Tofighi, for the Alzheimer's Disease Neuroimaging Initiativ* | [bioRxiv](https://doi.org/10.1101/070441)\n\n- **2016-12** | Training Genotype Callers with Neural Networks | *R\u00e9mi Torracinta, Fabien Campagne* | [bioRxiv](https://doi.org/10.1101/097469)\n\n- **2016-12** | EP-DNN: A Deep Neural Network-Based Global Enhancer Prediction Algorithm | *Seong Gon Kim, Mrudul Harwani, Ananth Grama, Somali Chaterji* | [Nature Scientific Reports](https://doi.org/10.1038/srep38433)\n\n- **2016-12** | EnhancerPred: a predictor for discovering enhancers based on the combination and selection of multiple features | *Cangzhi Jia, Wenying He* | [Nature Scientific Reports](https://doi.org/10.1038/srep38741)\n\n- **2016-12** | DeepEnhancer: Predicting enhancers by convolutional neural networks | *Min, Xu, Ning Chen, Ting Chen, and Rui Jiang* | [2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)](https://doi.org/10.1109/BIBM.2016.7822593)\n\n- **2016-12** | DeepSplice: Deep classification of novel splice junctions revealed by RNA-seq | *Zhang, Yi, Xinan Liu, James N. MacLeod, and Jinze Liu* | [2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)](https://doi.org/10.1109/BIBM.2016.7822541)\n\n- **2016-12** | Deep convolutional neural networks for detecting secondary structures in protein density maps from cryo-electron microscopy | *Li, Rongjian, Dong Si, Tao Zeng, Shuiwang Ji, and Jing He* | [2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)](https://doi.org/10.1109/BIBM.2016.7822490)\n\n- **2016-12** | Towards recognition of protein function based on its structure using deep convolutional networks | *Tavanaei, Amirhossein, Anthony S. Maida, Arun Kaniymattam, and Rasiah Loganantharaj* | [2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)](https://doi.org/10.1109/BIBM.2016.7822509)\n\n- **2016-12** | Emotion recognition from multi-channel EEG data through Convolutional Recurrent Neural Network | *Li, Xiang, Dawei Song, Peng Zhang, Guangliang Yu, Yuexian Hou, and Bin Hu* | [2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)](https://doi.org/10.1109/BIBM.2016.7822545)\n\n- **2016-12** | Coarse-to-Fine Stacked Fully Convolutional Nets for lymph node segmentation in ultrasound images | *Zhang, Yizhe, Michael TC Ying, Lin Yang, Anil T. Ahuja, and Danny Z. Chen* | [2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)](https://doi.org/10.1109/BIBM.2016.7822557)\n\n- **2016-12** | CNNsite: Prediction of DNA-binding residues in proteins using Convolutional Neural Network with sequence features | *Zhou, Jiyun, Qin Lu, Ruifeng Xu, Lin Gui, and Hongpeng Wang* | [2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)](https://doi.org/10.1109/BIBM.2016.7822496)\n\n- **2016-12** | A predictive model of gene expression using a deep learning framework | *Xie, Rui, Andrew Quitadamo, Jianlin Cheng, and Xinghua Shi* | [2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)](https://doi.org/10.1109/BIBM.2016.7822599)\n\n- **2016-12** | Deep convolutional neural network for survival analysis with pathological images | *Zhu, Xinliang, Jiawen Yao, and Junzhou Huang* | [2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)](https://doi.org/10.1109/BIBM.2016.7822579)\n\n- **2016-12** | Dependency-based convolutional neural network for drug-drug interaction extraction | *Liu, Shengyu, Kai Chen, Qingcai Chen, and Buzhou Tang* | [2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)](https://doi.org/10.1109/BIBM.2016.7822671)\n\n- **2016-12** | Pervasive EEG diagnosis of depression using Deep Belief Network with three-electrodes EEG collector | *Cai, Hanshu, Xiaocong Sha, Xue Han, Shixin Wei, and Bin Hu* | [2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)](https://doi.org/10.1109/BIBM.2016.7822696)\n\n- **2016-12** | Cardiac left ventricular volumes prediction method based on atlas location and deep learning | *Luo, Gongning, Suyu Dong, Kuanquan Wang, and Henggui Zhang* | [2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)](https://doi.org/10.1109/BIBM.2016.7822759)\n\n- **2016-12** | A high-precision shallow Convolutional Neural Network based strategy for the detection of Genomic Deletions | *Wang, Jing, Cheng Ling, and Jingyang Gao* | [2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)](https://doi.org/10.1109/BIBM.2016.7822793)\n\n- **2016-12** | The cornucopia of meaningful leads: Applying deep adversarial autoencoders for new molecule development in oncology | *Kadurin, Artur, Alexander Aliper, Andrey Kazennov, Polina Mamoshina, Quentin Vanhaelen, Kuzma Khrabrov, and Alex Zhavoronkov* | [Oncotarget](https://doi.org/10.18632/oncotarget.14073)\n\n- **2016-12** | Medical Image Synthesis with Context-Aware Generative Adversarial Networks | *Dong Nie, Roger Trullo, Caroline Petitjean, Su Ruan, Dinggang Shen* | [Arxiv](https://arxiv.org/abs/1612.05362)\n\n- **2016-12** | Unsupervised Learning from Noisy Networks with Applications to Hi-C Data | *Wang, Bo, Junjie Zhu, Armin Pourshafeie, Oana Ursu, Serafim Batzoglou, and Anshul Kundaje* | [Advances in Neural Information Processing Systems (NIPS 2016)](http://papers.nips.cc/paper/6291-unsupervised-learning-from-noisy-networks-with-applications-to-hi-c-data)\n\n- **2016-12** | Deep Learning for Health Informatics | *Daniele Rav\u00ec, Charence Wong, Fani Deligianni, Melissa Berthelot, Javier Andreu-Perez, Benny Lo, and Guang-Zhong Yang* | [IEEE Journal of Biomedical and Health Informatics](https://doi.org/10.1109/JBHI.2016.2636665)\n\n- **2017-01** | A Deep Learning Approach for Cancer Detection and Relevant Gene Identification | *Wang, Jing, Cheng Ling, and Jingyang Gao* | [Pacific Symposium on Biocomputing 2017](http://dx.doi.org/10.1142/9789813207813_0022)\n\n- **2017-01** | Deep Motif Dashboard: Visualizing and Understanding Genomic Sequences Using Deep Neural Networks | *Lanchantin, Jack, Ritambhara Singh, Beilun Wang, and Yanjun Qi* | [Pacific Symposium on Biocomputing 2017](http://dx.doi.org/10.1142/9789813207813_0025)\n\n- **2017-01** | HLA class I binding prediction via convolutional neural networks | *Yeeleng Scott Vang, Xiaohui Xie* | [bioRxiv](https://doi.org/10.1101/099358)\n\n- **2017-01** | DeadNet: Identifying Phototoxicity from Label-free Microscopy Images of Cells using Deep ConvNets | *David Richmond, Anna Payne-Tobin Jost, Talley Lambert, Jennifer Waters, Hunter Elliott* | [arXiv](https://arxiv.org/abs/1701.06109)\n\n- **2017-01** | Dermatologist-level classification of skin cancer with deep neural networks | *Andre Esteva, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau & Sebastian Thrun* | [Nature](https://doi.org/10.1038/nature21056)\n\n- **2017-01** | Understanding sequence conservation with deep learning | *Yi Li, Daniel Quang, Xiaohui Xie* | [Biorxiv](https://doi.org/10.1101/103929)\n\n- **2017-01** | Learning the Structural Vocabulary of a Network | *Saket Navlakha* | [Neural Computation](https://doi.org/10.1162/NECO_a_00924)\n\n- **2017-01** | Mining the Unknown: Assigning Function to Noncoding Single Nucleotide Polymorphisms | *Sierra S. Nishizaki, Alan P. Boyle* | [Trends in Genetics](http://dx.doi.org/10.1016/j.tig.2016.10.008)\n\n- **2017-01** | Reverse-complement parameter sharing improves deep learning models for genomics | *Avanti Shrikumar, Peyton Greenside, Anshul Kundaje* | [bioRxiv](https://doi.org/10.1101/103663)\n\n- **2017-01** | TIDE: predicting translation initiation sites by deep learning | *Sai Zhang, Hailin Hu, Tao Jiang, Lei Zhang, Jianyang Zeng* | [bioRxiv](https://doi.org/10.1101/103374)\n\n- **2017-01** | Integrative Deep Models for Alternative Splicing | *Anupama Jha, Matthew R Gazzara, Yoseph Barash* | [bioRxiv](https://doi.org/10.1101/104869)\n\n- **2017-01** | Deep Recurrent Neural Network for Protein Function Prediction from Sequence | *Xueliang\u00a0Leon\u00a0Liu* | [bioRxiv](https://doi.org/10.1101/103994)\n\n- **2017-01** | Nucleotide sequence and DNaseI sensitivity are predictive of 3D chromatin architecture | *Jacob Schreiber, Maxwell Libbrecht, Jeffrey Bilmes, William Noble* | [bioRxiv](https://doi.org/10.1101/103614)\n\n- **2017-01** | Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model | *Sheng Wang, Siqi Sun, Zhen Li, Renyu Zhang, Jinbo Xu* | [PloS Computational Biology](https://doi.org/10.1371/journal.pcbi.1005324)\n\n- **2017-02** | Imputation for transcription factor binding predictions based on deep learning | *Qian Qin, Jianxing Feng* | [PloS Computational Biology](http://dx.doi.org/10.1371/journal.pcbi.1005403)\n\n- **2017-02** | Deep Learning based multi-omics integration robustly predicts survival in liver cancer | *Kumardeep Chaudhary, Olivier B. Poirion, Liangqun Lu, Lana Garmire* | [bioRxiv](https://doi.org/10.1101/114892)\n\n- **2017-03** | Predicting the impact of non-coding variants on DNA methylation | *Zeng, Haoyang, and David K. Gifford* | [Nucleic Acids Research](https://doi.org/10.1093/nar/gkx177)\n\n- **2017-03** | H&E-stained Whole Slide Image Deep Learning Predicts SPOP Mutation State in Prostate Cancer | *Andrew J Schaumberg, Mark A Rubin, Thomas J Fuchs* | [bioRxiv](https://doi.org/10.1101/064279)\n\n- **2017-04** | DeepCpG: accurate prediction of single-cell DNA methylation states using deep learning | *Christof Angermueller, Heather J. Lee, Wolf Reik, and Oliver Stegle* | [Genome Biology](https://doi.org/10.1186/s13059-017-1189-z)\n\n- **2017-04** | Generalising Better: Applying Deep Learning To Integrate Deleteriousness Prediction Scores For Whole-Exome SNV Studies | *Ilia Korvigo, Andrey Afanasyev, Nikolay Romashchenko, Mihail Skoblov* | [bioRxiv](https://doi.org/10.1101/126532)\n\n- **2017-09** | DeepLoc: prediction of protein subcellular localization using deep learning | *Jos\u00e9 JA Armenteros, Casper K S\u00f8nderby, S\u00f8ren K S\u00f8nderby, Henrik Nielsen, Ole Winther* | [Bioinformatics](https://doi.org/10.1093/bioinformatics/btx431)\n\n- **2017-11** | Modeling positional effects of regulatory sequences with spline transformations increases prediction accuracy of deep neural networks | *\u017diga Avsec, Mohammadamin Barekatain, Jun Cheng, Julien Gagneur* | [Bioinformatics](https://doi.org/10.1093/bioinformatics/btx727)\n\n- **2017-11** | Protein Loop Modeling Using Deep Generative Adversarial Network | *Zhaoyu Li, Son P. Nguyen, Dong Xu, Yi Shang* | [ICTAI](https://ieeexplore.ieee.org/abstract/document/8372069)\n\n- **2017-12** | Variational auto-encoding of protein sequences | *Sam Sinai, Eric Kelsic, George M. Church and Martin A. Nowak* | [arxiv](https://arxiv.org/abs/1712.03346)\n\n- **2017-12** | Predicting enhancers with deep convolutional neural networks | *Xu Min, Wanwen Zeng, Shengquan Chen, Ning Chen, Ting Chen and Rui Jiang* | [BMC Bioinformatics](https://doi.org/10.1186/s12859-017-1878-3)\n\n- **2018-06** | Convolutional neural networks for classification of alignments of non-coding RNA sequences | *Genta Aoki, Yasubumi Sakakibara* | [Bioinformatics](https://doi.org/10.1093/bioinformatics/bty228)\n\n- **2019-02** | DeepRibo: a neural network for precise gene annotation of prokaryotes by combining ribosome profiling signal and binding site patterns | *Jim Clauwaert, Gerben Menschaert, Willem Waegeman* | [Nucleic Acids Research](https://doi.org/10.1093/nar/gkz061)\n\n### Contribution\n\nFeel free to send a pull request.\n\n### License\n\n[![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)\n"
 },
 {
  "repo": "buriburisuri/ByteNet",
  "language": "Python",
  "readme_contents": "# ByteNet - Fast Neural Machine Translation\nA tensorflow implementation of French-to-English machine translation using DeepMind's ByteNet \nfrom the paper [Nal et al's Neural Machine Translation in Linear Time](https://arxiv.org/abs/1610.10099).\nThis paper proposed the fancy method which replaced the traditional RNNs with conv1d dilated and causal conv1d, \nand they achieved fast training and state-of-the-art performance on character-level translation. \n\nThe architecture ( from the paper )\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/buriburisuri/ByteNet/master/png/architecture.png\" width=\"1024\"/>\n</p>\n\n## Version\n\nCurrent Version : __***0.0.0.2***__\n\n## Dependencies ( VERSION MUST BE MATCHED EXACTLY! )\n\n1. [tensorflow](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#pip-installation) == 1.0.0\n1. [sugartensor](https://github.com/buriburisuri/sugartensor) == 1.0.0.2\n1. [nltk](http://www.nltk.org/install.html) == 3.2.2\n\n## Datasets\n\nI've used NLTK's comtrans English-French parallel corpus for convenience.  You can easily download it as follows:\n<pre><code>\npython\n>>>> import nltk\n>>>> nltk.download_shell()\nNLTK Downloader\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> d\n\nDownload which package (l=list; x=cancel)?\n  Identifier> comtrans\n  \n</code></pre>\n\n## Implementation differences from the paper.\n\n1. I've replaced the Sub Batch Normal with [Layer Normalization](https://arxiv.org/abs/1607.06450) for convenience.\n1. No bags of characters applied for simplicity.\n1. Latent dimension is 400 because Comtrans corpus in NLTK is small. ( 892 in the paper )\n1. Generation code not optimized.\n\n## Training the network\n\nExecute\n<pre><code>\npython train.py\n</code></pre>\nto train the network. You can see the result ckpt files and log files in the 'asset/train' directory.\nLaunch tensorboard --logdir asset/train/log to monitor training process.\n\nI've trained this model on a single Titan X GPU during 10 hours until 50 epochs. \nIf you don't have a Titan X GPU, reduce batch_size in the train.py file from 16 to 8.  \n\n## Translate sample French sentences\n \nExecute\n<pre><code>\npython translate.py\n</code></pre>\nto translate sample French sentences to English. The result will be printed on the console. \n\n## Sample translations\n\nThe result looks messy but promising. \nThough Comtrans corpus in NLTK is very small(in my experiment only 17,163 pairs used), \nthe model have learned English words structures and syntax by character level.  \nI think that the translation accuracy will be better if we use big corpus. \n  \n\n| French (sources) | English (translated by ByteNet) | English (translated by Google translator) |\n| :------------- | :------------- | :------------- |\n| Et pareil ph\u00e9nom\u00e8ne ne devrait pas occuper nos d\u00e9bats ? | And they applied commitments have been satisfied ? | And such a phenomenon should not occupy our debates? |\n| Mais nous devons les aider sur la question de la formation . | However , we must addruss that climate condition . | But we need help on the issue of training. |\n| Les videurs de soci\u00e9t\u00e9s sont punis . | The existing considerations in the coming years ago . | Corporate bouncers are punished. |\n| Apr\u00e8s cette p\u00e9riode , ces \u00e9chantillons ont \u00e9t\u00e9 analys\u00e9s et les r\u00e9sultats illustrent bien la quantit\u00e9 de dioxine \u00e9mise au cours des mois \u00e9coul\u00e9s . | According to the relevant continent with the intentions and for all , the points of building situation by the directive butchers . | After this period, the samples were analyzed and the results illustrate the amount of dioxins emitted during the past months. |\n| Merci beaucoup , Madame la Commissaire . | Thank you very much for the Commissioner against this perfect . | Thank you very much, Commissioner. |\n| Le Zimbabwe a beaucoup \u00e0 gagner de l ' accord de partenariat et a un urgent besoin d ' aide et d ' all\u00e9gement de la dette . | The AIDR problem is carried out corperation in the waken home after a peaceful future and not have their different parts . | Zimbabwe has much to gain from the Partnership Agreement and urgently needs aid and debt relief. |\n| Le gouvernement travailliste de Grande-Bretagne a \u00e9galement des raisons d ' \u00eatre fier de ses performances . | The Larning wants to have a former colleague with the United States is indeed all of the population . | The Labour government in Britain also has reason to be proud of its performance. |\n| La plupart d' entre nous n' a pas l' intention de se vanter des 3 millions d' euros . | Most of us here would not wish to boast about EUR 3 million . | Most of us do not have the intention to boast of 3 million euros. |\n| Si le Conseil avait travaill\u00e9 aussi vite que ne l' a fait M. Brok , nous serions effectivement bien plus avanc\u00e9s . | If the Council had worked as quickly as Mr Brok then have been done and general support . | If the Council had worked as quickly as did the did Mr Brok, we would indeed well advanced. |\n| Le deuxi\u00e8me th\u00e8me important concerne la question de la gestion des contingents tarifaires . | The second important area is the issue of managing tariff quotas .| The second important issue concerns the question of the management of tariff quotas. |\n\n\n## pre-trained models\n\nYou can translate French sentences to English sentences with the pre-trained model on the Comtrans corpus in NLTK. \nExtract [the following zip file](https://drive.google.com/open?id=0B3ILZKxzcrUyeXBVeVZoWlN5XzA) in 'asset/train'.\nAnd try another sample French sentences in the 'translate.py' file.  \n \n## Other resources\n\n1. [ByteNet language model tensorflow implementation](https://github.com/paarthneekhara/byteNet-tensorflow)\n\n## My other repositories\n\n1. [SugarTensor](https://github.com/buriburisuri/sugartensor)\n1. [EBGAN tensorflow implementation](https://github.com/buriburisuri/ebgan)\n1. [Timeseries gan tensorflow implementation](https://github.com/buriburisuri/timeseries_gan)\n1. [Supervised InfoGAN tensorflow implementation](https://github.com/buriburisuri/supervised_infogan)\n1. [AC-GAN tensorflow implementation](https://github.com/buriburisuri/ac-gan)\n1. [SRGAN tensorflow implementation](https://github.com/buriburisuri/SRGAN)\n\n# Authors\nNamju Kim (buriburisuri@gmail.com) at Jamonglabs Co., Ltd."
 },
 {
  "repo": "josephmisiti/awesome-machine-learning",
  "language": "Python",
  "readme_contents": "# Awesome Machine Learning [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\nA curated list of awesome machine learning frameworks, libraries and software (by language). Inspired by `awesome-php`.\n\n_If you want to contribute to this list (please do), send me a pull request or contact me [@josephmisiti](https://twitter.com/josephmisiti)._\nAlso, a listed repository should be deprecated if:\n\n* Repository's owner explicitly say that \"this library is not maintained\".\n* Not committed for a long time (2~3 years).\n\nFurther resources:\n\n* For a list of free machine learning books available for download, go [here](https://github.com/josephmisiti/awesome-machine-learning/blob/master/books.md).\n\n* For a list of professional machine learning events, go [here](https://github.com/josephmisiti/awesome-machine-learning/blob/master/events.md).\n\n* For a list of (mostly) free machine learning courses available online, go [here](https://github.com/josephmisiti/awesome-machine-learning/blob/master/courses.md).\n\n* For a list of blogs and newsletters on data science and machine learning, go [here](https://github.com/josephmisiti/awesome-machine-learning/blob/master/blogs.md).\n\n* For a list of free-to-attend meetups and local events, go [here](https://github.com/josephmisiti/awesome-machine-learning/blob/master/meetups.md).\n\n## Table of Contents\n\n### Frameworks and Libraries\n<!-- MarkdownTOC depth=4 -->\n\n- [Awesome Machine Learning ![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](#awesome-machine-learning-awesomehttpsgithubcomsindresorhusawesome)\n  - [Table of Contents](#table-of-contents)\n    - [Frameworks and Libraries](#frameworks-and-libraries)\n    - [Tools](#tools)\n  - [APL](#apl)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning)\n  - [C](#c)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-1)\n      - [Computer Vision](#computer-vision)\n  - [C++](#c)\n      - [Computer Vision](#computer-vision-1)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-2)\n      - [Natural Language Processing](#natural-language-processing)\n      - [Speech Recognition](#speech-recognition)\n      - [Sequence Analysis](#sequence-analysis)\n      - [Gesture Detection](#gesture-detection)\n  - [Common Lisp](#common-lisp)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-3)\n  - [Clojure](#clojure)\n      - [Natural Language Processing](#clojure-nlp)\n      - [General-Purpose Machine Learning](#clojure-general-purpose)\n      - [Deep Learning](#clojure-deep-learning)\n      - [Data Analysis](#clojure-data-analysis)\n      - [Data Visualization](#clojure-data-visualization)\n      - [Interop](#clojure-interop)\n      - [Misc](#clojure-misc)\n      - [Extra](#clojure-extra)\n  - [Crystal](#crystal)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-5)\n  - [Elixir](#elixir)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-6)\n      - [Natural Language Processing](#natural-language-processing-2)\n  - [Erlang](#erlang)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-7)\n  - [Fortran](#fortran)\n      - [General-Purpose Machine Learning](#fortran-general-purpose-machine-learning)\n      - [Data Analysis / Data Visualization](#fortran-data-analysis-visualization)\n  - [Go](#go)\n      - [Natural Language Processing](#natural-language-processing-3)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-8)\n      - [Spatial analysis and geometry](#spatial-analysis-and-geometry)\n      - [Data Analysis / Data Visualization](#data-analysis--data-visualization-1)\n      - [Computer vision](#computer-vision-2)\n      - [Reinforcement learning](#reinforcement-learning)\n  - [Haskell](#haskell)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-9)\n  - [Java](#java)\n      - [Natural Language Processing](#natural-language-processing-4)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-10)\n      - [Speech Recognition](#speech-recognition-1)\n      - [Data Analysis / Data Visualization](#data-analysis--data-visualization-2)\n      - [Deep Learning](#deep-learning)\n  - [Javascript](#javascript)\n      - [Natural Language Processing](#natural-language-processing-5)\n      - [Data Analysis / Data Visualization](#data-analysis--data-visualization-3)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-11)\n      - [Misc](#misc)\n      - [Demos and Scripts](#demos-and-scripts)\n  - [Julia](#julia)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-12)\n      - [Natural Language Processing](#natural-language-processing-6)\n      - [Data Analysis / Data Visualization](#data-analysis--data-visualization-4)\n      - [Misc Stuff / Presentations](#misc-stuff--presentations)\n  - [Lua](#lua)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-13)\n      - [Demos and Scripts](#demos-and-scripts-1)\n  - [Matlab](#matlab)\n      - [Computer Vision](#computer-vision-3)\n      - [Natural Language Processing](#natural-language-processing-7)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-14)\n      - [Data Analysis / Data Visualization](#data-analysis--data-visualization-5)\n  - [.NET](#net)\n      - [Computer Vision](#computer-vision-4)\n      - [Natural Language Processing](#natural-language-processing-8)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-15)\n      - [Data Analysis / Data Visualization](#data-analysis--data-visualization-6)\n  - [Objective C](#objective-c)\n    - [General-Purpose Machine Learning](#general-purpose-machine-learning-16)\n  - [OCaml](#ocaml)\n    - [General-Purpose Machine Learning](#general-purpose-machine-learning-17)\n  - [Perl](#perl)\n    - [Data Analysis / Data Visualization](#data-analysis--data-visualization-7)\n    - [General-Purpose Machine Learning](#general-purpose-machine-learning-18)\n  - [Perl 6](#perl-6)\n    - [Data Analysis / Data Visualization](#data-analysis--data-visualization-8)\n    - [General-Purpose Machine Learning](#general-purpose-machine-learning-19)\n  - [PHP](#php)\n    - [Natural Language Processing](#natural-language-processing-9)\n    - [General-Purpose Machine Learning](#general-purpose-machine-learning-20)\n  - [Python](#python)\n      - [Computer Vision](#computer-vision-5)\n      - [Natural Language Processing](#natural-language-processing-10)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-21)\n      - [Data Analysis / Data Visualization](#data-analysis--data-visualization-9)\n      - [Misc Scripts / iPython Notebooks / Codebases](#misc-scripts--ipython-notebooks--codebases)\n      - [Neural Networks](#neural-networks)\n      - [Kaggle Competition Source Code](#kaggle-competition-source-code)\n      - [Reinforcement Learning](#reinforcement-learning-1)\n  - [Ruby](#ruby)\n      - [Natural Language Processing](#natural-language-processing-11)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-22)\n      - [Data Analysis / Data Visualization](#data-analysis--data-visualization-10)\n      - [Misc](#misc-1)\n  - [Rust](#rust)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-23)\n  - [R](#r)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-24)\n      - [Data Analysis / Data Visualization](#data-analysis--data-visualization-11)\n  - [SAS](#sas)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-25)\n      - [Data Analysis / Data Visualization](#data-analysis--data-visualization-12)\n      - [Natural Language Processing](#natural-language-processing-12)\n      - [Demos and Scripts](#demos-and-scripts-2)\n  - [Scala](#scala)\n      - [Natural Language Processing](#natural-language-processing-13)\n      - [Data Analysis / Data Visualization](#data-analysis--data-visualization-13)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-26)\n  - [Scheme](#scheme)\n      - [Neural Networks](#neural-networks-1)\n  - [Swift](#swift)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-27)\n  - [TensorFlow](#tensorflow)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-28)\n\n### [Tools](#tools-1)\n\n- [Neural Networks](#tools-neural-networks)\n- [Misc](#tools-misc)\n\n\n[Credits](#credits)\n\n<!-- /MarkdownTOC -->\n\n<a name=\"apl\"></a>\n## APL\n\n<a name=\"apl-general-purpose\"></a>\n#### General-Purpose Machine Learning\n* [naive-apl](https://github.com/mattcunningham/naive-apl) - Naive Bayesian Classifier implementation in APL. **[Deprecated]**\n\n<a name=\"c\"></a>\n## C\n\n<a name=\"c-general-purpose\"></a>\n#### General-Purpose Machine Learning\n* [Darknet](https://github.com/pjreddie/darknet) - Darknet is an open source neural network framework written in C and CUDA. It is fast, easy to install, and supports CPU and GPU computation.\n* [Recommender](https://github.com/GHamrouni/Recommender) - A C library for product recommendations/suggestions using collaborative filtering (CF).\n* [Hybrid Recommender System](https://github.com/SeniorSA/hybrid-rs-trainner) - A hybrid recommender system based upon scikit-learn algorithms. **[Deprecated]**\n* [neonrvm](https://github.com/siavashserver/neonrvm) - neonrvm is an open source machine learning library based on RVM technique. It's written in C programming language and comes with Python programming language bindings.\n* [cONNXr](https://github.com/alrevuelta/cONNXr) - An `ONNX` runtime written in pure C (99) with zero dependencies focused on small embedded devices. Run inference on your machine learning models no matter which framework you train it with. Easy to install and compiles everywhere, even in very old devices.\n\n<a name=\"c-cv\"></a>\n#### Computer Vision\n\n* [CCV](https://github.com/liuliu/ccv) - C-based/Cached/Core Computer Vision Library, A Modern Computer Vision Library.\n* [VLFeat](http://www.vlfeat.org/) - VLFeat is an open and portable library of computer vision algorithms, which has a Matlab toolbox.\n\n<a name=\"cpp\"></a>\n## C++\n\n<a name=\"cpp-cv\"></a>\n#### Computer Vision\n\n* [DLib](http://dlib.net/imaging.html) - DLib has C++ and Python interfaces for face detection and training general object detectors.\n* [EBLearn](http://eblearn.sourceforge.net/) - Eblearn is an object-oriented C++ library that implements various machine learning models **[Deprecated]**\n* [OpenCV](https://opencv.org) - OpenCV has C++, C, Python, Java and MATLAB interfaces and supports Windows, Linux, Android and Mac OS.\n* [VIGRA](https://github.com/ukoethe/vigra) - VIGRA is a genertic cross-platform C++ computer vision and machine learning library for volumes of arbitrary dimensionality with Python bindings.\n* [Openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) - A real-time multi-person keypoint detection library for body, face, hands, and foot estimation\n\n<a name=\"cpp-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [BanditLib](https://github.com/jkomiyama/banditlib) - A simple Multi-armed Bandit library. **[Deprecated]**\n* [Caffe](https://github.com/BVLC/caffe) - A deep learning framework developed with cleanliness, readability, and speed in mind. [DEEP LEARNING]\n* [CatBoost](https://github.com/catboost/catboost) - General purpose gradient boosting on decision trees library with categorical features support out of the box. It is easy to install, contains fast inference implementation and supports CPU and GPU (even multi-GPU) computation.\n* [CNTK](https://github.com/Microsoft/CNTK) - The Computational Network Toolkit (CNTK) by Microsoft Research, is a unified deep-learning toolkit that describes neural networks as a series of computational steps via a directed graph.\n* [CUDA](https://code.google.com/p/cuda-convnet/) - This is a fast C++/CUDA implementation of convolutional [DEEP LEARNING]\n* [DeepDetect](https://github.com/jolibrain/deepdetect) - A machine learning API and server written in C++11. It makes state of the art machine learning easy to work with and integrate into existing applications.\n* [Distributed Machine learning Tool Kit (DMTK)](http://www.dmtk.io/) - A distributed machine learning (parameter server) framework by Microsoft. Enables training models on large data sets across multiple machines. Current tools bundled with it include: LightLDA and Distributed (Multisense) Word Embedding.\n* [DLib](http://dlib.net/ml.html) - A suite of ML tools designed to be easy to imbed in other applications.\n* [DSSTNE](https://github.com/amznlabs/amazon-dsstne) - A software library created by Amazon for training and deploying deep neural networks using GPUs which emphasizes speed and scale over experimental flexibility.\n* [DyNet](https://github.com/clab/dynet) - A dynamic neural network library working well with networks that have dynamic structures that change for every training instance. Written in C++ with bindings in Python.\n* [Fido](https://github.com/FidoProject/Fido) - A highly-modular C++ machine learning library for embedded electronics and robotics.\n* [igraph](http://igraph.org/) - General purpose graph library.\n* [Intel(R) DAAL](https://github.com/intel/daal) - A high performance software library developed by Intel and optimized for Intel's architectures. Library provides algorithmic building blocks for all stages of data analytics and allows to process data in batch, online and distributed modes.\n* [LightGBM](https://github.com/Microsoft/LightGBM) - Microsoft's fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.\n* [libfm](https://github.com/srendle/libfm) - A generic approach that allows to mimic most factorization models by feature engineering.\n* [MLDB](https://mldb.ai) - The Machine Learning Database is a database designed for machine learning. Send it commands over a RESTful API to store data, explore it using SQL, then train machine learning models and expose them as APIs.\n* [mlpack](https://www.mlpack.org/) - A scalable C++ machine learning library.\n* [MXNet](https://github.com/apache/incubator-mxnet) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [ParaMonte](https://github.com/cdslaborg/paramonte) - A general-purpose library with C/C++ interface for Bayesian data analysis and visualization via serial/parallel Monte Carlo and MCMC simulations. Documentation can be found [here](https://www.cdslab.org/paramonte/).\n* [proNet-core](https://github.com/cnclabs/proNet-core) - A general-purpose network embedding framework: pair-wise representations optimization Network Edit.\n* [PyCUDA](https://mathema.tician.de/software/pycuda/) - Python interface to CUDA\n* [ROOT](https://root.cern.ch) - A modular scientific software framework. It provides all the functionalities needed to deal with big data processing, statistical analysis, visualization and storage.\n* [shark](http://image.diku.dk/shark/sphinx_pages/build/html/index.html) - A fast, modular, feature-rich open-source C++ machine learning library.\n* [Shogun](https://github.com/shogun-toolbox/shogun) - The Shogun Machine Learning Toolbox.\n* [sofia-ml](https://code.google.com/archive/p/sofia-ml) - Suite of fast incremental algorithms.\n* [Stan](http://mc-stan.org/) - A probabilistic programming language implementing full Bayesian statistical inference with Hamiltonian Monte Carlo sampling.\n* [Timbl](https://languagemachines.github.io/timbl/) - A software package/C++ library implementing several memory-based learning algorithms, among which IB1-IG, an implementation of k-nearest neighbor classification, and IGTree, a decision-tree approximation of IB1-IG. Commonly used for NLP.\n* [Vowpal Wabbit (VW)](https://github.com/VowpalWabbit/vowpal_wabbit) - A fast out-of-core learning system.\n* [Warp-CTC](https://github.com/baidu-research/warp-ctc) - A fast parallel implementation of Connectionist Temporal Classification (CTC), on both CPU and GPU.\n* [XGBoost](https://github.com/dmlc/xgboost) - A parallelized optimized general purpose gradient boosting library.\n* [ThunderGBM](https://github.com/Xtra-Computing/thundergbm) - A fast library for GBDTs and Random Forests on GPUs.\n* [ThunderSVM](https://github.com/Xtra-Computing/thundersvm) - A fast SVM library on GPUs and CPUs.\n* [LKYDeepNN](https://github.com/mosdeo/LKYDeepNN) - A header-only C++11 Neural Network library. Low dependency, native traditional chinese document.\n* [xLearn](https://github.com/aksnzhy/xlearn) - A high performance, easy-to-use, and scalable machine learning package, which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data, which is very common in Internet services such as online advertising and recommender systems.\n* [Featuretools](https://github.com/featuretools/featuretools) - A library for automated feature engineering. It excels at transforming transactional and relational datasets into feature matrices for machine learning using reusable feature engineering \"primitives\".\n* [skynet](https://github.com/Tyill/skynet) - A library for learning neural networks, has C-interface, net set in JSON. Written in C++ with bindings in Python, C++ and C#.\n* [Feast](https://github.com/gojek/feast) - A feature store for the management, discovery, and access of machine learning features. Feast provides a consistent view of feature data for both model training and model serving.\n* [Hopsworks](https://github.com/logicalclocks/hopsworks) - A data-intensive platform for AI with the industry's first open-source feature store. The Hopsworks Feature Store provides both a feature warehouse for training and batch based on Apache Hive and a feature serving database, based on MySQL Cluster, for online applications.\n* [Polyaxon](https://github.com/polyaxon/polyaxon) - A platform for reproducible and scalable machine learning and deep learning.\n\n<a name=\"cpp-nlp\"></a>\n#### Natural Language Processing\n\n* [BLLIP Parser](https://github.com/BLLIP/bllip-parser) - BLLIP Natural Language Parser (also known as the Charniak-Johnson parser).\n* [colibri-core](https://github.com/proycon/colibri-core) - C++ library, command line tools, and Python binding for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.\n* [CRF++](https://taku910.github.io/crfpp/) - Open source implementation of Conditional Random Fields (CRFs) for segmenting/labeling sequential data & other Natural Language Processing tasks. **[Deprecated]**\n* [CRFsuite](http://www.chokkan.org/software/crfsuite/) - CRFsuite is an implementation of Conditional Random Fields (CRFs) for labeling sequential data. **[Deprecated]**\n* [frog](https://github.com/LanguageMachines/frog) - Memory-based NLP suite developed for Dutch: PoS tagger, lemmatiser, dependency parser, NER, shallow parser, morphological analyzer.\n* [libfolia](https://github.com/LanguageMachines/libfolia) - C++ library for the [FoLiA format](https://proycon.github.io/folia/)\n* [MeTA](https://github.com/meta-toolkit/meta) - [MeTA : ModErn Text Analysis](https://meta-toolkit.org/) is a C++ Data Sciences Toolkit that facilitates mining big text data.\n* [MIT Information Extraction Toolkit](https://github.com/mit-nlp/MITIE) - C, C++, and Python tools for named entity recognition and relation extraction\n* [ucto](https://github.com/LanguageMachines/ucto) - Unicode-aware regular-expression based tokenizer for various languages. Tool and C++ library. Supports FoLiA format.\n\n<a name=\"cpp-speech-recognition\"></a>\n#### Speech Recognition\n* [Kaldi](https://github.com/kaldi-asr/kaldi) - Kaldi is a toolkit for speech recognition written in C++ and licensed under the Apache License v2.0. Kaldi is intended for use by speech recognition researchers.\n\n<a name=\"cpp-sequence\"></a>\n#### Sequence Analysis\n* [ToPS](https://github.com/ayoshiaki/tops) - This is an object-oriented framework that facilitates the integration of probabilistic models for sequences over a user defined alphabet. **[Deprecated]**\n\n<a name=\"cpp-gestures\"></a>\n#### Gesture Detection\n* [grt](https://github.com/nickgillian/grt) - The Gesture Recognition Toolkit (GRT) is a cross-platform, open-source, C++ machine learning library designed for real-time gesture recognition.\n\n<a name=\"common-lisp\"></a>\n## Common Lisp\n\n<a name=\"common-lisp-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [mgl](https://github.com/melisgl/mgl/) - Neural networks (boltzmann machines, feed-forward and recurrent nets), Gaussian Processes.\n* [mgl-gpr](https://github.com/melisgl/mgl-gpr/) - Evolutionary algorithms. **[Deprecated]**\n* [cl-libsvm](https://github.com/melisgl/cl-libsvm/) - Wrapper for the libsvm support vector machine library. **[Deprecated]**\n* [cl-online-learning](https://github.com/masatoi/cl-online-learning) - Online learning algorithms (Perceptron, AROW, SCW, Logistic Regression).\n* [cl-random-forest](https://github.com/masatoi/cl-random-forest) - Implementation of Random Forest in Common Lisp.\n\n<a name=\"clojure\"></a>\n## Clojure\n\n<a name=\"clojure-nlp\"></a>\n#### Natural Language Processing\n\n* [Clojure-openNLP](https://github.com/dakrone/clojure-opennlp) - Natural Language Processing in Clojure (opennlp).\n* [Infections-clj](https://github.com/r0man/inflections-clj) - Rails-like inflection library for Clojure and ClojureScript.\n\n<a name=\"clojure-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [tech.ml](https://github.com/techascent/tech.ml) - A machine learning platform based on tech.ml.dataset, supporting not just ml algorithms, but also relevant ETL processing; wraps multiple machine learning libraries\n* [clj-ml](https://github.com/joshuaeckroth/clj-ml/) - A machine learning library for Clojure built on top of Weka and friends. \n* [clj-boost](https://gitlab.com/alanmarazzi/clj-boost) - Wrapper for XGBoost\n* [Touchstone](https://github.com/ptaoussanis/touchstone) - Clojure A/B testing library. \n* [Clojush](https://github.com/lspector/Clojush) - The Push programming language and the PushGP genetic programming system implemented in Clojure.\n* [lambda-ml](https://github.com/cloudkj/lambda-ml) - Simple, concise implementations of machine learning techniques and utilities in Clojure.\n* [Infer](https://github.com/aria42/infer) - Inference and machine learning in Clojure. **[Deprecated]**\n* [Encog](https://github.com/jimpil/enclog) - Clojure wrapper for Encog (v3) (Machine-Learning framework that specializes in neural-nets). **[Deprecated]**\n* [Fungp](https://github.com/vollmerm/fungp) - A genetic programming library for Clojure. **[Deprecated]**\n* [Statistiker](https://github.com/clojurewerkz/statistiker) - Basic Machine Learning algorithms in Clojure. **[Deprecated]**\n* [clortex](https://github.com/htm-community/clortex) - General Machine Learning library using Numenta\u2019s Cortical Learning Algorithm. **[Deprecated]**\n* [comportex](https://github.com/htm-community/comportex) - Functionally composable Machine Learning library using Numenta\u2019s Cortical Learning Algorithm. **[Deprecated]**\n\n<a name=\"clojure-deep-learning\"></a>\n#### Deep Learning\n* [MXNet](https://mxnet.apache.org/versions/1.7.0/api/clojure) - Bindings to Apache MXNet - part of the MXNet project\n* [Deep Diamond](https://github.com/uncomplicate/deep-diamond) - A fast Clojure Tensor & Deep Learning library\n* [jutsu.ai](https://github.com/hswick/jutsu.ai) - Clojure wrapper for deeplearning4j with some added syntactic sugar.\n* [cortex](https://github.com/originrose/cortex) - Neural networks, regression and feature learning in Clojure.\n* [Flare](https://github.com/aria42/flare) - Dynamic Tensor Graph library in Clojure (think PyTorch, DynNet, etc.) \n* [dl4clj](https://github.com/yetanalytics/dl4clj) - Clojure wrapper for Deeplearning4j.\n\n<a name=\"clojure-data-analysis\"></a>\n#### Data Analysis \n* [tech.ml.dataset](https://github.com/techascent/tech.ml.dataset) - Clojure dataframe library and pipeline for data processing and machine learning  \n* [Tablecloth](https://github.com/scicloj/tablecloth) - A dataframe grammar wrapping tech.ml.dataset, inspired by several R libraries\n* [Panthera](https://github.com/alanmarazzi/panthera) - Clojure API wrapping Python's Pandas library\n* [Incanter](http://incanter.org/) - Incanter is a Clojure-based, R-like platform for statistical computing and graphics.\n* [PigPen](https://github.com/Netflix/PigPen) - Map-Reduce for Clojure.\n* [Geni](https://github.com/zero-one-group/geni) - a Clojure dataframe library that runs on Apache Spark\n\n<a name=\"clojure-data-visualization\"></a>\n#### Data Visualization\n* [Hanami](https://github.com/jsa-aerial/hanami) : Clojure(Script) library and framework for creating interactive visualization applications based in Vega-Lite (VGL) and/or Vega (VG) specifications. Automatic framing and layouts along with a powerful templating system for abstracting visualization specs\n* [Saite](https://github.com/jsa-aerial/saite) -  Clojure(Script) client/server application for dynamic interactive explorations and the creation of live shareable documents capturing them using Vega/Vega-Lite, CodeMirror, markdown, and LaTeX\n* [Oz](https://github.com/metasoarous/oz) - Data visualisation using Vega/Vega-Lite and Hiccup, and a live-reload platform for literate-programming\n* [Envision](https://github.com/clojurewerkz/envision) - Clojure Data Visualisation library, based on Statistiker and D3.\n* [Pink Gorilla Notebook](https://github.com/pink-gorilla/gorilla-notebook) - A Clojure/Clojurescript notebook application/-library based on Gorilla-REPL \n* [clojupyter](https://github.com/clojupyter/clojupyter) -  A Jupyter kernel for Clojure - run Clojure code in Jupyter Lab, Notebook and Console.\n* [notespace](https://github.com/scicloj/notespace) - Notebook experience in your Clojure namespace \n* [Delight](https://github.com/datamechanics/delight) - A listener that streams your spark events logs to delight, a free and improved spark UI \n\n<a name=\"clojure-interop\"></a>\n#### Interop\n\n* [Java Interop](https://clojure.org/reference/java_interop) - Clojure has Native Java Interop from which Java's ML ecosystem can be accessed\n* [JavaScript Interop](https://clojurescript.org/reference/javascript-api) - ClojureScript has Native JavaScript Interop from which JavaScript's ML ecosystem can be accessed\n* [Libpython-clj](https://github.com/clj-python/libpython-clj) - Interop with Python\n* [ClojisR](https://github.com/scicloj/clojisr) - Interop with R and Renjin (R on the JVM)\n\n<a name=\"clojure-misc\"></a>\n#### Misc\n* [Neanderthal](https://neanderthal.uncomplicate.org/) - Fast Clojure Matrix Library (native CPU, GPU, OpenCL, CUDA)\n* [kixistats](https://github.com/MastodonC/kixi.stats) - A library of statistical distribution sampling and transducing functions \n* [fastmath](https://github.com/generateme/fastmath) - A collection of functions for mathematical and statistical computing, macine learning, etc., wrapping several JVM libraries\n* [matlib](https://github.com/atisharma/matlib) - a Clojure library of optimisation and control theory tools and convenience functions based on Neanderthal.\n\n<a name=\"clojure-extra\"></a>\n#### Extra\n* [Scicloj](https://scicloj.github.io/pages/libraries/) - Curated list of ML related resources for Clojure.\n\n<a name=\"crystal\"></a>\n## Crystal\n\n<a name=\"crystal-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [machine](https://github.com/mathieulaporte/machine) - Simple machine learning algorithm.\n* [crystal-fann](https://github.com/NeuraLegion/crystal-fann) - FANN (Fast Artificial Neural Network) binding.\n\n<a name=\"elixir\"></a>\n## Elixir\n\n<a name=\"elixir-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [Simple Bayes](https://github.com/fredwu/simple_bayes) - A Simple Bayes / Naive Bayes implementation in Elixir.\n* [emel](https://github.com/mrdimosthenis/emel) - A simple and functional machine learning library written in Elixir.\n* [Tensorflex](https://github.com/anshuman23/tensorflex) - Tensorflow bindings for the Elixir programming language.\n\n<a name=\"elixir-nlp\"></a>\n#### Natural Language Processing\n\n* [Stemmer](https://github.com/fredwu/stemmer) - An English (Porter2) stemming implementation in Elixir.\n\n<a name=\"erlang\"></a>\n## Erlang\n\n<a name=\"erlang-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [Disco](https://github.com/discoproject/disco/) - Map Reduce in Erlang. **[Deprecated]**\n\n<a name=\"fortran\"></a>\n## Fortran\n\n<a name=\"fortran-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [neural-fortran](https://github.com/modern-fortran/neural-fortran) - A parallel neural net microframework. \nRead the paper [here](https://arxiv.org/abs/1902.06714).\n\n<a name=\"fortran-data-analysis-visualization\"></a>\n#### Data Analysis / Data Visualization\n\n* [ParaMonte](https://github.com/cdslaborg/paramonte) - A general-purpose Fortran library for Bayesian data analysis and visualization via serial/parallel Monte Carlo and MCMC simulations. Documentation can be found [here](https://www.cdslab.org/paramonte/).\n\n<a name=\"go\"></a>\n## Go\n\n<a name=\"go-nlp\"></a>\n#### Natural Language Processing\n\n* [snowball](https://github.com/tebeka/snowball) - Snowball Stemmer for Go.\n* [word-embedding](https://github.com/ynqa/word-embedding) - Word Embeddings: the full implementation of word2vec, GloVe in Go.\n* [sentences](https://github.com/neurosnap/sentences) - Golang implementation of Punkt sentence tokenizer.\n* [go-ngram](https://github.com/Lazin/go-ngram) - In-memory n-gram index with compression. *[Deprecated]*\n* [paicehusk](https://github.com/Rookii/paicehusk) - Golang implementation of the Paice/Husk Stemming Algorithm. *[Deprecated]*\n* [go-porterstemmer](https://github.com/reiver/go-porterstemmer) - A native Go clean room implementation of the Porter Stemming algorithm. **[Deprecated]**\n\n<a name=\"go-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [birdland](https://github.com/rlouf/birdland) - A recommendation library in Go.\n* [eaopt](https://github.com/MaxHalford/eaopt) - An evolutionary optimization library.\n* [leaves](https://github.com/dmitryikh/leaves) - A pure Go implementation of the prediction part of GBRTs, including XGBoost and LightGBM.\n* [gobrain](https://github.com/goml/gobrain) - Neural Networks written in Go.\n* [go-mxnet-predictor](https://github.com/songtianyi/go-mxnet-predictor) - Go binding for MXNet c_predict_api to do inference with a pre-trained model.\n* [go-ml-transpiler](https://github.com/znly/go-ml-transpiler) - An open source Go transpiler for machine learning models.\n* [golearn](https://github.com/sjwhitworth/golearn) - Machine learning for Go.\n* [goml](https://github.com/cdipaolo/goml) - Machine learning library written in pure Go.\n* [gorgonia](https://github.com/gorgonia/gorgonia) - Deep learning in Go.\n* [goro](https://github.com/aunum/goro) - A high-level machine learning library in the vein of Keras.\n* [gorse](https://github.com/zhenghaoz/gorse) - An offline recommender system backend based on collaborative filtering written in Go.\n* [therfoo](https://github.com/therfoo/therfoo) - An embedded deep learning library for Go.\n* [neat](https://github.com/jinyeom/neat) - Plug-and-play, parallel Go framework for NeuroEvolution of Augmenting Topologies (NEAT). **[Deprecated]**\n* [go-pr](https://github.com/daviddengcn/go-pr) - Pattern recognition package in Go lang. **[Deprecated]**\n* [go-ml](https://github.com/alonsovidales/go_ml) - Linear / Logistic regression, Neural Networks, Collaborative Filtering and Gaussian Multivariate Distribution. **[Deprecated]**\n* [GoNN](https://github.com/fxsjy/gonn) - GoNN is an implementation of Neural Network in Go Language, which includes BPNN, RBF, PCN. **[Deprecated]**\n* [bayesian](https://github.com/jbrukh/bayesian) - Naive Bayesian Classification for Golang. **[Deprecated]**\n* [go-galib](https://github.com/thoj/go-galib) - Genetic Algorithms library written in Go / Golang. **[Deprecated]**\n* [Cloudforest](https://github.com/ryanbressler/CloudForest) - Ensembles of decision trees in Go/Golang. **[Deprecated]**\n* [go-dnn](https://github.com/sudachen/go-dnn) - Deep Neural Networks for Golang (powered by MXNet)\n\n<a name=\"go-spatial-analysis\"></a>\n#### Spatial analysis and geometry\n\n* [go-geom](https://github.com/twpayne/go-geom) - Go library to handle geometries.\n* [gogeo](https://github.com/golang/geo) - Spherical geometry in Go.\n\n<a name=\"go-data-analysis\"></a>\n#### Data Analysis / Data Visualization\n\n* [dataframe-go](https://github.com/rocketlaunchr/dataframe-go) - Dataframes for machine-learning and statistics (similar to pandas).\n* [gota](https://github.com/go-gota/gota) - Dataframes.\n* [gonum/mat](https://godoc.org/gonum.org/v1/gonum/mat) - A linear algebra package for Go.\n* [gonum/optimize](https://godoc.org/gonum.org/v1/gonum/optimize) - Implementations of optimization algorithms.\n* [gonum/plot](https://godoc.org/gonum.org/v1/plot) - A plotting library.\n* [gonum/stat](https://godoc.org/gonum.org/v1/gonum/stat) - A statistics library.\n* [SVGo](https://github.com/ajstarks/svgo) - The Go Language library for SVG generation.\n* [glot](https://github.com/arafatk/glot) - Glot is a plotting library for Golang built on top of gnuplot.\n* [globe](https://github.com/mmcloughlin/globe) - Globe wireframe visualization.\n* [gonum/graph](https://godoc.org/gonum.org/v1/gonum/graph) - General-purpose graph library.\n* [go-graph](https://github.com/StepLg/go-graph) - Graph library for Go/Golang language. **[Deprecated]**\n* [RF](https://github.com/fxsjy/RF.go) - Random forests implementation in Go. **[Deprecated]**\n\n<a name=\"go-computer-vision\"></a>\n#### Computer vision\n\n* [GoCV](https://github.com/hybridgroup/gocv) - Package for computer vision using OpenCV 4 and beyond.\n\n<a name=\"go-reinforcement-learning\"></a>\n#### Reinforcement learning\n\n* [gold](https://github.com/aunum/gold) - A reinforcement learning library.\n\n<a name=\"haskell\"></a>\n## Haskell\n\n<a name=\"haskell-general-purpose\"></a>\n#### General-Purpose Machine Learning\n* [haskell-ml](https://github.com/ajtulloch/haskell-ml) - Haskell implementations of various ML algorithms. **[Deprecated]**\n* [HLearn](https://github.com/mikeizbicki/HLearn) - a suite of libraries for interpreting machine learning models according to their algebraic structure. **[Deprecated]**\n* [hnn](https://github.com/alpmestan/HNN) - Haskell Neural Network library.\n* [hopfield-networks](https://github.com/ajtulloch/hopfield-networks) - Hopfield Networks for unsupervised learning in Haskell. **[Deprecated]**\n* [DNNGraph](https://github.com/ajtulloch/dnngraph) - A DSL for deep neural networks. **[Deprecated]**\n* [LambdaNet](https://github.com/jbarrow/LambdaNet) - Configurable Neural Networks in Haskell. **[Deprecated]**\n\n<a name=\"java\"></a>\n## Java\n\n<a name=\"java-nlp\"></a>\n#### Natural Language Processing\n* [Cortical.io](https://www.cortical.io/) - Retina: an API performing complex NLP operations (disambiguation, classification, streaming text filtering, etc...) as quickly and intuitively as the brain.\n* [IRIS](https://github.com/cortical-io/Iris) - [Cortical.io's](https://cortical.io) FREE NLP, Retina API Analysis Tool (written in JavaFX!) - [See the Tutorial Video](https://www.youtube.com/watch?v=CsF4pd7fGF0).\n* [CoreNLP](https://nlp.stanford.edu/software/corenlp.shtml) - Stanford CoreNLP provides a set of natural language analysis tools which can take raw English language text input and give the base forms of words.\n* [Stanford Parser](https://nlp.stanford.edu/software/lex-parser.shtml) - A natural language parser is a program that works out the grammatical structure of sentences.\n* [Stanford POS Tagger](https://nlp.stanford.edu/software/tagger.shtml) - A Part-Of-Speech Tagger (POS Tagger).\n* [Stanford Name Entity Recognizer](https://nlp.stanford.edu/software/CRF-NER.shtml) - Stanford NER is a Java implementation of a Named Entity Recognizer.\n* [Stanford Word Segmenter](https://nlp.stanford.edu/software/segmenter.shtml) - Tokenization of raw text is a standard pre-processing step for many NLP tasks.\n* [Tregex, Tsurgeon and Semgrex](https://nlp.stanford.edu/software/tregex.shtml) - Tregex is a utility for matching patterns in trees, based on tree relationships and regular expression matches on nodes (the name is short for \"tree regular expressions\").\n* [Stanford Phrasal: A Phrase-Based Translation System](https://nlp.stanford.edu/phrasal/)\n* [Stanford English Tokenizer](https://nlp.stanford.edu/software/tokenizer.shtml) - Stanford Phrasal is a state-of-the-art statistical phrase-based machine translation system, written in Java.\n* [Stanford Tokens Regex](https://nlp.stanford.edu/software/tokensregex.shtml) - A tokenizer divides text into a sequence of tokens, which roughly correspond to \"words\".\n* [Stanford Temporal Tagger](https://nlp.stanford.edu/software/sutime.shtml) - SUTime is a library for recognizing and normalizing time expressions.\n* [Stanford SPIED](https://nlp.stanford.edu/software/patternslearning.shtml) - Learning entities from unlabeled text starting with seed sets using patterns in an iterative fashion.\n* [Twitter Text Java](https://github.com/twitter/twitter-text/tree/master/java) - A Java implementation of Twitter's text processing library.\n* [MALLET](http://mallet.cs.umass.edu/) - A Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.\n* [OpenNLP](https://opennlp.apache.org/) - a machine learning based toolkit for the processing of natural language text.\n* [LingPipe](http://alias-i.com/lingpipe/index.html) - A tool kit for processing text using computational linguistics.\n* [ClearTK](https://github.com/ClearTK/cleartk) - ClearTK provides a framework for developing statistical natural language processing (NLP) components in Java and is built on top of Apache UIMA. **[Deprecated]**\n* [Apache cTAKES](https://ctakes.apache.org/) - Apache Clinical Text Analysis and Knowledge Extraction System (cTAKES) is an open-source natural language processing system for information extraction from electronic medical record clinical free-text.\n* [NLP4J](https://github.com/emorynlp/nlp4j) - The NLP4J project provides software and resources for natural language processing. The project started at the Center for Computational Language and EducAtion Research, and is currently developed by the Center for Language and Information Research at Emory University. **[Deprecated]**\n* [CogcompNLP](https://github.com/CogComp/cogcomp-nlp) - This project collects a number of core libraries for Natural Language Processing (NLP) developed in the University of Illinois' Cognitive Computation Group, for example `illinois-core-utilities` which provides a set of NLP-friendly data structures and a number of NLP-related utilities that support writing NLP applications, running experiments, etc, `illinois-edison` a library for feature extraction from illinois-core-utilities data structures and many other packages.\n\n<a name=\"java-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [aerosolve](https://github.com/airbnb/aerosolve) - A machine learning library by Airbnb designed from the ground up to be human friendly.\n* [AMIDST Toolbox](http://www.amidsttoolbox.com/) - A Java Toolbox for Scalable Probabilistic Machine Learning.\n* [Datumbox](https://github.com/datumbox/datumbox-framework) - Machine Learning framework for rapid development of Machine Learning and Statistical applications.\n* [ELKI](https://elki-project.github.io/) - Java toolkit for data mining. (unsupervised: clustering, outlier detection etc.)\n* [Encog](https://github.com/encog/encog-java-core) - An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.\n* [FlinkML in Apache Flink](https://ci.apache.org/projects/flink/flink-docs-master/dev/libs/ml/index.html) - Distributed machine learning library in Flink.\n* [H2O](https://github.com/h2oai/h2o-3) - ML engine that supports distributed learning on Hadoop, Spark or your laptop via APIs in R, Python, Scala, REST/JSON.\n* [htm.java](https://github.com/numenta/htm.java) - General Machine Learning library using Numenta\u2019s Cortical Learning Algorithm.\n* [liblinear-java](https://github.com/bwaldvogel/liblinear-java) - Java version of liblinear.\n* [Mahout](https://github.com/apache/mahout) - Distributed machine learning.\n* [Meka](http://meka.sourceforge.net/) - An open source implementation of methods for multi-label classification and evaluation (extension to Weka).\n* [MLlib in Apache Spark](https://spark.apache.org/docs/latest/mllib-guide.html) - Distributed machine learning library in Spark\n* [Hydrosphere Mist](https://github.com/Hydrospheredata/mist) - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.\n* [Neuroph](http://neuroph.sourceforge.net/) - Neuroph is lightweight Java neural network framework\n* [ORYX](https://github.com/oryxproject/oryx) - Lambda Architecture Framework using Apache Spark and Apache Kafka with a specialization for real-time large-scale machine learning.\n* [Samoa](https://samoa.incubator.apache.org/) SAMOA is a framework that includes distributed machine learning for data streams with an interface to plug-in different stream processing platforms.\n* [RankLib](https://sourceforge.net/p/lemur/wiki/RankLib/) - RankLib is a library of learning to rank algorithms. **[Deprecated]**\n* [rapaio](https://github.com/padreati/rapaio) - statistics, data mining and machine learning toolbox in Java.\n* [RapidMiner](https://rapidminer.com) - RapidMiner integration into Java code.\n* [Stanford Classifier](https://nlp.stanford.edu/software/classifier.shtml) - A classifier is a machine learning tool that will take data items and place them into one of k classes.\n* [Smile](https://haifengl.github.io/) - Statistical Machine Intelligence & Learning Engine.\n* [SystemML](https://github.com/apache/systemml) - flexible, scalable machine learning (ML) language.\n* [Weka](https://www.cs.waikato.ac.nz/ml/weka/) - Weka is a collection of machine learning algorithms for data mining tasks.\n* [LBJava](https://github.com/CogComp/lbjava) - Learning Based Java is a modeling language for the rapid development of software systems, offers a convenient, declarative syntax for classifier and constraint definition directly in terms of the objects in the programmer's application.\n\n\n<a name=\"java-speech-recognition\"></a>\n#### Speech Recognition\n* [CMU Sphinx](https://cmusphinx.github.io) - Open Source Toolkit For Speech Recognition purely based on Java speech recognition library.\n\n<a name=\"java-data-analysis\"></a>\n#### Data Analysis / Data Visualization\n\n* [Flink](https://flink.apache.org/) - Open source platform for distributed stream and batch data processing.\n* [Hadoop](https://github.com/apache/hadoop) - Hadoop/HDFS.\n* [Onyx](https://github.com/onyx-platform/onyx) - Distributed, masterless, high performance, fault tolerant data processing. Written entirely in Clojure.\n* [Spark](https://github.com/apache/spark) - Spark is a fast and general engine for large-scale data processing.\n* [Storm](https://storm.apache.org/) - Storm is a distributed realtime computation system.\n* [Impala](https://github.com/cloudera/impala) - Real-time Query for Hadoop.\n* [DataMelt](https://jwork.org/dmelt/) - Mathematics software for numeric computation, statistics, symbolic calculations, data analysis and data visualization.\n* [Dr. Michael Thomas Flanagan's Java Scientific Library](https://www.ee.ucl.ac.uk/~mflanaga/java/) **[Deprecated]**\n\n<a name=\"java-deep-learning\"></a>\n#### Deep Learning\n\n* [Deeplearning4j](https://github.com/deeplearning4j/deeplearning4j) - Scalable deep learning for industry with parallel GPUs.\n* [Keras Beginner Tutorial](https://victorzhou.com/blog/keras-neural-network-tutorial/) - Friendly guide on using Keras to implement a simple Neural Network in Python\n\n<a name=\"javascript\"></a>\n## Javascript\n\n<a name=\"javascript-nlp\"></a>\n#### Natural Language Processing\n\n* [Twitter-text](https://github.com/twitter/twitter-text) - A JavaScript implementation of Twitter's text processing library.\n* [natural](https://github.com/NaturalNode/natural) - General natural language facilities for node.\n* [Knwl.js](https://github.com/loadfive/Knwl.js) - A Natural Language Processor in JS.\n* [Retext](https://github.com/retextjs/retext) - Extensible system for analyzing and manipulating natural language.\n* [NLP Compromise](https://github.com/spencermountain/compromise) - Natural Language processing in the browser.\n* [nlp.js](https://github.com/axa-group/nlp.js) - An NLP library built in node over Natural, with entity extraction, sentiment analysis, automatic language identify, and so more\n\n\n\n<a name=\"javascript-data-analysis\"></a>\n#### Data Analysis / Data Visualization\n\n* [D3.js](https://d3js.org/)\n* [High Charts](https://www.highcharts.com/)\n* [NVD3.js](http://nvd3.org/)\n* [dc.js](https://dc-js.github.io/dc.js/)\n* [chartjs](https://www.chartjs.org/)\n* [dimple](http://dimplejs.org/)\n* [amCharts](https://www.amcharts.com/)\n* [D3xter](https://github.com/NathanEpstein/D3xter) - Straight forward plotting built on D3. **[Deprecated]**\n* [statkit](https://github.com/rigtorp/statkit) - Statistics kit for JavaScript. **[Deprecated]**\n* [datakit](https://github.com/nathanepstein/datakit) - A lightweight framework for data analysis in JavaScript\n* [science.js](https://github.com/jasondavies/science.js/) - Scientific and statistical computing in JavaScript. **[Deprecated]**\n* [Z3d](https://github.com/NathanEpstein/Z3d) - Easily make interactive 3d plots built on Three.js **[Deprecated]**\n* [Sigma.js](http://sigmajs.org/) - JavaScript library dedicated to graph drawing.\n* [C3.js](https://c3js.org/) - customizable library based on D3.js for easy chart drawing.\n* [Datamaps](https://datamaps.github.io/) - Customizable SVG map/geo visualizations using D3.js. **[Deprecated]**\n* [ZingChart](https://www.zingchart.com/) - library written on Vanilla JS for big data visualization.\n* [cheminfo](https://www.cheminfo.org/) - Platform for data visualization and analysis, using the [visualizer](https://github.com/npellet/visualizer) project.\n* [Learn JS Data](http://learnjsdata.com/)\n* [AnyChart](https://www.anychart.com/)\n* [FusionCharts](https://www.fusioncharts.com/)\n* [Nivo](https://nivo.rocks) - built on top of the awesome d3 and Reactjs libraries\n\n\n<a name=\"javascript-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [Auto ML](https://github.com/ClimbsRocks/auto_ml) - Automated machine learning, data formatting, ensembling, and hyperparameter optimization for competitions and exploration- just give it a .csv file!\n* [Convnet.js](https://cs.stanford.edu/people/karpathy/convnetjs/) - ConvNetJS is a Javascript library for training Deep Learning models[DEEP LEARNING] **[Deprecated]**\n* [Clusterfck](https://harthur.github.io/clusterfck/) - Agglomerative hierarchical clustering implemented in Javascript for Node.js and the browser. **[Deprecated]**\n* [Clustering.js](https://github.com/emilbayes/clustering.js) - Clustering algorithms implemented in Javascript for Node.js and the browser. **[Deprecated]**\n* [Decision Trees](https://github.com/serendipious/nodejs-decision-tree-id3) - NodeJS Implementation of Decision Tree using ID3 Algorithm. **[Deprecated]**\n* [DN2A](https://github.com/antoniodeluca/dn2a.js) - Digital Neural Networks Architecture. **[Deprecated]**\n* [figue](https://code.google.com/archive/p/figue) - K-means, fuzzy c-means and agglomerative clustering.\n* [Gaussian Mixture Model](https://github.com/lukapopijac/gaussian-mixture-model) - Unsupervised machine learning with multivariate Gaussian mixture model.\n* [Node-fann](https://github.com/rlidwka/node-fann) - FANN (Fast Artificial Neural Network Library) bindings for Node.js **[Deprecated]**\n* [Keras.js](https://github.com/transcranial/keras-js) - Run Keras models in the browser, with GPU support provided by WebGL 2.\n* [Kmeans.js](https://github.com/emilbayes/kMeans.js) - Simple Javascript implementation of the k-means algorithm, for node.js and the browser. **[Deprecated]**\n* [LDA.js](https://github.com/primaryobjects/lda) - LDA topic modeling for Node.js\n* [Learning.js](https://github.com/yandongliu/learningjs) - Javascript implementation of logistic regression/c4.5 decision tree **[Deprecated]**\n* [machinelearn.js](https://github.com/machinelearnjs/machinelearnjs) - Machine Learning library for the web, Node.js and developers\n* [mil-tokyo](https://github.com/mil-tokyo) - List of several machine learning libraries.\n* [Node-SVM](https://github.com/nicolaspanel/node-svm) - Support Vector Machine for Node.js\n* [Brain](https://github.com/harthur/brain) - Neural networks in JavaScript **[Deprecated]**\n* [Brain.js](https://github.com/BrainJS/brain.js) - Neural networks in JavaScript - continued community fork of [Brain](https://github.com/harthur/brain).\n* [Bayesian-Bandit](https://github.com/omphalos/bayesian-bandit.js) - Bayesian bandit implementation for Node and the browser. **[Deprecated]**\n* [Synaptic](https://github.com/cazala/synaptic) - Architecture-free neural network library for Node.js and the browser.\n* [kNear](https://github.com/NathanEpstein/kNear) - JavaScript implementation of the k nearest neighbors algorithm for supervised learning.\n* [NeuralN](https://github.com/totemstech/neuraln) - C++ Neural Network library for Node.js. It has advantage on large dataset and multi-threaded training. **[Deprecated]**\n* [kalman](https://github.com/itamarwe/kalman) - Kalman filter for Javascript. **[Deprecated]**\n* [shaman](https://github.com/luccastera/shaman) - Node.js library with support for both simple and multiple linear regression. **[Deprecated]**\n* [ml.js](https://github.com/mljs/ml) - Machine learning and numerical analysis tools for Node.js and the Browser!\n* [ml5](https://github.com/ml5js/ml5-library) - Friendly machine learning for the web!\n* [Pavlov.js](https://github.com/NathanEpstein/Pavlov.js) - Reinforcement learning using Markov Decision Processes.\n* [MXNet](https://github.com/apache/incubator-mxnet) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [TensorFlow.js](https://js.tensorflow.org/) - A WebGL accelerated, browser based JavaScript library for training and deploying ML models.\n* [JSMLT](https://github.com/jsmlt/jsmlt) - Machine learning toolkit with classification and clustering for Node.js; supports visualization (see [visualml.io](https://visualml.io)).\n* [xgboost-node](https://github.com/nuanio/xgboost-node) - Run XGBoost model and make predictions in Node.js.\n* [Netron](https://github.com/lutzroeder/netron) - Visualizer for machine learning models.\n* [WebDNN](https://github.com/mil-tokyo/webdnn) - Fast Deep Neural Network Javascript Framework. WebDNN uses next generation JavaScript API, WebGPU for GPU execution, and WebAssembly for CPU execution.  \n\n<a name=\"javascript-misc\"></a>\n#### Misc\n\n* [stdlib](https://github.com/stdlib-js/stdlib) - A standard library for JavaScript and Node.js, with an emphasis on numeric computing. The library provides a collection of robust, high performance libraries for mathematics, statistics, streams, utilities, and more.\n* [sylvester](https://github.com/jcoglan/sylvester) - Vector and Matrix math for JavaScript. **[Deprecated]**\n* [simple-statistics](https://github.com/simple-statistics/simple-statistics) - A JavaScript implementation of descriptive, regression, and inference statistics. Implemented in literate JavaScript with no dependencies, designed to work in all modern browsers (including IE) as well as in Node.js.\n* [regression-js](https://github.com/Tom-Alexander/regression-js) - A javascript library containing a collection of least squares fitting methods for finding a trend in a set of data.\n* [Lyric](https://github.com/flurry/Lyric) - Linear Regression library. **[Deprecated]**\n* [GreatCircle](https://github.com/mwgg/GreatCircle) - Library for calculating great circle distance.\n* [MLPleaseHelp](https://github.com/jgreenemi/MLPleaseHelp) - MLPleaseHelp is a simple ML resource search engine. You can use this search engine right now at [https://jgreenemi.github.io/MLPleaseHelp/](https://jgreenemi.github.io/MLPleaseHelp/), provided via Github Pages.\n* [Pipcook](https://github.com/alibaba/pipcook) - A JavaScript application framework for machine learning and its engineering.\n\n<a name=\"javascript-demos\"></a>\n#### Demos and Scripts\n* [The Bot](https://github.com/sta-ger/TheBot) - Example of how the neural network learns to predict the angle between two points created with [Synaptic](https://github.com/cazala/synaptic).\n* [Half Beer](https://github.com/sta-ger/HalfBeer) - Beer glass classifier created with [Synaptic](https://github.com/cazala/synaptic).\n* [NSFWJS](http://nsfwjs.com) - Indecent content checker with TensorFlow.js\n* [Rock Paper Scissors](https://rps-tfjs.netlify.com/) - Rock Paper Scissors trained in the browser with TensorFlow.js\n\n<a name=\"julia\"></a>\n## Julia\n\n<a name=\"julia-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [MachineLearning](https://github.com/benhamner/MachineLearning.jl) - Julia Machine Learning library. **[Deprecated]**\n* [MLBase](https://github.com/JuliaStats/MLBase.jl) - A set of functions to support the development of machine learning algorithms.\n* [PGM](https://github.com/JuliaStats/PGM.jl) - A Julia framework for probabilistic graphical models.\n* [DA](https://github.com/trthatcher/DiscriminantAnalysis.jl) - Julia package for Regularized Discriminant Analysis.\n* [Regression](https://github.com/lindahua/Regression.jl) - Algorithms for regression analysis (e.g. linear regression and logistic regression). **[Deprecated]**\n* [Local Regression](https://github.com/JuliaStats/Loess.jl) - Local regression, so smooooth!\n* [Naive Bayes](https://github.com/nutsiepully/NaiveBayes.jl) - Simple Naive Bayes implementation in Julia. **[Deprecated]**\n* [Mixed Models](https://github.com/dmbates/MixedModels.jl) - A Julia package for fitting (statistical) mixed-effects models.\n* [Simple MCMC](https://github.com/fredo-dedup/SimpleMCMC.jl) - basic mcmc sampler implemented in Julia. **[Deprecated]**\n* [Distances](https://github.com/JuliaStats/Distances.jl) - Julia module for Distance evaluation.\n* [Decision Tree](https://github.com/bensadeghi/DecisionTree.jl) - Decision Tree Classifier and Regressor.\n* [Neural](https://github.com/compressed/BackpropNeuralNet.jl) - A neural network in Julia.\n* [MCMC](https://github.com/doobwa/MCMC.jl) - MCMC tools for Julia. **[Deprecated]**\n* [Mamba](https://github.com/brian-j-smith/Mamba.jl) - Markov chain Monte Carlo (MCMC) for Bayesian analysis in Julia.\n* [GLM](https://github.com/JuliaStats/GLM.jl) - Generalized linear models in Julia.\n* [Gaussian Processes](https://github.com/STOR-i/GaussianProcesses.jl) - Julia package for Gaussian processes.\n* [Online Learning](https://github.com/lendle/OnlineLearning.jl) **[Deprecated]**\n* [GLMNet](https://github.com/simonster/GLMNet.jl) - Julia wrapper for fitting Lasso/ElasticNet GLM models using glmnet.\n* [Clustering](https://github.com/JuliaStats/Clustering.jl) - Basic functions for clustering data: k-means, dp-means, etc.\n* [SVM](https://github.com/JuliaStats/SVM.jl) - SVM for Julia. **[Deprecated]**\n* [Kernel Density](https://github.com/JuliaStats/KernelDensity.jl) - Kernel density estimators for julia.\n* [MultivariateStats](https://github.com/JuliaStats/MultivariateStats.jl) - Methods for dimensionality reduction.\n* [NMF](https://github.com/JuliaStats/NMF.jl) - A Julia package for non-negative matrix factorization.\n* [ANN](https://github.com/EricChiang/ANN.jl) - Julia artificial neural networks. **[Deprecated]**\n* [Mocha](https://github.com/pluskid/Mocha.jl) - Deep Learning framework for Julia inspired by Caffe. **[Deprecated]**\n* [XGBoost](https://github.com/dmlc/XGBoost.jl) - eXtreme Gradient Boosting Package in Julia.\n* [ManifoldLearning](https://github.com/wildart/ManifoldLearning.jl) - A Julia package for manifold learning and nonlinear dimensionality reduction.\n* [MXNet](https://github.com/apache/incubator-mxnet) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [Merlin](https://github.com/hshindo/Merlin.jl) - Flexible Deep Learning Framework in Julia.\n* [ROCAnalysis](https://github.com/davidavdav/ROCAnalysis.jl) - Receiver Operating Characteristics and functions for evaluation probabilistic binary classifiers.\n* [GaussianMixtures](https://github.com/davidavdav/GaussianMixtures.jl) - Large scale Gaussian Mixture Models.\n* [ScikitLearn](https://github.com/cstjean/ScikitLearn.jl) - Julia implementation of the scikit-learn API.\n* [Knet](https://github.com/denizyuret/Knet.jl) - Ko\u00e7 University Deep Learning Framework.\n* [Flux](https://fluxml.ai/) - Relax! Flux is the ML library that doesn't make you tensor\n* [MLJ](https://github.com/alan-turing-institute/MLJ.jl) - A Julia machine learning framework\n\n<a name=\"julia-nlp\"></a>\n#### Natural Language Processing\n\n* [Topic Models](https://github.com/slycoder/TopicModels.jl) - TopicModels for Julia. **[Deprecated]**\n* [Text Analysis](https://github.com/JuliaText/TextAnalysis.jl) - Julia package for text analysis.\n* [Word Tokenizers](https://github.com/JuliaText/WordTokenizers.jl) - Tokenizers for Natural Language Processing in Julia\n* [Corpus Loaders](https://github.com/JuliaText/CorpusLoaders.jl) - A julia package providing a variety of loaders for various NLP corpora.\n* [Embeddings](https://github.com/JuliaText/Embeddings.jl) - Functions and data dependencies for loading various word embeddings\n* [Languages](https://github.com/JuliaText/Languages.jl) - Julia package for working with various human languages\n* [WordNet](https://github.com/JuliaText/WordNet.jl) - A Julia package for Princeton's WordNet\n\n<a name=\"julia-data-analysis\"></a>\n#### Data Analysis / Data Visualization\n\n* [Graph Layout](https://github.com/IainNZ/GraphLayout.jl) - Graph layout algorithms in pure Julia.\n* [LightGraphs](https://github.com/JuliaGraphs/LightGraphs.jl) - Graph modeling and analysis.\n* [Data Frames Meta](https://github.com/JuliaData/DataFramesMeta.jl) - Metaprogramming tools for DataFrames.\n* [Julia Data](https://github.com/nfoti/JuliaData) - library for working with tabular data in Julia. **[Deprecated]**\n* [Data Read](https://github.com/queryverse/ReadStat.jl) - Read files from Stata, SAS, and SPSS.\n* [Hypothesis Tests](https://github.com/JuliaStats/HypothesisTests.jl) - Hypothesis tests for Julia.\n* [Gadfly](https://github.com/GiovineItalia/Gadfly.jl) - Crafty statistical graphics for Julia.\n* [Stats](https://github.com/JuliaStats/StatsKit.jl) - Statistical tests for Julia.\n* [RDataSets](https://github.com/johnmyleswhite/RDatasets.jl) - Julia package for loading many of the data sets available in R.\n* [DataFrames](https://github.com/JuliaData/DataFrames.jl) - library for working with tabular data in Julia.\n* [Distributions](https://github.com/JuliaStats/Distributions.jl) - A Julia package for probability distributions and associated functions.\n* [Data Arrays](https://github.com/JuliaStats/DataArrays.jl) - Data structures that allow missing values. **[Deprecated]**\n* [Time Series](https://github.com/JuliaStats/TimeSeries.jl) - Time series toolkit for Julia.\n* [Sampling](https://github.com/lindahua/Sampling.jl) - Basic sampling algorithms for Julia.\n\n<a name=\"julia-misc\"></a>\n#### Misc Stuff / Presentations\n\n* [DSP](https://github.com/JuliaDSP/DSP.jl) - Digital Signal Processing (filtering, periodograms, spectrograms, window functions).\n* [JuliaCon Presentations](https://github.com/JuliaCon/presentations) - Presentations for JuliaCon.\n* [SignalProcessing](https://github.com/JuliaDSP/DSP.jl) - Signal Processing tools for Julia.\n* [Images](https://github.com/JuliaImages/Images.jl) - An image library for Julia.\n* [DataDeps](https://github.com/oxinabox/DataDeps.jl) - Reproducible data setup for reproducible science.\n\n<a name=\"lua\"></a>\n## Lua\n\n<a name=\"lua-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [Torch7](http://torch.ch/)\n  * [cephes](https://github.com/deepmind/torch-cephes) - Cephes mathematical functions library, wrapped for Torch. Provides and wraps the 180+ special mathematical functions from the Cephes mathematical library, developed by Stephen L. Moshier. It is used, among many other places, at the heart of SciPy. **[Deprecated]**\n  * [autograd](https://github.com/twitter/torch-autograd) - Autograd automatically differentiates native Torch code. Inspired by the original Python version.\n  * [graph](https://github.com/torch/graph) - Graph package for Torch. **[Deprecated]**\n  * [randomkit](https://github.com/deepmind/torch-randomkit) - Numpy's randomkit, wrapped for Torch. **[Deprecated]**\n  * [signal](https://github.com/soumith/torch-signal) - A signal processing toolbox for Torch-7. FFT, DCT, Hilbert, cepstrums, stft.\n  * [nn](https://github.com/torch/nn) - Neural Network package for Torch.\n  * [torchnet](https://github.com/torchnet/torchnet) - framework for torch which provides a set of abstractions aiming at encouraging code re-use as well as encouraging modular programming.\n  * [nngraph](https://github.com/torch/nngraph) - This package provides graphical computation for nn library in Torch7.\n  * [nnx](https://github.com/clementfarabet/lua---nnx) - A completely unstable and experimental package that extends Torch's builtin nn library.\n  * [rnn](https://github.com/Element-Research/rnn) - A Recurrent Neural Network library that extends Torch's nn. RNNs, LSTMs, GRUs, BRNNs, BLSTMs, etc.\n  * [dpnn](https://github.com/Element-Research/dpnn) - Many useful features that aren't part of the main nn package.\n  * [dp](https://github.com/nicholas-leonard/dp) - A deep learning library designed for streamlining research and development using the Torch7 distribution. It emphasizes flexibility through the elegant use of object-oriented design patterns. **[Deprecated]**\n  * [optim](https://github.com/torch/optim) - An optimization library for Torch. SGD, Adagrad, Conjugate-Gradient, LBFGS, RProp and more.\n  * [unsup](https://github.com/koraykv/unsup) - A package for unsupervised learning in Torch. Provides modules that are compatible with nn (LinearPsd, ConvPsd, AutoEncoder, ...), and self-contained algorithms (k-means, PCA). **[Deprecated]**\n  * [manifold](https://github.com/clementfarabet/manifold) - A package to manipulate manifolds.\n  * [svm](https://github.com/koraykv/torch-svm) - Torch-SVM library. **[Deprecated]**\n  * [lbfgs](https://github.com/clementfarabet/lbfgs) - FFI Wrapper for liblbfgs. **[Deprecated]**\n  * [vowpalwabbit](https://github.com/clementfarabet/vowpal_wabbit) - An old vowpalwabbit interface to torch. **[Deprecated]**\n  * [OpenGM](https://github.com/clementfarabet/lua---opengm) - OpenGM is a C++ library for graphical modeling, and inference. The Lua bindings provide a simple way of describing graphs, from Lua, and then optimizing them with OpenGM. **[Deprecated]**\n  * [spaghetti](https://github.com/MichaelMathieu/lua---spaghetti) - Spaghetti (sparse linear) module for torch7 by @MichaelMathieu **[Deprecated]**\n  * [LuaSHKit](https://github.com/ocallaco/LuaSHkit) - A lua wrapper around the Locality sensitive hashing library SHKit **[Deprecated]**\n  * [kernel smoothing](https://github.com/rlowrance/kernel-smoothers) - KNN, kernel-weighted average, local linear regression smoothers. **[Deprecated]**\n  * [cutorch](https://github.com/torch/cutorch) - Torch CUDA Implementation.\n  * [cunn](https://github.com/torch/cunn) - Torch CUDA Neural Network Implementation.\n  * [imgraph](https://github.com/clementfarabet/lua---imgraph) - An image/graph library for Torch. This package provides routines to construct graphs on images, segment them, build trees out of them, and convert them back to images. **[Deprecated]**\n  * [videograph](https://github.com/clementfarabet/videograph) - A video/graph library for Torch. This package provides routines to construct graphs on videos, segment them, build trees out of them, and convert them back to videos. **[Deprecated]**\n  * [saliency](https://github.com/marcoscoffier/torch-saliency) - code and tools around integral images. A library for finding interest points based on fast integral histograms. **[Deprecated]**\n  * [stitch](https://github.com/marcoscoffier/lua---stitch) - allows us to use hugin to stitch images and apply same stitching to a video sequence. **[Deprecated]**\n  * [sfm](https://github.com/marcoscoffier/lua---sfm) - A bundle adjustment/structure from motion package. **[Deprecated]**\n  * [fex](https://github.com/koraykv/fex) - A package for feature extraction in Torch. Provides SIFT and dSIFT modules. **[Deprecated]**\n  * [OverFeat](https://github.com/sermanet/OverFeat) - A state-of-the-art generic dense feature extractor. **[Deprecated]**\n  * [wav2letter](https://github.com/facebookresearch/wav2letter) - a simple and efficient end-to-end Automatic Speech Recognition (ASR) system from Facebook AI Research.\n* [Numeric Lua](http://numlua.luaforge.net/)\n* [Lunatic Python](https://labix.org/lunatic-python)\n* [SciLua](http://scilua.org/)\n* [Lua - Numerical Algorithms](https://bitbucket.org/lucashnegri/lna) **[Deprecated]**\n* [Lunum](https://github.com/jzrake/lunum) **[Deprecated]**\n\n<a name=\"lua-demos\"></a>\n#### Demos and Scripts\n* [Core torch7 demos repository](https://github.com/e-lab/torch7-demos).\n  * linear-regression, logistic-regression\n  * face detector (training and detection as separate demos)\n  * mst-based-segmenter\n  * train-a-digit-classifier\n  * train-autoencoder\n  * optical flow demo\n  * train-on-housenumbers\n  * train-on-cifar\n  * tracking with deep nets\n  * kinect demo\n  * filter-bank visualization\n  * saliency-networks\n* [Training a Convnet for the Galaxy-Zoo Kaggle challenge(CUDA demo)](https://github.com/soumith/galaxyzoo)\n* [Music Tagging](https://github.com/mbhenaff/MusicTagging) - Music Tagging scripts for torch7.\n* [torch-datasets](https://github.com/rosejn/torch-datasets) - Scripts to load several popular datasets including:\n  * BSR 500\n  * CIFAR-10\n  * COIL\n  * Street View House Numbers\n  * MNIST\n  * NORB\n* [Atari2600](https://github.com/fidlej/aledataset) - Scripts to generate a dataset with static frames from the Arcade Learning Environment.\n\n\n\n<a name=\"matlab\"></a>\n## Matlab\n\n<a name=\"matlab-cv\"></a>\n#### Computer Vision\n\n* [Contourlets](http://www.ifp.illinois.edu/~minhdo/software/contourlet_toolbox.tar) - MATLAB source code that implements the contourlet transform and its utility functions.\n* [Shearlets](https://www3.math.tu-berlin.de/numerik/www.shearlab.org/software) - MATLAB code for shearlet transform.\n* [Curvelets](http://www.curvelet.org/software.html) - The Curvelet transform is a higher dimensional generalization of the Wavelet transform designed to represent images at different scales and different angles.\n* [Bandlets](http://www.cmap.polytechnique.fr/~peyre/download/) - MATLAB code for bandlet transform.\n* [mexopencv](https://kyamagu.github.io/mexopencv/) - Collection and a development kit of MATLAB mex functions for OpenCV library.\n\n<a name=\"matlab-nlp\"></a>\n#### Natural Language Processing\n\n* [NLP](https://amplab.cs.berkeley.edu/an-nlp-library-for-matlab/) - A NLP library for Matlab.\n\n<a name=\"matlab-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [Training a deep autoencoder or a classifier\non MNIST digits](https://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html) - Training a deep autoencoder or a classifier\non MNIST digits[DEEP LEARNING].\n* [Convolutional-Recursive Deep Learning for 3D Object Classification](https://www.socher.org/index.php/Main/Convolutional-RecursiveDeepLearningFor3DObjectClassification) - Convolutional-Recursive Deep Learning for 3D Object Classification[DEEP LEARNING].\n* [Spider](https://people.kyb.tuebingen.mpg.de/spider/) - The spider is intended to be a complete object orientated environment for machine learning in Matlab.\n* [LibSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/#matlab) - A Library for Support Vector Machines.\n* [ThunderSVM](https://github.com/Xtra-Computing/thundersvm) - An Open-Source SVM Library on GPUs and CPUs\n* [LibLinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/#download) - A Library for Large Linear Classification.\n* [Machine Learning Module](https://github.com/josephmisiti/machine-learning-module) - Class on machine w/ PDF, lectures, code\n* [Caffe](https://github.com/BVLC/caffe) - A deep learning framework developed with cleanliness, readability, and speed in mind.\n* [Pattern Recognition Toolbox](https://github.com/covartech/PRT) - A complete object-oriented environment for machine learning in Matlab.\n* [Pattern Recognition and Machine Learning](https://github.com/PRML/PRMLT) - This package contains the matlab implementation of the algorithms described in the book Pattern Recognition and Machine Learning by C. Bishop.\n* [Optunity](https://optunity.readthedocs.io/en/latest/) - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly with MATLAB.\n* [MXNet](https://github.com/apache/incubator-mxnet/) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [Machine Learning in MatLab/Octave](https://github.com/trekhleb/machine-learning-octave) - examples of popular machine learning algorithms (neural networks, linear/logistic regressions, K-Means, etc.) with code examples and mathematics behind them being explained.\n\n\n<a name=\"matlab-data-analysis\"></a>\n#### Data Analysis / Data Visualization\n\n* [ParaMonte](https://github.com/cdslaborg/paramonte) - A general-purpose MATLAB library for Bayesian data analysis and visualization via serial/parallel Monte Carlo and MCMC simulations. Documentation can be found [here](https://www.cdslab.org/paramonte/).\n* [matlab_bgl](https://www.cs.purdue.edu/homes/dgleich/packages/matlab_bgl/) - MatlabBGL is a Matlab package for working with graphs.\n* [gaimc](https://www.mathworks.com/matlabcentral/fileexchange/24134-gaimc---graph-algorithms-in-matlab-code) - Efficient pure-Matlab implementations of graph algorithms to complement MatlabBGL's mex functions.\n\n<a name=\"net\"></a>\n## .NET\n\n<a name=\"net-cv\"></a>\n#### Computer Vision\n\n* [OpenCVDotNet](https://code.google.com/archive/p/opencvdotnet) - A wrapper for the OpenCV project to be used with .NET applications.\n* [Emgu CV](http://www.emgu.com/wiki/index.php/Main_Page) - Cross platform wrapper of OpenCV which can be compiled in Mono to be run on Windows, Linus, Mac OS X, iOS, and Android.\n* [AForge.NET](http://www.aforgenet.com/framework/) - Open source C# framework for developers and researchers in the fields of Computer Vision and Artificial Intelligence. Development has now shifted to GitHub.\n* [Accord.NET](http://accord-framework.net) - Together with AForge.NET, this library can provide image processing and computer vision algorithms to Windows, Windows RT and Windows Phone. Some components are also available for Java and Android.\n\n<a name=\"net-nlp\"></a>\n#### Natural Language Processing\n\n* [Stanford.NLP for .NET](https://github.com/sergey-tihon/Stanford.NLP.NET/) - A full port of Stanford NLP packages to .NET and also available precompiled as a NuGet package.\n\n<a name=\"net-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [Accord-Framework](http://accord-framework.net/) -The Accord.NET Framework is a complete framework for building machine learning, computer vision, computer audition, signal processing and statistical applications.\n* [Accord.MachineLearning](https://www.nuget.org/packages/Accord.MachineLearning/) - Support Vector Machines, Decision Trees, Naive Bayesian models, K-means, Gaussian Mixture models and general algorithms such as Ransac, Cross-validation and Grid-Search for machine-learning applications. This package is part of the Accord.NET Framework.\n* [DiffSharp](https://diffsharp.github.io/DiffSharp/) - An automatic differentiation (AD) library providing exact and efficient derivatives (gradients, Hessians, Jacobians, directional derivatives, and matrix-free Hessian- and Jacobian-vector products) for machine learning and optimization applications. Operations can be nested to any level, meaning that you can compute exact higher-order derivatives and differentiate functions that are internally making use of differentiation, for applications such as hyperparameter optimization.\n* [Encog](https://www.nuget.org/packages/encog-dotnet-core/) - An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.\n* [GeneticSharp](https://github.com/giacomelli/GeneticSharp) - Multi-platform genetic algorithm library for .NET Core and .NET Framework. The library has several implementations of GA operators, like: selection, crossover, mutation, reinsertion and termination.\n* [Infer.NET](https://dotnet.github.io/infer/) - Infer.NET is a framework for running Bayesian inference in graphical models. One can use Infer.NET to solve many different kinds of machine learning problems, from standard problems like classification, recommendation or clustering through to customized solutions to domain-specific problems. Infer.NET has been used in a wide variety of domains including information retrieval, bioinformatics, epidemiology, vision, and many others.\n* [ML.NET](https://github.com/dotnet/machinelearning) - ML.NET is a cross-platform open-source machine learning framework which makes machine learning accessible to .NET developers. ML.NET was originally developed in Microsoft Research and evolved into a significant framework over the last decade and is used across many product groups in Microsoft like Windows, Bing, PowerPoint, Excel and more.\n* [Neural Network Designer](https://sourceforge.net/projects/nnd/) - DBMS management system and designer for neural networks. The designer application is developed using WPF, and is a user interface which allows you to design your neural network, query the network, create and configure chat bots that are capable of asking questions and learning from your feedback. The chat bots can even scrape the internet for information to return in their output as well as to use for learning.\n* [Synapses](https://github.com/mrdimosthenis/Synapses) - Neural network library in F#.\n* [Vulpes](https://github.com/fsprojects/Vulpes) - Deep belief and deep learning implementation written in F# and leverages CUDA GPU execution with Alea.cuBase.\n* [MxNet.Sharp](https://github.com/tech-quantum/MxNet.Sharp) - .NET Standard bindings for Apache MxNet with Imperative, Symbolic and Gluon Interface for developing, training and deploying Machine Learning models in C#. https://mxnet.tech-quantum.com/\n\n<a name=\"net-data-analysis\"></a>\n#### Data Analysis / Data Visualization\n\n* [numl](https://www.nuget.org/packages/numl/) - numl is a machine learning library intended to ease the use of using standard modeling techniques for both prediction and clustering.\n* [Math.NET Numerics](https://www.nuget.org/packages/MathNet.Numerics/) - Numerical foundation of the Math.NET project, aiming to provide methods and algorithms for numerical computations in science, engineering and everyday use. Supports .Net 4.0, .Net 3.5 and Mono on Windows, Linux and Mac; Silverlight 5, WindowsPhone/SL 8, WindowsPhone 8.1 and Windows 8 with PCL Portable Profiles 47 and 344; Android/iOS with Xamarin.\n* [Sho](https://www.microsoft.com/en-us/research/project/sho-the-net-playground-for-data/) - Sho is an interactive environment for data analysis and scientific computing that lets you seamlessly connect scripts (in IronPython) with compiled code (in .NET) to enable fast and flexible prototyping. The environment includes powerful and efficient libraries for linear algebra as well as data visualization that can be used from any .NET language, as well as a feature-rich interactive shell for rapid development.\n\n<a name=\"objectivec\"></a>\n## Objective C\n\n<a name=\"objectivec-general-purpose\"></a>\n### General-Purpose Machine Learning\n\n* [YCML](https://github.com/yconst/YCML) - A Machine Learning framework for Objective-C and Swift (OS X / iOS).\n* [MLPNeuralNet](https://github.com/nikolaypavlov/MLPNeuralNet) - Fast multilayer perceptron neural network library for iOS and Mac OS X. MLPNeuralNet predicts new examples by trained neural networks. It is built on top of the Apple's Accelerate Framework, using vectorized operations and hardware acceleration if available. **[Deprecated]**\n* [MAChineLearning](https://github.com/gianlucabertani/MAChineLearning) - An Objective-C multilayer perceptron library, with full support for training through backpropagation. Implemented using vDSP and vecLib, it's 20 times faster than its Java equivalent. Includes sample code for use from Swift.\n* [BPN-NeuralNetwork](https://github.com/Kalvar/ios-BPN-NeuralNetwork) - It implemented 3 layers of neural networks ( Input Layer, Hidden Layer and Output Layer ) and it was named Back Propagation Neural Networks (BPN). This network can be used in products recommendation, user behavior analysis, data mining and data analysis. **[Deprecated]**\n* [Multi-Perceptron-NeuralNetwork](https://github.com/Kalvar/ios-Multi-Perceptron-NeuralNetwork) - it implemented multi-perceptrons neural network (\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af) based on Back Propagation Neural Networks (BPN) and designed unlimited-hidden-layers.\n* [KRHebbian-Algorithm](https://github.com/Kalvar/ios-KRHebbian-Algorithm) - It is a non-supervisor and self-learning algorithm (adjust the weights) in the neural network of Machine Learning. **[Deprecated]**\n* [KRKmeans-Algorithm](https://github.com/Kalvar/ios-KRKmeans-Algorithm) - It implemented K-Means  clustering and classification algorithm. It could be used in data mining and image compression. **[Deprecated]**\n* [KRFuzzyCMeans-Algorithm](https://github.com/Kalvar/ios-KRFuzzyCMeans-Algorithm) - It implemented Fuzzy C-Means (FCM) the fuzzy clustering / classification algorithm on Machine Learning. It could be used in data mining and image compression. **[Deprecated]**\n\n<a name=\"ocaml\"></a>\n## OCaml\n\n<a name=\"ocaml-general-purpose\"></a>\n### General-Purpose Machine Learning\n\n* [Oml](https://github.com/rleonid/oml) - A general statistics and machine learning library.\n* [GPR](https://mmottl.github.io/gpr/) - Efficient Gaussian Process Regression in OCaml.\n* [Libra-Tk](https://libra.cs.uoregon.edu) - Algorithms for learning and inference with discrete probabilistic models.\n* [TensorFlow](https://github.com/LaurentMazare/tensorflow-ocaml) - OCaml bindings for TensorFlow.\n\n<a name=\"perl\"></a>\n## Perl\n\n<a name=\"perl-data\"></a>\n### Data Analysis / Data Visualization\n\n* [Perl Data Language](https://metacpan.org/pod/Paws::MachineLearning), a pluggable architecture for data and image processing, which can\nbe [used for machine learning](https://github.com/zenogantner/PDL-ML).\n\n<a name=\"perl-ml\"></a>\n### General-Purpose Machine Learning\n\n* [MXnet for Deep Learning, in Perl](https://github.com/apache/incubator-mxnet/tree/master/perl-package),\nalso [released in CPAN](https://metacpan.org/pod/AI::MXNet).\n* [Perl Data Language](https://metacpan.org/pod/Paws::MachineLearning),\nusing AWS machine learning platform from Perl.\n* [Algorithm::SVMLight](https://metacpan.org/pod/Algorithm::SVMLight),\n  implementation of Support Vector Machines with SVMLight under it. **[Deprecated]**\n* Several machine learning and artificial intelligence models are\n  included in the [`AI`](https://metacpan.org/search?size=20&q=AI)\n  namespace. For instance, you can\n  find [Na\u00efve Bayes](https://metacpan.org/pod/AI::NaiveBayes).\n\n<a name=\"perl6\"></a>\n## Perl 6\n\n* [Support Vector Machines](https://github.com/titsuki/p6-Algorithm-LibSVM)\n* [Na\u00efve Bayes](https://github.com/titsuki/p6-Algorithm-NaiveBayes)\n\n### Data Analysis / Data Visualization\n\n* [Perl Data Language](https://metacpan.org/pod/Paws::MachineLearning),\na pluggable architecture for data and image processing, which can\nbe\n[used for machine learning](https://github.com/zenogantner/PDL-ML).\n\n### General-Purpose Machine Learning\n\n<a name=\"php\"></a>\n## PHP\n\n<a name=\"php-nlp\"></a>\n### Natural Language Processing\n\n* [jieba-php](https://github.com/fukuball/jieba-php) - Chinese Words Segmentation Utilities.\n\n<a name=\"php-general-purpose\"></a>\n### General-Purpose Machine Learning\n\n* [PHP-ML](https://github.com/php-ai/php-ml) - Machine Learning library for PHP. Algorithms, Cross Validation, Neural Network, Preprocessing, Feature Extraction and much more in one library.\n* [PredictionBuilder](https://github.com/denissimon/prediction-builder) - A library for machine learning that builds predictions using a linear regression.\n* [Rubix ML](https://github.com/RubixML) - A high-level machine learning (ML) library that lets you build programs that learn from data using the PHP language.\n* [19 Questions](https://github.com/fulldecent/19-questions) - A machine learning / bayesian inference assigning attributes to objects.\n\n<a name=\"python\"></a>\n## Python\n\n<a name=\"python-cv\"></a>\n#### Computer Vision\n\n* [Scikit-Image](https://github.com/scikit-image/scikit-image) - A collection of algorithms for image processing in Python.\n* [Scikit-Opt](https://github.com/guofei9987/scikit-opt) - Swarm Intelligence in Python (Genetic Algorithm, Particle Swarm Optimization, Simulated Annealing, Ant Colony Algorithm, Immune Algorithm,Artificial Fish Swarm Algorithm in Python)\n* [SimpleCV](http://simplecv.org/) - An open source computer vision framework that gives access to several high-powered computer vision libraries, such as OpenCV. Written on Python and runs on Mac, Windows, and Ubuntu Linux.\n* [Vigranumpy](https://github.com/ukoethe/vigra) - Python bindings for the VIGRA C++ computer vision library.\n* [OpenFace](https://cmusatyalab.github.io/openface/) - Free and open source face recognition with deep neural networks.\n* [PCV](https://github.com/jesolem/PCV) - Open source Python module for computer vision. **[Deprecated]**\n* [face_recognition](https://github.com/ageitgey/face_recognition) - Face recognition library that recognizes and manipulates faces from Python or from the command line.\n* [dockerface](https://github.com/natanielruiz/dockerface) - Easy to install and use deep learning Faster R-CNN face detection for images and video in a docker container.\n* [Detectron](https://github.com/facebookresearch/Detectron) - FAIR's software system that implements state-of-the-art object detection algorithms, including Mask R-CNN. It is written in Python and powered by the Caffe2 deep learning framework. **[Deprecated]**\n* [detectron2](https://github.com/facebookresearch/detectron2) - FAIR's next-generation research platform for object detection and segmentation. It is a ground-up rewrite of the previous version, Detectron, and is powered by the PyTorch deep learning framework. \n* [albumentations](https://github.com/albu/albumentations) - \u0410 fast and framework agnostic image augmentation library that implements a diverse set of augmentation techniques. Supports classification, segmentation, detection out of the box. Was used to win a number of Deep Learning competitions at Kaggle, Topcoder and those that were a part of the CVPR workshops.\n* [pytessarct](https://github.com/madmaze/pytesseract) - Python-tesseract is an optical character recognition (OCR) tool for python. That is, it will recognize and \"read\" the text embedded in images. Python-tesseract is a wrapper for [Google's Tesseract-OCR Engine](https://github.com/tesseract-ocr/tesseract).\n* [imutils](https://github.com/jrosebr1/imutils) - A library containing Convenience functions to make basic image processing operations such as translation, rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and Python.\n* [PyTorchCV](https://github.com/donnyyou/PyTorchCV) - A PyTorch-Based Framework for Deep Learning in Computer Vision.\n* [Self-supervised learning](https://pytorch-lightning-bolts.readthedocs.io/en/latest/self_supervised_models.html)\n* [neural-style-pt](https://github.com/ProGamerGov/neural-style-pt) - A PyTorch implementation of Justin Johnson's neural-style (neural style transfer).\n* [Detecto](https://github.com/alankbi/detecto) - Train and run a computer vision model with 5-10 lines of code.\n* [neural-dream](https://github.com/ProGamerGov/neural-dream) - A PyTorch implementation of DeepDream.\n* [Openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) - A real-time multi-person keypoint detection library for body, face, hands, and foot estimation\n* [Deep High-Resolution-Net](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch) - A PyTorch implementation of CVPR2019 paper \"Deep High-Resolution Representation Learning for Human Pose Estimation\"\n* [dream-creator](https://github.com/ProGamerGov/dream-creator) - A PyTorch implementation of DeepDream. Allows individuals to quickly and easily train their own custom GoogleNet models with custom datasets for DeepDream.\n* [Lucent](https://github.com/greentfrapp/lucent) - Tensorflow and OpenAI Clarity's Lucid adapted for PyTorch.\n* [lightly](https://github.com/lightly-ai/lightly) - Lightly is a computer vision framework for self-supervised learning.\n\n<a name=\"python-nlp\"></a>\n#### Natural Language Processing\n\n* [pkuseg-python](https://github.com/lancopku/pkuseg-python) - A better version of Jieba, developed by Peking University.\n* [NLTK](https://www.nltk.org/) - A leading platform for building Python programs to work with human language data.\n* [Pattern](https://github.com/clips/pattern) - A web mining module for the Python programming language. It has tools for natural language processing, machine learning, among others.\n* [Quepy](https://github.com/machinalis/quepy) - A python framework to transform natural language questions to queries in a database query language.\n* [TextBlob](http://textblob.readthedocs.io/en/dev/) - Providing a consistent API for diving into common natural language processing (NLP) tasks. Stands on the giant shoulders of NLTK and Pattern, and plays nicely with both.\n* [YAlign](https://github.com/machinalis/yalign) - A sentence aligner, a friendly tool for extracting parallel sentences from comparable corpora. **[Deprecated]**\n* [jieba](https://github.com/fxsjy/jieba#jieba-1) - Chinese Words Segmentation Utilities.\n* [SnowNLP](https://github.com/isnowfy/snownlp) - A library for processing Chinese text.\n* [spammy](https://github.com/tasdikrahman/spammy) - A library for email Spam filtering built on top of nltk\n* [loso](https://github.com/fangpenlin/loso) - Another Chinese segmentation library. **[Deprecated]**\n* [genius](https://github.com/duanhongyi/genius) - A Chinese segment based on Conditional Random Field.\n* [KoNLPy](http://konlpy.org) - A Python package for Korean natural language processing.\n* [nut](https://github.com/pprett/nut) - Natural language Understanding Toolkit. **[Deprecated]**\n* [Rosetta](https://github.com/columbia-applied-data-science/rosetta) - Text processing tools and wrappers (e.g. Vowpal Wabbit)\n* [BLLIP Parser](https://pypi.org/project/bllipparser/) - Python bindings for the BLLIP Natural Language Parser (also known as the Charniak-Johnson parser). **[Deprecated]**\n* [PyNLPl](https://github.com/proycon/pynlpl) - Python Natural Language Processing Library. General purpose NLP library for Python. Also contains some specific modules for parsing common NLP formats, most notably for [FoLiA](https://proycon.github.io/folia/), but also ARPA language models, Moses phrasetables, GIZA++ alignments.\n* [PySS3](https://github.com/sergioburdisso/pyss3) - Python package that implements a novel white-box machine learning model for text classification, called SS3. Since SS3 has the ability to visually explain its rationale, this package also comes with easy-to-use interactive visualizations tools ([online demos](http://tworld.io/ss3/)).\n* [python-ucto](https://github.com/proycon/python-ucto) - Python binding to ucto (a unicode-aware rule-based tokenizer for various languages).\n* [python-frog](https://github.com/proycon/python-frog) - Python binding to Frog, an NLP suite for Dutch. (pos tagging, lemmatisation, dependency parsing, NER)\n* [python-zpar](https://github.com/EducationalTestingService/python-zpar) - Python bindings for [ZPar](https://github.com/frcchang/zpar), a statistical part-of-speech-tagger, constituency parser, and dependency parser for English.\n* [colibri-core](https://github.com/proycon/colibri-core) - Python binding to C++ library for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.\n* [spaCy](https://github.com/explosion/spaCy) - Industrial strength NLP with Python and Cython.\n* [PyStanfordDependencies](https://github.com/dmcc/PyStanfordDependencies) - Python interface for converting Penn Treebank trees to Stanford Dependencies.\n* [Distance](https://github.com/doukremt/distance) - Levenshtein and Hamming distance computation. **[Deprecated]**\n* [Fuzzy Wuzzy](https://github.com/seatgeek/fuzzywuzzy) - Fuzzy String Matching in Python.\n* [jellyfish](https://github.com/jamesturk/jellyfish) - a python library for doing approximate and phonetic matching of strings.\n* [editdistance](https://pypi.org/project/editdistance/) - fast implementation of edit distance.\n* [textacy](https://github.com/chartbeat-labs/textacy) - higher-level NLP built on Spacy.\n* [stanford-corenlp-python](https://github.com/dasmith/stanford-corenlp-python) - Python wrapper for [Stanford CoreNLP](https://github.com/stanfordnlp/CoreNLP) **[Deprecated]**\n* [CLTK](https://github.com/cltk/cltk) - The Classical Language Toolkit.\n* [rasa_nlu](https://github.com/RasaHQ/rasa_nlu) - turn natural language into structured data.\n* [yase](https://github.com/PPACI/yase) - Transcode sentence (or other sequence) to list of word vector .\n* [Polyglot](https://github.com/aboSamoor/polyglot) - Multilingual text (NLP) processing toolkit.\n* [DrQA](https://github.com/facebookresearch/DrQA) - Reading Wikipedia to answer open-domain questions.\n* [Dedupe](https://github.com/dedupeio/dedupe) - A python library for accurate and scalable fuzzy matching, record deduplication and entity-resolution.\n* [Snips NLU](https://github.com/snipsco/snips-nlu) - Natural Language Understanding library for intent classification and entity extraction\n* [NeuroNER](https://github.com/Franck-Dernoncourt/NeuroNER) - Named-entity recognition using neural networks providing state-of-the-art-results\n* [DeepPavlov](https://github.com/deepmipt/DeepPavlov/) - conversational AI library with many pre-trained Russian NLP models.\n* [BigARTM](https://github.com/bigartm/bigartm) - topic modelling platform.\n\n<a name=\"python-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n * [igel](https://github.com/nidhaloff/igel) -> A delightful machine learning tool that allows you to train/fit, test and use models **without writing code**\n * [ML Model building](https://github.com/Shanky-21/Machine_learning) -> A Repository Containing Classification, Clustering, Regression, Recommender Notebooks with illustration to make them.\n * [ML/DL project template](https://github.com/PyTorchLightning/deep-learning-project-template)\n * [PyTorch Geometric Temporal](https://github.com/benedekrozemberczki/pytorch_geometric_temporal) -> A temporal extension of PyTorch Geometric for dynamic graph representation learning.\n * [Little Ball of Fur](https://github.com/benedekrozemberczki/littleballoffur) -> A graph sampling extension library for NetworkX with a Scikit-Learn like API.\n * [Karate Club](https://github.com/benedekrozemberczki/karateclub) -> An unsupervised machine learning extension library for NetworkX with a Scikit-Learn like API.\n* [Auto_ViML](https://github.com/AutoViML/Auto_ViML) -> Automatically Build Variant Interpretable ML models fast! Auto_ViML is pronounced \"auto vimal\", is a comprehensive and scalable Python AutoML toolkit with imbalanced handling, ensembling, stacking and built-in feature selection. Featured in <a href=\"https://towardsdatascience.com/why-automl-is-an-essential-new-tool-for-data-scientists-2d9ab4e25e46?source=friends_link&sk=d03a0cc55c23deb497d546d6b9be0653\">Medium article</a>.\n* [PyOD](https://github.com/yzhao062/pyod) -> Python Outlier Detection, comprehensive and scalable Python toolkit for detecting outlying objects in multivariate data. Featured for Advanced models, including Neural Networks/Deep Learning and Outlier Ensembles.\n* [steppy](https://github.com/neptune-ml/steppy) -> Lightweight, Python library for fast and reproducible machine learning experimentation. Introduces a very simple interface that enables clean machine learning pipeline design.\n* [steppy-toolkit](https://github.com/neptune-ml/steppy-toolkit) -> Curated collection of the neural networks, transformers and models that make your machine learning work faster and more effective.\n* [CNTK](https://github.com/Microsoft/CNTK) - Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit. Documentation can be found [here](https://docs.microsoft.com/cognitive-toolkit/).\n* [Couler](https://github.com/couler-proj/couler) - Unified interface for constructing and managing machine learning workflows on different workflow engines, such as Argo Workflows, Tekton Pipelines, and Apache Airflow.\n* [auto_ml](https://github.com/ClimbsRocks/auto_ml) - Automated machine learning for production and analytics. Lets you focus on the fun parts of ML, while outputting production-ready code, and detailed analytics of your dataset and results. Includes support for NLP, XGBoost, CatBoost, LightGBM, and soon, deep learning.\n* [machine learning](https://github.com/jeff1evesque/machine-learning) - automated build consisting of a [web-interface](https://github.com/jeff1evesque/machine-learning#web-interface), and set of [programmatic-interface](https://github.com/jeff1evesque/machine-learning#programmatic-interface) API, for support vector machines. Corresponding dataset(s) are stored into a SQL database, then generated model(s) used for prediction(s), are stored into a NoSQL datastore.\n* [XGBoost](https://github.com/dmlc/xgboost) - Python bindings for eXtreme Gradient Boosting (Tree) Library.\n* [Apache SINGA](https://singa.apache.org) - An Apache Incubating project for developing an open source machine learning library.\n* [Bayesian Methods for Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers) - Book/iPython notebooks on Probabilistic Programming in Python.\n* [Featureforge](https://github.com/machinalis/featureforge) A set of tools for creating and testing machine learning features, with a scikit-learn compatible API.\n* [MLlib in Apache Spark](http://spark.apache.org/docs/latest/mllib-guide.html) - Distributed machine learning library in Spark\n* [Hydrosphere Mist](https://github.com/Hydrospheredata/mist) - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.\n* [scikit-learn](https://scikit-learn.org/) - A Python module for machine learning built on top of SciPy.\n* [metric-learn](https://github.com/metric-learn/metric-learn) - A Python module for metric learning.\n* [SimpleAI](https://github.com/simpleai-team/simpleai) Python implementation of many of the artificial intelligence algorithms described in the book \"Artificial Intelligence, a Modern Approach\". It focuses on providing an easy to use, well documented and tested library.\n* [astroML](https://www.astroml.org/) - Machine Learning and Data Mining for Astronomy.\n* [graphlab-create](https://turi.com/products/create/docs/) - A library with various machine learning models (regression, clustering, recommender systems, graph analytics, etc.) implemented on top of a disk-backed DataFrame.\n* [BigML](https://bigml.com) - A library that contacts external servers.\n* [pattern](https://github.com/clips/pattern) - Web mining module for Python.\n* [NuPIC](https://github.com/numenta/nupic) - Numenta Platform for Intelligent Computing.\n* [Pylearn2](https://github.com/lisa-lab/pylearn2) - A Machine Learning library based on [Theano](https://github.com/Theano/Theano). **[Deprecated]**\n* [keras](https://github.com/keras-team/keras) - High-level neural networks frontend for [TensorFlow](https://github.com/tensorflow/tensorflow), [CNTK](https://github.com/Microsoft/CNTK) and [Theano](https://github.com/Theano/Theano).\n* [Lasagne](https://github.com/Lasagne/Lasagne) - Lightweight library to build and train neural networks in Theano.\n* [hebel](https://github.com/hannes-brt/hebel) - GPU-Accelerated Deep Learning Library in Python. **[Deprecated]**\n* [Chainer](https://github.com/chainer/chainer) - Flexible neural network framework.\n* [prophet](https://facebook.github.io/prophet/) - Fast and automated time series forecasting framework by Facebook.\n* [gensim](https://github.com/RaRe-Technologies/gensim) - Topic Modelling for Humans.\n* [topik](https://github.com/ContinuumIO/topik) - Topic modelling toolkit. **[Deprecated]**\n* [PyBrain](https://github.com/pybrain/pybrain) - Another Python Machine Learning Library.\n* [Brainstorm](https://github.com/IDSIA/brainstorm) - Fast, flexible and fun neural networks. This is the successor of PyBrain.\n* [Surprise](https://surpriselib.com) - A scikit for building and analyzing recommender systems.\n* [implicit](https://implicit.readthedocs.io/en/latest/quickstart.html) - Fast Python Collaborative Filtering for Implicit Datasets.\n* [LightFM](https://making.lyst.com/lightfm/docs/home.html) -  A Python implementation of a number of popular recommendation algorithms for both implicit and explicit feedback.\n* [Crab](https://github.com/muricoca/crab) - A flexible, fast recommender engine. **[Deprecated]**\n* [python-recsys](https://github.com/ocelma/python-recsys) - A Python library for implementing a Recommender System.\n* [thinking bayes](https://github.com/AllenDowney/ThinkBayes) - Book on Bayesian Analysis.\n* [Image-to-Image Translation with Conditional Adversarial Networks](https://github.com/williamFalcon/pix2pix-keras) - Implementation of image to image (pix2pix) translation from the paper by [isola et al](https://arxiv.org/pdf/1611.07004.pdf).[DEEP LEARNING]\n* [Restricted Boltzmann Machines](https://github.com/echen/restricted-boltzmann-machines) -Restricted Boltzmann Machines in Python. [DEEP LEARNING]\n* [Bolt](https://github.com/pprett/bolt) - Bolt Online Learning Toolbox. **[Deprecated]**\n* [CoverTree](https://github.com/patvarilly/CoverTree) - Python implementation of cover trees, near-drop-in replacement for scipy.spatial.kdtree **[Deprecated]**\n* [nilearn](https://github.com/nilearn/nilearn) - Machine learning for NeuroImaging in Python.\n* [neuropredict](https://github.com/raamana/neuropredict) - Aimed at novice machine learners and non-expert programmers, this package offers easy (no coding needed) and comprehensive machine learning (evaluation and full report of predictive performance WITHOUT requiring you to code) in Python for NeuroImaging and any other type of features. This is aimed at absorbing much of the ML workflow, unlike other packages like nilearn and pymvpa, which require you to learn their API and code to produce anything useful.\n* [imbalanced-learn](https://imbalanced-learn.org/stable/) - Python module to perform under sampling and oversampling with various techniques.\n* [Shogun](https://github.com/shogun-toolbox/shogun) - The Shogun Machine Learning Toolbox.\n* [Pyevolve](https://github.com/perone/Pyevolve) - Genetic algorithm framework. **[Deprecated]**\n* [Caffe](https://github.com/BVLC/caffe) - A deep learning framework developed with cleanliness, readability, and speed in mind.\n* [breze](https://github.com/breze-no-salt/breze) - Theano based library for deep and recurrent neural networks. \n* [Cortex](https://github.com/cortexlabs/cortex) - Open source platform for deploying machine learning models in production.\n* [pyhsmm](https://github.com/mattjj/pyhsmm) - library for approximate unsupervised inference in Bayesian Hidden Markov Models (HMMs) and explicit-duration Hidden semi-Markov Models (HSMMs), focusing on the Bayesian Nonparametric extensions, the HDP-HMM and HDP-HSMM, mostly with weak-limit approximations.\n* [SKLL](https://github.com/EducationalTestingService/skll) - A wrapper around scikit-learn that makes it simpler to conduct experiments.\n* [neurolab](https://github.com/zueve/neurolab)\n* [Spearmint](https://github.com/HIPS/Spearmint) - Spearmint is a package to perform Bayesian optimization according to the algorithms outlined in the paper: Practical Bayesian Optimization of Machine Learning Algorithms. Jasper Snoek, Hugo Larochelle and Ryan P. Adams. Advances in Neural Information Processing Systems, 2012. **[Deprecated]**\n* [Pebl](https://github.com/abhik/pebl/) - Python Environment for Bayesian Learning. **[Deprecated]**\n* [Theano](https://github.com/Theano/Theano/) - Optimizing GPU-meta-programming code generating array oriented optimizing math compiler in Python.\n* [TensorFlow](https://github.com/tensorflow/tensorflow/) - Open source software library for numerical computation using data flow graphs.\n* [pomegranate](https://github.com/jmschrei/pomegranate) - Hidden Markov Models for Python, implemented in Cython for speed and efficiency.\n* [python-timbl](https://github.com/proycon/python-timbl) - A Python extension module wrapping the full TiMBL C++ programming interface. Timbl is an elaborate k-Nearest Neighbours machine learning toolkit.\n* [deap](https://github.com/deap/deap) - Evolutionary algorithm framework.\n* [pydeep](https://github.com/andersbll/deeppy) - Deep Learning In Python. **[Deprecated]**\n* [mlxtend](https://github.com/rasbt/mlxtend) - A library consisting of useful tools for data science and machine learning tasks.\n* [neon](https://github.com/NervanaSystems/neon) - Nervana's [high-performance](https://github.com/soumith/convnet-benchmarks) Python-based Deep Learning framework [DEEP LEARNING]. **[Deprecated]**\n* [Optunity](https://optunity.readthedocs.io/en/latest/) - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search.\n* [Neural Networks and Deep Learning](https://github.com/mnielsen/neural-networks-and-deep-learning) - Code samples for my book \"Neural Networks and Deep Learning\" [DEEP LEARNING].\n* [Annoy](https://github.com/spotify/annoy) - Approximate nearest neighbours implementation.\n* [TPOT](https://github.com/EpistasisLab/tpot) - Tool that automatically creates and optimizes machine learning pipelines using genetic programming. Consider it your personal data science assistant, automating a tedious part of machine learning.\n* [pgmpy](https://github.com/pgmpy/pgmpy) A python library for working with Probabilistic Graphical Models.\n* [DIGITS](https://github.com/NVIDIA/DIGITS) - The Deep Learning GPU Training System (DIGITS) is a web application for training deep learning models.\n* [Orange](https://orange.biolab.si/) - Open source data visualization and data analysis for novices and experts.\n* [MXNet](https://github.com/apache/incubator-mxnet) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [milk](https://github.com/luispedro/milk) - Machine learning toolkit focused on supervised classification. **[Deprecated]**\n* [TFLearn](https://github.com/tflearn/tflearn) - Deep learning library featuring a higher-level API for TensorFlow.\n* [REP](https://github.com/yandex/rep) - an IPython-based environment for conducting data-driven research in a consistent and reproducible way. REP is not trying to substitute scikit-learn, but extends it and provides better user experience. **[Deprecated]**\n* [rgf_python](https://github.com/RGF-team/rgf) - Python bindings for Regularized Greedy Forest (Tree) Library.\n* [skbayes](https://github.com/AmazaspShumik/sklearn-bayes) - Python package for Bayesian Machine Learning with scikit-learn API.\n* [fuku-ml](https://github.com/fukuball/fuku-ml) - Simple machine learning library, including Perceptron, Regression, Support Vector Machine, Decision Tree and more, it's easy to use and easy to learn for beginners.\n* [Xcessiv](https://github.com/reiinakano/xcessiv) - A web-based application for quick, scalable, and automated hyperparameter tuning and stacked ensembling.\n* [PyTorch](https://github.com/pytorch/pytorch) - Tensors and Dynamic neural networks in Python with strong GPU acceleration\n* [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) - The lightweight PyTorch wrapper for high-performance AI research.\n* [PyTorch Lightning Bolts](https://github.com/PyTorchLightning/pytorch-lightning-bolts) - Toolbox of models, callbacks, and datasets for AI/ML researchers.\n* [skorch](https://github.com/skorch-dev/skorch) - A scikit-learn compatible neural network library that wraps PyTorch.\n* [ML-From-Scratch](https://github.com/eriklindernoren/ML-From-Scratch) - Implementations of Machine Learning models from scratch in Python with a focus on transparency. Aims to showcase the nuts and bolts of ML in an accessible way.\n* [Edward](http://edwardlib.org/) - A library for probabilistic modeling, inference, and criticism. Built on top of TensorFlow.\n* [xRBM](https://github.com/omimo/xRBM) - A library for Restricted Boltzmann Machine (RBM) and its conditional variants in Tensorflow.\n* [CatBoost](https://github.com/catboost/catboost) - General purpose gradient boosting on decision trees library with categorical features support out of the box. It is easy to install, well documented and supports CPU and GPU (even multi-GPU) computation.\n* [stacked_generalization](https://github.com/fukatani/stacked_generalization) - Implementation of machine learning stacking technique as a handy library in Python.\n* [modAL](https://github.com/modAL-python/modAL) - A modular active learning framework for Python, built on top of scikit-learn.\n* [Cogitare](https://github.com/cogitare-ai/cogitare): A Modern, Fast, and Modular Deep Learning and Machine Learning framework for Python.\n* [Parris](https://github.com/jgreenemi/Parris) - Parris, the automated infrastructure setup tool for machine learning algorithms.\n* [neonrvm](https://github.com/siavashserver/neonrvm) - neonrvm is an open source machine learning library based on RVM technique. It's written in C programming language and comes with Python programming language bindings.\n* [Turi Create](https://github.com/apple/turicreate) - Machine learning from Apple. Turi Create simplifies the development of custom machine learning models. You don't have to be a machine learning expert to add recommendations, object detection, image classification, image similarity or activity classification to your app.\n* [xLearn](https://github.com/aksnzhy/xlearn) - A high performance, easy-to-use, and scalable machine learning package, which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data, which is very common in Internet services such as online advertisement and recommender systems.\n* [mlens](https://github.com/flennerhag/mlens) - A high performance, memory efficient, maximally parallelized ensemble learning, integrated with scikit-learn.\n* [Netron](https://github.com/lutzroeder/netron) - Visualizer for machine learning models.\n* [Thampi](https://github.com/scoremedia/thampi) - Machine Learning Prediction System on AWS Lambda\n* [MindsDB](https://github.com/mindsdb/mindsdb) - Open Source framework to streamline use of neural networks.\n* [Microsoft Recommenders](https://github.com/Microsoft/Recommenders): Examples and best practices for building recommendation systems, provided as Jupyter notebooks. The repo contains some of the latest state of the art algorithms from Microsoft Research as well as from other companies and institutions.\n* [StellarGraph](https://github.com/stellargraph/stellargraph): Machine Learning on Graphs, a Python library for machine learning on graph-structured (network-structured) data.\n* [BentoML](https://github.com/bentoml/bentoml): Toolkit for package and deploy machine learning models for serving in production\n* [MiraiML](https://github.com/arthurpaulino/miraiml): An asynchronous engine for continuous & autonomous machine learning, built for real-time usage.\n* [numpy-ML](https://github.com/ddbourgin/numpy-ml): Reference implementations of ML models written in numpy\n* [creme](https://github.com/creme-ml/creme): A framework for online machine learning.\n* [Neuraxle](https://github.com/Neuraxio/Neuraxle): A framework providing the right abstractions to ease research, development, and deployment of your ML pipelines.\n* [Cornac](https://github.com/PreferredAI/cornac) - A comparative framework for multimodal recommender systems with a focus on models leveraging auxiliary data.\n* [JAX](https://github.com/google/jax) - JAX is Autograd and XLA, brought together for high-performance machine learning research.\n* [Catalyst](https://github.com/catalyst-team/catalyst) - High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing. Being able to research/develop something new, rather than write another regular train loop.\n* [Fastai](https://github.com/fastai/fastai) - High-level wrapper built on the top of Pytorch which supports vision, text, tabular data and collaborative filtering.\n* [scikit-multiflow](https://github.com/scikit-multiflow/scikit-multiflow) - A machine learning framework for multi-output/multi-label and stream data.\n* [Lightwood](https://github.com/mindsdb/lightwood) - A Pytorch based framework that breaks down machine learning problems into smaller blocks that can be glued together seamlessly with objective to build predictive models with one line of code.\n* [bayeso](https://github.com/jungtaekkim/bayeso) - A simple, but essential Bayesian optimization package, written in Python.\n* [mljar-supervised](https://github.com/mljar/mljar-supervised) - An Automated Machine Learning (AutoML) python package for tabular data. It can handle: Binary Classification, MultiClass Classification and Regression. It provides explanations and markdown reports.\n* [evostra](https://github.com/alirezamika/evostra) - A fast Evolution Strategy implementation in Python.\n* [Determined](https://github.com/determined-ai/determined) - Scalable deep learning training platform, including integrated support for distributed training, hyperparameter tuning, experiment tracking, and model management.\n* [PySyft](https://github.com/OpenMined/PySyft) - A Python library for secure and private Deep Learning built on PyTorch and TensorFlow.\n* [PyGrid](https://github.com/OpenMined/PyGrid/) - Peer-to-peer network of data owners and data scientists who can collectively train AI models using PySyft\n* [sktime](https://github.com/alan-turing-institute/sktime) - A unified framework for machine learning with time series\n\n<a name=\"python-data-analysis\"></a>\n#### Data Analysis / Data Visualization\n* [DataVisualization](https://github.com/Shanky-21/Data_visualization) - A Github Repository Where you can Learn Datavisualizatoin Basics to Intermediate level.\n* [Cartopy](https://scitools.org.uk/cartopy/docs/latest/) - Cartopy is a Python package designed for geospatial data processing in order to produce maps and other geospatial data analyses.\n* [SciPy](https://www.scipy.org/) - A Python-based ecosystem of open-source software for mathematics, science, and engineering.\n* [NumPy](https://www.numpy.org/) - A fundamental package for scientific computing with Python.\n* [AutoViz](https://github.com/AutoViML/AutoViz) AutoViz performs automatic visualization of any dataset with a single line of Python code. Give it any input file (CSV, txt or json) of any size and AutoViz will visualize it. See <a href=\"https://towardsdatascience.com/autoviz-a-new-tool-for-automated-visualization-ec9c1744a6ad?source=friends_link&sk=c9e9503ec424b191c6096d7e3f515d10\">Medium article</a>.\n* [Numba](https://numba.pydata.org/) - Python JIT (just in time) compiler to LLVM aimed at scientific Python by the developers of Cython and NumPy.\n* [Mars](https://github.com/mars-project/mars) - A tensor-based framework for large-scale data computation which is often regarded as a parallel and distributed version of NumPy.\n* [NetworkX](https://networkx.github.io/) - A high-productivity software for complex networks.\n* [igraph](https://igraph.org/python/) - binding to igraph library - General purpose graph library.\n* [Pandas](https://pandas.pydata.org/) - A library providing high-performance, easy-to-use data structures and data analysis tools.\n* [ParaMonte](https://github.com/cdslaborg/paramonte) - A general-purpose Python library for Bayesian data analysis and visualization via serial/parallel Monte Carlo and MCMC simulations. Documentation can be found [here](https://www.cdslab.org/paramonte/).\n* [Open Mining](https://github.com/mining/mining) - Business Intelligence (BI) in Python (Pandas web interface) **[Deprecated]**\n* [PyMC](https://github.com/pymc-devs/pymc) - Markov Chain Monte Carlo sampling toolkit.\n* [zipline](https://github.com/quantopian/zipline) - A Pythonic algorithmic trading library.\n* [PyDy](https://www.pydy.org/) - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion based around NumPy, SciPy, IPython, and matplotlib.\n* [SymPy](https://github.com/sympy/sympy) - A Python library for symbolic mathematics.\n* [statsmodels](https://github.com/statsmodels/statsmodels) - Statistical modeling and econometrics in Python.\n* [astropy](https://www.astropy.org/) - A community Python library for Astronomy.\n* [matplotlib](https://matplotlib.org/) - A Python 2D plotting library.\n* [bokeh](https://github.com/bokeh/bokeh) - Interactive Web Plotting for Python.\n* [plotly](https://plot.ly/python/) - Collaborative web plotting for Python and matplotlib.\n* [altair](https://github.com/altair-viz/altair) - A Python to Vega translator.\n* [d3py](https://github.com/mikedewar/d3py) - A plotting library for Python, based on [D3.js](https://d3js.org/).\n* [PyDexter](https://github.com/D3xterjs/pydexter) - Simple plotting for Python. Wrapper for D3xterjs; easily render charts in-browser.\n* [ggplot](https://github.com/yhat/ggpy) - Same API as ggplot2 for R. **[Deprecated]**\n* [ggfortify](https://github.com/sinhrks/ggfortify) - Unified interface to ggplot2 popular R packages.\n* [Kartograph.py](https://github.com/kartograph/kartograph.py) - Rendering beautiful SVG maps in Python.\n* [pygal](http://pygal.org/en/stable/) - A Python SVG Charts Creator.\n* [PyQtGraph](https://github.com/pyqtgraph/pyqtgraph) - A pure-python graphics and GUI library built on PyQt4 / PySide and NumPy.\n* [pycascading](https://github.com/twitter/pycascading) **[Deprecated]**\n* [Petrel](https://github.com/AirSage/Petrel) - Tools for writing, submitting, debugging, and monitoring Storm topologies in pure Python.\n* [Blaze](https://github.com/blaze/blaze) - NumPy and Pandas interface to Big Data.\n* [emcee](https://github.com/dfm/emcee) - The Python ensemble sampling toolkit for affine-invariant MCMC.\n* [windML](https://github.com/cigroup-ol/windml) - A Python Framework for Wind Energy Analysis and Prediction.\n* [vispy](https://github.com/vispy/vispy) - GPU-based high-performance interactive OpenGL 2D/3D data visualization library.\n* [cerebro2](https://github.com/numenta/nupic.cerebro2) A web-based visualization and debugging platform for NuPIC. **[Deprecated]**\n* [NuPIC Studio](https://github.com/htm-community/nupic.studio) An all-in-one NuPIC Hierarchical Temporal Memory visualization and debugging super-tool! **[Deprecated]**\n* [SparklingPandas](https://github.com/sparklingpandas/sparklingpandas) Pandas on PySpark (POPS).\n* [Seaborn](https://seaborn.pydata.org/) - A python visualization library based on matplotlib.\n* [bqplot](https://github.com/bloomberg/bqplot) - An API for plotting in Jupyter (IPython).\n* [pastalog](https://github.com/rewonc/pastalog) - Simple, realtime visualization of neural network training performance.\n* [Superset](https://github.com/apache/incubator-superset) - A data exploration platform designed to be visual, intuitive, and interactive.\n* [Dora](https://github.com/nathanepstein/dora) - Tools for exploratory data analysis in Python.\n* [Ruffus](http://www.ruffus.org.uk) - Computation Pipeline library for python.\n* [SOMPY](https://github.com/sevamoo/SOMPY) - Self Organizing Map written in Python (Uses neural networks for data analysis).\n* [somoclu](https://github.com/peterwittek/somoclu) Massively parallel self-organizing maps: accelerate training on multicore CPUs, GPUs, and clusters, has python API.\n* [HDBScan](https://github.com/lmcinnes/hdbscan) - implementation of the hdbscan algorithm in Python - used for clustering\n* [visualize_ML](https://github.com/ayush1997/visualize_ML) - A python package for data exploration and data analysis. **[Deprecated]**\n* [scikit-plot](https://github.com/reiinakano/scikit-plot) - A visualization library for quick and easy generation of common plots in data analysis and machine learning.\n* [Bowtie](https://github.com/jwkvam/bowtie) - A dashboard library for interactive visualizations using flask socketio and react.\n* [lime](https://github.com/marcotcr/lime) - Lime is about explaining what machine learning classifiers (or models) are doing. It is able to explain any black box classifier, with two or more classes.\n* [PyCM](https://github.com/sepandhaghighi/pycm) - PyCM is a multi-class confusion matrix library written in Python that supports both input data vectors and direct matrix, and a proper tool for post-classification model evaluation that supports most classes and overall statistics parameters\n* [Dash](https://github.com/plotly/dash) - A framework for creating analytical web applications built on top of Plotly.js, React, and Flask\n* [Lambdo](https://github.com/asavinov/lambdo) - A workflow engine for solving machine learning problems by combining in one analysis pipeline (i) feature engineering and machine learning (ii) model training and prediction (iii) table population and column evaluation via user-defined (Python) functions.\n* [TensorWatch](https://github.com/microsoft/tensorwatch) - Debugging and visualization tool for machine learning and data science. It extensively leverages Jupyter Notebook to show real-time visualizations of data in running processes such as machine learning training.\n* [dowel](https://github.com/rlworkgroup/dowel) - A little logger for machine learning research. Output any object to the terminal, CSV, TensorBoard, text logs on disk, and more with just one call to `logger.log()`.\n\n<a name=\"python-misc\"></a>\n#### Misc Scripts / iPython Notebooks / Codebases\n* [MiniGrad](https://github.com/kennysong/minigrad) \u2013\u00a0A minimal, educational, Pythonic implementation of autograd (~100 loc).\n* [Map/Reduce implementations of common ML algorithms](https://github.com/Yannael/BigDataAnalytics_INFOH515): Jupyter notebooks that cover how to implement from scratch different ML algorithms (ordinary least squares, gradient descent, k-means, alternating least squares), using Python NumPy, and how to then make these implementations scalable using Map/Reduce and Spark.\n* [BioPy](https://github.com/jaredthecoder/BioPy) - Biologically-Inspired and Machine Learning Algorithms in Python. **[Deprecated]**\n* [SVM Explorer](https://github.com/plotly/dash-svm) - Interactive SVM Explorer, using Dash and scikit-learn\n* [pattern_classification](https://github.com/rasbt/pattern_classification)\n* [thinking stats 2](https://github.com/Wavelets/ThinkStats2)\n* [hyperopt](https://github.com/hyperopt/hyperopt-sklearn)\n* [numpic](https://github.com/numenta/nupic)\n* [2012-paper-diginorm](https://github.com/dib-lab/2012-paper-diginorm)\n* [A gallery of interesting IPython notebooks](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks)\n* [ipython-notebooks](https://github.com/ogrisel/notebooks)\n* [data-science-ipython-notebooks](https://github.com/donnemartin/data-science-ipython-notebooks) - Continually updated Data Science Python Notebooks: Spark, Hadoop MapReduce, HDFS, AWS, Kaggle, scikit-learn, matplotlib, pandas, NumPy, SciPy, and various command lines.\n* [decision-weights](https://github.com/CamDavidsonPilon/decision-weights)\n* [Sarah Palin LDA](https://github.com/Wavelets/sarah-palin-lda) - Topic Modeling the Sarah Palin emails.\n* [Diffusion Segmentation](https://github.com/Wavelets/diffusion-segmentation) - A collection of image segmentation algorithms based on diffusion methods.\n* [Scipy Tutorials](https://github.com/Wavelets/scipy-tutorials) - SciPy tutorials. This is outdated, check out scipy-lecture-notes.\n* [Crab](https://github.com/marcelcaraciolo/crab) - A recommendation engine library for Python.\n* [BayesPy](https://github.com/maxsklar/BayesPy) - Bayesian Inference Tools in Python.\n* [scikit-learn tutorials](https://github.com/GaelVaroquaux/scikit-learn-tutorial) - Series of notebooks for learning scikit-learn.\n* [sentiment-analyzer](https://github.com/madhusudancs/sentiment-analyzer) - Tweets Sentiment Analyzer\n* [sentiment_classifier](https://github.com/kevincobain2000/sentiment_classifier) - Sentiment classifier using word sense disambiguation.\n* [group-lasso](https://github.com/fabianp/group_lasso) - Some experiments with the coordinate descent algorithm used in the (Sparse) Group Lasso model.\n* [jProcessing](https://github.com/kevincobain2000/jProcessing) - Kanji / Hiragana / Katakana to Romaji Converter. Edict Dictionary & parallel sentences Search. Sentence Similarity between two JP Sentences. Sentiment Analysis of Japanese Text. Run Cabocha(ISO--8859-1 configured) in Python.\n* [mne-python-notebooks](https://github.com/mne-tools/mne-python-notebooks) - IPython notebooks for EEG/MEG data processing using mne-python.\n* [Neon Course](https://github.com/NervanaSystems/neon_course) - IPython notebooks for a complete course around understanding Nervana's Neon.\n* [pandas cookbook](https://github.com/jvns/pandas-cookbook) - Recipes for using Python's pandas library.\n* [climin](https://github.com/BRML/climin) - Optimization library focused on machine learning, pythonic implementations of gradient descent, LBFGS, rmsprop, adadelta and others.\n* [Allen Downey\u2019s Data Science Course](https://github.com/AllenDowney/DataScience) - Code for Data Science at Olin College, Spring 2014.\n* [Allen Downey\u2019s Think Bayes Code](https://github.com/AllenDowney/ThinkBayes) - Code repository for Think Bayes.\n* [Allen Downey\u2019s Think Complexity Code](https://github.com/AllenDowney/ThinkComplexity) - Code for Allen Downey's book Think Complexity.\n* [Allen Downey\u2019s Think OS Code](https://github.com/AllenDowney/ThinkOS) - Text and supporting code for Think OS: A Brief Introduction to Operating Systems.\n* [Python Programming for the Humanities](https://www.karsdorp.io/python-course/) - Course for Python programming for the Humanities, assuming no prior knowledge. Heavy focus on text processing / NLP.\n* [GreatCircle](https://github.com/mwgg/GreatCircle) - Library for calculating great circle distance.\n* [Optunity examples](http://optunity.readthedocs.io/en/latest/notebooks/index.html) - Examples demonstrating how to use Optunity in synergy with machine learning libraries.\n* [Dive into Machine Learning  with Python Jupyter notebook and scikit-learn](https://github.com/hangtwenty/dive-into-machine-learning) - \"I learned Python by hacking first, and getting serious *later.* I wanted to do this with Machine Learning. If this is your style, join me in getting a bit ahead of yourself.\"\n* [TDB](https://github.com/ericjang/tdb) - TensorDebugger (TDB) is a visual debugger for deep learning. It features interactive, node-by-node debugging and visualization for TensorFlow.\n* [Suiron](https://github.com/kendricktan/suiron/) - Machine Learning for RC Cars.\n* [Introduction to machine learning with scikit-learn](https://github.com/justmarkham/scikit-learn-videos) - IPython notebooks from Data School's video tutorials on scikit-learn.\n* [Practical XGBoost in Python](https://parrotprediction.teachable.com/p/practical-xgboost-in-python) - comprehensive online course about using XGBoost in Python.\n* [Introduction to Machine Learning with Python](https://github.com/amueller/introduction_to_ml_with_python) - Notebooks and code for the book \"Introduction to Machine Learning with Python\"\n* [Pydata book](https://github.com/wesm/pydata-book) - Materials and IPython notebooks for \"Python for Data Analysis\" by Wes McKinney, published by O'Reilly Media\n* [Homemade Machine Learning](https://github.com/trekhleb/homemade-machine-learning) - Python examples of popular machine learning algorithms with interactive Jupyter demos and math being explained\n* [Prodmodel](https://github.com/prodmodel/prodmodel) - Build tool for data science pipelines.\n* [the-elements-of-statistical-learning](https://github.com/maitbayev/the-elements-of-statistical-learning) - This repository contains Jupyter notebooks implementing the algorithms found in the book and summary of the textbook.\n* [Hyperparameter-Optimization-of-Machine-Learning-Algorithms](https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms) - Code for hyperparameter tuning/optimization of machine learning and deep learning algorithms.\n\n<a name=\"python-neural-networks\"></a>\n#### Neural Networks\n\n* [nn_builder](https://github.com/p-christ/nn_builder) - nn_builder is a python package that lets you build neural networks in 1 line\n* [NeuralTalk](https://github.com/karpathy/neuraltalk) - NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences.\n* [Neuron](https://github.com/molcik/python-neuron) - Neuron is simple class for time series predictions. It's utilize LNU (Linear Neural Unit), QNU (Quadratic Neural Unit), RBF (Radial Basis Function), MLP (Multi Layer Perceptron), MLP-ELM (Multi Layer Perceptron - Extreme Learning Machine) neural networks learned with Gradient descent or LeLevenberg\u2013Marquardt algorithm.\n\n* [NeuralTalk](https://github.com/karpathy/neuraltalk2) - NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences. **[Deprecated]**\n* [Neuron](https://github.com/molcik/python-neuron) - Neuron is simple class for time series predictions. It's utilize LNU (Linear Neural Unit), QNU (Quadratic Neural Unit), RBF (Radial Basis Function), MLP (Multi Layer Perceptron), MLP-ELM (Multi Layer Perceptron - Extreme Learning Machine) neural networks learned with Gradient descent or LeLevenberg\u2013Marquardt algorithm. **[Deprecated]**\n* [Data Driven Code](https://github.com/atmb4u/data-driven-code) - Very simple implementation of neural networks for dummies in python without using any libraries, with detailed comments.\n* [Machine Learning, Data Science and Deep Learning with Python](https://www.manning.com/livevideo/machine-learning-data-science-and-deep-learning-with-python) - LiveVideo course that covers machine learning, Tensorflow, artificial intelligence, and neural networks.\n* [TResNet: High Performance GPU-Dedicated Architecture](https://github.com/mrT23/TResNet) - TResNet models were designed and optimized to give the best speed-accuracy tradeoff out there on GPUs. \n* [TResNet: Simple and powerful neural network library for python](https://github.com/zueve/neurolab) - Variety of supported types of Artificial Neural Network and learning algorithms.\n\n\n<a name=\"python-kaggle\"></a>\n#### Kaggle Competition Source Code\n* [open-solution-home-credit](https://github.com/neptune-ml/open-solution-home-credit) -> source code and [experiments results](https://app.neptune.ml/neptune-ml/Home-Credit-Default-Risk) for [Home Credit Default Risk](https://www.kaggle.com/c/home-credit-default-risk).\n* [open-solution-googleai-object-detection](https://github.com/neptune-ml/open-solution-googleai-object-detection) -> source code and [experiments results](https://app.neptune.ml/neptune-ml/Google-AI-Object-Detection-Challenge) for [Google AI Open Images - Object Detection Track](https://www.kaggle.com/c/google-ai-open-images-object-detection-track).\n* [open-solution-salt-identification](https://github.com/neptune-ml/open-solution-salt-identification) -> source code and [experiments results](https://app.neptune.ml/neptune-ml/Salt-Detection) for [TGS Salt Identification Challenge](https://www.kaggle.com/c/tgs-salt-identification-challenge).\n* [open-solution-ship-detection](https://github.com/neptune-ml/open-solution-ship-detection) -> source code and [experiments results](https://app.neptune.ml/neptune-ml/Ships) for [Airbus Ship Detection Challenge](https://www.kaggle.com/c/airbus-ship-detection).\n* [open-solution-data-science-bowl-2018](https://github.com/neptune-ml/open-solution-data-science-bowl-2018) -> source code and [experiments results](https://app.neptune.ml/neptune-ml/Data-Science-Bowl-2018) for [2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018).\n* [open-solution-value-prediction](https://github.com/neptune-ml/open-solution-value-prediction) -> source code and [experiments results](https://app.neptune.ml/neptune-ml/Santander-Value-Prediction-Challenge) for [Santander Value Prediction Challenge](https://www.kaggle.com/c/santander-value-prediction-challenge).\n* [open-solution-toxic-comments](https://github.com/neptune-ml/open-solution-toxic-comments) -> source code for [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge).\n* [wiki challenge](https://github.com/hammer/wikichallenge) - An implementation of Dell Zhang's solution to Wikipedia's Participation Challenge on Kaggle.\n* [kaggle insults](https://github.com/amueller/kaggle_insults) - Kaggle Submission for \"Detecting Insults in Social Commentary\".\n* [kaggle_acquire-valued-shoppers-challenge](https://github.com/MLWave/kaggle_acquire-valued-shoppers-challenge) - Code for the Kaggle acquire valued shoppers challenge.\n* [kaggle-cifar](https://github.com/zygmuntz/kaggle-cifar) - Code for the CIFAR-10 competition at Kaggle, uses cuda-convnet.\n* [kaggle-blackbox](https://github.com/zygmuntz/kaggle-blackbox) - Deep learning made easy.\n* [kaggle-accelerometer](https://github.com/zygmuntz/kaggle-accelerometer) - Code for Accelerometer Biometric Competition at Kaggle.\n* [kaggle-advertised-salaries](https://github.com/zygmuntz/kaggle-advertised-salaries) - Predicting job salaries from ads - a Kaggle competition.\n* [kaggle amazon](https://github.com/zygmuntz/kaggle-amazon) - Amazon access control challenge.\n* [kaggle-bestbuy_big](https://github.com/zygmuntz/kaggle-bestbuy_big) - Code for the Best Buy competition at Kaggle.\n* [kaggle-bestbuy_small](https://github.com/zygmuntz/kaggle-bestbuy_small)\n* [Kaggle Dogs vs. Cats](https://github.com/kastnerkyle/kaggle-dogs-vs-cats) - Code for Kaggle Dogs vs. Cats competition.\n* [Kaggle Galaxy Challenge](https://github.com/benanne/kaggle-galaxies) - Winning solution for the Galaxy Challenge on Kaggle.\n* [Kaggle Gender](https://github.com/zygmuntz/kaggle-gender) - A Kaggle competition: discriminate gender based on handwriting.\n* [Kaggle Merck](https://github.com/zygmuntz/kaggle-merck) - Merck challenge at Kaggle.\n* [Kaggle Stackoverflow](https://github.com/zygmuntz/kaggle-stackoverflow) - Predicting closed questions on Stack Overflow.\n* [kaggle_acquire-valued-shoppers-challenge](https://github.com/MLWave/kaggle_acquire-valued-shoppers-challenge) - Code for the Kaggle acquire valued shoppers challenge.\n* [wine-quality](https://github.com/zygmuntz/wine-quality) - Predicting wine quality.\n\n<a name=\"python-reinforcement-learning\"></a>\n#### Reinforcement Learning\n* [DeepMind Lab](https://github.com/deepmind/lab) - DeepMind Lab is a 3D learning environment based on id Software's Quake III Arena via ioquake3 and other open source software. Its primary purpose is to act as a testbed for research in artificial intelligence, especially deep reinforcement learning.\n* [Gym](https://github.com/openai/gym) - OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms.\n* [Serpent.AI](https://github.com/SerpentAI/SerpentAI) - Serpent.AI is a game agent framework that allows you to turn any video game you own into a sandbox to develop AI and machine learning experiments. For both researchers and hobbyists.\n* [ViZDoom](https://github.com/mwydmuch/ViZDoom) - ViZDoom allows developing AI bots that play Doom using only the visual information (the screen buffer). It is primarily intended for research in machine visual learning, and deep reinforcement learning, in particular.\n* [Roboschool](https://github.com/openai/roboschool) - Open-source software for robot simulation, integrated with OpenAI Gym.\n* [Retro](https://github.com/openai/retro) - Retro Games in Gym\n* [SLM Lab](https://github.com/kengz/SLM-Lab) - Modular Deep Reinforcement Learning framework in PyTorch.\n* [Coach](https://github.com/NervanaSystems/coach) - Reinforcement Learning Coach by Intel\u00ae AI Lab enables easy experimentation with state of the art Reinforcement Learning algorithms\n* [garage](https://github.com/rlworkgroup/garage) - A toolkit for reproducible reinforcement learning research\n* [metaworld](https://github.com/rlworkgroup/metaworld) - An open source robotics benchmark for meta- and multi-task reinforcement learning\n* [acme](https://deepmind.com/research/publications/Acme) - An Open Source Distributed Framework for Reinforcement Learning that makes build and train your agents easily.\n* [Spinning Up](https://spinningup.openai.com) - An educational resource designed to let anyone learn to become a skilled practitioner in deep reinforcement learning\n\n<a name=\"ruby\"></a>\n## Ruby\n\n<a name=\"ruby-nlp\"></a>\n#### Natural Language Processing\n\n* [Awesome NLP with Ruby](https://github.com/arbox/nlp-with-ruby) - Curated link list for practical natural language processing in Ruby.\n* [Treat](https://github.com/louismullie/treat) - Text REtrieval and Annotation Toolkit, definitely the most comprehensive toolkit I\u2019ve encountered so far for Ruby.\n* [Stemmer](https://github.com/aurelian/ruby-stemmer) - Expose libstemmer_c to Ruby. **[Deprecated]**\n* [Raspell](https://sourceforge.net/projects/raspell/) - raspell is an interface binding for ruby. **[Deprecated]**\n* [UEA Stemmer](https://github.com/ealdent/uea-stemmer) - Ruby port of UEALite Stemmer - a conservative stemmer for search and indexing.\n* [Twitter-text-rb](https://github.com/twitter/twitter-text/tree/master/rb) - A library that does auto linking and extraction of usernames, lists and hashtags in tweets.\n\n<a name=\"ruby-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [Awesome Machine Learning with Ruby](https://github.com/arbox/machine-learning-with-ruby) - Curated list of ML related resources for Ruby.\n* [Ruby Machine Learning](https://github.com/tsycho/ruby-machine-learning) - Some Machine Learning algorithms, implemented in Ruby. **[Deprecated]**\n* [Machine Learning Ruby](https://github.com/mizoR/machine-learning-ruby) **[Deprecated]**\n* [jRuby Mahout](https://github.com/vasinov/jruby_mahout) - JRuby Mahout is a gem that unleashes the power of Apache Mahout in the world of JRuby. **[Deprecated]**\n* [CardMagic-Classifier](https://github.com/cardmagic/classifier) - A general classifier module to allow Bayesian and other types of classifications.\n* [rb-libsvm](https://github.com/febeling/rb-libsvm) - Ruby language bindings for LIBSVM which is a Library for Support Vector Machines.\n* [Scoruby](https://github.com/asafschers/scoruby) - Creates Random Forest classifiers from PMML files.\n* [rumale](https://github.com/yoshoku/rumale) - Rumale is a machine learning library in Ruby\n\n<a name=\"ruby-data-analysis\"></a>\n#### Data Analysis / Data Visualization\n\n* [rsruby](https://github.com/alexgutteridge/rsruby) - Ruby - R bridge.\n* [data-visualization-ruby](https://github.com/chrislo/data_visualisation_ruby) - Source code and supporting content for my Ruby Manor presentation on Data Visualisation with Ruby. **[Deprecated]**\n* [ruby-plot](https://www.ruby-toolbox.com/projects/ruby-plot) - gnuplot wrapper for Ruby, especially for plotting ROC curves into SVG files. **[Deprecated]**\n* [plot-rb](https://github.com/zuhao/plotrb) - A plotting library in Ruby built on top of Vega and D3. **[Deprecated]**\n* [scruffy](https://github.com/delano/scruffy) - A beautiful graphing toolkit for Ruby.\n* [SciRuby](http://sciruby.com/)\n* [Glean](https://github.com/glean/glean) - A data management tool for humans. **[Deprecated]**\n* [Bioruby](https://github.com/bioruby/bioruby)\n* [Arel](https://github.com/nkallen/arel) **[Deprecated]**\n\n<a name=\"ruby-misc\"></a>\n#### Misc\n\n* [Big Data For Chimps](https://github.com/infochimps-labs/big_data_for_chimps)\n* [Listof](https://github.com/kevincobain2000/listof) - Community based data collection, packed in gem. Get list of pretty much anything (stop words, countries, non words) in txt, json or hash. [Demo/Search for a list](http://kevincobain2000.github.io/listof/)\n\n\n<a name=\"rust\"></a>\n## Rust\n\n<a name=\"rust-general-purpose\"></a>\n#### General-Purpose Machine Learning\n* [deeplearn-rs](https://github.com/tedsta/deeplearn-rs) - deeplearn-rs provides simple networks that use matrix multiplication, addition, and ReLU under the MIT license.\n* [rustlearn](https://github.com/maciejkula/rustlearn) - a machine learning framework featuring logistic regression, support vector machines, decision trees and random forests.\n* [rusty-machine](https://github.com/AtheMathmo/rusty-machine) - a pure-rust machine learning library.\n* [leaf](https://github.com/autumnai/leaf) - open source framework for machine intelligence, sharing concepts from TensorFlow and Caffe. Available under the MIT license. [**[Deprecated]**](https://medium.com/@mjhirn/tensorflow-wins-89b78b29aafb#.s0a3uy4cc)\n* [RustNN](https://github.com/jackm321/RustNN) - RustNN is a feedforward neural network library. **[Deprecated]**\n* [RusticSOM](https://github.com/avinashshenoy97/RusticSOM) - A Rust library for Self Organising Maps (SOM).\n\n\n<a name=\"r\"></a>\n## R\n\n<a name=\"r-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [ahaz](https://cran.r-project.org/web/packages/ahaz/index.html) - ahaz: Regularization for semiparametric additive hazards regression. **[Deprecated]**\n* [arules](https://cran.r-project.org/web/packages/arules/index.html) - arules: Mining Association Rules and Frequent Itemsets\n* [biglasso](https://cran.r-project.org/web/packages/biglasso/index.html) - biglasso: Extending Lasso Model Fitting to Big Data in R.\n* [bmrm](https://cran.r-project.org/web/packages/bmrm/index.html) - bmrm: Bundle Methods for Regularized Risk Minimization Package.\n* [Boruta](https://cran.r-project.org/web/packages/Boruta/index.html) - Boruta: A wrapper algorithm for all-relevant feature selection.\n* [bst](https://cran.r-project.org/web/packages/bst/index.html) - bst: Gradient Boosting.\n* [C50](https://cran.r-project.org/web/packages/C50/index.html) - C50: C5.0 Decision Trees and Rule-Based Models.\n* [caret](https://topepo.github.io/caret/index.html) - Classification and Regression Training: Unified interface to ~150 ML algorithms in R.\n* [caretEnsemble](https://cran.r-project.org/web/packages/caretEnsemble/index.html) - caretEnsemble: Framework for fitting multiple caret models as well as creating ensembles of such models. **[Deprecated]**\n* [CatBoost](https://github.com/catboost/catboost) - General purpose gradient boosting on decision trees library with categorical features support out of the box for R.\n* [Clever Algorithms For Machine Learning](https://machinelearningmastery.com/)\n* [CORElearn](https://cran.r-project.org/web/packages/CORElearn/index.html) - CORElearn: Classification, regression, feature evaluation and ordinal evaluation.\n* [CoxBoost](https://cran.r-project.org/web/packages/CoxBoost/index.html) - CoxBoost: Cox models by likelihood based boosting for a single survival endpoint or competing risks **[Deprecated]**\n* [Cubist](https://cran.r-project.org/web/packages/Cubist/index.html) - Cubist: Rule- and Instance-Based Regression Modeling.\n* [e1071](https://cran.r-project.org/web/packages/e1071/index.html) - e1071: Misc Functions of the Department of Statistics (e1071), TU Wien\n* [earth](https://cran.r-project.org/web/packages/earth/index.html) - earth: Multivariate Adaptive Regression Spline Models\n* [elasticnet](https://cran.r-project.org/web/packages/elasticnet/index.html) - elasticnet: Elastic-Net for Sparse Estimation and Sparse PCA.\n* [ElemStatLearn](https://cran.r-project.org/web/packages/ElemStatLearn/index.html) - ElemStatLearn: Data sets, functions and examples from the book: \"The Elements of Statistical Learning, Data Mining, Inference, and Prediction\" by Trevor Hastie, Robert Tibshirani and Jerome Friedman Prediction\" by Trevor Hastie, Robert Tibshirani and Jerome Friedman.\n* [evtree](https://cran.r-project.org/web/packages/evtree/index.html) - evtree: Evolutionary Learning of Globally Optimal Trees.\n* [forecast](https://cran.r-project.org/web/packages/forecast/index.html) - forecast: Timeseries forecasting using ARIMA, ETS, STLM, TBATS, and neural network models.\n* [forecastHybrid](https://cran.r-project.org/web/packages/forecastHybrid/index.html) - forecastHybrid: Automatic ensemble and cross validation of ARIMA, ETS, STLM, TBATS, and neural network models from the \"forecast\" package.\n* [fpc](https://cran.r-project.org/web/packages/fpc/index.html) - fpc: Flexible procedures for clustering.\n* [frbs](https://cran.r-project.org/web/packages/frbs/index.html) - frbs: Fuzzy Rule-based Systems for Classification and Regression Tasks. **[Deprecated]**\n* [GAMBoost](https://cran.r-project.org/web/packages/GAMBoost/index.html) - GAMBoost: Generalized linear and additive models by likelihood based boosting. **[Deprecated]**\n* [gamboostLSS](https://cran.r-project.org/web/packages/gamboostLSS/index.html) - gamboostLSS: Boosting Methods for GAMLSS.\n* [gbm](https://cran.r-project.org/web/packages/gbm/index.html) - gbm: Generalized Boosted Regression Models.\n* [glmnet](https://cran.r-project.org/web/packages/glmnet/index.html) - glmnet: Lasso and elastic-net regularized generalized linear models.\n* [glmpath](https://cran.r-project.org/web/packages/glmpath/index.html) - glmpath: L1 Regularization Path for Generalized Linear Models and Cox Proportional Hazards Model.\n* [GMMBoost](https://cran.r-project.org/web/packages/GMMBoost/index.html) - GMMBoost: Likelihood-based Boosting for Generalized mixed models. **[Deprecated]**\n* [grplasso](https://cran.r-project.org/web/packages/grplasso/index.html) - grplasso: Fitting user specified models with Group Lasso penalty.\n* [grpreg](https://cran.r-project.org/web/packages/grpreg/index.html) - grpreg: Regularization paths for regression models with grouped covariates.\n* [h2o](https://cran.r-project.org/web/packages/h2o/index.html) - A framework for fast, parallel, and distributed machine learning algorithms at scale -- Deeplearning, Random forests, GBM, KMeans, PCA, GLM.\n* [hda](https://cran.r-project.org/web/packages/hda/index.html) - hda: Heteroscedastic Discriminant Analysis. **[Deprecated]**\n* [Introduction to Statistical Learning](https://www-bcf.usc.edu/~gareth/ISL/)\n* [ipred](https://cran.r-project.org/web/packages/ipred/index.html) - ipred: Improved Predictors.\n* [kernlab](https://cran.r-project.org/web/packages/kernlab/index.html) - kernlab: Kernel-based Machine Learning Lab.\n* [klaR](https://cran.r-project.org/web/packages/klaR/index.html) - klaR: Classification and visualization.\n* [L0Learn](https://cran.r-project.org/web/packages/L0Learn/index.html) - L0Learn: Fast algorithms for best subset selection.\n* [lars](https://cran.r-project.org/web/packages/lars/index.html) - lars: Least Angle Regression, Lasso and Forward Stagewise. **[Deprecated]**\n* [lasso2](https://cran.r-project.org/web/packages/lasso2/index.html) - lasso2: L1 constrained estimation aka \u2018lasso\u2019.\n* [LiblineaR](https://cran.r-project.org/web/packages/LiblineaR/index.html) - LiblineaR: Linear Predictive Models Based On The Liblinear C/C++ Library.\n* [LogicReg](https://cran.r-project.org/web/packages/LogicReg/index.html) - LogicReg: Logic Regression.\n* [Machine Learning For Hackers](https://github.com/johnmyleswhite/ML_for_Hackers)\n* [maptree](https://cran.r-project.org/web/packages/maptree/index.html) - maptree: Mapping, pruning, and graphing tree models. **[Deprecated]**\n* [mboost](https://cran.r-project.org/web/packages/mboost/index.html) - mboost: Model-Based Boosting.\n* [medley](https://www.kaggle.com/general/3661) - medley: Blending regression models, using a greedy stepwise approach.\n* [mlr](https://cran.r-project.org/web/packages/mlr/index.html) - mlr: Machine Learning in R.\n* [ncvreg](https://cran.r-project.org/web/packages/ncvreg/index.html) - ncvreg: Regularization paths for SCAD- and MCP-penalized regression models.\n* [nnet](https://cran.r-project.org/web/packages/nnet/index.html) - nnet: Feed-forward Neural Networks and Multinomial Log-Linear Models. **[Deprecated]**\n* [pamr](https://cran.r-project.org/web/packages/pamr/index.html) - pamr: Pam: prediction analysis for microarrays. **[Deprecated]**\n* [party](https://cran.r-project.org/web/packages/party/index.html) - party: A Laboratory for Recursive Partitioning\n* [partykit](https://cran.r-project.org/web/packages/partykit/index.html) - partykit: A Toolkit for Recursive Partitioning.\n* [penalized](https://cran.r-project.org/web/packages/penalized/index.html) - penalized: L1 (lasso and fused lasso) and L2 (ridge) penalized estimation in GLMs and in the Cox model.\n* [penalizedLDA](https://cran.r-project.org/web/packages/penalizedLDA/index.html) - penalizedLDA: Penalized classification using Fisher's linear discriminant. **[Deprecated]**\n* [penalizedSVM](https://cran.r-project.org/web/packages/penalizedSVM/index.html) - penalizedSVM: Feature Selection SVM using penalty functions.\n* [quantregForest](https://cran.r-project.org/web/packages/quantregForest/index.html) - quantregForest: Quantile Regression Forests.\n* [randomForest](https://cran.r-project.org/web/packages/randomForest/index.html) - randomForest: Breiman and Cutler's random forests for classification and regression.\n* [randomForestSRC](https://cran.r-project.org/web/packages/randomForestSRC/index.html) - randomForestSRC: Random Forests for Survival, Regression and Classification (RF-SRC).\n* [rattle](https://cran.r-project.org/web/packages/rattle/index.html) - rattle: Graphical user interface for data mining in R.\n* [rda](https://cran.r-project.org/web/packages/rda/index.html) - rda: Shrunken Centroids Regularized Discriminant Analysis.\n* [rdetools](https://cran.r-project.org/web/packages/rdetools/index.html) - rdetools: Relevant Dimension Estimation (RDE) in Feature Spaces. **[Deprecated]**\n* [REEMtree](https://cran.r-project.org/web/packages/REEMtree/index.html) - REEMtree: Regression Trees with Random Effects for Longitudinal (Panel) Data. **[Deprecated]**\n* [relaxo](https://cran.r-project.org/web/packages/relaxo/index.html) - relaxo: Relaxed Lasso. **[Deprecated]**\n* [rgenoud](https://cran.r-project.org/web/packages/rgenoud/index.html) - rgenoud: R version of GENetic Optimization Using Derivatives\n* [Rmalschains](https://cran.r-project.org/web/packages/Rmalschains/index.html) - Rmalschains: Continuous Optimization using Memetic Algorithms with Local Search Chains (MA-LS-Chains) in R.\n* [rminer](https://cran.r-project.org/web/packages/rminer/index.html) - rminer: Simpler use of data mining methods (e.g. NN and SVM) in classification and regression. **[Deprecated]**\n* [ROCR](https://cran.r-project.org/web/packages/ROCR/index.html) - ROCR: Visualizing the performance of scoring classifiers. **[Deprecated]**\n* [RoughSets](https://cran.r-project.org/web/packages/RoughSets/index.html) - RoughSets: Data Analysis Using Rough Set and Fuzzy Rough Set Theories. **[Deprecated]**\n* [rpart](https://cran.r-project.org/web/packages/rpart/index.html) - rpart: Recursive Partitioning and Regression Trees.\n* [RPMM](https://cran.r-project.org/web/packages/RPMM/index.html) - RPMM: Recursively Partitioned Mixture Model.\n* [RSNNS](https://cran.r-project.org/web/packages/RSNNS/index.html) - RSNNS: Neural Networks in R using the Stuttgart Neural Network Simulator (SNNS).\n* [RWeka](https://cran.r-project.org/web/packages/RWeka/index.html) - RWeka: R/Weka interface.\n* [RXshrink](https://cran.r-project.org/web/packages/RXshrink/index.html) - RXshrink: Maximum Likelihood Shrinkage via Generalized Ridge or Least Angle Regression.\n* [sda](https://cran.r-project.org/web/packages/sda/index.html) - sda: Shrinkage Discriminant Analysis and CAT Score Variable Selection. **[Deprecated]**\n* [spectralGraphTopology](https://cran.r-project.org/web/packages/spectralGraphTopology/index.html) - spectralGraphTopology: Learning Graphs from Data via Spectral Constraints.\n* [SuperLearner](https://github.com/ecpolley/SuperLearner) - Multi-algorithm ensemble learning packages.\n* [svmpath](https://cran.r-project.org/web/packages/svmpath/index.html) - svmpath: svmpath: the SVM Path algorithm. **[Deprecated]**\n* [tgp](https://cran.r-project.org/web/packages/tgp/index.html) - tgp: Bayesian treed Gaussian process models. **[Deprecated]**\n* [tree](https://cran.r-project.org/web/packages/tree/index.html) - tree: Classification and regression trees.\n* [varSelRF](https://cran.r-project.org/web/packages/varSelRF/index.html) - varSelRF: Variable selection using random forests.\n* [XGBoost.R](https://github.com/tqchen/xgboost/tree/master/R-package) - R binding for eXtreme Gradient Boosting (Tree) Library.\n* [Optunity](https://optunity.readthedocs.io/en/latest/) - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly to R.\n* [igraph](https://igraph.org/r/) - binding to igraph library - General purpose graph library.\n* [MXNet](https://github.com/apache/incubator-mxnet) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [TDSP-Utilities](https://github.com/Azure/Azure-TDSP-Utilities) - Two data science utilities in R from Microsoft: 1) Interactive Data Exploration, Analysis, and Reporting (IDEAR) ; 2) Automated Modeling and Reporting (AMR).\n\n<a name=\"r-data-analysis\"></a>\n#### Data Manipulation | Data Analysis | Data Visualization\n\n* [dplyr](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8) - A data manipulation package that helps to solve the most common data manipulation problems.\n* [ggplot2](https://ggplot2.tidyverse.org/) - A data visualization package based on the grammar of graphics.\n* [tmap](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html) for visualizing geospatial data with static maps and [leaflet](https://rstudio.github.io/leaflet/) for interactive maps\n* [tm](https://www.rdocumentation.org/packages/tm/) and [quanteda](https://quanteda.io/) are the main packages for managing,  analyzing, and visualizing textual data.\n* [shiny](https://shiny.rstudio.com/) is the basis for truly interactive displays and dashboards in R. However, some measure of interactivity can be achieved with [htmlwidgets](https://www.htmlwidgets.org/) bringing javascript libraries to R. These include, [plotly](https://plot.ly/r/), [dygraphs](http://rstudio.github.io/dygraphs), [highcharter](http://jkunst.com/highcharter/), and several others.\n\n<a name=\"sas\"></a>\n## SAS\n\n<a name=\"sas-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [Visual Data Mining and Machine Learning](https://www.sas.com/en_us/software/visual-data-mining-machine-learning.html) - Interactive, automated, and programmatic modeling with the latest machine learning algorithms in and end-to-end analytics environment, from data prep to deployment. Free trial available.\n* [Enterprise Miner](https://www.sas.com/en_us/software/enterprise-miner.html) - Data mining and machine learning that creates deployable models using a GUI or code.\n* [Factory Miner](https://www.sas.com/en_us/software/factory-miner.html) - Automatically creates deployable machine learning models across numerous market or customer segments using a GUI.\n\n<a name=\"sas-data-analysis\"></a>\n#### Data Analysis / Data Visualization\n\n* [SAS/STAT](https://www.sas.com/en_us/software/stat.html) - For conducting advanced statistical analysis.\n* [University Edition](https://www.sas.com/en_us/software/university-edition.html) - FREE! Includes all SAS packages necessary for data analysis and visualization, and includes online SAS courses.\n\n<a name=\"sas-nlp\"></a>\n#### Natural Language Processing\n\n* [Contextual Analysis](https://www.sas.com/en_us/software/contextual-analysis.html) - Add structure to unstructured text using a GUI.\n* [Sentiment Analysis](https://www.sas.com/en_us/software/sentiment-analysis.html) - Extract sentiment from text using a GUI.\n* [Text Miner](https://www.sas.com/en_us/software/text-miner.html) - Text mining using a GUI or code.\n\n<a name=\"sas-demos\"></a>\n#### Demos and Scripts\n\n* [ML_Tables](https://github.com/sassoftware/enlighten-apply/tree/master/ML_tables) - Concise cheat sheets containing machine learning best practices.\n* [enlighten-apply](https://github.com/sassoftware/enlighten-apply) - Example code and materials that illustrate applications of SAS machine learning techniques.\n* [enlighten-integration](https://github.com/sassoftware/enlighten-integration) - Example code and materials that illustrate techniques for integrating SAS with other analytics technologies in Java, PMML, Python and R.\n* [enlighten-deep](https://github.com/sassoftware/enlighten-deep) - Example code and materials that illustrate using neural networks with several hidden layers in SAS.\n* [dm-flow](https://github.com/sassoftware/dm-flow) - Library of SAS Enterprise Miner process flow diagrams to help you learn by example about specific data mining topics.\n\n\n<a name=\"scala\"></a>\n## Scala\n\n<a name=\"scala-nlp\"></a>\n#### Natural Language Processing\n\n* [ScalaNLP](http://www.scalanlp.org/) - ScalaNLP is a suite of machine learning and numerical computing libraries.\n* [Breeze](https://github.com/scalanlp/breeze) - Breeze is a numerical processing library for Scala.\n* [Chalk](https://github.com/scalanlp/chalk) - Chalk is a natural language processing library. **[Deprecated]**\n* [FACTORIE](https://github.com/factorie/factorie) - FACTORIE is a toolkit for deployable probabilistic modeling, implemented as a software library in Scala. It provides its users with a succinct language for creating relational factor graphs, estimating parameters and performing inference.\n* [Montague](https://github.com/Workday/upshot-montague) - Montague is a semantic parsing library for Scala with an easy-to-use DSL.\n* [Spark NLP](https://github.com/JohnSnowLabs/spark-nlp) - Natural language processing library built on top of Apache Spark ML to provide simple, performant, and accurate NLP annotations for machine learning pipelines, that scale easily in a distributed environment.\n\n<a name=\"scala-data-analysis\"></a>\n#### Data Analysis / Data Visualization\n\n* [MLlib in Apache Spark](https://spark.apache.org/docs/latest/mllib-guide.html) - Distributed machine learning library in Spark\n* [Hydrosphere Mist](https://github.com/Hydrospheredata/mist) - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.\n* [Scalding](https://github.com/twitter/scalding) - A Scala API for Cascading.\n* [Summing Bird](https://github.com/twitter/summingbird) - Streaming MapReduce with Scalding and Storm.\n* [Algebird](https://github.com/twitter/algebird) - Abstract Algebra for Scala.\n* [xerial](https://github.com/xerial/xerial) - Data management utilities for Scala. **[Deprecated]**\n* [PredictionIO](https://github.com/apache/predictionio) - PredictionIO, a machine learning server for software developers and data engineers.\n* [BIDMat](https://github.com/BIDData/BIDMat) - CPU and GPU-accelerated matrix library intended to support large-scale exploratory data analysis.\n* [Flink](https://flink.apache.org/) - Open source platform for distributed stream and batch data processing.\n* [Spark Notebook](http://spark-notebook.io) - Interactive and Reactive Data Science using Scala and Spark.\n\n<a name=\"scala-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [DeepLearning.scala](https://deeplearning.thoughtworks.school/) - Creating statically typed dynamic neural networks from object-oriented & functional programming constructs.\n* [Conjecture](https://github.com/etsy/Conjecture) - Scalable Machine Learning in Scalding.\n* [brushfire](https://github.com/stripe/brushfire) - Distributed decision tree ensemble learning in Scala.\n* [ganitha](https://github.com/tresata/ganitha) - Scalding powered machine learning. **[Deprecated]**\n* [adam](https://github.com/bigdatagenomics/adam) - A genomics processing engine and specialized file format built using Apache Avro, Apache Spark and Parquet. Apache 2 licensed.\n* [bioscala](https://github.com/bioscala/bioscala) - Bioinformatics for the Scala programming language\n* [BIDMach](https://github.com/BIDData/BIDMach) - CPU and GPU-accelerated Machine Learning Library.\n* [Figaro](https://github.com/p2t2/figaro) - a Scala library for constructing probabilistic models.\n* [H2O Sparkling Water](https://github.com/h2oai/sparkling-water) - H2O and Spark interoperability.\n* [FlinkML in Apache Flink](https://ci.apache.org/projects/flink/flink-docs-master/dev/libs/ml/index.html) - Distributed machine learning library in Flink.\n* [DynaML](https://github.com/transcendent-ai-labs/DynaML) - Scala Library/REPL for Machine Learning Research.\n* [Saul](https://github.com/CogComp/saul) - Flexible Declarative Learning-Based Programming.\n* [SwiftLearner](https://github.com/valdanylchuk/swiftlearner/) - Simply written algorithms to help study ML or write your own implementations.\n* [Smile](https://haifengl.github.io/) - Statistical Machine Intelligence and Learning Engine.\n* [doddle-model](https://github.com/picnicml/doddle-model) - An in-memory machine learning library built on top of Breeze. It provides immutable objects and exposes its functionality through a scikit-learn-like API.\n* [TensorFlow Scala](https://github.com/eaplatanios/tensorflow_scala) -   Strongly-typed Scala API for TensorFlow.\n\n<a name=\"scheme\"></a>\n## Scheme\n\n<a name=\"scheme-neural-networks\"></a>\n#### Neural Networks\n\n* [layer](https://github.com/cloudkj/layer) - Neural network inference from the command line, implemented in [CHICKEN Scheme](https://www.call-cc.org/).\n\n<a name=\"swift\"></a>\n## Swift\n\n<a name=\"swift-general-purpose\"></a>\n#### General-Purpose Machine Learning\n\n* [Bender](https://github.com/xmartlabs/Bender) - Fast Neural Networks framework built on top of Metal. Supports TensorFlow models.\n* [Swift AI](https://github.com/Swift-AI/Swift-AI) - Highly optimized artificial intelligence and machine learning library written in Swift.\n* [Swift for Tensorflow](https://github.com/tensorflow/swift) - a next-generation platform for machine learning, incorporating the latest research across machine learning, compilers, differentiable programming, systems design, and beyond.\n* [BrainCore](https://github.com/alejandro-isaza/BrainCore) - The iOS and OS X neural network framework.\n* [swix](https://github.com/stsievert/swix) - A bare bones library that includes a general matrix language and wraps some OpenCV for iOS development. **[Deprecated]**\n* [AIToolbox](https://github.com/KevinCoble/AIToolbox) - A toolbox framework of AI modules written in Swift: Graphs/Trees, Linear Regression, Support Vector Machines, Neural Networks, PCA, KMeans, Genetic Algorithms, MDP, Mixture of Gaussians.\n* [MLKit](https://github.com/Somnibyte/MLKit) - A simple Machine Learning Framework written in Swift. Currently features Simple Linear Regression, Polynomial Regression, and Ridge Regression.\n* [Swift Brain](https://github.com/vlall/Swift-Brain) - The first neural network / machine learning library written in Swift. This is a project for AI algorithms in Swift for iOS and OS X development. This project includes algorithms focused on Bayes theorem, neural networks, SVMs, Matrices, etc...\n* [Perfect TensorFlow](https://github.com/PerfectlySoft/Perfect-TensorFlow) - Swift Language Bindings of TensorFlow. Using native TensorFlow models on both macOS / Linux.\n* [PredictionBuilder](https://github.com/denissimon/prediction-builder-swift) - A library for machine learning that builds predictions using a linear regression.\n* [Awesome CoreML](https://github.com/SwiftBrain/awesome-CoreML-models) - A curated list of pretrained CoreML models.\n* [Awesome Core ML Models](https://github.com/likedan/Awesome-CoreML-Models) - A curated list of machine learning models in CoreML format.\n\n<a name=\"tensor\"></a>\n## TensorFlow\n\n<a name=\"tensor-general-purpose\"></a>\n#### General-Purpose Machine Learning\n* [Awesome TensorFlow](https://github.com/jtoy/awesome-tensorflow) - A list of all things related to TensorFlow.\n* [Golden TensorFlow](https://golden.com/wiki/TensorFlow) - A page of content on TensorFlow, including academic papers and links to related topics.\n\n<a name=\"tools\"></a>\n## Tools\n\n<a name=\"tools-neural-networks\"></a>\n#### Neural Networks\n* [layer](https://github.com/cloudkj/layer) - Neural network inference from the command line\n\n<a name=\"tools-misc\"></a>\n#### Misc\n* [CatalyzeX](https://chrome.google.com/webstore/detail/code-finder-for-research/aikkeehnlfpamidigaffhfmgbkdeheil) - Browser extension ([Chrome](https://chrome.google.com/webstore/detail/code-finder-for-research/aikkeehnlfpamidigaffhfmgbkdeheil) and [Firefox](https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/)) that automatically finds and shows code implementations for machine learning papers anywhere: Google, Twitter, Arxiv, Scholar, etc.\n* [ML Workspace](https://github.com/ml-tooling/ml-workspace) - All-in-one web-based IDE for machine learning and data science. The workspace is deployed as a docker container and is preloaded with a variety of popular data science libraries (e.g., Tensorflow, PyTorch) and dev tools (e.g., Jupyter, VS Code).\n* [Notebooks](https://github.com/rlan/notebooks) - A starter kit for Jupyter notebooks and machine learning. Companion docker images consist of all combinations of python versions, machine learning frameworks (Keras, PyTorch and Tensorflow) and CPU/CUDA versions.\n* [DVC](https://github.com/iterative/dvc) - Data Science Version Control is an open-source version control system for machine learning projects with pipelines support. It makes ML projects reproducible and shareable.\n* [Kedro](https://github.com/quantumblacklabs/kedro/) - Kedro is a data and development workflow framework that implements best practices for data pipelines with an eye towards productionizing machine learning models.\n* [guild.ai](https://guild.ai/) - Tool to log, analyze, compare and \"optimize\" experiments. It's cross-platform and framework independent, and provided integrated visualizers such as tensorboard.\n* [Sacred](https://github.com/IDSIA/sacred) - Python tool to help  you configure, organize, log and reproduce experiments. Like a notebook lab in the context of Chemistry/Biology. The community has built multiple add-ons leveraging the proposed standard.\n* [MLFlow](https://mlflow.org/) - platform to manage the ML lifecycle, including experimentation, reproducibility and deployment. Framework and language agnostic, take a look at all the built-in integrations.\n* [Weights & Biases](https://www.wandb.com/) - Machine learning experiment tracking, dataset versioning, hyperparameter search, visualization, and collaboration\n* More tools to improve the ML lifecycle: [Catalyst](https://github.com/catalyst-team/catalyst), [PachydermIO](https://www.pachyderm.io/). The following are Github-alike and targeting teams [Weights & Biases](https://www.wandb.com/), [Neptune.Ml](https://neptune.ml/), [Comet.ml](https://www.comet.ml/), [Valohai.ai](https://valohai.com/), [DAGsHub](https://DAGsHub.com/).\n* [MachineLearningWithTensorFlow2ed](https://www.manning.com/books/machine-learning-with-tensorflow-second-edition) - a book on general purpose machine learning techniques regression, classification, unsupervised clustering, reinforcement learning, auto encoders, convolutional neural networks, RNNs, LSTMs, using TensorFlow 1.14.1.\n* [m2cgen](https://github.com/BayesWitnesses/m2cgen) - A tool that allows the conversion of ML models into native code (Java, C, Python, Go, JavaScript, Visual Basic, C#, R, PowerShell, PHP, Dart) with zero dependencies.\n* [CML](https://github.com/iterative/cml) - A library for doing continuous integration with ML projects. Use GitHub Actions & GitLab CI to train and evaluate models in production like environments and automatically generate visual reports with metrics and graphs in pull/merge requests. Framework & language agnostic.\n* [Pythonizr](https://pythonizr.com) - An online tool to generate boilerplate machine learning code that uses scikit-learn.\n<a name=\"credits\"></a>\n## Credits\n\n* Some of the python libraries were cut-and-pasted from [vinta](https://github.com/vinta/awesome-python)\n* References for Go were mostly cut-and-pasted from [gopherdata](https://github.com/gopherdata/resources/tree/master/tooling)\n"
 },
 {
  "repo": "ujjwalkarn/Machine-Learning-Tutorials",
  "language": null,
  "readme_contents": "\n# Machine Learning & Deep Learning Tutorials [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\n- This repository contains a topic-wise curated list of Machine Learning and Deep Learning tutorials, articles and other resources. Other awesome lists can be found in this [list](https://github.com/sindresorhus/awesome).\n\n- If you want to contribute to this list, please read [Contributing Guidelines](https://github.com/ujjwalkarn/Machine-Learning-Tutorials/blob/master/contributing.md).\n\n- [Curated list of R tutorials for Data Science, NLP and Machine Learning](https://github.com/ujjwalkarn/DataScienceR).\n\n- [Curated list of Python tutorials for Data Science, NLP and Machine Learning](https://github.com/ujjwalkarn/DataSciencePython).\n\n\n## Contents\n- [Introduction](#general)\n- [Interview Resources](#interview)\n- [Artificial Intelligence](#ai)\n- [Genetic Algorithms](#ga)\n- [Statistics](#stat)\n- [Useful Blogs](#blogs)\n- [Resources on Quora](#quora)\n- [Resources on Kaggle](#kaggle)\n- [Cheat Sheets](#cs)\n- [Classification](#classification)\n- [Linear Regression](#linear)\n- [Logistic Regression](#logistic)\n- [Model Validation using Resampling](#validation)\n    - [Cross Validation](#cross)\n    - [Bootstraping](#boot)\n- [Deep Learning](#deep)\n    - [Frameworks](#frame)\n    - [Feed Forward Networks](#feed)\n    - [Recurrent Neural Nets, LSTM, GRU](#rnn)\n    - [Restricted Boltzmann Machine, DBNs](#rbm)\n    - [Autoencoders](#auto)\n    - [Convolutional Neural Nets](#cnn)\n    - [Graph Representation Learning](#nrl)\n- [Natural Language Processing](#nlp)\n    - [Topic Modeling, LDA](#topic)\n    - [Word2Vec](#word2vec)\n- [Computer Vision](#vision)\n- [Support Vector Machine](#svm)\n- [Reinforcement Learning](#rl)\n- [Decision Trees](#dt)\n- [Random Forest / Bagging](#rf)\n- [Boosting](#gbm)\n- [Ensembles](#ensem)\n- [Stacking Models](#stack)\n- [VC Dimension](#vc)\n- [Bayesian Machine Learning](#bayes)\n- [Semi Supervised Learning](#semi)\n- [Optimizations](#opt)\n- [Other Useful Tutorials](#other)\n\n<a name=\"general\" />\n\n## Introduction\n\n- [Machine Learning Course by Andrew Ng (Stanford University)](https://www.coursera.org/learn/machine-learning)\n\n- [Curated List of Machine Learning Resources](https://hackr.io/tutorials/learn-machine-learning-ml)\n\n- [In-depth introduction to machine learning in 15 hours of expert videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/)\n\n- [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)\n\n- [List of Machine Learning University Courses](https://github.com/prakhar1989/awesome-courses#machine-learning)\n\n- [Machine Learning for Software Engineers](https://github.com/ZuzooVn/machine-learning-for-software-engineers)\n\n- [Dive into Machine Learning](https://github.com/hangtwenty/dive-into-machine-learning)\n\n- [A curated list of awesome Machine Learning frameworks, libraries and software](https://github.com/josephmisiti/awesome-machine-learning)\n\n- [A curated list of awesome data visualization libraries and resources.](https://github.com/fasouto/awesome-dataviz)\n\n- [An awesome Data Science repository to learn and apply for real world problems](https://github.com/okulbilisim/awesome-datascience)\n\n- [The Open Source Data Science Masters](http://datasciencemasters.org/)\n\n- [Machine Learning FAQs on Cross Validated](http://stats.stackexchange.com/questions/tagged/machine-learning)\n\n- [Machine Learning algorithms that you should always have a strong understanding of](https://www.quora.com/What-are-some-Machine-Learning-algorithms-that-you-should-always-have-a-strong-understanding-of-and-why)\n\n- [Difference between Linearly Independent, Orthogonal, and Uncorrelated Variables](http://terpconnect.umd.edu/~bmomen/BIOM621/LineardepCorrOrthogonal.pdf)\n\n- [List of Machine Learning Concepts](https://en.wikipedia.org/wiki/List_of_machine_learning_concepts)\n\n- [Slides on Several Machine Learning Topics](http://www.slideshare.net/pierluca.lanzi/presentations)\n\n- [MIT Machine Learning Lecture Slides](http://www.ai.mit.edu/courses/6.867-f04/lectures.html)\n\n- [Comparison Supervised Learning Algorithms](http://www.dataschool.io/comparing-supervised-learning-algorithms/)\n\n- [Learning Data Science Fundamentals](http://www.dataschool.io/learning-data-science-fundamentals/)\n\n- [Machine Learning mistakes to avoid](https://medium.com/@nomadic_mind/new-to-machine-learning-avoid-these-three-mistakes-73258b3848a4#.lih061l3l)\n\n- [Statistical Machine Learning Course](http://www.stat.cmu.edu/~larry/=sml/)\n\n- [TheAnalyticsEdge edX Notes and Codes](https://github.com/pedrosan/TheAnalyticsEdge)\n\n- [Have Fun With Machine Learning](https://github.com/humphd/have-fun-with-machine-learning)\n\n- [Twitter's Most Shared #machineLearning Content From The Past 7 Days](http://theherdlocker.com/tweet/popularity/machinelearning)\n\n<a name=\"interview\" />\n\n## Interview Resources\n\n- [41 Essential Machine Learning Interview Questions (with answers)](https://www.springboard.com/blog/machine-learning-interview-questions/)\n\n- [How can a computer science graduate student prepare himself for data scientist interviews?](https://www.quora.com/How-can-a-computer-science-graduate-student-prepare-himself-for-data-scientist-machine-learning-intern-interviews)\n\n- [How do I learn Machine Learning?](https://www.quora.com/How-do-I-learn-machine-learning-1)\n\n- [FAQs about Data Science Interviews](https://www.quora.com/topic/Data-Science-Interviews/faq)\n\n- [What are the key skills of a data scientist?](https://www.quora.com/What-are-the-key-skills-of-a-data-scientist)\n\n- [The Big List of DS/ML Interview Resources](https://towardsdatascience.com/the-big-list-of-ds-ml-interview-resources-2db4f651bd63)\n\n<a name=\"ai\" />\n\n## Artificial Intelligence\n\n- [Awesome Artificial Intelligence (GitHub Repo)](https://github.com/owainlewis/awesome-artificial-intelligence)\n\n- [UC Berkeley CS188 Intro to AI](http://ai.berkeley.edu/home.html), [Lecture Videos](http://ai.berkeley.edu/lecture_videos.html), [2](https://www.youtube.com/watch?v=W1S-HSakPTM)\n\n- [Programming Community Curated Resources for learning Artificial Intelligence](https://hackr.io/tutorials/learn-artificial-intelligence-ai) \n\n- [MIT 6.034 Artificial Intelligence Lecture Videos](https://www.youtube.com/playlist?list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi), [Complete Course](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/)\n\n- [edX course | Klein & Abbeel](https://courses.edx.org/courses/BerkeleyX/CS188x_1/1T2013/info)\n\n- [Udacity Course | Norvig & Thrun](https://www.udacity.com/course/intro-to-artificial-intelligence--cs271)\n\n- [TED talks on AI](http://www.ted.com/playlists/310/talks_on_artificial_intelligen)\n\n<a name=\"ga\" />\n\n## Genetic Algorithms\n\n- [Genetic Algorithms Wikipedia Page](https://en.wikipedia.org/wiki/Genetic_algorithm)\n\n- [Simple Implementation of Genetic Algorithms in Python (Part 1)](http://outlace.com/miniga.html), [Part 2](http://outlace.com/miniga_addendum.html)\n\n- [Genetic Algorithms vs Artificial Neural Networks](http://stackoverflow.com/questions/1402370/when-to-use-genetic-algorithms-vs-when-to-use-neural-networks)\n\n- [Genetic Algorithms Explained in Plain English](http://www.ai-junkie.com/ga/intro/gat1.html)\n\n- [Genetic Programming](https://en.wikipedia.org/wiki/Genetic_programming)\n\n    - [Genetic Programming in Python (GitHub)](https://github.com/trevorstephens/gplearn)\n    \n    - [Genetic Alogorithms vs Genetic Programming (Quora)](https://www.quora.com/Whats-the-difference-between-Genetic-Algorithms-and-Genetic-Programming), [StackOverflow](http://stackoverflow.com/questions/3819977/what-are-the-differences-between-genetic-algorithms-and-genetic-programming)\n\n<a name=\"stat\" />\n\n## Statistics\n\n- [Stat Trek Website](http://stattrek.com/) - A dedicated website to teach yourselves Statistics\n\n- [Learn Statistics Using Python](https://github.com/rouseguy/intro2stats) - Learn Statistics using an application-centric programming approach\n\n- [Statistics for Hackers | Slides | @jakevdp](https://speakerdeck.com/jakevdp/statistics-for-hackers) - Slides by Jake VanderPlas\n\n- [Online Statistics Book](http://onlinestatbook.com/2/index.html) - An Interactive Multimedia Course for Studying Statistics\n\n- [What is a Sampling Distribution?](http://stattrek.com/sampling/sampling-distribution.aspx)\n\n- Tutorials\n\n    - [AP Statistics Tutorial](http://stattrek.com/tutorials/ap-statistics-tutorial.aspx)\n    \n    - [Statistics and Probability Tutorial](http://stattrek.com/tutorials/statistics-tutorial.aspx)\n    \n    - [Matrix Algebra Tutorial](http://stattrek.com/tutorials/matrix-algebra-tutorial.aspx)\n    \n- [What is an Unbiased Estimator?](https://www.physicsforums.com/threads/what-is-an-unbiased-estimator.547728/)\n\n- [Goodness of Fit Explained](https://en.wikipedia.org/wiki/Goodness_of_fit)\n\n- [What are QQ Plots?](http://onlinestatbook.com/2/advanced_graphs/q-q_plots.html)\n\n- [OpenIntro Statistics](https://www.openintro.org/stat/textbook.php?stat_book=os) - Free PDF textbook\n\n<a name=\"blogs\" />\n\n## Useful Blogs\n\n- [Edwin Chen's Blog](http://blog.echen.me/) - A blog about Math, stats, ML, crowdsourcing, data science\n\n- [The Data School Blog](http://www.dataschool.io/) - Data science for beginners!\n\n- [ML Wave](http://mlwave.com/) - A blog for Learning Machine Learning\n\n- [Andrej Karpathy](http://karpathy.github.io/) - A blog about Deep Learning and Data Science in general\n\n- [Colah's Blog](http://colah.github.io/) - Awesome Neural Networks Blog\n\n- [Alex Minnaar's Blog](http://alexminnaar.com/) - A blog about Machine Learning and Software Engineering\n\n- [Statistically Significant](http://andland.github.io/) - Andrew Landgraf's Data Science Blog\n\n- [Simply Statistics](http://simplystatistics.org/) - A blog by three biostatistics professors\n\n- [Yanir Seroussi's Blog](https://yanirseroussi.com/) - A blog about Data Science and beyond\n\n- [fastML](http://fastml.com/) - Machine learning made easy\n\n- [Trevor Stephens Blog](http://trevorstephens.com/) - Trevor Stephens Personal Page\n\n- [no free hunch | kaggle](http://blog.kaggle.com/) - The Kaggle Blog about all things Data Science\n\n- [A Quantitative Journey | outlace](http://outlace.com/) -  learning quantitative applications\n\n- [r4stats](http://r4stats.com/) - analyze the world of data science, and to help people learn to use R\n\n- [Variance Explained](http://varianceexplained.org/) - David Robinson's Blog\n\n- [AI Junkie](http://www.ai-junkie.com/) - a blog about Artificial Intellingence\n\n- [Deep Learning Blog by Tim Dettmers](http://timdettmers.com/) - Making deep learning accessible\n\n- [J Alammar's Blog](http://jalammar.github.io/)- Blog posts about Machine Learning and Neural Nets\n\n- [Adam Geitgey](https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471#.f7vwrtfne) - Easiest Introduction to machine learning\n\n- [Ethen's Notebook Collection](https://github.com/ethen8181/machine-learning) - Continuously updated machine learning documentations (mainly in Python3). Contents include educational implementation of machine learning algorithms from scratch and open-source library usage\n\n<a name=\"quora\" />\n\n## Resources on Quora\n\n- [Most Viewed Machine Learning writers](https://www.quora.com/topic/Machine-Learning/writers)\n\n- [Data Science Topic on Quora](https://www.quora.com/Data-Science)\n\n- [William Chen's Answers](https://www.quora.com/William-Chen-6/answers)\n\n- [Michael Hochster's Answers](https://www.quora.com/Michael-Hochster/answers)\n\n- [Ricardo Vladimiro's Answers](https://www.quora.com/Ricardo-Vladimiro-1/answers)\n\n- [Storytelling with Statistics](https://datastories.quora.com/)\n\n- [Data Science FAQs on Quora](https://www.quora.com/topic/Data-Science/faq)\n\n- [Machine Learning FAQs on Quora](https://www.quora.com/topic/Machine-Learning/faq)\n\n<a name=\"kaggle\" />\n\n## Kaggle Competitions WriteUp\n\n- [How to almost win Kaggle Competitions](https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/)\n\n- [Convolution Neural Networks for EEG detection](http://blog.kaggle.com/2015/10/05/grasp-and-lift-eeg-detection-winners-interview-3rd-place-team-hedj/)\n\n- [Facebook Recruiting III Explained](http://alexminnaar.com/tag/kaggle-competitions.html)\n\n- [Predicting CTR with Online ML](http://mlwave.com/predicting-click-through-rates-with-online-machine-learning/)\n\n- [How to Rank 10% in Your First Kaggle Competition](https://dnc1994.com/2016/05/rank-10-percent-in-first-kaggle-competition-en/)\n\n<a name=\"cs\" />\n\n## Cheat Sheets\n\n- [Probability Cheat Sheet](http://static1.squarespace.com/static/54bf3241e4b0f0d81bf7ff36/t/55e9494fe4b011aed10e48e5/1441352015658/probability_cheatsheet.pdf),\n[Source](http://www.wzchen.com/probability-cheatsheet/)\n\n- [Machine Learning Cheat Sheet](https://github.com/soulmachine/machine-learning-cheat-sheet)\n\n- [ML Compiled](https://ml-compiled.readthedocs.io/en/latest/)\n\n<a name=\"classification\" />\n\n## Classification\n\n- [Does Balancing Classes Improve Classifier Performance?](http://www.win-vector.com/blog/2015/02/does-balancing-classes-improve-classifier-performance/)\n\n- [What is Deviance?](http://stats.stackexchange.com/questions/6581/what-is-deviance-specifically-in-cart-rpart)\n\n- [When to choose which machine learning classifier?](http://stackoverflow.com/questions/2595176/when-to-choose-which-machine-learning-classifier)\n\n- [What are the advantages of different classification algorithms?](https://www.quora.com/What-are-the-advantages-of-different-classification-algorithms)\n\n- [ROC and AUC Explained](http://www.dataschool.io/roc-curves-and-auc-explained/) ([related video](https://youtu.be/OAl6eAyP-yo))\n\n- [An introduction to ROC analysis](https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf)\n\n- [Simple guide to confusion matrix terminology](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)\n\n\n<a name=\"linear\" />\n\n## Linear Regression\n\n- [General](#general-)\n\n    - [Assumptions of Linear Regression](http://pareonline.net/getvn.asp?n=2&v=8), [Stack Exchange](http://stats.stackexchange.com/questions/16381/what-is-a-complete-list-of-the-usual-assumptions-for-linear-regression)\n    \n    - [Linear Regression Comprehensive Resource](http://people.duke.edu/~rnau/regintro.htm)\n    \n    - [Applying and Interpreting Linear Regression](http://www.dataschool.io/applying-and-interpreting-linear-regression/)\n    \n    - [What does having constant variance in a linear regression model mean?](http://stats.stackexchange.com/questions/52089/what-does-having-constant-variance-in-a-linear-regression-model-mean/52107?stw=2#52107)\n    \n    - [Difference between linear regression on y with x and x with y](http://stats.stackexchange.com/questions/22718/what-is-the-difference-between-linear-regression-on-y-with-x-and-x-with-y?lq=1)\n    \n    - [Is linear regression valid when the dependant variable is not normally distributed?](https://www.researchgate.net/post/Is_linear_regression_valid_when_the_outcome_dependant_variable_not_normally_distributed)\n- Multicollinearity and VIF\n\n    - [Dummy Variable Trap | Multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)\n    \n    - [Dealing with multicollinearity using VIFs](https://jonlefcheck.net/2012/12/28/dealing-with-multicollinearity-using-variance-inflation-factors/)\n\n- [Residual Analysis](#residuals-)\n\n    - [Interpreting plot.lm() in R](http://stats.stackexchange.com/questions/58141/interpreting-plot-lm)\n    \n    - [How to interpret a QQ plot?](http://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot?lq=1)\n    \n    - [Interpreting Residuals vs Fitted Plot](http://stats.stackexchange.com/questions/76226/interpreting-the-residuals-vs-fitted-values-plot-for-verifying-the-assumptions)\n\n- [Outliers](#outliers-)\n\n    - [How should outliers be dealt with?](http://stats.stackexchange.com/questions/175/how-should-outliers-be-dealt-with-in-linear-regression-analysis)\n\n- [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization)\n    - [Regularization and Variable Selection via the\nElastic Net](https://web.stanford.edu/~hastie/Papers/elasticnet.pdf)\n\n<a name=\"logistic\" />\n\n## Logistic Regression\n\n- [Logistic Regression Wiki](https://en.wikipedia.org/wiki/Logistic_regression)\n\n- [Geometric Intuition of Logistic Regression](http://florianhartl.com/logistic-regression-geometric-intuition.html)\n\n- [Obtaining predicted categories (choosing threshold)](http://stats.stackexchange.com/questions/25389/obtaining-predicted-values-y-1-or-0-from-a-logistic-regression-model-fit)\n\n- [Residuals in logistic regression](http://stats.stackexchange.com/questions/1432/what-do-the-residuals-in-a-logistic-regression-mean)\n\n- [Difference between logit and probit models](http://stats.stackexchange.com/questions/20523/difference-between-logit-and-probit-models#30909), [Logistic Regression Wiki](https://en.wikipedia.org/wiki/Logistic_regression), [Probit Model Wiki](https://en.wikipedia.org/wiki/Probit_model)\n\n- [Pseudo R2 for Logistic Regression](http://stats.stackexchange.com/questions/3559/which-pseudo-r2-measure-is-the-one-to-report-for-logistic-regression-cox-s), [How to calculate](http://stats.stackexchange.com/questions/8511/how-to-calculate-pseudo-r2-from-rs-logistic-regression), [Other Details](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/Psuedo_RSquareds.htm)\n\n- [Guide to an in-depth understanding of logistic regression](http://www.dataschool.io/guide-to-logistic-regression/)\n\n<a name=\"validation\" />\n\n## Model Validation using Resampling\n\n- [Resampling Explained](https://en.wikipedia.org/wiki/Resampling_(statistics))\n\n- [Partioning data set in R](http://stackoverflow.com/questions/13536537/partitioning-data-set-in-r-based-on-multiple-classes-of-observations)\n\n- [Implementing hold-out Validaion in R](http://stackoverflow.com/questions/22972854/how-to-implement-a-hold-out-validation-in-r), [2](http://www.gettinggeneticsdone.com/2011/02/split-data-frame-into-testing-and.html)\n\n<a name=\"cross\" />\n\n- [Cross Validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))\n    - [How to use cross-validation in predictive modeling](http://stuartlacy.co.uk/2016/02/04/how-to-correctly-use-cross-validation-in-predictive-modelling/)\n    - [Training with Full dataset after CV?](http://stats.stackexchange.com/questions/11602/training-with-the-full-dataset-after-cross-validation)\n    \n    - [Which CV method is best?](http://stats.stackexchange.com/questions/103459/how-do-i-know-which-method-of-cross-validation-is-best)\n    \n    - [Variance Estimates in k-fold CV](http://stats.stackexchange.com/questions/31190/variance-estimates-in-k-fold-cross-validation)\n    \n    - [Is CV a subsitute for Validation Set?](http://stats.stackexchange.com/questions/18856/is-cross-validation-a-proper-substitute-for-validation-set)\n    \n    - [Choice of k in k-fold CV](http://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation)\n    \n    - [CV for ensemble learning](http://stats.stackexchange.com/questions/102631/k-fold-cross-validation-of-ensemble-learning)\n    \n    - [k-fold CV in R](http://stackoverflow.com/questions/22909197/creating-folds-for-k-fold-cv-in-r-using-caret)\n    \n    - [Good Resources](http://www.chioka.in/tag/cross-validation/)\n    \n    - Overfitting and Cross Validation\n    \n        - [Preventing Overfitting the Cross Validation Data | Andrew Ng](http://ai.stanford.edu/~ang/papers/cv-final.pdf)\n        \n        - [Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation](http://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf)\n\n        - [CV for detecting and preventing Overfitting](http://www.autonlab.org/tutorials/overfit10.pdf)\n        \n        - [How does CV overcome the Overfitting Problem](http://stats.stackexchange.com/questions/9053/how-does-cross-validation-overcome-the-overfitting-problem)\n\n\n<a name=\"boot\" />\n\n- [Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))\n\n    - [Why Bootstrapping Works?](http://stats.stackexchange.com/questions/26088/explaining-to-laypeople-why-bootstrapping-works)\n    \n    - [Good Animation](https://www.stat.auckland.ac.nz/~wild/BootAnim/)\n    \n    - [Example of Bootstapping](http://statistics.about.com/od/Applications/a/Example-Of-Bootstrapping.htm)\n    \n    - [Understanding Bootstapping for Validation and Model Selection](http://stats.stackexchange.com/questions/14516/understanding-bootstrapping-for-validation-and-model-selection?rq=1)\n    \n    - [Cross Validation vs Bootstrap to estimate prediction error](http://stats.stackexchange.com/questions/18348/differences-between-cross-validation-and-bootstrapping-to-estimate-the-predictio), [Cross-validation vs .632 bootstrapping to evaluate classification performance](http://stats.stackexchange.com/questions/71184/cross-validation-or-bootstrapping-to-evaluate-classification-performance)\n\n\n<a name=\"deep\" />\n\n## Deep Learning\n\n- [fast.ai - Practical Deep Learning For Coders](http://course.fast.ai/)\n\n- [fast.ai - Cutting Edge Deep Learning For Coders](http://course.fast.ai/part2.html)\n\n- [A curated list of awesome Deep Learning tutorials, projects and communities](https://github.com/ChristosChristofidis/awesome-deep-learning)\n\n- **[Deep Learning Papers Reading Roadmap](https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap/blob/master/README.md)**\n\n- [Lots of Deep Learning Resources](http://deeplearning4j.org/documentation.html)\n\n- [Interesting Deep Learning and NLP Projects (Stanford)](http://cs224d.stanford.edu/reports.html), [Website](http://cs224d.stanford.edu/)\n\n- [Core Concepts of Deep Learning](https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/)\n\n- [Understanding Natural Language with Deep Neural Networks Using Torch](https://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)\n\n- [Stanford Deep Learning Tutorial](http://ufldl.stanford.edu/tutorial/)\n\n- [Deep Learning FAQs on Quora](https://www.quora.com/topic/Deep-Learning/faq)\n\n- [Google+ Deep Learning Page](https://plus.google.com/communities/112866381580457264725)\n\n- [Recent Reddit AMAs related to Deep Learning](http://deeplearning.net/2014/11/22/recent-reddit-amas-about-deep-learning/), [Another AMA](https://www.reddit.com/r/IAmA/comments/3mdk9v/we_are_google_researchers_working_on_deep/)\n\n- [Where to Learn Deep Learning?](http://www.kdnuggets.com/2014/05/learn-deep-learning-courses-tutorials-overviews.html)\n\n- [Deep Learning nvidia concepts](http://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/)\n\n- [Introduction to Deep Learning Using Python (GitHub)](https://github.com/rouseguy/intro2deeplearning), [Good Introduction Slides](https://speakerdeck.com/bargava/introduction-to-deep-learning)\n\n- [Video Lectures Oxford 2015](https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu), [Video Lectures Summer School Montreal](http://videolectures.net/deeplearning2015_montreal/)\n\n- [Deep Learning Software List](http://deeplearning.net/software_links/)\n\n- [Hacker's guide to Neural Nets](http://karpathy.github.io/neuralnets/)\n\n- [Top arxiv Deep Learning Papers explained](http://www.kdnuggets.com/2015/10/top-arxiv-deep-learning-papers-explained.html)\n\n- [Geoff Hinton Youtube Vidoes on Deep Learning](https://www.youtube.com/watch?v=IcOMKXAw5VA)\n\n- [Awesome Deep Learning Reading List](http://deeplearning.net/reading-list/)\n\n- [Deep Learning Comprehensive Website](http://deeplearning.net/), [Software](http://deeplearning.net/software_links/)\n\n- [deeplearning Tutorials](http://deeplearning4j.org/)\n\n- [AWESOME! Deep Learning Tutorial](https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks)\n\n- [Deep Learning Basics](http://alexminnaar.com/deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html)\n\n- [Intuition Behind Backpropagation](https://medium.com/spidernitt/breaking-down-neural-networks-an-intuitive-approach-to-backpropagation-3b2ff958794c)\n\n- [Stanford Tutorials](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/)\n\n- [Train, Validation & Test in Artificial Neural Networks](http://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-networ)\n\n- [Artificial Neural Networks Tutorials](http://stackoverflow.com/questions/478947/what-are-some-good-resources-for-learning-about-artificial-neural-networks)\n\n- [Neural Networks FAQs on Stack Overflow](http://stackoverflow.com/questions/tagged/neural-network?sort=votes&pageSize=50)\n\n- [Deep Learning Tutorials on deeplearning.net](http://deeplearning.net/tutorial/index.html)\n\n- [Neural Networks and Deep Learning Online Book](http://neuralnetworksanddeeplearning.com/)\n\n- Neural Machine Translation\n\n    - **[Machine Translation Reading List](https://github.com/THUNLP-MT/MT-Reading-List#machine-translation-reading-list)**\n\n    - [Introduction to Neural Machine Translation with GPUs (part 1)](https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-with-gpus/), [Part 2](https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/), [Part 3](https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/)\n    \n    - [Deep Speech: Accurate Speech Recognition with GPU-Accelerated Deep Learning](https://devblogs.nvidia.com/parallelforall/deep-speech-accurate-speech-recognition-gpu-accelerated-deep-learning/)\n\n<a name=\"frame\" />\n\n- Deep Learning Frameworks\n\n    - [Torch vs. Theano](http://fastml.com/torch-vs-theano/)\n    \n    - [dl4j vs. torch7 vs. theano](http://deeplearning4j.org/compare-dl4j-torch7-pylearn.html)\n    \n    - [Deep Learning Libraries by Language](http://www.teglor.com/b/deep-learning-libraries-language-cm569/)\n    \n\n    - [Theano](https://en.wikipedia.org/wiki/Theano_(software))\n    \n        - [Website](http://deeplearning.net/software/theano/)\n        \n        - [Theano Introduction](http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/)\n        \n        - [Theano Tutorial](http://outlace.com/Beginner-Tutorial-Theano/)\n        \n        - [Good Theano Tutorial](http://deeplearning.net/software/theano/tutorial/)\n        \n        - [Logistic Regression using Theano for classifying digits](http://deeplearning.net/tutorial/logreg.html#logreg)\n        \n        - [MLP using Theano](http://deeplearning.net/tutorial/mlp.html#mlp)\n        \n        - [CNN using Theano](http://deeplearning.net/tutorial/lenet.html#lenet)\n        \n        - [RNNs using Theano](http://deeplearning.net/tutorial/rnnslu.html#rnnslu)\n        \n        - [LSTM for Sentiment Analysis in Theano](http://deeplearning.net/tutorial/lstm.html#lstm)\n        \n        - [RBM using Theano](http://deeplearning.net/tutorial/rbm.html#rbm)\n        \n        - [DBNs using Theano](http://deeplearning.net/tutorial/DBN.html#dbn)\n        \n        - [All Codes](https://github.com/lisa-lab/DeepLearningTutorials)\n        \n        - [Deep Learning Implementation Tutorials - Keras and Lasagne](https://github.com/vict0rsch/deep_learning/)\n\n    - [Torch](http://torch.ch/)\n    \n        - [Torch ML Tutorial](http://code.madbits.com/wiki/doku.php), [Code](https://github.com/torch/tutorials)\n        \n        - [Intro to Torch](http://ml.informatik.uni-freiburg.de/_media/teaching/ws1415/presentation_dl_lect3.pdf)\n        \n        - [Learning Torch GitHub Repo](https://github.com/chetannaik/learning_torch)\n        \n        - [Awesome-Torch (Repository on GitHub)](https://github.com/carpedm20/awesome-torch)\n        \n        - [Machine Learning using Torch Oxford Univ](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/), [Code](https://github.com/oxford-cs-ml-2015)\n        \n        - [Torch Internals Overview](https://apaszke.github.io/torch-internals.html)\n        \n        - [Torch Cheatsheet](https://github.com/torch/torch7/wiki/Cheatsheet)\n        \n        - [Understanding Natural Language with Deep Neural Networks Using Torch](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)\n\n    - Caffe\n        - [Deep Learning for Computer Vision with Caffe and cuDNN](https://devblogs.nvidia.com/parallelforall/deep-learning-computer-vision-caffe-cudnn/)\n\n    - TensorFlow\n        - [Website](http://tensorflow.org/)\n        \n        - [TensorFlow Examples for Beginners](https://github.com/aymericdamien/TensorFlow-Examples)\n        \n        - [Stanford Tensorflow for Deep Learning Research Course](https://web.stanford.edu/class/cs20si/syllabus.html)\n        \n            - [GitHub Repo](https://github.com/chiphuyen/tf-stanford-tutorials)\n            \n        - [Simplified Scikit-learn Style Interface to TensorFlow](https://github.com/tensorflow/skflow)\n        \n        - [Learning TensorFlow GitHub Repo](https://github.com/chetannaik/learning_tensorflow)\n        \n        - [Benchmark TensorFlow GitHub](https://github.com/soumith/convnet-benchmarks/issues/66)\n        \n        - [Awesome TensorFlow List](https://github.com/jtoy/awesome-tensorflow)\n        \n        - [TensorFlow Book](https://github.com/BinRoot/TensorFlow-Book)\n        \n        - [Android TensorFlow Machine Learning Example](https://blog.mindorks.com/android-tensorflow-machine-learning-example-ff0e9b2654cc)\n        \n            - [GitHub Repo](https://github.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample)\n        - [Creating Custom Model For Android Using TensorFlow](https://blog.mindorks.com/creating-custom-model-for-android-using-tensorflow-3f963d270bfb)\n            - [GitHub Repo](https://github.com/MindorksOpenSource/AndroidTensorFlowMNISTExample)            \n\n<a name=\"feed\" />\n\n- Feed Forward Networks\n\n    - [A Quick Introduction to Neural Networks](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/)\n    \n    - [Implementing a Neural Network from scratch](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/), [Code](https://github.com/dennybritz/nn-from-scratch)\n    \n    - [Speeding up your Neural Network with Theano and the gpu](http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/), [Code](https://github.com/dennybritz/nn-theano)\n    \n    - [Basic ANN Theory](https://takinginitiative.wordpress.com/2008/04/03/basic-neural-network-tutorial-theory/)\n    \n    - [Role of Bias in Neural Networks](http://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks)\n    \n    - [Choosing number of hidden layers and nodes](http://stackoverflow.com/questions/3345079/estimating-the-number-of-neurons-and-number-of-layers-of-an-artificial-neural-ne),[2](http://stackoverflow.com/questions/10565868/multi-layer-perceptron-mlp-architecture-criteria-for-choosing-number-of-hidde?lq=1),[3](http://stackoverflow.com/questions/9436209/how-to-choose-number-of-hidden-layers-and-nodes-in-neural-network/2#)\n    \n    - [Backpropagation in Matrix Form](http://sudeepraja.github.io/Neural/)\n    \n    - [ANN implemented in C++ | AI Junkie](http://www.ai-junkie.com/ann/evolved/nnt6.html)\n    \n    - [Simple Implementation](http://stackoverflow.com/questions/15395835/simple-multi-layer-neural-network-implementation)\n    \n    - [NN for Beginners](http://www.codeproject.com/Articles/16419/AI-Neural-Network-for-beginners-Part-of)\n    \n    - [Regression and Classification with NNs (Slides)](http://www.autonlab.org/tutorials/neural13.pdf)\n    \n    - [Another Intro](http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html)\n\n<a name=\"rnn\" />\n\n- Recurrent and LSTM Networks\n    - [awesome-rnn: list of resources (GitHub Repo)](https://github.com/kjw0612/awesome-rnn)\n    \n    - [Recurrent Neural Net Tutorial Part 1](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/), [Part 2](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/), [Part 3](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/), [Code](https://github.com/dennybritz/rnn-tutorial-rnnlm/)\n    \n    - [NLP RNN Representations](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\n    \n    - [The Unreasonable effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), [Torch Code](https://github.com/karpathy/char-rnn), [Python Code](https://gist.github.com/karpathy/d4dee566867f8291f086)\n    \n    - [Intro to RNN](http://deeplearning4j.org/recurrentnetwork.html), [LSTM](http://deeplearning4j.org/lstm.html)\n    \n    - [An application of RNN](http://hackaday.com/2015/10/15/73-computer-scientists-created-a-neural-net-and-you-wont-believe-what-happened-next/)\n    \n    - [Optimizing RNN Performance](http://svail.github.io/)\n    \n    - [Simple RNN](http://outlace.com/Simple-Recurrent-Neural-Network/)\n    \n    - [Auto-Generating Clickbait with RNN](https://larseidnes.com/2015/10/13/auto-generating-clickbait-with-recurrent-neural-networks/)\n    \n    - [Sequence Learning using RNN (Slides)](http://www.slideshare.net/indicods/general-sequence-learning-with-recurrent-neural-networks-for-next-ml)\n    \n    - [Machine Translation using RNN (Paper)](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf)\n    \n    - [Music generation using RNNs (Keras)](https://github.com/MattVitelli/GRUV)\n    \n    - [Using RNN to create on-the-fly dialogue (Keras)](http://neuralniche.com/post/tutorial/)\n    \n    - Long Short Term Memory (LSTM)\n    \n        - [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n        \n        - [LSTM explained](https://apaszke.github.io/lstm-explained.html)\n        \n        - [Beginner\u2019s Guide to LSTM](http://deeplearning4j.org/lstm.html)\n        \n        - [Implementing LSTM from scratch](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/), [Python/Theano code](https://github.com/dennybritz/rnn-tutorial-gru-lstm)\n        \n        - [Torch Code for character-level language models using LSTM](https://github.com/karpathy/char-rnn)\n        \n        - [LSTM for Kaggle EEG Detection competition (Torch Code)](https://github.com/apaszke/kaggle-grasp-and-lift)\n        \n        - [LSTM for Sentiment Analysis in Theano](http://deeplearning.net/tutorial/lstm.html#lstm)\n        \n        - [Deep Learning for Visual Q&A | LSTM | CNN](http://avisingh599.github.io/deeplearning/visual-qa/), [Code](https://github.com/avisingh599/visual-qa)\n        \n        - [Computer Responds to email using LSTM | Google](http://googleresearch.blogspot.in/2015/11/computer-respond-to-this-email.html)\n        \n        - [LSTM dramatically improves Google Voice Search](http://googleresearch.blogspot.ch/2015/09/google-voice-search-faster-and-more.html), [Another Article](http://deeplearning.net/2015/09/30/long-short-term-memory-dramatically-improves-google-voice-etc-now-available-to-a-billion-users/)\n        \n        - [Understanding Natural Language with LSTM Using Torch](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)\n        \n        - [Torch code for Visual Question Answering using a CNN+LSTM model](https://github.com/abhshkdz/neural-vqa)\n        \n        - [LSTM for Human Activity Recognition](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/)\n        \n    - Gated Recurrent Units (GRU)\n    \n        - [LSTM vs GRU](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)\n    \n    - [Time series forecasting with Sequence-to-Sequence (seq2seq) rnn models](https://github.com/guillaume-chevalier/seq2seq-signal-prediction)\n\n\n<a name=\"rnn2\" />\n\n- [Recursive Neural Network (not Recurrent)](https://en.wikipedia.org/wiki/Recursive_neural_network)\n\n    - [Recursive Neural Tensor Network (RNTN)](http://deeplearning4j.org/recursiveneuraltensornetwork.html)\n    \n    - [word2vec, DBN, RNTN for Sentiment Analysis ](http://deeplearning4j.org/zh-sentiment_analysis_word2vec.html)\n\n<a name=\"rbm\" />\n\n- Restricted Boltzmann Machine\n\n    - [Beginner's Guide about RBMs](http://deeplearning4j.org/restrictedboltzmannmachine.html)\n    \n    - [Another Good Tutorial](http://deeplearning.net/tutorial/rbm.html)\n    \n    - [Introduction to RBMs](http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/)\n    \n    - [Hinton's Guide to Training RBMs](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)\n    \n    - [RBMs in R](https://github.com/zachmayer/rbm)\n    \n    - [Deep Belief Networks Tutorial](http://deeplearning4j.org/deepbeliefnetwork.html)\n    \n    - [word2vec, DBN, RNTN for Sentiment Analysis ](http://deeplearning4j.org/zh-sentiment_analysis_word2vec.html)\n\n<a name=\"auto\" />\n\n- Autoencoders: Unsupervised (applies BackProp after setting target = input)\n\n    - [Andrew Ng Sparse Autoencoders pdf](https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf)\n    \n    - [Deep Autoencoders Tutorial](http://deeplearning4j.org/deepautoencoder.html)\n    \n    - [Denoising Autoencoders](http://deeplearning.net/tutorial/dA.html), [Theano Code](http://deeplearning.net/tutorial/code/dA.py)\n    \n    - [Stacked Denoising Autoencoders](http://deeplearning.net/tutorial/SdA.html#sda)\n\n\n<a name=\"cnn\" />\n\n- Convolutional Neural Networks\n\n    - [An Intuitive Explanation of Convolutional Neural Networks](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n    \n    - [Awesome Deep Vision: List of Resources (GitHub)](https://github.com/kjw0612/awesome-deep-vision)\n    \n    - [Intro to CNNs](http://deeplearning4j.org/convolutionalnets.html)\n    \n    - [Understanding CNN for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n    \n    - [Stanford Notes](http://vision.stanford.edu/teaching/cs231n/), [Codes](http://cs231n.github.io/), [GitHub](https://github.com/cs231n/cs231n.github.io)\n    \n    - [JavaScript Library (Browser Based) for CNNs](http://cs.stanford.edu/people/karpathy/convnetjs/)\n    \n    - [Using CNNs to detect facial keypoints](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)\n    \n    - [Deep learning to classify business photos at Yelp](http://engineeringblog.yelp.com/2015/10/how-we-use-deep-learning-to-classify-business-photos-at-yelp.html)\n    \n    - [Interview with Yann LeCun | Kaggle](http://blog.kaggle.com/2014/12/22/convolutional-nets-and-cifar-10-an-interview-with-yan-lecun/)\n    \n    - [Visualising and Understanding CNNs](https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf)\n\n<a name=\"nrl\" />\n\n- Network Representation Learning\n\n    - [Awesome Graph Embedding](https://github.com/benedekrozemberczki/awesome-graph-embedding)\n    \n    - [Awesome Network Embedding](https://github.com/chihming/awesome-network-embedding)\n    \n    - [Network Representation Learning Papers](https://github.com/thunlp)\n    \n    - [Knowledge Representation Learning Papers](https://github.com/thunlp/KRLPapers)\n    \n    - [Graph Based Deep Learning Literature](https://github.com/naganandy/graph-based-deep-learning-literature)\n\n<a name=\"nlp\" />\n\n## Natural Language Processing\n\n- [A curated list of speech and natural language processing resources](https://github.com/edobashira/speech-language-processing)\n\n- [Understanding Natural Language with Deep Neural Networks Using Torch](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)\n\n- [tf-idf explained](http://michaelerasm.us/post/tf-idf-in-10-minutes/)\n\n- [Interesting Deep Learning NLP Projects Stanford](http://cs224d.stanford.edu/reports.html), [Website](http://cs224d.stanford.edu/)\n\n- [The Stanford NLP Group](https://nlp.stanford.edu/)\n\n- [NLP from Scratch | Google Paper](https://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/35671.pdf)\n\n- [Graph Based Semi Supervised Learning for NLP](http://graph-ssl.wdfiles.com/local--files/blog%3A_start/graph_ssl_acl12_tutorial_slides_final.pdf)\n\n- [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model)\n\n    - [Classification text with Bag of Words](http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/)\n    \n<a name=\"topic\" />\n\n- Topic Modeling\n    - [Topic Modeling Wikipedia](https://en.wikipedia.org/wiki/Topic_model) \n    - [**Probabilistic Topic Models Princeton PDF**](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf)\n\n    - [LDA Wikipedia](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation), [LSA Wikipedia](https://en.wikipedia.org/wiki/Latent_semantic_analysis), [Probabilistic LSA Wikipedia](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis)\n    \n    - [What is a good explanation of Latent Dirichlet Allocation (LDA)?](https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation)\n    \n    - [**Introduction to LDA**](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/), [Another good explanation](http://confusedlanguagetech.blogspot.in/2012/07/jordan-boyd-graber-and-philip-resnik.html)\n    \n    - [The LDA Buffet - Intuitive Explanation](http://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/)\n    \n    - [Your Guide to Latent Dirichlet Allocation (LDA)](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d)\n    \n    - [Difference between LSI and LDA](https://www.quora.com/Whats-the-difference-between-Latent-Semantic-Indexing-LSI-and-Latent-Dirichlet-Allocation-LDA)\n    \n    - [Original LDA Paper](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf)\n    \n    - [alpha and beta in LDA](http://datascience.stackexchange.com/questions/199/what-does-the-alpha-and-beta-hyperparameters-contribute-to-in-latent-dirichlet-a)\n    \n    - [Intuitive explanation of the Dirichlet distribution](https://www.quora.com/What-is-an-intuitive-explanation-of-the-Dirichlet-distribution)\n    - [topicmodels: An R Package for Fitting Topic Models](https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf)\n\n    - [Topic modeling made just simple enough](https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/)\n    \n    - [Online LDA](http://alexminnaar.com/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html), [Online LDA with Spark](http://alexminnaar.com/distributed-online-latent-dirichlet-allocation-with-apache-spark.html)\n    \n    - [LDA in Scala](http://alexminnaar.com/latent-dirichlet-allocation-in-scala-part-i-the-theory.html), [Part 2](http://alexminnaar.com/latent-dirichlet-allocation-in-scala-part-ii-the-code.html)\n    \n    - [Segmentation of Twitter Timelines via Topic Modeling](https://alexisperrier.com/nlp/2015/09/16/segmentation_twitter_timelines_lda_vs_lsa.html)\n    \n    - [Topic Modeling of Twitter Followers](http://alexperrier.github.io/jekyll/update/2015/09/04/topic-modeling-of-twitter-followers.html)\n\n    - [Multilingual Latent Dirichlet Allocation (LDA)](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA). ([Tutorial here](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA/blob/master/Multilingual-LDA-Pipeline-Tutorial.ipynb))\n\n    - [Deep Belief Nets for Topic Modeling](https://github.com/larsmaaloee/deep-belief-nets-for-topic-modeling)\n    - [Gaussian LDA for Topic Models with Word Embeddings](http://www.cs.cmu.edu/~rajarshd/papers/acl2015.pdf)\n    - Python\n        - [Series of lecture notes for probabilistic topic models written in ipython notebook](https://github.com/arongdari/topic-model-lecture-note)\n        - [Implementation of various topic models in Python](https://github.com/arongdari/python-topic-model)\n           \n<a name=\"word2vec\" />\n\n- word2vec\n\n    - [Google word2vec](https://code.google.com/archive/p/word2vec)\n    \n    - [Bag of Words Model Wiki](https://en.wikipedia.org/wiki/Bag-of-words_model)\n    \n    - [word2vec Tutorial](https://rare-technologies.com/word2vec-tutorial/)\n    \n    - [A closer look at Skip Gram Modeling](http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf)\n    \n    - [Skip Gram Model Tutorial](http://alexminnaar.com/word2vec-tutorial-part-i-the-skip-gram-model.html), [CBoW Model](http://alexminnaar.com/word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html)\n    \n    - [Word Vectors Kaggle Tutorial Python](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors), [Part 2](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors)\n    \n    - [Making sense of word2vec](http://rare-technologies.com/making-sense-of-word2vec/)\n    \n    - [word2vec explained on deeplearning4j](http://deeplearning4j.org/word2vec.html)\n    \n    - [Quora word2vec](https://www.quora.com/How-does-word2vec-work)\n    \n    - [Other Quora Resources](https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures-in-laymans-terms), [2](https://www.quora.com/What-is-the-difference-between-the-Bag-of-Words-model-and-the-Continuous-Bag-of-Words-model), [3](https://www.quora.com/Is-skip-gram-negative-sampling-better-than-CBOW-NS-for-word2vec-If-so-why)\n    \n    - [word2vec, DBN, RNTN for Sentiment Analysis ](http://deeplearning4j.org/zh-sentiment_analysis_word2vec.html)\n\n- Text Clustering\n\n    - [How string clustering works](http://stackoverflow.com/questions/8196371/how-clustering-works-especially-string-clustering)\n    \n    - [Levenshtein distance for measuring the difference between two sequences](https://en.wikipedia.org/wiki/Levenshtein_distance)\n    \n    - [Text clustering with Levenshtein distances](http://stackoverflow.com/questions/21511801/text-clustering-with-levenshtein-distances)\n\n- Text Classification\n\n    - [Classification Text with Bag of Words](http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/)\n\n- Named Entity Recognitation \n    \n     - [Stanford Named Entity Recognizer (NER)](https://nlp.stanford.edu/software/CRF-NER.shtml)\n\n     - [Named Entity Recognition: Applications and Use Cases- Towards Data Science](https://towardsdatascience.com/named-entity-recognition-applications-and-use-cases-acdbf57d595e)\n\t\n- [Language learning with NLP and reinforcement learning](http://blog.dennybritz.com/2015/09/11/reimagining-language-learning-with-nlp-and-reinforcement-learning/)\n\n- [Kaggle Tutorial Bag of Words and Word vectors](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words), [Part 2](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors), [Part 3](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors)\n\n- [What would Shakespeare say (NLP Tutorial)](https://gigadom.wordpress.com/2015/10/02/natural-language-processing-what-would-shakespeare-say/)\n\n- [A closer look at Skip Gram Modeling](http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf)\n\n<a name=\"vision\" />\n\n## Computer Vision\n- [Awesome computer vision (github)](https://github.com/jbhuang0604/awesome-computer-vision)\n\n- [Awesome deep vision (github)](https://github.com/kjw0612/awesome-deep-vision)\n\n\n<a name=\"svm\" />\n\n## Support Vector Machine\n\n- [Highest Voted Questions about SVMs on Cross Validated](http://stats.stackexchange.com/questions/tagged/svm)\n\n- [Help me Understand SVMs!](http://stats.stackexchange.com/questions/3947/help-me-understand-support-vector-machines)\n\n- [SVM in Layman's terms](https://www.quora.com/What-does-support-vector-machine-SVM-mean-in-laymans-terms)\n\n- [How does SVM Work | Comparisons](http://stats.stackexchange.com/questions/23391/how-does-a-support-vector-machine-svm-work)\n\n- [A tutorial on SVMs](http://alex.smola.org/papers/2003/SmoSch03b.pdf)\n\n- [Practical Guide to SVC](http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf), [Slides](http://www.csie.ntu.edu.tw/~cjlin/talks/freiburg.pdf)\n\n- [Introductory Overview of SVMs](http://www.statsoft.com/Textbook/Support-Vector-Machines)\n\n- Comparisons\n\n    - [SVMs > ANNs](http://stackoverflow.com/questions/6699222/support-vector-machines-better-than-artificial-neural-networks-in-which-learn?rq=1), [ANNs > SVMs](http://stackoverflow.com/questions/11632516/what-are-advantages-of-artificial-neural-networks-over-support-vector-machines), [Another Comparison](http://www.svms.org/anns.html)\n    \n    - [Trees > SVMs](http://stats.stackexchange.com/questions/57438/why-is-svm-not-so-good-as-decision-tree-on-the-same-data)\n    \n    - [Kernel Logistic Regression vs SVM](http://stats.stackexchange.com/questions/43996/kernel-logistic-regression-vs-svm)\n    \n    - [Logistic Regression vs SVM](http://stats.stackexchange.com/questions/58684/regularized-logistic-regression-and-support-vector-machine), [2](http://stats.stackexchange.com/questions/95340/svm-v-s-logistic-regression), [3](https://www.quora.com/Support-Vector-Machines/What-is-the-difference-between-Linear-SVMs-and-Logistic-Regression)\n    \n- [Optimization Algorithms in Support Vector Machines](http://pages.cs.wisc.edu/~swright/talks/sjw-complearning.pdf)\n\n- [Variable Importance from SVM](http://stats.stackexchange.com/questions/2179/variable-importance-from-svm)\n\n- Software\n\n    - [LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)\n    \n    - [Intro to SVM in R](http://cbio.ensmp.fr/~jvert/svn/tutorials/practical/svmbasic/svmbasic_notes.pdf)\n    \n- Kernels\n    - [What are Kernels in ML and SVM?](https://www.quora.com/What-are-Kernels-in-Machine-Learning-and-SVM)\n    \n    - [Intuition Behind Gaussian Kernel in SVMs?](https://www.quora.com/Support-Vector-Machines/What-is-the-intuition-behind-Gaussian-kernel-in-SVM)\n    \n- Probabilities post SVM\n\n    - [Platt's Probabilistic Outputs for SVM](http://www.csie.ntu.edu.tw/~htlin/paper/doc/plattprob.pdf)\n    \n    - [Platt Calibration Wiki](https://en.wikipedia.org/wiki/Platt_scaling)\n    \n    - [Why use Platts Scaling](http://stats.stackexchange.com/questions/5196/why-use-platts-scaling)\n    \n    - [Classifier Classification with Platt's Scaling](http://fastml.com/classifier-calibration-with-platts-scaling-and-isotonic-regression/)\n\n\n<a name=\"rl\" />\n\n## Reinforcement Learning\n\n- [Awesome Reinforcement Learning (GitHub)](https://github.com/aikorea/awesome-rl)\n\n- [RL Tutorial Part 1](http://outlace.com/Reinforcement-Learning-Part-1/), [Part 2](http://outlace.com/Reinforcement-Learning-Part-2/)\n\n<a name=\"dt\" />\n\n## Decision Trees\n\n- [Wikipedia Page - Lots of Good Info](https://en.wikipedia.org/wiki/Decision_tree_learning)\n\n- [FAQs about Decision Trees](http://stats.stackexchange.com/questions/tagged/cart)\n\n- [Brief Tour of Trees and Forests](https://statistical-research.com/index.php/2013/04/29/a-brief-tour-of-the-trees-and-forests/)\n\n- [Tree Based Models in R](http://www.statmethods.net/advstats/cart.html)\n\n- [How Decision Trees work?](http://www.aihorizon.com/essays/generalai/decision_trees.htm)\n\n- [Weak side of Decision Trees](http://stats.stackexchange.com/questions/1292/what-is-the-weak-side-of-decision-trees)\n\n- [Thorough Explanation and different algorithms](http://www.ise.bgu.ac.il/faculty/liorr/hbchap9.pdf)\n\n- [What is entropy and information gain in the context of building decision trees?](http://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain)\n\n- [Slides Related to Decision Trees](http://www.slideshare.net/pierluca.lanzi/machine-learning-and-data-mining-11-decision-trees)\n\n- [How do decision tree learning algorithms deal with missing values?](http://stats.stackexchange.com/questions/96025/how-do-decision-tree-learning-algorithms-deal-with-missing-values-under-the-hoo)\n\n- [Using Surrogates to Improve Datasets with Missing Values](https://www.salford-systems.com/videos/tutorials/tips-and-tricks/using-surrogates-to-improve-datasets-with-missing-values)\n\n- [Good Article](https://www.mindtools.com/dectree.html)\n\n- [Are decision trees almost always binary trees?](http://stats.stackexchange.com/questions/12187/are-decision-trees-almost-always-binary-trees)\n\n- [Pruning Decision Trees](https://en.wikipedia.org/wiki/Pruning_(decision_trees)), [Grafting of Decision Trees](https://en.wikipedia.org/wiki/Grafting_(decision_trees))\n\n- [What is Deviance in context of Decision Trees?](http://stats.stackexchange.com/questions/6581/what-is-deviance-specifically-in-cart-rpart)\n\n- [Discover structure behind data with decision trees](http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/) - Grow and plot a decision tree to automatically figure out hidden rules in your data\n\n- Comparison of Different Algorithms\n\n    - [CART vs CTREE](http://stats.stackexchange.com/questions/12140/conditional-inference-trees-vs-traditional-decision-trees)\n    \n    - [Comparison of complexity or performance](https://stackoverflow.com/questions/9979461/different-decision-tree-algorithms-with-comparison-of-complexity-or-performance)\n    \n    - [CHAID vs CART](http://stats.stackexchange.com/questions/61230/chaid-vs-crt-or-cart) , [CART vs CHAID](http://www.bzst.com/2006/10/classification-trees-cart-vs-chaid.html)\n    \n    - [Good Article on comparison](http://www.ftpress.com/articles/article.aspx?p=2248639&seqNum=11)\n    \n- CART\n\n    - [Recursive Partitioning Wikipedia](https://en.wikipedia.org/wiki/Recursive_partitioning)\n    \n    - [CART Explained](http://documents.software.dell.com/Statistics/Textbook/Classification-and-Regression-Trees)\n    \n    - [How to measure/rank \u201cvariable importance\u201d when using CART?](http://stats.stackexchange.com/questions/6478/how-to-measure-rank-variable-importance-when-using-cart-specifically-using)\n    \n    - [Pruning a Tree in R](http://stackoverflow.com/questions/15318409/how-to-prune-a-tree-in-r)\n    \n    - [Does rpart use multivariate splits by default?](http://stats.stackexchange.com/questions/4356/does-rpart-use-multivariate-splits-by-default)\n    \n    - [FAQs about Recursive Partitioning](http://stats.stackexchange.com/questions/tagged/rpart)\n    \n- CTREE\n\n    - [party package in R](https://cran.r-project.org/web/packages/party/party.pdf)\n    \n    - [Show volumne in each node using ctree in R](http://stackoverflow.com/questions/13772715/show-volume-in-each-node-using-ctree-plot-in-r)\n    \n    - [How to extract tree structure from ctree function?](http://stackoverflow.com/questions/8675664/how-to-extract-tree-structure-from-ctree-function)\n    \n- CHAID\n\n    - [Wikipedia Artice on CHAID](https://en.wikipedia.org/wiki/CHAID)\n    \n    - [Basic Introduction to CHAID](https://smartdrill.com/Introduction-to-CHAID.html)\n    \n    - [Good Tutorial on CHAID](http://www.statsoft.com/Textbook/CHAID-Analysis)\n    \n- MARS\n\n    - [Wikipedia Article on MARS](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines)\n    \n- Probabilistic Decision Trees\n\n    - [Bayesian Learning in Probabilistic Decision Trees](http://www.stats.org.uk/bayesian/Jordan.pdf)\n    \n    - [Probabilistic Trees Research Paper](http://people.stern.nyu.edu/adamodar/pdfiles/papers/probabilistic.pdf)\n\n<a name=\"rf\" />\n\n## Random Forest / Bagging\n\n- [Awesome Random Forest (GitHub)**](https://github.com/kjw0612/awesome-random-forest)\n\n- [How to tune RF parameters in practice?](https://www.kaggle.com/forums/f/15/kaggle-forum/t/4092/how-to-tune-rf-parameters-in-practice)\n\n- [Measures of variable importance in random forests](http://stats.stackexchange.com/questions/12605/measures-of-variable-importance-in-random-forests)\n\n- [Compare R-squared from two different Random Forest models](http://stats.stackexchange.com/questions/13869/compare-r-squared-from-two-different-random-forest-models)\n\n- [OOB Estimate Explained | RF vs LDA](https://stat.ethz.ch/education/semesters/ss2012/ams/slides/v10.2.pdf)\n\n- [Evaluating Random Forests for Survival Analysis Using Prediction Error Curve](https://www.jstatsoft.org/index.php/jss/article/view/v050i11)\n\n- [Why doesn't Random Forest handle missing values in predictors?](http://stats.stackexchange.com/questions/98953/why-doesnt-random-forest-handle-missing-values-in-predictors)\n\n- [How to build random forests in R with missing (NA) values?](http://stackoverflow.com/questions/8370455/how-to-build-random-forests-in-r-with-missing-na-values)\n\n- [FAQs about Random Forest](http://stats.stackexchange.com/questions/tagged/random-forest), [More FAQs](http://stackoverflow.com/questions/tagged/random-forest)\n\n- [Obtaining knowledge from a random forest](http://stats.stackexchange.com/questions/21152/obtaining-knowledge-from-a-random-forest)\n\n- [Some Questions for R implementation](http://stackoverflow.com/questions/20537186/getting-predictions-after-rfimpute), [2](http://stats.stackexchange.com/questions/81609/whether-preprocessing-is-needed-before-prediction-using-finalmodel-of-randomfore), [3](http://stackoverflow.com/questions/17059432/random-forest-package-in-r-shows-error-during-prediction-if-there-are-new-fact)\n\n<a name=\"gbm\" />\n\n## Boosting\n\n- [Boosting for Better Predictions](http://www.datasciencecentral.com/profiles/blogs/boosting-algorithms-for-better-predictions)\n\n- [Boosting Wikipedia Page](https://en.wikipedia.org/wiki/Boosting_(machine_learning))\n\n- [Introduction to Boosted Trees | Tianqi Chen](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)\n\n- Gradient Boosting Machine\n\n    - [Gradiet Boosting Wiki](https://en.wikipedia.org/wiki/Gradient_boosting)\n    \n    - [Guidelines for GBM parameters in R](http://stats.stackexchange.com/questions/25748/what-are-some-useful-guidelines-for-gbm-parameters), [Strategy to set parameters](http://stats.stackexchange.com/questions/35984/strategy-to-set-the-gbm-parameters)\n    \n    - [Meaning of Interaction Depth](http://stats.stackexchange.com/questions/16501/what-does-interaction-depth-mean-in-gbm), [2](http://stats.stackexchange.com/questions/16501/what-does-interaction-depth-mean-in-gbm)\n    \n    - [Role of n.minobsinnode parameter of GBM in R](http://stats.stackexchange.com/questions/30645/role-of-n-minobsinnode-parameter-of-gbm-in-r)\n    \n    - [GBM in R](http://www.slideshare.net/mark_landry/gbm-package-in-r)\n    \n    - [FAQs about GBM](http://stats.stackexchange.com/tags/gbm/hot)\n    \n    - [GBM vs xgboost](https://www.kaggle.com/c/higgs-boson/forums/t/9497/r-s-gbm-vs-python-s-xgboost)\n\n- xgboost\n\n    - [xgboost tuning kaggle](https://www.kaggle.com/khozzy/rossmann-store-sales/xgboost-parameter-tuning-template/log)\n    \n    - [xgboost vs gbm](https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13012/question-to-experienced-kagglers-and-anyone-who-wants-to-take-a-shot/68296#post68296)\n    \n    - [xgboost survey](https://www.kaggle.com/c/higgs-boson/forums/t/10335/xgboost-post-competition-survey)\n    \n    - [Practical XGBoost in Python online course (free)](http://education.parrotprediction.teachable.com/courses/practical-xgboost-in-python)\n    \n- AdaBoost\n\n    - [AdaBoost Wiki](https://en.wikipedia.org/wiki/AdaBoost), [Python Code](https://gist.github.com/tristanwietsma/5486024)\n    \n    - [AdaBoost Sparse Input Support](http://hamzehal.blogspot.com/2014/06/adaboost-sparse-input-support.html)\n    \n    - [adaBag R package](https://cran.r-project.org/web/packages/adabag/adabag.pdf)\n    \n    - [Tutorial](http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf)\n\n- CatBoost\n\n    - [CatBoost Documentation](https://catboost.ai/docs/)\n\n    - [Benchmarks](https://catboost.ai/#benchmark)\n\n    - [Tutorial](https://github.com/catboost/tutorials)\n\n    - [GitHub Project](https://github.com/catboost)\n\n    - [CatBoost vs. Light GBM vs. XGBoost](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)\n\n<a name=\"ensem\" />\n\n## Ensembles\n\n- [Wikipedia Article on Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning)\n\n- [Kaggle Ensembling Guide](http://mlwave.com/kaggle-ensembling-guide/)\n\n- [The Power of Simple Ensembles](http://www.overkillanalytics.net/more-is-always-better-the-power-of-simple-ensembles/)\n\n- [Ensemble Learning Intro](http://machine-learning.martinsewell.com/ensembles/)\n\n- [Ensemble Learning Paper](http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf)\n\n- [Ensembling models with R](http://amunategui.github.io/blending-models/), [Ensembling Regression Models in R](http://stats.stackexchange.com/questions/26790/ensembling-regression-models), [Intro to Ensembles in R](http://www.vikparuchuri.com/blog/intro-to-ensemble-learning-in-r/)\n\n- [Ensembling Models with caret](http://stats.stackexchange.com/questions/27361/stacking-ensembling-models-with-caret)\n\n- [Bagging vs Boosting vs Stacking](http://stats.stackexchange.com/questions/18891/bagging-boosting-and-stacking-in-machine-learning)\n\n- [Good Resources | Kaggle Africa Soil Property Prediction](https://www.kaggle.com/c/afsis-soil-properties/forums/t/10391/best-ensemble-references)\n\n- [Boosting vs Bagging](http://www.chioka.in/which-is-better-boosting-or-bagging/)\n\n- [Resources for learning how to implement ensemble methods](http://stats.stackexchange.com/questions/32703/resources-for-learning-how-to-implement-ensemble-methods)\n\n- [How are classifications merged in an ensemble classifier?](http://stats.stackexchange.com/questions/21502/how-are-classifications-merged-in-an-ensemble-classifier)\n\n<a name=\"stack\" />\n\n## Stacking Models\n\n- [Stacking, Blending and Stacked Generalization](http://www.chioka.in/stacking-blending-and-stacked-generalization/)\n\n- [Stacked Generalization (Stacking)](http://machine-learning.martinsewell.com/ensembles/stacking/)\n\n- [Stacked Generalization: when does it work?](http://www.ijcai.org/Proceedings/97-2/011.pdf)\n\n- [Stacked Generalization Paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.1533&rep=rep1&type=pdf)\n\n<a name=\"vc\" />\n\n## Vapnik\u2013Chervonenkis Dimension\n\n- [Wikipedia article on VC Dimension](https://en.wikipedia.org/wiki/VC_dimension)\n\n- [Intuitive Explanantion of VC Dimension](https://www.quora.com/Explain-VC-dimension-and-shattering-in-lucid-Way)\n\n- [Video explaining VC Dimension](https://www.youtube.com/watch?v=puDzy2XmR5c)\n\n- [Introduction to VC Dimension](http://www.svms.org/vc-dimension/)\n\n- [FAQs about VC Dimension](http://stats.stackexchange.com/questions/tagged/vc-dimension)\n\n- [Do ensemble techniques increase VC-dimension?](http://stats.stackexchange.com/questions/78076/do-ensemble-techniques-increase-vc-dimension)\n\n\n<a name=\"bayes\" />\n\n## Bayesian Machine Learning\n\n- [Bayesian Methods for Hackers (using pyMC)](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)\n\n- [Should all Machine Learning be Bayesian?](http://videolectures.net/bark08_ghahramani_samlbb/)\n\n- [Tutorial on Bayesian Optimisation for Machine Learning](http://www.iro.umontreal.ca/~bengioy/cifar/NCAP2014-summerschool/slides/Ryan_adams_140814_bayesopt_ncap.pdf)\n\n- [Bayesian Reasoning and Deep Learning](http://blog.shakirm.com/2015/10/bayesian-reasoning-and-deep-learning/), [Slides](http://blog.shakirm.com/wp-content/uploads/2015/10/Bayes_Deep.pdf)\n\n- [Bayesian Statistics Made Simple](http://greenteapress.com/wp/think-bayes/)\n\n- [Kalman & Bayesian Filters in Python](https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python)\n\n- [Markov Chain Wikipedia Page](https://en.wikipedia.org/wiki/Markov_chain)\n\n\n<a name=\"semi\" />\n\n## Semi Supervised Learning\n\n- [Wikipedia article on Semi Supervised Learning](https://en.wikipedia.org/wiki/Semi-supervised_learning)\n\n- [Tutorial on Semi Supervised Learning](http://pages.cs.wisc.edu/~jerryzhu/pub/sslicml07.pdf)\n\n- [Graph Based Semi Supervised Learning for NLP](http://graph-ssl.wdfiles.com/local--files/blog%3A_start/graph_ssl_acl12_tutorial_slides_final.pdf)\n\n- [Taxonomy](http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/taxo_[0].pdf)\n\n- [Video Tutorial Weka](https://www.youtube.com/watch?v=sWxcIjZFGNM)\n\n- [Unsupervised, Supervised and Semi Supervised learning](http://stats.stackexchange.com/questions/517/unsupervised-supervised-and-semi-supervised-learning)\n\n- [Research Papers 1](http://mlg.eng.cam.ac.uk/zoubin/papers/zglactive.pdf), [2](http://mlg.eng.cam.ac.uk/zoubin/papers/zgl.pdf), [3](http://icml.cc/2012/papers/616.pdf)\n\n\n<a name=\"opt\" />\n\n## Optimization\n\n- [Mean Variance Portfolio Optimization with R and Quadratic Programming](http://www.wdiam.com/2012/06/10/mean-variance-portfolio-optimization-with-r-and-quadratic-programming/?utm_content=buffer04c12&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer)\n\n- [Algorithms for Sparse Optimization and Machine Learning](http://www.ima.umn.edu/2011-2012/W3.26-30.12/activities/Wright-Steve/sjw-ima12)\n\n- [Optimization Algorithms in Machine Learning](http://pages.cs.wisc.edu/~swright/nips2010/sjw-nips10.pdf), [Video Lecture](http://videolectures.net/nips2010_wright_oaml/)\n\n- [Optimization Algorithms for Data Analysis](http://www.birs.ca/workshops/2011/11w2035/files/Wright.pdf)\n\n- [Video Lectures on Optimization](http://videolectures.net/stephen_j_wright/)\n\n- [Optimization Algorithms in Support Vector Machines](http://pages.cs.wisc.edu/~swright/talks/sjw-complearning.pdf)\n\n- [The Interplay of Optimization and Machine Learning Research](http://jmlr.org/papers/volume7/MLOPT-intro06a/MLOPT-intro06a.pdf)\n\n- [Hyperopt tutorial for Optimizing Neural Networks\u2019 Hyperparameters](http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/)\n\n\n<a name=\"other\" />\n\n## Other Tutorials\n\n- For a collection of Data Science Tutorials using R, please refer to [this list](https://github.com/ujjwalkarn/DataScienceR).\n\n- For a collection of Data Science Tutorials using Python, please refer to [this list](https://github.com/ujjwalkarn/DataSciencePython).\n"
 },
 {
  "repo": "ChristosChristofidis/awesome-deep-learning",
  "language": null,
  "readme_contents": "\ufeff# Awesome Deep Learning [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\n## Table of Contents\n\n* **[Books](#books)**\n\n* **[Courses](#courses)**  \n\n* **[Videos and Lectures](#videos-and-lectures)**  \n\n* **[Papers](#papers)**  \n\n* **[Tutorials](#tutorials)**  \n\n* **[Researchers](#researchers)**  \n\n* **[Websites](#websites)**  \n\n* **[Datasets](#datasets)**\n\n* **[Conferences](#Conferences)**\n\n* **[Frameworks](#frameworks)**  \n\n* **[Tools](#tools)**  \n\n* **[Miscellaneous](#miscellaneous)**  \n\n* **[Contributing](#contributing)**  \n\n\n### Books\n\n1.  [Deep Learning](http://www.deeplearningbook.org/) by Yoshua Bengio, Ian Goodfellow and Aaron Courville  (05/07/2015)\n2.  [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) by  Michael Nielsen (Dec 2014)\n3.  [Deep Learning](http://research.microsoft.com/pubs/209355/DeepLearning-NowPublishing-Vol7-SIG-039.pdf) by Microsoft Research (2013)\n4.  [Deep Learning Tutorial](http://deeplearning.net/tutorial/deeplearning.pdf) by LISA lab, University of Montreal (Jan 6 2015)\n5.  [neuraltalk](https://github.com/karpathy/neuraltalk) by Andrej Karpathy : numpy-based RNN/LSTM implementation\n6.  [An introduction to genetic algorithms](http://www.boente.eti.br/fuzzy/ebook-fuzzy-mitchell.pdf)\n7.  [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/)\n8.  [Deep Learning in Neural Networks: An Overview](http://arxiv.org/pdf/1404.7828v4.pdf)\n9.  [Artificial intelligence and machine learning: Topic wise explanation](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/)\n10.[Grokking Deep Learning for Computer Vision](https://www.manning.com/books/grokking-deep-learning-for-computer-vision)\n11. [Dive into Deep Learning](https://d2l.ai/) - numpy based interactive Deep Learning book\n12. [Practical Deep Learning for Cloud, Mobile, and Edge](https://www.oreilly.com/library/view/practical-deep-learning/9781492034858/) - A book for optimization techniques during production.\n13. [Math and Architectures of Deep Learning](https://www.manning.com/books/math-and-architectures-of-deep-learning) - by Krishnendu Chaudhury\n\n \n### Courses\n\n1.  [Machine Learning - Stanford](https://class.coursera.org/ml-005) by Andrew Ng in Coursera (2010-2014)\n2.  [Machine Learning - Caltech](http://work.caltech.edu/lectures.html) by Yaser Abu-Mostafa (2012-2014)\n3.  [Machine Learning - Carnegie Mellon](http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml) by Tom Mitchell (Spring 2011)\n2.  [Neural Networks for Machine Learning](https://class.coursera.org/neuralnets-2012-001) by Geoffrey Hinton in Coursera (2012)\n3.  [Neural networks class](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH) by Hugo Larochelle from Universit\u00e9 de Sherbrooke (2013)\n4.  [Deep Learning Course](http://cilvr.cs.nyu.edu/doku.php?id=deeplearning:slides:start) by CILVR lab @ NYU (2014)\n5.  [A.I - Berkeley](https://courses.edx.org/courses/BerkeleyX/CS188x_1/1T2013/courseware/) by Dan Klein and Pieter Abbeel (2013)\n6.  [A.I - MIT](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/) by Patrick Henry Winston (2010)\n7.  [Vision and learning - computers and brains](http://web.mit.edu/course/other/i2course/www/vision_and_learning_fall_2013.html) by Shimon Ullman, Tomaso Poggio, Ethan Meyers @ MIT (2013)\n9.  [Convolutional Neural Networks for Visual Recognition - Stanford](http://vision.stanford.edu/teaching/cs231n/syllabus.html) by Fei-Fei Li, Andrej Karpathy (2017)\n10.  [Deep Learning for Natural Language Processing - Stanford](http://cs224d.stanford.edu/)\n11.  [Neural Networks - usherbrooke](http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html)\n12.  [Machine Learning - Oxford](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/) (2014-2015)\n13.  [Deep Learning - Nvidia](https://developer.nvidia.com/deep-learning-courses) (2015)\n14.  [Graduate Summer School: Deep Learning, Feature Learning](https://www.youtube.com/playlist?list=PLHyI3Fbmv0SdzMHAy0aN59oYnLy5vyyTA) by Geoffrey Hinton, Yoshua Bengio, Yann LeCun, Andrew Ng, Nando de Freitas and several others @ IPAM, UCLA (2012)\n15.  [Deep Learning - Udacity/Google](https://www.udacity.com/course/deep-learning--ud730) by Vincent Vanhoucke and Arpan Chakraborty (2016)\n16.  [Deep Learning - UWaterloo](https://www.youtube.com/playlist?list=PLehuLRPyt1Hyi78UOkMPWCGRxGcA9NVOE) by Prof. Ali Ghodsi at University of Waterloo (2015)\n17.  [Statistical Machine Learning - CMU](https://www.youtube.com/watch?v=azaLcvuql_g&list=PLjbUi5mgii6BWEUZf7He6nowWvGne_Y8r) by Prof. Larry Wasserman\n18.  [Deep Learning Course](https://www.college-de-france.fr/site/en-yann-lecun/course-2015-2016.htm) by Yann LeCun (2016)\n19. [Designing, Visualizing and Understanding Deep Neural Networks-UC Berkeley](https://www.youtube.com/playlist?list=PLkFD6_40KJIxopmdJF_CLNqG3QuDFHQUm)\n20. [UVA Deep Learning Course](http://uvadlc.github.io) MSc in Artificial Intelligence for the University of Amsterdam.\n21. [MIT 6.S094: Deep Learning for Self-Driving Cars](http://selfdrivingcars.mit.edu/)\n22. [MIT 6.S191: Introduction to Deep Learning](http://introtodeeplearning.com/)\n23. [Berkeley CS 294: Deep Reinforcement Learning](http://rll.berkeley.edu/deeprlcourse/)\n24. [Keras in Motion video course](https://www.manning.com/livevideo/keras-in-motion)\n25. [Practical Deep Learning For Coders](http://course.fast.ai/) by Jeremy Howard - Fast.ai\n26. [Introduction to Deep Learning](http://deeplearning.cs.cmu.edu/) by Prof. Bhiksha Raj (2017)\n27. [AI for Everyone](https://www.deeplearning.ai/ai-for-everyone/) by Andrew Ng (2019)\n28. [MIT Intro to Deep Learning 7 day bootcamp](https://introtodeeplearning.com) - A seven day bootcamp designed in MIT to introduce deep learning methods and applications (2019)\n29. [Deep Blueberry: Deep Learning](https://mithi.github.io/deep-blueberry) - A free five-weekend plan to self-learners to learn the basics of deep-learning architectures like CNNs, LSTMs, RNNs, VAEs, GANs, DQN, A3C and more (2019)\n30. [Spinning Up in Deep Reinforcement Learning](https://spinningup.openai.com/) - A free deep reinforcement learning course by OpenAI (2019)\n31. [Deep Learning Specialization - Coursera](https://www.coursera.org/specializations/deep-learning) - Breaking into AI with the best course from Andrew NG.\n32. [Deep Learning - UC Berkeley | STAT-157](https://www.youtube.com/playlist?list=PLZSO_6-bSqHQHBCoGaObUljoXAyyqhpFW) by Alex Smola and Mu Li (2019)\n33. [Machine Learning for Mere Mortals video course](https://www.manning.com/livevideo/machine-learning-for-mere-mortals) by Nick Chase\n34. [Machine Learning Crash Course with TensorFlow APIs](https://developers.google.com/machine-learning/crash-course/) -Google AI\n35. [Deep Learning from the Foundations](https://course.fast.ai/part2) Jeremy Howard - Fast.ai\n36. [Deep Reinforcement Learning (nanodegree) - Udacity](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) a 3-6 month Udacity nanodegree, spanning multiple courses (2018)\n37. [Grokking Deep Learning in Motion](https://www.manning.com/livevideo/grokking-deep-learning-in-motion) by Beau Carnes (2018)\n38. [Face Detection with Computer Vision and Deep Learning](https://www.udemy.com/share/1000gAA0QdcV9aQng=/) by Hakan Cebeci\n39. [Deep Learning Online Course list at Classpert](https://classpert.com/deep-learning) List of Deep Learning online courses (some are free) from Classpert Online Course Search\n\n### Videos and Lectures\n\n1.  [How To Create A Mind](https://www.youtube.com/watch?v=RIkxVci-R4k) By Ray Kurzweil\n2.  [Deep Learning, Self-Taught Learning and Unsupervised Feature Learning](https://www.youtube.com/watch?v=n1ViNeWhC24) By Andrew Ng\n3.  [Recent Developments in Deep Learning](https://www.youtube.com/watch?v=vShMxxqtDDs&amp;index=3&amp;list=PL78U8qQHXgrhP9aZraxTT5-X1RccTcUYT) By Geoff Hinton\n4.  [The Unreasonable Effectiveness of Deep Learning](https://www.youtube.com/watch?v=sc-KbuZqGkI) by Yann LeCun\n5.  [Deep Learning of Representations](https://www.youtube.com/watch?v=4xsVFLnHC_0) by Yoshua bengio\n6.  [Principles of Hierarchical Temporal Memory](https://www.youtube.com/watch?v=6ufPpZDmPKA) by Jeff Hawkins\n7.  [Machine Learning Discussion Group - Deep Learning w/ Stanford AI Lab](https://www.youtube.com/watch?v=2QJi0ArLq7s&amp;list=PL78U8qQHXgrhP9aZraxTT5-X1RccTcUYT) by Adam Coates\n8.  [Making Sense of the World with Deep Learning](http://vimeo.com/80821560) By Adam Coates\n9.  [Demystifying Unsupervised Feature Learning ](https://www.youtube.com/watch?v=wZfVBwOO0-k) By Adam Coates\n10.  [Visual Perception with Deep Learning](https://www.youtube.com/watch?v=3boKlkPBckA) By Yann LeCun\n11.  [The Next Generation of Neural Networks](https://www.youtube.com/watch?v=AyzOUbkUf3M) By Geoffrey Hinton at GoogleTechTalks\n12.  [The wonderful and terrifying implications of computers that can learn](http://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn) By Jeremy Howard at TEDxBrussels\n13.  [Unsupervised Deep Learning - Stanford](http://web.stanford.edu/class/cs294a/handouts.html) by Andrew Ng in Stanford (2011)\n14.  [Natural Language Processing](http://web.stanford.edu/class/cs224n/handouts/) By Chris Manning in Stanford\n15.  [A beginners Guide to Deep Neural Networks](http://googleresearch.blogspot.com/2015/09/a-beginners-guide-to-deep-neural.html) By Natalie Hammel and Lorraine Yurshansky\n16.  [Deep Learning: Intelligence from Big Data](https://www.youtube.com/watch?v=czLI3oLDe8M) by Steve Jurvetson (and panel) at VLAB in Stanford.\n17. [Introduction to Artificial Neural Networks and Deep Learning](https://www.youtube.com/watch?v=FoO8qDB8gUU) by Leo Isikdogan at Motorola Mobility HQ\n18. [NIPS 2016 lecture and workshop videos](https://nips.cc/Conferences/2016/Schedule) - NIPS 2016\n19. [Deep Learning Crash Course](https://www.youtube.com/watch?v=oS5fz_mHVz0&list=PLWKotBjTDoLj3rXBL-nEIPRN9V3a9Cx07): a series of mini-lectures by Leo Isikdogan on YouTube (2018)\n20. [Deep Learning Crash Course](https://www.manning.com/livevideo/deep-learning-crash-course) By Oliver Zeigermann\n21. [Deep Learning with R in Motion](https://www.manning.com/livevideo/deep-learning-with-r-in-motion): a live video course that teaches how to apply deep learning to text and images using the powerful Keras library and its R language interface.\n22. [Medical Imaging with Deep Learning Tutorial](https://www.youtube.com/playlist?list=PLheiZMDg_8ufxEx9cNVcOYXsT3BppJP4b): This tutorial is styled as a graduate lecture about medical imaging with deep learning. This will cover the background of popular medical image domains (chest X-ray and histology) as well as methods to tackle multi-modality/view, segmentation, and counting tasks.\n23. [Deepmind x UCL Deeplearning](https://www.youtube.com/playlist?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF): 2020 version \n24. [Deepmind x UCL Reinforcement Learning](https://www.youtube.com/playlist?list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb): Deep Reinforcement Learning\n25. [CMU 11-785 Intro to Deep learning Spring 2020](https://www.youtube.com/playlist?list=PLp-0K3kfddPzCnS4CqKphh-zT3aDwybDe) Course: 11-785, Intro to Deep Learning by Bhiksha Raj \n26. [Machine Learning CS 229](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) : End part focuses on deep learning By Andrew Ng\n\n### Papers\n*You can also find the most cited deep learning papers from [here](https://github.com/terryum/awesome-deep-learning-papers)*\n\n1.  [ImageNet Classification with Deep Convolutional Neural Networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n2.  [Using Very Deep Autoencoders for Content Based Image Retrieval](http://www.cs.toronto.edu/~hinton/absps/esann-deep-final.pdf)\n3.  [Learning Deep Architectures for AI](http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf)\n4.  [CMU\u2019s list of papers](http://deeplearning.cs.cmu.edu/)\n5.  [Neural Networks for Named Entity Recognition](http://nlp.stanford.edu/~socherr/pa4_ner.pdf) [zip](http://nlp.stanford.edu/~socherr/pa4-ner.zip)\n6. [Training tricks by YB](http://www.iro.umontreal.ca/~bengioy/papers/YB-tricks.pdf)\n7. [Geoff Hinton's reading list (all papers)](http://www.cs.toronto.edu/~hinton/deeprefs.html)\n8. [Supervised Sequence Labelling with Recurrent Neural Networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n9.  [Statistical Language Models based on Neural Networks](http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf)\n10.  [Training Recurrent Neural Networks](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)\n11.  [Recursive Deep Learning for Natural Language Processing and Computer Vision](http://nlp.stanford.edu/~socherr/thesis.pdf)\n12.  [Bi-directional RNN](http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf)\n13.  [LSTM](http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf)\n14.  [GRU - Gated Recurrent Unit](http://arxiv.org/pdf/1406.1078v3.pdf)\n15.  [GFRNN](http://arxiv.org/pdf/1502.02367v3.pdf) [.](http://jmlr.org/proceedings/papers/v37/chung15.pdf) [.](http://jmlr.org/proceedings/papers/v37/chung15-supp.pdf)\n16.  [LSTM: A Search Space Odyssey](http://arxiv.org/pdf/1503.04069v1.pdf)\n17.  [A Critical Review of Recurrent Neural Networks for Sequence Learning](http://arxiv.org/pdf/1506.00019v1.pdf)\n18.  [Visualizing and Understanding Recurrent Networks](http://arxiv.org/pdf/1506.02078v1.pdf)\n19.  [Wojciech Zaremba, Ilya Sutskever, An Empirical Exploration of Recurrent Network Architectures](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n20.  [Recurrent Neural Network based Language Model](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n21.  [Extensions of Recurrent Neural Network Language Model](http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf)\n22.  [Recurrent Neural Network based Language Modeling in Meeting Recognition](http://www.fit.vutbr.cz/~imikolov/rnnlm/ApplicationOfRNNinMeetingRecognition_IS2011.pdf)\n23.  [Deep Neural Networks for Acoustic Modeling in Speech Recognition](http://cs224d.stanford.edu/papers/maas_paper.pdf)\n24.  [Speech Recognition with Deep Recurrent Neural Networks](http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf)\n25.  [Reinforcement Learning Neural Turing Machines](http://arxiv.org/pdf/1505.00521v1)\n26.  [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](http://arxiv.org/pdf/1406.1078v3.pdf)\n27. [Google - Sequence to Sequence  Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n28. [Memory Networks](http://arxiv.org/pdf/1410.3916v10)\n29. [Policy Learning with Continuous Memory States for Partially Observed Robotic Control](http://arxiv.org/pdf/1507.01273v1)\n30. [Microsoft - Jointly Modeling Embedding and Translation to Bridge Video and Language](http://arxiv.org/pdf/1505.01861v1.pdf)\n31. [Neural Turing Machines](http://arxiv.org/pdf/1410.5401v2.pdf)\n32. [Ask Me Anything: Dynamic Memory Networks for Natural Language Processing](http://arxiv.org/pdf/1506.07285v1.pdf)\n33. [Mastering the Game of Go with Deep Neural Networks and Tree Search](http://www.nature.com/nature/journal/v529/n7587/pdf/nature16961.pdf)\n34. [Batch Normalization](https://arxiv.org/abs/1502.03167)\n35. [Residual Learning](https://arxiv.org/pdf/1512.03385v1.pdf)\n36. [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/pdf/1611.07004v1.pdf)\n37. [Berkeley AI Research (BAIR) Laboratory](https://arxiv.org/pdf/1611.07004v1.pdf)\n38. [MobileNets by Google](https://arxiv.org/abs/1704.04861)\n39. [Cross Audio-Visual Recognition in the Wild Using Deep Learning](https://arxiv.org/abs/1706.05739)\n40. [Dynamic Routing Between Capsules](https://arxiv.org/abs/1710.09829)\n41. [Matrix Capsules With Em Routing](https://openreview.net/pdf?id=HJWLfGWRb)\n42. [Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n43. [Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661v1.pdf)\n44. [Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf)\n45. [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n46. [Siamese Neural Networks for One-shot Image Recognition](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n47. [Unsupervised Translation of Programming Languages](https://arxiv.org/pdf/2006.03511.pdf)\n48. [Matching Networks for One Shot Learning](http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf)\n\n### Tutorials\n\n1.  [UFLDL Tutorial 1](http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial)\n2.  [UFLDL Tutorial 2](http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/)\n3.  [Deep Learning for NLP (without Magic)](http://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial)\n4.  [A Deep Learning Tutorial: From Perceptrons to Deep Networks](http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks)\n5.  [Deep Learning from the Bottom up](http://www.metacademy.org/roadmaps/rgrosse/deep_learning)\n6.  [Theano Tutorial](http://deeplearning.net/tutorial/deeplearning.pdf)\n7.  [Neural Networks for Matlab](http://uk.mathworks.com/help/pdf_doc/nnet/nnet_ug.pdf)\n8.  [Using convolutional neural nets to detect facial keypoints tutorial](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)\n9.  [Torch7 Tutorials](https://github.com/clementfarabet/ipam-tutorials/tree/master/th_tutorials)\n10.  [The Best Machine Learning Tutorials On The Web](https://github.com/josephmisiti/machine-learning-module)\n11. [VGG Convolutional Neural Networks Practical](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/index.html)\n12. [TensorFlow tutorials](https://github.com/nlintz/TensorFlow-Tutorials)\n13. [More TensorFlow tutorials](https://github.com/pkmital/tensorflow_tutorials)\n13. [TensorFlow Python Notebooks](https://github.com/aymericdamien/TensorFlow-Examples)\n14. [Keras and Lasagne Deep Learning Tutorials](https://github.com/Vict0rSch/deep_learning)\n15. [Classification on raw time series in TensorFlow with a LSTM RNN](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition)\n16. [Using convolutional neural nets to detect facial keypoints tutorial](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)\n17. [TensorFlow-World](https://github.com/astorfi/TensorFlow-World)\n18. [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)\n19. [Grokking Deep Learning](https://www.manning.com/books/grokking-deep-learning)\n20. [Deep Learning for Search](https://www.manning.com/books/deep-learning-for-search)\n21. [Keras Tutorial: Content Based Image Retrieval Using a Convolutional Denoising Autoencoder](https://blog.sicara.com/keras-tutorial-content-based-image-retrieval-convolutional-denoising-autoencoder-dc91450cc511)\n22. [Pytorch Tutorial by Yunjey Choi](https://github.com/yunjey/pytorch-tutorial)\n23. [Understanding deep Convolutional Neural Networks with a practical use-case in Tensorflow and Keras](https://ahmedbesbes.com/understanding-deep-convolutional-neural-networks-with-a-practical-use-case-in-tensorflow-and-keras.html)\n24. [Overview and benchmark of traditional and deep learning models in text classification](https://ahmedbesbes.com/overview-and-benchmark-of-traditional-and-deep-learning-models-in-text-classification.html)\n25. [Hardware for AI: Understanding computer hardware & build your own computer](https://github.com/MelAbgrall/HardwareforAI)\n26. [Programming Community Curated Resources](https://hackr.io/tutorials/learn-artificial-intelligence-ai)\n27. [The Illustrated Self-Supervised Learning](https://amitness.com/2020/02/illustrated-self-supervised-learning/)\n28. [Visual Paper Summary: ALBERT (A Lite BERT)](https://amitness.com/2020/02/albert-visual-summary/)\n\n\n\n## Researchers\n\n1. [Aaron Courville](http://aaroncourville.wordpress.com)\n2. [Abdel-rahman Mohamed](http://www.cs.toronto.edu/~asamir/)\n3. [Adam Coates](http://cs.stanford.edu/~acoates/)\n4. [Alex Acero](http://research.microsoft.com/en-us/people/alexac/)\n5. [ Alex Krizhevsky ](http://www.cs.utoronto.ca/~kriz/index.html)\n6. [ Alexander Ilin ](http://users.ics.aalto.fi/alexilin/)\n7. [ Amos Storkey ](http://homepages.inf.ed.ac.uk/amos/)\n8. [ Andrej Karpathy ](http://cs.stanford.edu/~karpathy/)\n9. [ Andrew M. Saxe ](http://www.stanford.edu/~asaxe/)\n10. [ Andrew Ng ](http://www.cs.stanford.edu/people/ang/)\n11. [ Andrew W. Senior ](http://research.google.com/pubs/author37792.html)\n12. [ Andriy Mnih ](http://www.gatsby.ucl.ac.uk/~amnih/)\n13. [ Ayse Naz Erkan ](http://www.cs.nyu.edu/~naz/)\n14. [ Benjamin Schrauwen ](http://reslab.elis.ugent.be/benjamin)\n15. [ Bernardete Ribeiro ](https://www.cisuc.uc.pt/people/show/2020)\n16. [ Bo David Chen ](http://vision.caltech.edu/~bchen3/Site/Bo_David_Chen.html)\n17. [ Boureau Y-Lan ](http://cs.nyu.edu/~ylan/)\n18. [ Brian Kingsbury ](http://researcher.watson.ibm.com/researcher/view.php?person=us-bedk)\n19. [ Christopher Manning ](http://nlp.stanford.edu/~manning/)\n20. [ Clement Farabet ](http://www.clement.farabet.net/)\n21. [ Dan Claudiu Cire\u0219an ](http://www.idsia.ch/~ciresan/)\n22. [ David Reichert ](http://serre-lab.clps.brown.edu/person/david-reichert/)\n23. [ Derek Rose ](http://mil.engr.utk.edu/nmil/member/5.html)\n24. [ Dong Yu ](http://research.microsoft.com/en-us/people/dongyu/default.aspx)\n25. [ Drausin Wulsin ](http://www.seas.upenn.edu/~wulsin/)\n26. [ Erik M. Schmidt ](http://music.ece.drexel.edu/people/eschmidt)\n27. [ Eugenio Culurciello ](https://engineering.purdue.edu/BME/People/viewPersonById?resource_id=71333)\n28. [ Frank Seide ](http://research.microsoft.com/en-us/people/fseide/)\n29. [ Galen Andrew ](http://homes.cs.washington.edu/~galen/)\n30. [ Geoffrey Hinton ](http://www.cs.toronto.edu/~hinton/)\n31. [ George Dahl ](http://www.cs.toronto.edu/~gdahl/)\n32. [ Graham Taylor ](http://www.uoguelph.ca/~gwtaylor/)\n33. [ Gr\u00e9goire Montavon ](http://gregoire.montavon.name/)\n34. [ Guido Francisco Mont\u00fafar ](http://personal-homepages.mis.mpg.de/montufar/)\n35. [ Guillaume Desjardins ](http://brainlogging.wordpress.com/)\n36. [ Hannes Schulz ](http://www.ais.uni-bonn.de/~schulz/)\n37. [ H\u00e9l\u00e8ne Paugam-Moisy ](http://www.lri.fr/~hpaugam/)\n38. [ Honglak Lee ](http://web.eecs.umich.edu/~honglak/)\n39. [ Hugo Larochelle ](http://www.dmi.usherb.ca/~larocheh/index_en.html)\n40. [ Ilya Sutskever ](http://www.cs.toronto.edu/~ilya/)\n41. [ Itamar Arel ](http://mil.engr.utk.edu/nmil/member/2.html)\n42. [ James Martens ](http://www.cs.toronto.edu/~jmartens/)\n43. [ Jason Morton ](http://www.jasonmorton.com/)\n44. [ Jason Weston ](http://www.thespermwhale.com/jaseweston/)\n45. [ Jeff Dean ](http://research.google.com/pubs/jeff.html)\n46. [ Jiquan Mgiam ](http://cs.stanford.edu/~jngiam/)\n47. [ Joseph Turian ](http://www-etud.iro.umontreal.ca/~turian/)\n48. [ Joshua Matthew Susskind ](http://aclab.ca/users/josh/index.html)\n49. [ J\u00fcrgen Schmidhuber ](http://www.idsia.ch/~juergen/)\n50. [ Justin A. Blanco ](https://sites.google.com/site/blancousna/)\n51. [ Koray Kavukcuoglu ](http://koray.kavukcuoglu.org/)\n52. [ KyungHyun Cho ](http://users.ics.aalto.fi/kcho/)\n53. [ Li Deng ](http://research.microsoft.com/en-us/people/deng/)\n54. [ Lucas Theis ](http://www.kyb.tuebingen.mpg.de/nc/employee/details/lucas.html)\n55. [ Ludovic Arnold ](http://ludovicarnold.altervista.org/home/)\n56. [ Marc'Aurelio Ranzato ](http://www.cs.nyu.edu/~ranzato/)\n57. [ Martin L\u00e4ngkvist ](http://aass.oru.se/~mlt/)\n58. [ Misha Denil ](http://mdenil.com/)\n59. [ Mohammad Norouzi ](http://www.cs.toronto.edu/~norouzi/)\n60. [ Nando de Freitas ](http://www.cs.ubc.ca/~nando/)\n61. [ Navdeep Jaitly ](http://www.cs.utoronto.ca/~ndjaitly/)\n62. [ Nicolas Le Roux ](http://nicolas.le-roux.name/)\n63. [ Nitish Srivastava ](http://www.cs.toronto.edu/~nitish/)\n64. [ Noel Lopes ](https://www.cisuc.uc.pt/people/show/2028)\n65. [ Oriol Vinyals ](http://www.cs.berkeley.edu/~vinyals/)\n66. [ Pascal Vincent ](http://www.iro.umontreal.ca/~vincentp)\n67. [ Patrick Nguyen ](https://sites.google.com/site/drpngx/)\n68. [ Pedro Domingos ](http://homes.cs.washington.edu/~pedrod/)\n69. [ Peggy Series ](http://homepages.inf.ed.ac.uk/pseries/)\n70. [ Pierre Sermanet ](http://cs.nyu.edu/~sermanet)\n71. [ Piotr Mirowski ](http://www.cs.nyu.edu/~mirowski/)\n72. [ Quoc V. Le ](http://ai.stanford.edu/~quocle/)\n73. [ Reinhold Scherer ](http://bci.tugraz.at/scherer/)\n74. [ Richard Socher ](http://www.socher.org/)\n75. [ Rob Fergus ](http://cs.nyu.edu/~fergus/pmwiki/pmwiki.php)\n76. [ Robert Coop ](http://mil.engr.utk.edu/nmil/member/19.html)\n77. [ Robert Gens ](http://homes.cs.washington.edu/~rcg/)\n78. [ Roger Grosse ](http://people.csail.mit.edu/rgrosse/)\n79. [ Ronan Collobert ](http://ronan.collobert.com/)\n80. [ Ruslan Salakhutdinov ](http://www.utstat.toronto.edu/~rsalakhu/)\n81. [ Sebastian Gerwinn ](http://www.kyb.tuebingen.mpg.de/nc/employee/details/sgerwinn.html)\n82. [ St\u00e9phane Mallat ](http://www.cmap.polytechnique.fr/~mallat/)\n83. [ Sven Behnke ](http://www.ais.uni-bonn.de/behnke/)\n84. [ Tapani Raiko ](http://users.ics.aalto.fi/praiko/)\n85. [ Tara Sainath ](https://sites.google.com/site/tsainath/)\n86. [ Tijmen Tieleman ](http://www.cs.toronto.edu/~tijmen/)\n87. [ Tom Karnowski ](http://mil.engr.utk.edu/nmil/member/36.html)\n88. [ Tom\u00e1\u0161 Mikolov ](https://research.facebook.com/tomas-mikolov)\n89. [ Ueli Meier ](http://www.idsia.ch/~meier/)\n90. [ Vincent Vanhoucke ](http://vincent.vanhoucke.com)\n91. [ Volodymyr Mnih ](http://www.cs.toronto.edu/~vmnih/)\n92. [ Yann LeCun ](http://yann.lecun.com/)\n93. [ Yichuan Tang ](http://www.cs.toronto.edu/~tang/)\n94. [ Yoshua Bengio ](http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html)\n95. [ Yotaro Kubo ](http://yota.ro/)\n96. [ Youzhi (Will) Zou ](http://ai.stanford.edu/~wzou)\n97. [ Fei-Fei Li ](http://vision.stanford.edu/feifeili)\n98. [ Ian Goodfellow ](https://research.google.com/pubs/105214.html)\n99. [ Robert Lagani\u00e8re ](http://www.site.uottawa.ca/~laganier/)\n100. [Merve Ayy\u00fcce K\u0131zrak](http://www.ayyucekizrak.com/)\n\n\n### Websites\n\n1.  [deeplearning.net](http://deeplearning.net/)\n2.  [deeplearning.stanford.edu](http://deeplearning.stanford.edu/)\n3.  [nlp.stanford.edu](http://nlp.stanford.edu/)\n4.  [ai-junkie.com](http://www.ai-junkie.com/ann/evolved/nnt1.html)\n5.  [cs.brown.edu/research/ai](http://cs.brown.edu/research/ai/)\n6.  [eecs.umich.edu/ai](http://www.eecs.umich.edu/ai/)\n7.  [cs.utexas.edu/users/ai-lab](http://www.cs.utexas.edu/users/ai-lab/)\n8.  [cs.washington.edu/research/ai](http://www.cs.washington.edu/research/ai/)\n9.  [aiai.ed.ac.uk](http://www.aiai.ed.ac.uk/)\n10.  [www-aig.jpl.nasa.gov](http://www-aig.jpl.nasa.gov/)\n11.  [csail.mit.edu](http://www.csail.mit.edu/)\n12.  [cgi.cse.unsw.edu.au/~aishare](http://cgi.cse.unsw.edu.au/~aishare/)\n13.  [cs.rochester.edu/research/ai](http://www.cs.rochester.edu/research/ai/)\n14.  [ai.sri.com](http://www.ai.sri.com/)\n15.  [isi.edu/AI/isd.htm](http://www.isi.edu/AI/isd.htm)\n16.  [nrl.navy.mil/itd/aic](http://www.nrl.navy.mil/itd/aic/)\n17.  [hips.seas.harvard.edu](http://hips.seas.harvard.edu/)\n18.  [AI Weekly](http://aiweekly.co)\n19.  [stat.ucla.edu](http://www.stat.ucla.edu/~junhua.mao/m-RNN.html)\n20.  [deeplearning.cs.toronto.edu](http://deeplearning.cs.toronto.edu/i2t)\n21.  [jeffdonahue.com/lrcn/](http://jeffdonahue.com/lrcn/)\n22.  [visualqa.org](http://www.visualqa.org/)\n23.  [www.mpi-inf.mpg.de/departments/computer-vision...](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/)\n24.  [Deep Learning News](http://news.startup.ml/)\n25.  [Machine Learning is Fun! Adam Geitgey's Blog](https://medium.com/@ageitgey/)\n26.  [Guide to Machine Learning](http://yerevann.com/a-guide-to-deep-learning/)\n27.  [Deep Learning for Beginners](https://spandan-madan.github.io/DeepLearningProject/)\n28.  [Machine Learning Mastery blog](https://machinelearningmastery.com/blog/)\n29.  [ML Compiled](https://ml-compiled.readthedocs.io/en/latest/)\n30.  [Programming Community Curated Resources](https://hackr.io/tutorials/learn-artificial-intelligence-ai)\n31.  [A Beginner's Guide To Understanding Convolutional Neural Networks](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)\n32.  [ahmedbesbes.com](http://ahmedbesbes.com)\n33.  [amitness.com](https://amitness.com/)\n34.  [AI Summer](https://theaisummer.com/)\n35.  [AI Hub - supported by AAAI, NeurIPS](https://aihub.org/)\n36.  [CatalyzeX: Machine Learning Hub for Builders and Makers](https://www.catalyzeX.com)\n\n### Datasets\n\n1.  [MNIST](http://yann.lecun.com/exdb/mnist/) Handwritten digits\n2.  [Google House Numbers](http://ufldl.stanford.edu/housenumbers/) from street view\n3.  [CIFAR-10 and CIFAR-100](http://www.cs.toronto.edu/~kriz/cifar.html)\n4.  [IMAGENET](http://www.image-net.org/)\n5.  [Tiny Images](http://groups.csail.mit.edu/vision/TinyImages/) 80 Million tiny images6.  \n6.  [Flickr Data](https://yahooresearch.tumblr.com/post/89783581601/one-hundred-million-creative-commons-flickr-images) 100 Million Yahoo dataset\n7.  [Berkeley Segmentation Dataset 500](http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/)\n8.  [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/)\n9.  [Flickr 8k](http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/KCCA.html)\n10. [Flickr 30k](http://shannon.cs.illinois.edu/DenotationGraph/)\n11. [Microsoft COCO](http://mscoco.org/home/)\n12. [VQA](http://www.visualqa.org/)\n13. [Image QA](http://www.cs.toronto.edu/~mren/imageqa/data/cocoqa/)\n14. [AT&T Laboratories Cambridge face database](http://www.uk.research.att.com/facedatabase.html)\n15. [AVHRR Pathfinder](http://xtreme.gsfc.nasa.gov)\n16. [Air Freight](http://www.anc.ed.ac.uk/~amos/afreightdata.html) - The Air Freight data set is a ray-traced image sequence along with ground truth segmentation based on textural characteristics. (455 images + GT, each 160x120 pixels). (Formats: PNG)  \n17. [Amsterdam Library of Object Images](http://www.science.uva.nl/~aloi/) - ALOI is a color image collection of one-thousand small objects, recorded for scientific purposes. In order to capture the sensory variation in object recordings, we systematically varied viewing angle, illumination angle, and illumination color for each object, and additionally captured wide-baseline stereo images. We recorded over a hundred images of each object, yielding a total of 110,250 images for the collection. (Formats: png)\n18. [Annotated face, hand, cardiac & meat images](http://www.imm.dtu.dk/~aam/) - Most images & annotations are supplemented by various ASM/AAM analyses using the AAM-API. (Formats: bmp,asf)\n19. [Image Analysis and Computer Graphics](http://www.imm.dtu.dk/image/)  \n21. [Brown University Stimuli](http://www.cog.brown.edu/~tarr/stimuli.html) - A variety of datasets including geons, objects, and \"greebles\". Good for testing recognition algorithms. (Formats: pict)\n22. [CAVIAR video sequences of mall and public space behavior](http://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1/) - 90K video frames in 90 sequences of various human activities, with XML ground truth of detection and behavior classification (Formats: MPEG2 & JPEG)\n23. [Machine Vision Unit](http://www.ipab.inf.ed.ac.uk/mvu/)\n25. [CCITT Fax standard images](http://www.cs.waikato.ac.nz/~singlis/ccitt.html) - 8 images (Formats: gif)\n26. [CMU CIL's Stereo Data with Ground Truth](cil-ster.html) - 3 sets of 11 images, including color tiff images with spectroradiometry (Formats: gif, tiff)\n27. [CMU PIE Database](http://www.ri.cmu.edu/projects/project_418.html) - A database of 41,368 face images of 68 people captured under 13 poses, 43 illuminations conditions, and with 4 different expressions.\n28. [CMU VASC Image Database](http://www.ius.cs.cmu.edu/idb/) - Images, sequences, stereo pairs (thousands of images) (Formats: Sun Rasterimage)\n29. [Caltech Image Database](http://www.vision.caltech.edu/html-files/archive.html) - about 20 images - mostly top-down views of small objects and toys. (Formats: GIF)\n30. [Columbia-Utrecht Reflectance and Texture Database](http://www.cs.columbia.edu/CAVE/curet/) - Texture and reflectance measurements for over 60 samples of 3D texture, observed with over 200 different combinations of viewing and illumination directions. (Formats: bmp)\n31. [Computational Colour Constancy Data](http://www.cs.sfu.ca/~colour/data/index.html) - A dataset oriented towards computational color constancy, but useful for computer vision in general. It includes synthetic data, camera sensor data, and over 700 images. (Formats: tiff)\n32. [Computational Vision Lab](http://www.cs.sfu.ca/~colour/)\n34. [Content-based image retrieval database](http://www.cs.washington.edu/research/imagedatabase/groundtruth/) - 11 sets of color images for testing algorithms for content-based retrieval. Most sets have a description file with names of objects in each image. (Formats: jpg)\n35. [Efficient Content-based Retrieval Group](http://www.cs.washington.edu/research/imagedatabase/)\n37. [Densely Sampled View Spheres](http://ls7-www.cs.uni-dortmund.de/~peters/pages/research/modeladaptsys/modeladaptsys_vba_rov.html) - Densely sampled view spheres - upper half of the view sphere of two toy objects with 2500 images each. (Formats: tiff)\n38. [Computer Science VII (Graphical Systems)](http://ls7-www.cs.uni-dortmund.de/)\n40. [Digital Embryos](https://web-beta.archive.org/web/20011216051535/vision.psych.umn.edu/www/kersten-lab/demos/digitalembryo.html) - Digital embryos are novel objects which may be used to develop and test object recognition systems. They have an organic appearance. (Formats: various formats are available on request)\n41. [Univerity of Minnesota Vision Lab](http://vision.psych.umn.edu/users/kersten//kersten-lab/kersten-lab.html) \n42. [El Salvador Atlas of Gastrointestinal VideoEndoscopy](http://www.gastrointestinalatlas.com) - Images and Videos of his-res of studies taken from Gastrointestinal Video endoscopy. (Formats: jpg, mpg, gif)\n43. [FG-NET Facial Aging Database](http://sting.cycollege.ac.cy/~alanitis/fgnetaging/index.htm) - Database contains 1002 face images showing subjects at different ages. (Formats: jpg)\n44. [FVC2000 Fingerprint Databases](http://bias.csr.unibo.it/fvc2000/) - FVC2000 is the First International Competition for Fingerprint Verification Algorithms. Four fingerprint databases constitute the FVC2000 benchmark (3520 fingerprints in all).\n45. [Biometric Systems Lab](http://biolab.csr.unibo.it/home.asp) - University of Bologna\n46. [Face and Gesture images and image sequences](http://www.fg-net.org) - Several image datasets of faces and gestures that are ground truth annotated for benchmarking\n47. [German Fingerspelling Database](http://www-i6.informatik.rwth-aachen.de/~dreuw/database.html) - The database contains 35 gestures and consists of 1400 image sequences that contain gestures of 20 different persons recorded under non-uniform daylight lighting conditions. (Formats: mpg,jpg)  \n48. [Language Processing and Pattern Recognition](http://www-i6.informatik.rwth-aachen.de/)\n50. [Groningen Natural Image Database](http://hlab.phys.rug.nl/archive.html) - 4000+ 1536x1024 (16 bit) calibrated outdoor images (Formats: homebrew)\n51. [ICG Testhouse sequence](http://www.icg.tu-graz.ac.at/~schindler/Data) -  2 turntable sequences from ifferent viewing heights, 36 images each, resolution 1000x750, color (Formats: PPM)\n52. [Institute of Computer Graphics and Vision](http://www.icg.tu-graz.ac.at)\n54. [IEN Image Library](http://www.ien.it/is/vislib/) - 1000+ images, mostly outdoor sequences (Formats: raw, ppm)  \n55. [INRIA's Syntim images database](http://www-rocq.inria.fr/~tarel/syntim/images.html) - 15 color image of simple objects (Formats: gif)\n56. [INRIA](http://www.inria.fr/)\n57. [INRIA's Syntim stereo databases](http://www-rocq.inria.fr/~tarel/syntim/paires.html) - 34 calibrated color stereo pairs (Formats: gif)\n58. [Image Analysis Laboratory](http://www.ece.ncsu.edu/imaging/Archives/ImageDataBase/index.html) - Images obtained from a variety of imaging modalities -- raw CFA images, range images and a host of \"medical images\". (Formats: homebrew)\n59. [Image Analysis Laboratory](http://www.ece.ncsu.edu/imaging)\n61. [Image Database](http://www.prip.tuwien.ac.at/prip/image.html) - An image database including some textures  \n62. [JAFFE Facial Expression Image Database](http://www.mis.atr.co.jp/~mlyons/jaffe.html) - The JAFFE database consists of 213 images of Japanese female subjects posing 6 basic facial expressions as well as a neutral pose. Ratings on emotion adjectives are also available, free of charge, for research purposes. (Formats: TIFF Grayscale images.)\n63. [ATR Research, Kyoto, Japan](http://www.mic.atr.co.jp/)\n64. [JISCT Stereo Evaluation](ftp://ftp.vislist.com/IMAGERY/JISCT/) - 44 image pairs. These data have been used in an evaluation of stereo analysis, as described in the April 1993 ARPA Image Understanding Workshop paper ``The JISCT Stereo Evaluation'' by R.C.Bolles, H.H.Baker, and M.J.Hannah, 263--274 (Formats: SSI)\n65. [MIT Vision Texture](https://vismod.media.mit.edu/vismod/imagery/VisionTexture/vistex.html) - Image archive (100+ images) (Formats: ppm)\n66. [MIT face images and more](ftp://whitechapel.media.mit.edu/pub/images) - hundreds of images (Formats: homebrew)\n67. [Machine Vision](http://vision.cse.psu.edu/book/testbed/images/) - Images from the textbook by Jain, Kasturi, Schunck (20+ images) (Formats: GIF TIFF)\n68. [Mammography Image Databases](http://marathon.csee.usf.edu/Mammography/Database.html) - 100 or more images of mammograms with ground truth. Additional images available by request, and links to several other mammography databases are provided. (Formats: homebrew)\n69. [ftp://ftp.cps.msu.edu/pub/prip](ftp://ftp.cps.msu.edu/pub/prip) - many images (Formats: unknown)\n70. [Middlebury Stereo Data Sets with Ground Truth](http://www.middlebury.edu/stereo/data.html) - Six multi-frame stereo data sets of scenes containing planar regions. Each data set contains 9 color images and subpixel-accuracy ground-truth data. (Formats: ppm)\n71. [Middlebury Stereo Vision Research Page](http://www.middlebury.edu/stereo) - Middlebury College\n72. [Modis Airborne simulator, Gallery and data set](http://ltpwww.gsfc.nasa.gov/MODIS/MAS/) - High Altitude Imagery from around the world for environmental modeling in support of NASA EOS program (Formats: JPG and HDF)\n73. [NIST Fingerprint and handwriting](ftp://sequoyah.ncsl.nist.gov/pub/databases/data) - datasets - thousands of images (Formats: unknown)\n74. [NIST Fingerprint data](ftp://ftp.cs.columbia.edu/jpeg/other/uuencoded) - compressed multipart uuencoded tar file\n75. [NLM HyperDoc Visible Human Project](http://www.nlm.nih.gov/research/visible/visible_human.html) - Color, CAT and MRI image samples - over 30 images (Formats: jpeg)\n76. [National Design Repository](http://www.designrepository.org) - Over 55,000 3D CAD and solid models of (mostly) mechanical/machined engineering designs. (Formats: gif,vrml,wrl,stp,sat) \n77. [Geometric & Intelligent Computing Laboratory](http://gicl.mcs.drexel.edu)\n79. [OSU (MSU) 3D Object Model Database](http://eewww.eng.ohio-state.edu/~flynn/3DDB/Models/) - several sets of 3D object models collected over several years to use in object recognition research (Formats: homebrew, vrml)\n80. [OSU (MSU/WSU) Range Image Database](http://eewww.eng.ohio-state.edu/~flynn/3DDB/RID/) - Hundreds of real and synthetic images (Formats: gif, homebrew)\n81. [OSU/SAMPL Database: Range Images, 3D Models, Stills, Motion Sequences](http://sampl.eng.ohio-state.edu/~sampl/database.htm) - Over 1000 range images, 3D object models, still images and motion sequences (Formats: gif, ppm, vrml, homebrew)\n82. [Signal Analysis and Machine Perception Laboratory](http://sampl.eng.ohio-state.edu)\n84. [Otago Optical Flow Evaluation Sequences](http://www.cs.otago.ac.nz/research/vision/Research/OpticalFlow/opticalflow.html) - Synthetic and real sequences with machine-readable ground truth optical flow fields, plus tools to generate ground truth for new sequences. (Formats: ppm,tif,homebrew)\n85. [Vision Research Group](http://www.cs.otago.ac.nz/research/vision/index.html)\n87. [ftp://ftp.limsi.fr/pub/quenot/opflow/testdata/piv/](ftp://ftp.limsi.fr/pub/quenot/opflow/testdata/piv/) - Real and synthetic image sequences used for testing a Particle Image Velocimetry application. These images may be used for the test of optical flow and image matching algorithms. (Formats: pgm (raw))\n88. [LIMSI-CNRS/CHM/IMM/vision](http://www.limsi.fr/Recherche/IMM/PageIMM.html)\n89. [LIMSI-CNRS](http://www.limsi.fr/)\n90. [Photometric 3D Surface Texture Database](http://www.taurusstudio.net/research/pmtexdb/index.htm) - This is the first 3D texture database which provides both full real surface rotations and registered photometric stereo data (30 textures, 1680 images). (Formats: TIFF)\n91. [SEQUENCES FOR OPTICAL FLOW ANALYSIS (SOFA)](http://www.cee.hw.ac.uk/~mtc/sofa) - 9 synthetic sequences designed for testing motion analysis applications, including full ground truth of motion and camera parameters. (Formats: gif)\n92. [Computer Vision Group](http://www.cee.hw.ac.uk/~mtc/research.html)\n94. [Sequences for Flow Based Reconstruction](http://www.nada.kth.se/~zucch/CAMERA/PUB/seq.html) - synthetic sequence for testing structure from motion algorithms (Formats: pgm)\n95. [Stereo Images with Ground Truth Disparity and Occlusion](http://www-dbv.cs.uni-bonn.de/stereo_data/) - a small set of synthetic images of a hallway with varying amounts of noise added. Use these images to benchmark your stereo algorithm. (Formats: raw, viff (khoros), or tiff)\n96. [Stuttgart Range Image Database](http://range.informatik.uni-stuttgart.de) - A collection of synthetic range images taken from high-resolution polygonal models available on the web (Formats: homebrew)\n97. [Department Image Understanding](http://www.informatik.uni-stuttgart.de/ipvr/bv/bv_home_engl.html)\n99. [The AR Face Database](http://www2.ece.ohio-state.edu/~aleix/ARdatabase.html) - Contains over 4,000 color images corresponding to 126 people's faces (70 men and 56 women). Frontal views with variations in facial expressions, illumination, and occlusions. (Formats: RAW (RGB 24-bit))\n100. [Purdue Robot Vision Lab](http://rvl.www.ecn.purdue.edu/RVL/)\n101. [The MIT-CSAIL Database of Objects and Scenes](http://web.mit.edu/torralba/www/database.html) - Database for testing multiclass object detection and scene recognition algorithms. Over 72,000 images with 2873 annotated frames. More than 50 annotated object classes. (Formats: jpg)\n102. [The RVL SPEC-DB (SPECularity DataBase)](http://rvl1.ecn.purdue.edu/RVL/specularity_database/) - A collection of over 300 real images of 100 objects taken under three different illuminaiton conditions (Diffuse/Ambient/Directed). -- Use these images to test algorithms for detecting and compensating specular highlights in color images. (Formats: TIFF )\n103. [Robot Vision Laboratory](http://rvl1.ecn.purdue.edu/RVL/)\n105. [The Xm2vts database](http://xm2vtsdb.ee.surrey.ac.uk) - The XM2VTSDB contains four digital recordings of 295 people taken over a period of four months. This database contains both image and video data of faces.\n106. [Centre for Vision, Speech and Signal Processing](http://www.ee.surrey.ac.uk/Research/CVSSP)\n107. [Traffic Image Sequences and 'Marbled Block' Sequence](http://i21www.ira.uka.de/image_sequences) - thousands of frames of digitized traffic image sequences as well as the 'Marbled Block' sequence (grayscale images) (Formats: GIF)\n108. [IAKS/KOGS](http://i21www.ira.uka.de)\n110. [U Bern Face images](ftp://ftp.iam.unibe.ch/pub/Images/FaceImages) - hundreds of images (Formats: Sun rasterfile)\n111. [U Michigan textures](ftp://freebie.engin.umich.edu/pub/misc/textures) (Formats: compressed raw)\n112. [U Oulu wood and knots database](http://www.ee.oulu.fi/~olli/Projects/Lumber.Grading.html) - Includes classifications - 1000+ color images (Formats: ppm)\n113. [UCID - an Uncompressed Colour Image Database](http://vision.doc.ntu.ac.uk/datasets/UCID/ucid.html) - a benchmark database for image retrieval with predefined ground truth. (Formats: tiff)\n115. [UMass Vision Image Archive](http://vis-www.cs.umass.edu/~vislib/) - Large image database with aerial, space, stereo, medical images and more. (Formats: homebrew)\n116. [UNC's 3D image database](ftp://sunsite.unc.edu/pub/academic/computer-science/virtual-reality/3d) - many images (Formats: GIF)\n117. [USF Range Image Data with Segmentation Ground Truth](http://marathon.csee.usf.edu/range/seg-comp/SegComp.html) - 80 image sets (Formats: Sun rasterimage)\n118. [University of Oulu Physics-based Face Database](http://www.ee.oulu.fi/research/imag/color/pbfd.html) - contains color images of faces under different illuminants and camera calibration conditions as well as skin spectral reflectance measurements of each person.\n119. [Machine Vision and Media Processing Unit](http://www.ee.oulu.fi/mvmp/)\n121. [University of Oulu Texture Database](http://www.outex.oulu.fi) - Database of 320 surface textures, each captured under three illuminants, six spatial resolutions and nine rotation angles. A set of test suites is also provided so that texture segmentation, classification, and retrieval algorithms can be tested in a standard manner. (Formats: bmp, ras, xv)\n122. [Machine Vision Group](http://www.ee.oulu.fi/mvg)\n124. [Usenix face database](ftp://ftp.uu.net/published/usenix/faces) - Thousands of face images from many different sites (circa 994)\n125. [View Sphere Database](http://www-prima.inrialpes.fr/Prima/hall/view_sphere.html) - Images of 8 objects seen from many different view points. The view sphere is sampled using a geodesic with 172 images/sphere. Two sets for training and testing are available. (Formats: ppm)\n126. [PRIMA, GRAVIR](http://www-prima.inrialpes.fr/Prima/)\n127. [Vision-list Imagery Archive](ftp://ftp.vislist.com/IMAGERY/) - Many images, many formats\n128. [Wiry Object Recognition Database](http://www.cs.cmu.edu/~owenc/word.htm) - Thousands of images of a cart, ladder, stool, bicycle, chairs, and cluttered scenes with ground truth labelings of edges and regions. (Formats: jpg)\n129. [3D Vision Group](http://www.cs.cmu.edu/0.000000E+003dvision/)\n131. [Yale Face Database](http://cvc.yale.edu/projects/yalefaces/yalefaces.html) -  165 images (15 individuals) with different lighting, expression, and occlusion configurations.\n132. [Yale Face Database B](http://cvc.yale.edu/projects/yalefacesB/yalefacesB.html) - 5760 single light source images of 10 subjects each seen under 576 viewing conditions (9 poses x 64 illumination conditions). (Formats: PGM)\n133. [Center for Computational Vision and Control](http://cvc.yale.edu/)\n134. [DeepMind QA Corpus](https://github.com/deepmind/rc-data) - Textual QA corpus from CNN and DailyMail. More than 300K documents in total. [Paper](http://arxiv.org/abs/1506.03340) for reference.\n135. [YouTube-8M Dataset](https://research.google.com/youtube8m/) - YouTube-8M is a large-scale labeled video dataset that consists of 8 million YouTube video IDs and associated labels from a diverse vocabulary of 4800 visual entities.\n136. [Open Images dataset](https://github.com/openimages/dataset) - Open Images is a dataset of ~9 million URLs to images that have been annotated with labels spanning over 6000 categories.\n137. [Visual Object Classes Challenge 2012 (VOC2012)](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit) - VOC2012 dataset containing 12k images with 20 annotated classes for object detection and segmentation.\n138. [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) - MNIST like fashion product dataset consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n139. [Large-scale Fashion (DeepFashion) Database](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) - Contains over 800,000 diverse fashion images.  Each image in this dataset is labeled with 50 categories, 1,000 descriptive attributes, bounding box and clothing landmarks\n140. [FakeNewsCorpus](https://github.com/several27/FakeNewsCorpus) - Contains about 10 million news articles classified using [opensources.co](http://opensources.co) types\n\n### Conferences\n\n1. [CVPR - IEEE Conference on Computer Vision and Pattern Recognition](http://cvpr2018.thecvf.com)\n2. [AAMAS - International Joint Conference on Autonomous Agents and Multiagent Systems](http://celweb.vuse.vanderbilt.edu/aamas18/)\n3. [IJCAI - \tInternational Joint Conference on Artificial Intelligence](https://www.ijcai-18.org/)\n4. [ICML - \tInternational Conference on Machine Learning](https://icml.cc)\n5. [ECML - European Conference on Machine Learning](http://www.ecmlpkdd2018.org)\n6. [KDD - Knowledge Discovery and Data Mining](http://www.kdd.org/kdd2018/)\n7. [NIPS - Neural Information Processing Systems](https://nips.cc/Conferences/2018)\n8. [O'Reilly AI Conference - \tO'Reilly Artificial Intelligence Conference](https://conferences.oreilly.com/artificial-intelligence/ai-ny)\n9. [ICDM - International Conference on Data Mining](https://www.waset.org/conference/2018/07/istanbul/ICDM)\n10. [ICCV - International Conference on Computer Vision](http://iccv2017.thecvf.com)\n11. [AAAI - Association for the Advancement of Artificial Intelligence](https://www.aaai.org)\n12. [MAIS - Montreal AI Symposium](https://montrealaisymposium.wordpress.com/)\n\n### Frameworks\n\n1.  [Caffe](http://caffe.berkeleyvision.org/)  \n2.  [Torch7](http://torch.ch/)\n3.  [Theano](http://deeplearning.net/software/theano/)\n4.  [cuda-convnet](https://code.google.com/p/cuda-convnet2/)\n5.  [convetjs](https://github.com/karpathy/convnetjs)\n5.  [Ccv](http://libccv.org/doc/doc-convnet/)\n6.  [NuPIC](http://numenta.org/nupic.html)\n7.  [DeepLearning4J](http://deeplearning4j.org/)\n8.  [Brain](https://github.com/harthur/brain)\n9.  [DeepLearnToolbox](https://github.com/rasmusbergpalm/DeepLearnToolbox)\n10.  [Deepnet](https://github.com/nitishsrivastava/deepnet)\n11.  [Deeppy](https://github.com/andersbll/deeppy)\n12.  [JavaNN](https://github.com/ivan-vasilev/neuralnetworks)\n13.  [hebel](https://github.com/hannes-brt/hebel)\n14.  [Mocha.jl](https://github.com/pluskid/Mocha.jl)\n15.  [OpenDL](https://github.com/guoding83128/OpenDL)\n16.  [cuDNN](https://developer.nvidia.com/cuDNN)\n17.  [MGL](http://melisgl.github.io/mgl-pax-world/mgl-manual.html)\n18.  [Knet.jl](https://github.com/denizyuret/Knet.jl)\n19.  [Nvidia DIGITS - a web app based on Caffe](https://github.com/NVIDIA/DIGITS)\n20.  [Neon - Python based Deep Learning Framework](https://github.com/NervanaSystems/neon)\n21.  [Keras - Theano based Deep Learning Library](http://keras.io)\n22.  [Chainer - A flexible framework of neural networks for deep learning](http://chainer.org/)\n23.  [RNNLM Toolkit](http://rnnlm.org/)\n24.  [RNNLIB - A recurrent neural network library](http://sourceforge.net/p/rnnl/wiki/Home/)\n25.  [char-rnn](https://github.com/karpathy/char-rnn)\n26.  [MatConvNet: CNNs for MATLAB](https://github.com/vlfeat/matconvnet)\n27.  [Minerva - a fast and flexible tool for deep learning on multi-GPU](https://github.com/dmlc/minerva)\n28.  [Brainstorm - Fast, flexible and fun neural networks.](https://github.com/IDSIA/brainstorm)\n29.  [Tensorflow - Open source software library for numerical computation using data flow graphs](https://github.com/tensorflow/tensorflow)\n30.  [DMTK - Microsoft Distributed Machine Learning Tookit](https://github.com/Microsoft/DMTK)\n31.  [Scikit Flow - Simplified interface for TensorFlow (mimicking Scikit Learn)](https://github.com/google/skflow)\n32.  [MXnet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning framework](https://github.com/apache/incubator-mxnet)\n33.  [Veles - Samsung Distributed machine learning platform](https://github.com/Samsung/veles)\n34.  [Marvin - A Minimalist GPU-only N-Dimensional ConvNets Framework](https://github.com/PrincetonVision/marvin)\n35.  [Apache SINGA - A General Distributed Deep Learning Platform](http://singa.incubator.apache.org/)\n36.  [DSSTNE - Amazon's library for building Deep Learning models](https://github.com/amznlabs/amazon-dsstne)\n37.  [SyntaxNet - Google's syntactic parser - A TensorFlow dependency library](https://github.com/tensorflow/models/tree/master/syntaxnet)\n38.  [mlpack - A scalable Machine Learning library](http://mlpack.org/)\n39.  [Torchnet - Torch based Deep Learning Library](https://github.com/torchnet/torchnet)\n40.  [Paddle - PArallel Distributed Deep LEarning by Baidu](https://github.com/baidu/paddle)\n41.  [NeuPy - Theano based Python library for ANN and Deep Learning](http://neupy.com)\n42.  [Lasagne - a lightweight library to build and train neural networks in Theano](https://github.com/Lasagne/Lasagne)\n43.  [nolearn - wrappers and abstractions around existing neural network libraries, most notably Lasagne](https://github.com/dnouri/nolearn)\n44.  [Sonnet - a library for constructing neural networks by Google's DeepMind](https://github.com/deepmind/sonnet)\n45.  [PyTorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration](https://github.com/pytorch/pytorch)\n46.  [CNTK - Microsoft Cognitive Toolkit](https://github.com/Microsoft/CNTK)\n47.  [Serpent.AI - Game agent framework: Use any video game as a deep learning sandbox](https://github.com/SerpentAI/SerpentAI)\n48.  [Caffe2 - A New Lightweight, Modular, and Scalable Deep Learning Framework](https://github.com/caffe2/caffe2)\n49.  [deeplearn.js - Hardware-accelerated deep learning and linear algebra (NumPy) library for the web](https://github.com/PAIR-code/deeplearnjs)\n50.  [TVM - End to End Deep Learning Compiler Stack for CPUs, GPUs and specialized accelerators](https://tvm.ai/)\n51.  [Coach - Reinforcement Learning Coach by Intel\u00ae AI Lab](https://github.com/NervanaSystems/coach)\n52.  [albumentations - A fast and framework agnostic image augmentation library](https://github.com/albu/albumentations)\n53.  [Neuraxle - A general-purpose ML pipelining framework](https://github.com/Neuraxio/Neuraxle)\n54.  [Catalyst: High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing](https://github.com/catalyst-team/catalyst)\n55.  [garage - A toolkit for reproducible reinforcement learning research](https://github.com/rlworkgroup/garage)\n56.  [Detecto - Train and run object detection models with 5-10 lines of code](https://github.com/alankbi/detecto)\n57.  [Karate Club - An unsupervised machine learning library for graph structured data](https://github.com/benedekrozemberczki/karateclub)\n58.  [Synapses - A lightweight library for neural networks that runs anywhere](https://github.com/mrdimosthenis/Synapses)\n59.  [TensorForce - A TensorFlow library for applied reinforcement learning](https://github.com/reinforceio/tensorforce)\n60.  [Hopsworks - A Feature Store for ML and Data-Intensive AI](https://github.com/logicalclocks/hopsworks)\n61.  [Feast - A Feature Store for ML for GCP by Gojek/Google](https://github.com/gojek/feast)\n62.  [PyTorch Geometric Temporal - Representation learning on dynamic graphs](https://github.com/gojek/feast)\n\n### Tools\n\n1.  [Netron](https://github.com/lutzroeder/netron) - Visualizer for deep learning and machine learning models\n2.  [Jupyter Notebook](http://jupyter.org) - Web-based notebook environment for interactive computing\n3.  [TensorBoard](https://github.com/tensorflow/tensorboard) - TensorFlow's Visualization Toolkit\n4.  [Visual Studio Tools for AI](https://visualstudio.microsoft.com/downloads/ai-tools-vs) - Develop, debug and deploy deep learning and AI solutions\n5.  [TensorWatch](https://github.com/microsoft/tensorwatch) - Debugging and visualization for deep learning\n6. [ML Workspace](https://github.com/ml-tooling/ml-workspace) - All-in-one web-based IDE for machine learning and data science.\n7.  [dowel](https://github.com/rlworkgroup/dowel) - A little logger for machine learning research. Log any object to the console, CSVs, TensorBoard, text log files, and more with just one call to `logger.log()`\n8.  [Neptune](https://neptune.ml/) - Lightweight tool for experiment tracking and results visualization. \n9.  [CatalyzeX](https://chrome.google.com/webstore/detail/code-finder-for-research/aikkeehnlfpamidigaffhfmgbkdeheil) - Browser extension ([Chrome](https://chrome.google.com/webstore/detail/code-finder-for-research/aikkeehnlfpamidigaffhfmgbkdeheil) and [Firefox](https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/)) that automatically finds and links to code implementations for ML papers anywhere online: Google, Twitter, Arxiv, Scholar, etc.\n\n### Miscellaneous\n\n1.  [Google Plus - Deep Learning Community](https://plus.google.com/communities/112866381580457264725)\n2.  [Caffe Webinar](http://on-demand-gtc.gputechconf.com/gtcnew/on-demand-gtc.php?searchByKeyword=shelhamer&amp;searchItems=&amp;sessionTopic=&amp;sessionEvent=4&amp;sessionYear=2014&amp;sessionFormat=&amp;submit=&amp;select=+)\n3.  [100 Best Github Resources in Github for DL](http://meta-guide.com/software-meta-guide/100-best-github-deep-learning/)\n4.  [Word2Vec](https://code.google.com/p/word2vec/)\n5.  [Caffe DockerFile](https://github.com/tleyden/docker/tree/master/caffe)\n6.  [TorontoDeepLEarning convnet](https://github.com/TorontoDeepLearning/convnet)\n8.  [gfx.js](https://github.com/clementfarabet/gfx.js)\n9.  [Torch7 Cheat sheet](https://github.com/torch/torch7/wiki/Cheatsheet)\n10. [Misc from MIT's 'Advanced Natural Language Processing' course](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-864-advanced-natural-language-processing-fall-2005/)\n11. [Misc from MIT's 'Machine Learning' course](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/)\n12. [Misc from MIT's 'Networks for Learning: Regression and Classification' course](http://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-520-a-networks-for-learning-regression-and-classification-spring-2001/)\n13. [Misc from MIT's 'Neural Coding and Perception of Sound' course](http://ocw.mit.edu/courses/health-sciences-and-technology/hst-723j-neural-coding-and-perception-of-sound-spring-2005/index.htm)\n14. [Implementing a Distributed Deep Learning Network over Spark](http://www.datasciencecentral.com/profiles/blogs/implementing-a-distributed-deep-learning-network-over-spark)\n15. [A chess AI that learns to play chess using deep learning.](https://github.com/erikbern/deep-pink)\n16. [Reproducing the results of \"Playing Atari with Deep Reinforcement Learning\" by DeepMind](https://github.com/kristjankorjus/Replicating-DeepMind)\n17. [Wiki2Vec. Getting Word2vec vectors for entities and word from Wikipedia Dumps](https://github.com/idio/wiki2vec)\n18. [The original code from the DeepMind article + tweaks](https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner)\n19. [Google deepdream - Neural Network art](https://github.com/google/deepdream)\n20. [An efficient, batched LSTM.](https://gist.github.com/karpathy/587454dc0146a6ae21fc)\n21. [A recurrent neural network designed to generate classical music.](https://github.com/hexahedria/biaxial-rnn-music-composition)\n22. [Memory Networks Implementations - Facebook](https://github.com/facebook/MemNN)\n23. [Face recognition with Google's FaceNet deep neural network.](https://github.com/cmusatyalab/openface)\n24. [Basic digit recognition neural network](https://github.com/joeledenberg/DigitRecognition)\n25. [Emotion Recognition API Demo - Microsoft](https://www.projectoxford.ai/demo/emotion#detection)\n26. [Proof of concept for loading Caffe models in TensorFlow](https://github.com/ethereon/caffe-tensorflow)\n27. [YOLO: Real-Time Object Detection](http://pjreddie.com/darknet/yolo/#webcam)\n28. [YOLO: Practical Implementation using Python](https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/)\n29. [AlphaGo - A replication of DeepMind's 2016 Nature publication, \"Mastering the game of Go with deep neural networks and tree search\"](https://github.com/Rochester-NRT/AlphaGo)\n30. [Machine Learning for Software Engineers](https://github.com/ZuzooVn/machine-learning-for-software-engineers)\n31. [Machine Learning is Fun!](https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471#.oa4rzez3g)\n32. [Siraj Raval's Deep Learning tutorials](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A)\n33. [Dockerface](https://github.com/natanielruiz/dockerface) - Easy to install and use deep learning Faster R-CNN face detection for images and video in a docker container.\n34. [Awesome Deep Learning Music](https://github.com/ybayle/awesome-deep-learning-music) - Curated list of articles related to deep learning scientific research applied to music\n35. [Awesome Graph Embedding](https://github.com/benedekrozemberczki/awesome-graph-embedding) - Curated list of articles related to deep learning scientific research on graph structured data at the graph level.\n36. [Awesome Network Embedding](https://github.com/chihming/awesome-network-embedding) - Curated list of articles related to deep learning scientific research on graph structured data at the node level.\n37. [Microsoft Recommenders](https://github.com/Microsoft/Recommenders) contains examples, utilities and best practices for building recommendation systems. Implementations of several state-of-the-art algorithms are provided for self-study and customization in your own applications.\n38. [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) - Andrej Karpathy blog post about using RNN for generating text.\n39. [Ladder Network](https://github.com/divamgupta/ladder_network_keras) - Keras Implementation of Ladder Network for Semi-Supervised Learning \n40. [toolbox: Curated list of ML libraries](https://github.com/amitness/toolbox)\n41. [CNN Explainer](https://poloclub.github.io/cnn-explainer/)\n\n\n-----\n### Contributing\nHave anything in mind that you think is awesome and would fit in this list? Feel free to send a [pull request](https://github.com/ashara12/awesome-deeplearning/pulls).\n\n-----\n## License\n\n[![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)\n\nTo the extent possible under law, [Christos Christofidis](https://linkedin.com/in/Christofidis) has waived all copyright and related or neighboring rights to this work.\n"
 },
 {
  "repo": "fastai/courses",
  "language": "Jupyter Notebook",
  "readme_contents": "# Practical Deep Learning for Coders (fast.ai courses)\n\nThese are the lecture materials from [Practical Deep Learning for Coders](http://course.fast.ai/). Two important parts of the course are  [our online forums](http://forums.fast.ai/) and [our wiki](http://wiki.fast.ai/index.php/Main_Page).  If you are encountering an error, we recommend that you first search the forums and wiki for a solution.  If you can't find the answer there, the next step is to ask your question on the forums.  See this advice on [how to ask for help](http://wiki.fast.ai/index.php/How_to_ask_for_Help) in a way that will allow others to most quickly and effectively be able to help you.  Please don't use Github Issues to ask for help debugging (many questions have already been answered in the forums).\n"
 },
 {
  "repo": "Yorko/mlcourse.ai",
  "language": "Python",
  "readme_contents": "<div align=\"center\">\n\n![ODS stickers](https://github.com/Yorko/mlcourse.ai/blob/master/img/ods_stickers.jpg)\n\n**[mlcourse.ai](https://mlcourse.ai) \u2013 Open Machine Learning Course**\n\n[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-green)](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n[![Slack](https://img.shields.io/badge/slack-ods.ai-orange)](https://opendatascience.slack.com/archives/C91N8TL83/p1567408586359500)\n[![Donate](https://img.shields.io/badge/support-patreon-red)](https://www.patreon.com/ods_mlcourse)\n[![Donate](https://img.shields.io/badge/support-ko--fi-red)](https://ko-fi.com/mlcourse_ai)\n\n\n</div>\n  \n[mlcourse.ai](https://mlcourse.ai) is an open Machine Learning course by [OpenDataScience](https://ods.ai). The course is designed to perfectly balance theory and practice. You can take part in several Kaggle Inclass competitions held during the course. From spring 2017 to fall 2019, 6 sessions of mlcourse.ai took place - 26k participants applied, 10k converted to passing the first assignment, about 1500 participants finished the course. Currently, the course is in self-paced mode. Check out a thorough [Roadmap](https://mlcourse.ai/roadmap) guiding you through the self-paced mlcourse.ai.\n\nMirrors (:uk:-only): [mlcourse.ai](https://mlcourse.ai) (main site), [Kaggle Dataset](https://www.kaggle.com/kashnitsky/mlcourse) (same notebooks as Kaggle Notebooks)\n\n### Self-paced passing\nThe [Roadmap](https://mlcourse.ai/roadmap) will guide you through 11 weeks of mlcourse.ai. For each week, from Pandas to Gradient Boosting, instructions are given on what artciles to read, lectures to watch, what assignments to accomplish. \n\n### Articles\nThis is the list of published articles on medium.com [:uk:](https://medium.com/open-machine-learning-course), habr.com [:ru:](https://habr.com/company/ods/blog/344044/). Also notebooks in Chinese are mentioned :cn: and links to Kaggle Notebooks (in English) are given. Icons are clickable.\n\n1. Exploratory Data Analysis with Pandas [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-1-exploratory-data-analysis-with-pandas-de57880f1a68)  [:ru:](https://habrahabr.ru/company/ods/blog/322626/) [:cn:](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_chinese/topic01-%E4%BD%BF%E7%94%A8-Pandas-%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2.ipynb), [Kaggle Notebook](https://www.kaggle.com/kashnitsky/topic-1-exploratory-data-analysis-with-pandas)\n2. Visual Data Analysis with Python [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-2-visual-data-analysis-in-python-846b989675cd)  [:ru:](https://habrahabr.ru/company/ods/blog/323210/) [:cn:](http://nbviewer.ipython.org/urls/raw.github.com/Yorko/mlcourse.ai/master/jupyter_chinese/topic02-Python-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E5%88%86%E6%9E%90.ipynb), Kaggle Notebooks: [part1](https://www.kaggle.com/kashnitsky/topic-2-visual-data-analysis-in-python), [part2](https://www.kaggle.com/kashnitsky/topic-2-part-2-seaborn-and-plotly)\n3. Classification, Decision Trees and k Nearest Neighbors [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors-8613c6b6d2cd) [:ru:](https://habrahabr.ru/company/ods/blog/322534/) [:cn:](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_chinese/topic03-%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C-K-%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB.ipynb), [Kaggle Notebook](https://www.kaggle.com/kashnitsky/topic-3-decision-trees-and-knn)\n4. Linear Classification and Regression [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220) [:ru:](https://habrahabr.ru/company/ods/blog/323890/) [:cn:](http://nbviewer.ipython.org/urls/raw.github.com/Yorko/mlcourse.ai/master/jupyter_chinese/topic04-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8C%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8.ipynb), Kaggle Notebooks: [part1](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-1-ols), [part2](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-2-classification), [part3](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-3-regularization), [part4](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-4-more-of-logit), [part5](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-5-validation)\n5. Bagging and Random Forest [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-5-ensembles-of-algorithms-and-random-forest-8e05246cbba7) [:ru:](https://habrahabr.ru/company/ods/blog/324402/) [:cn:](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_chinese/topic05-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E6%96%B9%E6%B3%95.ipynb), Kaggle Notebooks: [part1](https://www.kaggle.com/kashnitsky/topic-5-ensembles-part-1-bagging), [part2](https://www.kaggle.com/kashnitsky/topic-5-ensembles-part-2-random-forest), [part3](https://www.kaggle.com/kashnitsky/topic-5-ensembles-part-3-feature-importance)\n6. Feature Engineering and Feature Selection [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-6-feature-engineering-and-feature-selection-8b94f870706a) [:ru:](https://habrahabr.ru/company/ods/blog/325422/) [:cn:](http://nbviewer.ipython.org/urls/raw.github.com/Yorko/mlcourse.ai/master/jupyter_chinese/topic06-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E5%92%8C%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9.ipynb), [Kaggle Notebook](https://www.kaggle.com/kashnitsky/topic-6-feature-engineering-and-feature-selection)\n7. Unsupervised Learning: Principal Component Analysis and Clustering [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-7-unsupervised-learning-pca-and-clustering-db7879568417) [:ru:](https://habrahabr.ru/company/ods/blog/325654/) [:cn:](http://nbviewer.ipython.org/urls/raw.github.com/Yorko/mlcourse.ai/master/jupyter_chinese/topic07-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E5%92%8C%E8%81%9A%E7%B1%BB.ipynb), [Kaggle Notebook](https://www.kaggle.com/kashnitsky/topic-7-unsupervised-learning-pca-and-clustering)\n8. Vowpal Wabbit: Learning with Gigabytes of Data [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-8-vowpal-wabbit-fast-learning-with-gigabytes-of-data-60f750086237) [:ru:](https://habrahabr.ru/company/ods/blog/326418/) [:cn:](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_chinese/topic08-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81.ipynb), [Kaggle Notebook](https://www.kaggle.com/kashnitsky/topic-8-online-learning-and-vowpal-wabbit)\n9. Time Series Analysis with Python, part 1 [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-9-time-series-analysis-in-python-a270cb05e0b3) [:ru:](https://habrahabr.ru/company/ods/blog/327242/) [:cn:](http://nbviewer.ipython.org/urls/raw.github.com/Yorko/mlcourse.ai/master/jupyter_chinese/topic09-%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%A4%84%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8.ipynb). Predicting future with Facebook Prophet, part 2 [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-9-part-3-predicting-the-future-with-facebook-prophet-3f3af145cdc), [:cn:](http://nbviewer.ipython.org/urls/raw.github.com/Yorko/mlcourse.ai/master/jupyter_chinese/topic09-%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%A4%84%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8.ipynb) Kaggle Notebooks: [part1](https://www.kaggle.com/kashnitsky/topic-9-part-1-time-series-analysis-in-python), [part2](https://www.kaggle.com/kashnitsky/topic-9-part-2-time-series-with-facebook-prophet)\n10. Gradient Boosting [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-10-gradient-boosting-c751538131ac) [:ru:](https://habrahabr.ru/company/ods/blog/327250/), [:cn:](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_chinese/topic05-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E6%96%B9%E6%B3%95.ipynb), [Kaggle Notebook](https://www.kaggle.com/kashnitsky/topic-10-gradient-boosting)\n\n### Lectures\nVideolectures are uploaded to [this](https://bit.ly/2zY6Xe2) YouTube playlist.\nIntroduction, [video](https://www.youtube.com/watch?v=DrohHdQa8u8), [slides](https://www.slideshare.net/festline/mlcourseai-fall2019-live-session-0)\n\n1. Exploratory data analysis with Pandas, [video](https://youtu.be/fwWCw_cE5aI)\n2. Visualization, main plots for EDA, [video](https://www.youtube.com/watch?v=WNoQTNOME5g)\n3. Decision trees: [theory](https://youtu.be/H4XlBTPv5rQ) and [practical part](https://youtu.be/RrVYO6Td9Js)\n4. Logistic regression: [theoretical foundations](https://www.youtube.com/watch?v=l3jiw-N544s), [practical part](https://www.youtube.com/watch?v=7o0SWgY89i8) (baselines in the \"Alice\" competition)\n5. Ensembles and Random Forest \u2013 [part 1](https://www.youtube.com/watch?v=neXJL-AqI_c). Classification metrics \u2013 [part 2](https://www.youtube.com/watch?v=aBOMYqGUlWQ). Example of a business task, predicting a customer payment \u2013 [part 3](https://www.youtube.com/watch?v=FmKU-1LZGoE) \n6. Linear regression and regularization - [theory](https://youtu.be/ne-MfRfYs_c), LASSO & Ridge, LTV prediction - [practice](https://youtu.be/B8yIaIEMyIc)\n7. Unsupervised learning - [Principal Component Analysis](https://youtu.be/-AswHf7h0I4) and [Clustering](https://youtu.be/eVplCo-w4XE)\n8. Stochastic Gradient Descent for classification and regression - [part 1](https://youtu.be/EUSXbdzaQE8), part 2 TBA\n9. Time series analysis with Python (ARIMA, Prophet) - [video](https://youtu.be/_9lBwXnbOd8)\n10. Gradient boosting: basic ideas - [part 1](https://youtu.be/g0ZOtzZqdqk), key ideas behind Xgboost, LightGBM, and CatBoost + practice - [part 2](https://youtu.be/V5158Oug4W8)\n\n### Assignments\n\n1. Exploratory data analysis with Pandas, [nbviewer](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment01_pandas_uci_adult.ipynb?flush_cache=true), [Kaggle Notebook](https://www.kaggle.com/kashnitsky/assignment-1-pandas-and-uci-adult-dataset), [solution](https://www.kaggle.com/kashnitsky/a1-demo-pandas-and-uci-adult-dataset-solution)\n2. Analyzing cardiovascular disease data, [nbviewer](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment02_analyzing_cardiovascular_desease_data.ipynb?flush_cache=true), [Kaggle Notebook](https://www.kaggle.com/kashnitsky/assignment-2-analyzing-cardiovascular-data), [solution](https://www.kaggle.com/kashnitsky/a2-demo-analyzing-cardiovascular-data-solution)\n3. Decision trees with a toy task and the UCI Adult dataset, [nbviewer](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment03_decision_trees.ipynb?flush_cache=true), [Kaggle Notebook](https://www.kaggle.com/kashnitsky/assignment-3-decision-trees), [solution](https://www.kaggle.com/kashnitsky/a3-demo-decision-trees-solution)\n4. Sarcasm detection, [Kaggle Notebook](https://www.kaggle.com/kashnitsky/a4-demo-sarcasm-detection-with-logit), [solution](https://www.kaggle.com/kashnitsky/a4-demo-sarcasm-detection-with-logit-solution). Linear Regression as an optimization problem, [nbviewer](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment04_linreg_optimization.ipynb?flush_cache=true), [Kaggle Notebook](https://www.kaggle.com/kashnitsky/assignment-4-linear-regression-as-optimization)\n5. Logistic Regression and Random Forest in the credit scoring problem, [nbviewer](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment05_logit_rf_credit_scoring.ipynb?flush_cache=true), [Kaggle Notebook](https://www.kaggle.com/kashnitsky/assignment-5-logit-and-rf-for-credit-scoring), [solution](https://www.kaggle.com/kashnitsky/a5-demo-logit-and-rf-for-credit-scoring-sol)\n6. Exploring OLS, Lasso and Random Forest in a regression task, [nbviewer](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment06_regression_wine.ipynb?flush_cache=true), [Kaggle Notebook](https://www.kaggle.com/kashnitsky/assignment-6-linear-models-and-rf-for-regression), [solution](https://www.kaggle.com/kashnitsky/a6-demo-regression-solution)\n7. Unsupervised learning, [nbviewer](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment07_unsupervised_learning.ipynb?flush_cache=true), [Kaggle Notebook](https://www.kaggle.com/kashnitsky/assignment-7-unupervised-learning), [solution](https://www.kaggle.com/kashnitsky/a7-demo-unsupervised-learning-solution)\n8. Implementing online regressor, [nbviewer](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment08_implement_sgd_regressor.ipynb?flush_cache=true), [Kaggle Notebook](https://www.kaggle.com/kashnitsky/assignment-8-implementing-online-regressor), [solution](https://www.kaggle.com/kashnitsky/a8-demo-implementing-online-regressor-solution)\n9. Time series analysis, [nbviewer](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment09_time_series.ipynb?flush_cache=true), [Kaggle Notebook](https://www.kaggle.com/kashnitsky/assignment-9-time-series-analysis), [solution](https://www.kaggle.com/kashnitsky/a9-demo-time-series-analysis-solution)\n10. Beating baseline in a competition, [Kaggle Notebook](https://www.kaggle.com/kashnitsky/assignment-10-gradient-boosting-and-flight-delays)\n\n### Kaggle competitions\n1. Catch Me If You Can: Intruder Detection through Webpage Session Tracking. [Kaggle Inclass](https://www.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2)\n2. DotA 2 winner prediction. [Kaggle Inclass](https://www.kaggle.com/c/mlcourse-dota2-win-prediction)\n\n### Citing mlcourse.ai\n\nIf you happen to cite mlcourse.ai in your work, you can use this bibtex\n```\n@misc{mlcourse_ai,\n    author = {Kashnitsky, Yury},\n    title = {mlcourse.ai \u2013 Open Machine Learning Course},\n    year = {2020},\n    publisher = {GitHub},\n    journal = {GitHub repository},\n    howpublished = {\\url{https://github.com/Yorko/mlcourse.ai}},\n}\n```\n\n### Community\nDiscussions are held in the **#mlcourse_ai** channel of the [OpenDataScience (ods.ai)](https://ods.ai) Slack team.\n\n*The course is free but you can support organizers by making a pledge on [Patreon](https://www.patreon.com/ods_mlcourse) (monthly support) or a one-time payment on [Ko-fi](https://ko-fi.com/mlcourse_ai). Thus you'll foster the spread of Machine Learning in the world!*\n\n[![Donate](https://img.shields.io/badge/support-patreon-red)](https://www.patreon.com/ods_mlcourse)\n[![Donate](https://img.shields.io/badge/support-ko--fi-red)](https://ko-fi.com/mlcourse_ai)\n"
 },
 {
  "repo": "jtoy/awesome-tensorflow",
  "language": null,
  "readme_contents": "# Awesome TensorFlow  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/jtoy/awesome)\n\nA curated list of awesome TensorFlow experiments, libraries, and projects. Inspired by awesome-machine-learning.\n\n## What is TensorFlow?\n\nTensorFlow is an open source software library for numerical computation using data flow graphs. In other words, the best way to build deep learning models.\n\nMore info [here](http://tensorflow.org).\n\n\n\n## Table of Contents\n\n<!-- MarkdownTOC depth=4 -->\n- [Tutorials](#github-tutorials)\n- [Models/Projects](#github-projects)\n- [Powered by TensorFlow](#github-powered-by)\n- [Libraries](#libraries)\n- [Tools/Utilities](#tools-utils)\n- [Videos](#video)\n- [Papers](#papers)\n- [Blog posts](#blogs)\n- [Community](#community)\n- [Books](#books)\n\n<!-- /MarkdownTOC -->\n\n\n<a name=\"github-tutorials\" />\n\n## Tutorials\n\n* [TensorFlow Tutorial 1](https://github.com/pkmital/tensorflow_tutorials) - From the basics to slightly more interesting applications of TensorFlow\n* [TensorFlow Tutorial 2](https://github.com/nlintz/TensorFlow-Tutorials) - Introduction to deep learning based on Google's TensorFlow framework. These tutorials are direct ports of Newmu's Theano\n* [TensorFlow Tutorial 3](https://github.com/Hvass-Labs/TensorFlow-Tutorials) - These tutorials are intended for beginners in Deep Learning and TensorFlow with well-documented code and YouTube videos.\n* [TensorFlow Examples](https://github.com/aymericdamien/TensorFlow-Examples) - TensorFlow tutorials and code examples for beginners\n* [Sungjoon's TensorFlow-101](https://github.com/sjchoi86/Tensorflow-101) - TensorFlow tutorials written in Python with Jupyter Notebook\n* [Terry Um\u2019s TensorFlow Exercises](https://github.com/terryum/TensorFlow_Exercises) - Re-create the codes from other TensorFlow examples\n* [Installing TensorFlow on Raspberry Pi 3](https://github.com/samjabrahams/tensorflow-on-raspberry-pi) - TensorFlow compiled and running properly on the Raspberry Pi\n* [Classification on time series](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition) - Recurrent Neural Network classification in TensorFlow with LSTM on cellphone sensor data\n* [Getting Started with TensorFlow on Android](https://omid.al/posts/2017-02-20-Tutorial-Build-Your-First-Tensorflow-Android-App.html) - Build your first TensorFlow Android app\n* [Predict time series](https://github.com/guillaume-chevalier/seq2seq-signal-prediction) - Learn to use a seq2seq model on simple datasets as an introduction to the vast array of possibilities that this architecture offers\n* [Single Image Random Dot Stereograms](https://github.com/Mazecreator/TensorFlow-SIRDS) - SIRDS is a means to present 3D data in a 2D image. It allows for scientific data display of a waterfall type plot with no hidden lines due to perspective.\n* [CS20 SI: TensorFlow for DeepLearning Research](http://web.stanford.edu/class/cs20si/syllabus.html) - Stanford Course about Tensorflow from 2017 - [Syllabus](http://web.stanford.edu/class/cs20si/syllabus.html) - [Unofficial Videos](https://youtu.be/g-EvyKpZjmQ?list=PLSPPwKHXGS2110rEaNH7amFGmaD5hsObs)\n* [TensorFlow World](https://github.com/astorfi/TensorFlow-World) - Concise and ready-to-use TensorFlow tutorials with detailed documentation are provided.\n* [Effective Tensorflow](https://github.com/vahidk/EffectiveTensorflow) - TensorFlow howtos and best practices. Covers the basics as well as advanced topics.\n* [TensorLayer](http://tensorlayer.readthedocs.io/en/latest/user/tutorial.html) - Modular implementation for TensorFlow's official tutorials. ([CN](https://tensorlayercn.readthedocs.io/zh/latest/user/tutorial.html)).\n* [Understanding The Tensorflow Estimator API](https://www.lighttag.io/blog/tensorflow-estimator-api/) A conceptual overview of the Estimator API, when you'd use it and why. \n* [Convolutional Neural Networks in TensorFlow](https://www.coursera.org/learn/convolutional-neural-networks-tensorflow) - Convolutional Neural Networks in Tensorflow, offered by Coursera\n\n<a name=\"github-projects\" />\n\n## Models/Projects\n\n* [SenseNet](https://github.com/jtoy/sensenetjey/dtn-tensorflow) - Robotics touch model with TensorFlow DQN example\n* [Tensorflow-Project-Template](https://github.com/Mrgemy95/Tensorflow-Project-Template) - A simple and well-designed template for your tensorflow project.\n* [Domain Transfer Network](https://github.com/yunjey/dtn-tensorflow) - Implementation of Unsupervised Cross-Domain Image Generation\n* [Show, Attend and Tell](https://github.com/yunjey/show_attend_and_tell) - Attention Based Image Caption Generator\n* [Neural Style](https://github.com/cysmith/neural-style-tf) Implementation of Neural Style\n* [SRGAN](https://github.com/tensorlayer/srgan) - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\n* [Pretty Tensor](https://github.com/google/prettytensor) - Pretty Tensor provides a high level builder API\n* [Neural Style](https://github.com/anishathalye/neural-style) - An implementation of neural style\n* [AlexNet3D](https://github.com/denti/AlexNet3D) - An implementations of AlexNet3D. Simple AlexNet model but with 3D convolutional layers (conv3d).\n* [TensorFlow White Paper Notes](https://github.com/samjabrahams/tensorflow-white-paper-notes) - Annotated notes and summaries of the TensorFlow white paper, along with SVG figures and links to documentation\n* [NeuralArt](https://github.com/ckmarkoh/neuralart_tensorflow) - Implementation of A Neural Algorithm of Artistic Style\n* [Deep-Q learning Pong with TensorFlow and PyGame](http://www.danielslater.net/2016/03/deep-q-learning-pong-with-tensorflow.html)\n* [Generative Handwriting Demo using TensorFlow](https://github.com/hardmaru/write-rnn-tensorflow) - An attempt to implement the random handwriting generation portion of Alex Graves' paper\n* [Neural Turing Machine in TensorFlow](https://github.com/carpedm20/NTM-tensorflow) - implementation of Neural Turing Machine\n* [GoogleNet Convolutional Neural Network Groups Movie Scenes By Setting](https://github.com/agermanidis/thingscoop) - Search, filter, and describe videos based on objects, places, and other things that appear in them\n* [Neural machine translation between the writings of Shakespeare and modern English using TensorFlow](https://github.com/tokestermw/tensorflow-shakespeare) - This performs a monolingual translation, going from modern English to Shakespeare and vice-versa.\n* [Chatbot](https://github.com/Conchylicultor/DeepQA) - Implementation of [\"A neural conversational model\"](http://arxiv.org/abs/1506.05869)\n* [Seq2seq-Chatbot](https://github.com/tensorlayer/seq2seq-chatbot) - Chatbot in 200 lines of code\n* [DCGAN](https://github.com/tensorlayer/dcgan) - Deep Convolutional Generative Adversarial Networks\n* [GAN-CLS](https://github.com/zsdonghao/text-to-image) -Generative Adversarial Text to Image Synthesis\n* [im2im](https://github.com/zsdonghao/Unsup-Im2Im) - Unsupervised Image to Image Translation with Generative Adversarial Networks\n* [Improved CycleGAN](https://github.com/luoxier/CycleGAN_Tensorlayer) - Unpaired Image to Image Translation\n* [DAGAN](https://github.com/nebulaV/DAGAN) - Fast Compressed Sensing MRI Reconstruction\n* [Colornet - Neural Network to colorize grayscale images](https://github.com/pavelgonchar/colornet) - Neural Network to colorize grayscale images\n* [Neural Caption Generator](https://github.com/jazzsaxmafia/show_attend_and_tell.tensorflow) - Implementation of [\"Show and Tell\"](http://arxiv.org/abs/1411.4555)\n* [Neural Caption Generator with Attention](https://github.com/jazzsaxmafia/show_attend_and_tell.tensorflow) - Implementation of [\"Show, Attend and Tell\"](http://arxiv.org/abs/1502.03044)\n* [Weakly_detector](https://github.com/jazzsaxmafia/Weakly_detector) - Implementation of [\"Learning Deep Features for Discriminative Localization\"](http://cnnlocalization.csail.mit.edu/)\n* [Dynamic Capacity Networks](https://github.com/jazzsaxmafia/dcn.tf) - Implementation of [\"Dynamic Capacity Networks\"](http://arxiv.org/abs/1511.07838)\n* [HMM in TensorFlow](https://github.com/dwiel/tensorflow_hmm) - Implementation of viterbi and forward/backward algorithms for HMM\n* [DeepOSM](https://github.com/trailbehind/DeepOSM) - Train TensorFlow neural nets with OpenStreetMap features and satellite imagery.\n* [DQN-tensorflow](https://github.com/devsisters/DQN-tensorflow) - TensorFlow implementation of DeepMind's 'Human-Level Control through Deep Reinforcement Learning' with OpenAI Gym by Devsisters.com\n* [Policy Gradient](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_atari_pong.py) - For Playing Atari Ping Pong\n* [Deep Q-Network](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_frozenlake_dqn.py) - For Playing Frozen Lake Game\n* [AC](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_cartpole_ac.py) - Actor Critic for Playing Discrete Action space Game (Cartpole)\n* [A3C](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_bipedalwalker_a3c_continuous_action.py) - Asynchronous Advantage Actor Critic (A3C) for Continuous Action Space (Bipedal Walker)\n* [DAGGER](https://github.com/zsdonghao/Imitation-Learning-Dagger-Torcs) - For Playing [Gym Torcs](https://github.com/ugo-nama-kun/gym_torcs)\n* [TRPO](https://github.com/jjkke88/RL_toolbox) - For Continuous and Discrete Action Space by\n* [Highway Network](https://github.com/fomorians/highway-cnn) - TensorFlow implementation of [\"Training Very Deep Networks\"](http://arxiv.org/abs/1507.06228) with a [blog post](https://medium.com/jim-fleming/highway-networks-with-tensorflow-1e6dfa667daa#.ndicn1i27)\n* [Hierarchical Attention Networks](https://github.com/tqtg/hierarchical-attention-networks) - TensorFlow implementation of [\"Hierarchical Attention Networks for Document Classification\"](https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf)\n* [Sentence Classification with CNN](https://github.com/dennybritz/cnn-text-classification-tf) - TensorFlow implementation of [\"Convolutional Neural Networks for Sentence Classification\"](http://arxiv.org/abs/1408.5882) with a [blog post](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)\n* [End-To-End Memory Networks](https://github.com/domluna/memn2n) - Implementation of [End-To-End Memory Networks](http://arxiv.org/abs/1503.08895)\n* [Character-Aware Neural Language Models](https://github.com/carpedm20/lstm-char-cnn-tensorflow) - TensorFlow implementation of [Character-Aware Neural Language Models](http://arxiv.org/abs/1508.06615)\n* [YOLO TensorFlow ++](https://github.com/thtrieu/yolotf) - TensorFlow implementation of 'YOLO: Real-Time Object Detection', with training and an actual support for real-time running on mobile devices.\n* [Wavenet](https://github.com/ibab/tensorflow-wavenet) - This is a TensorFlow implementation of the [WaveNet generative neural network architecture](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) for audio generation.\n* [Mnemonic Descent Method](https://github.com/trigeorgis/mdm) - Tensorflow implementation of [\"Mnemonic Descent Method: A recurrent process applied for end-to-end face alignment\"](http://ibug.doc.ic.ac.uk/media/uploads/documents/trigeorgis2016mnemonic.pdf)\n* [CNN visualization using Tensorflow](https://github.com/InFoCusp/tf_cnnvis) - Tensorflow implementation of [\"Visualizing and Understanding Convolutional Networks\"](https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf)\n* [VGAN Tensorflow](https://github.com/Singularity42/VGAN-Tensorflow) - Tensorflow implementation for MIT [\"Generating Videos with Scene Dynamics\"](http://carlvondrick.com/tinyvideo/) by Vondrick et al.\n* [3D Convolutional Neural Networks in TensorFlow](https://github.com/astorfi/3D-convolutional-speaker-recognition) - Implementation of [\"3D Convolutional Neural Networks for Speaker Verification application\"](https://arxiv.org/abs/1705.09422) in TensorFlow by Torfi et al.\n* [U-Net](https://github.com/zsdonghao/u-net-brain-tumor) - For Brain Tumor Segmentation\n* [Spatial Transformer Networks](https://github.com/zsdonghao/Spatial-Transformer-Nets) - Learn the Transformation Function \n* [Lip Reading - Cross Audio-Visual Recognition using 3D Architectures in TensorFlow](https://github.com/astorfi/lip-reading-deeplearning) - TensorFlow Implementation of [\"Cross Audio-Visual Recognition in the Wild Using Deep Learning\"](https://arxiv.org/abs/1706.05739) by Torfi et al.\n* [Attentive Object Tracking](https://github.com/akosiorek/hart) - Implementation of [\"Hierarchical Attentive Recurrent Tracking\"](https://arxiv.org/abs/1706.09262)\n* [Holographic Embeddings for Graph Completion and Link Prediction](https://github.com/laxatives/TensorFlow-TransX) - Implementation of [Holographic Embeddings of Knowledge Graphs](http://arxiv.org/abs/1510.04935)\n* [Unsupervised Object Counting](https://github.com/akosiorek/attend_infer_repeat) - Implementation of [\"Attend, Infer, Repeat\"](https://papers.nips.cc/paper/6230-attend-infer-repeat-fast-scene-understanding-with-generative-models)\n* [Tensorflow FastText](https://github.com/apcode/tensorflow_fasttext) - A simple embedding based text classifier inspired by Facebook's fastText.\n* [MusicGenreClassification](https://github.com/mlachmish/MusicGenreClassification) - Classify music genre from a 10 second sound stream using a Neural Network.\n* [Kubeflow](https://github.com/kubeflow/kubeflow) - Framework for easily using Tensorflow with Kubernetes.\n* [TensorNets](https://github.com/taehoonlee/tensornets) - 40+ Popular Computer Vision Models With Pre-trained Weights.\n* [Ladder Network](https://github.com/divamgupta/ladder_network_keras) - Implementation of Ladder Network for Semi-Supervised Learning in Keras and Tensorflow\n<a name=\"github-powered-by\" />\n* [TF-Unet](https://github.com/juniorxsound/TF-Unet) - General purpose U-Network implemented in Keras for image segmentation\n* [Sarus TF2 Models](https://github.com/sarus-tech/tf2-published-models) - A long list of recent generative models implemented in clean, easy to reuse, Tensorflow 2 code (Plain Autoencoder, VAE, VQ-VAE, PixelCNN, Gated PixelCNN, PixelCNN++, PixelSNAIL, Conditional Neural Processes).\n\n## Powered by TensorFlow\n\n* [YOLO TensorFlow](https://github.com/gliese581gg/YOLO_tensorflow) - Implementation of 'YOLO : Real-Time Object Detection'\n* [android-yolo](https://github.com/natanielruiz/android-yolo) - Real-time object detection on Android using the YOLO network, powered by TensorFlow.\n* [Magenta](https://github.com/tensorflow/magenta) - Research project to advance the state of the art in machine intelligence for music and art generation\n\n\n<a name=\"libraries\" />\n\n## Libraries\n\n* [TensorFlow Estimators](https://www.tensorflow.org/guide/estimators) - high-level TensorFlow API that greatly simplifies machine learning programming (originally [tensorflow/skflow](https://github.com/tensorflow/skflow))\n* [R Interface to TensorFlow](https://tensorflow.rstudio.com/) - R interface to TensorFlow APIs, including Estimators, Keras, Datasets, etc.\n* [Lattice](https://github.com/tensorflow/lattice) - Implementation of Monotonic Calibrated Interpolated Look-Up Tables in TensorFlow\n* [tensorflow.rb](https://github.com/somaticio/tensorflow.rb) - TensorFlow native interface for ruby using SWIG\n* [tflearn](https://github.com/tflearn/tflearn) - Deep learning library featuring a higher-level API\n* [TensorLayer](https://github.com/tensorlayer/tensorlayer) - Deep learning and reinforcement learning library for researchers and engineers\n* [TensorFlow-Slim](https://github.com/tensorflow/models/tree/master/inception/inception/slim) - High-level library for defining models\n* [TensorFrames](https://github.com/tjhunter/tensorframes) - TensorFlow binding for Apache Spark\n* [TensorForce](https://github.com/reinforceio/tensorforce) - TensorForce: A TensorFlow library for applied reinforcement learning\n* [TensorFlowOnSpark](https://github.com/yahoo/TensorFlowOnSpark) - initiative from Yahoo! to enable distributed TensorFlow with Apache Spark.\n* [caffe-tensorflow](https://github.com/ethereon/caffe-tensorflow) - Convert Caffe models to TensorFlow format\n* [keras](http://keras.io) - Minimal, modular deep learning library for TensorFlow and Theano\n* [SyntaxNet: Neural Models of Syntax](https://github.com/tensorflow/models/tree/master/syntaxnet) - A TensorFlow implementation of the models described in [Globally Normalized Transition-Based Neural Networks, Andor et al. (2016)](http://arxiv.org/pdf/1603.06042.pdf)\n* [keras-js](https://github.com/transcranial/keras-js) - Run Keras models (tensorflow backend) in the browser, with GPU support\n* [NNFlow](https://github.com/welschma/NNFlow) - Simple framework allowing to read-in ROOT NTuples by converting them to a Numpy array and then use them in Google Tensorflow.\n* [Sonnet](https://github.com/deepmind/sonnet) - Sonnet is DeepMind's library built on top of TensorFlow for building complex neural networks.\n* [tensorpack](https://github.com/ppwwyyxx/tensorpack) - Neural Network Toolbox on TensorFlow focusing on training speed and on large datasets.\n* [tf-encrypted](https://github.com/mortendahl/tf-encrypted) - Layer on top of TensorFlow for doing machine learning on encrypted data\n* [pytorch2keras](https://github.com/nerox8664/pytorch2keras) - Convert PyTorch models to Keras (with TensorFlow backend) format\n* [gluon2keras](https://github.com/nerox8664/gluon2keras) - Convert Gluon models to Keras (with TensorFlow backend) format\n* [TensorIO](https://doc-ai.github.io/tensorio/) - Lightweight, cross-platform library for deploying TensorFlow Lite models to mobile devices. \n* [StellarGraph](https://github.com/stellargraph/stellargraph) - Machine Learning on Graphs, a Python library for machine learning on graph-structured (network-structured) data.\n* [DeepBay](https://github.com/ElPapi42/DeepBay) - High-Level Keras Complement for implement common architectures stacks, served as easy to use plug-n-play modules\n\n<a name=\"tools-utils\" />\n\n## Tools/Utilities\n\n* [Guild AI](https://guild.ai) - Task runner and package manager for TensorFlow\n* [ML Workspace](https://github.com/ml-tooling/ml-workspace) - All-in-one web IDE for machine learning and data science. Combines Tensorflow, Jupyter, VS Code, Tensorboard, and many other tools/libraries into one Docker image.\n\n<a name=\"video\" />\n\n## Videos\n\n* [TensorFlow Guide 1](http://bit.ly/1OX8s8Y) - A guide to installation and use\n* [TensorFlow Guide 2](http://bit.ly/1R27Ki9) - Continuation of first video\n* [TensorFlow Basic Usage](http://bit.ly/1TCNmEY) - A guide going over basic usage\n* [TensorFlow Deep MNIST for Experts](http://bit.ly/1L9IfJx) - Goes over Deep MNIST\n* [TensorFlow Udacity Deep Learning](https://www.youtube.com/watch?v=ReaxoSIM5XQ) - Basic steps to install TensorFlow for free on the Cloud 9 online service with 1Gb of data\n* [Why Google wants everyone to have access to TensorFlow](http://video.foxnews.com/v/4611174773001/why-google-wants-everyone-to-have-access-to-tensorflow/?#sp=show-clips)\n* [Videos from TensorFlow Silicon Valley Meet Up 1/19/2016](http://blog.altoros.com/videos-from-tensorflow-silicon-valley-meetup-january-19-2016.html)\n* [Videos from TensorFlow Silicon Valley Meet Up 1/21/2016](http://blog.altoros.com/videos-from-tensorflow-seattle-meetup-jan-21-2016.html)\n* [Stanford CS224d Lecture 7 - Introduction to TensorFlow, 19th Apr 2016](https://www.youtube.com/watch?v=L8Y2_Cq2X5s&index=7&list=PLmImxx8Char9Ig0ZHSyTqGsdhb9weEGam) - CS224d Deep Learning for Natural Language Processing by Richard Socher\n* [Diving into Machine Learning through TensorFlow](https://youtu.be/GZBIPwdGtkk?list=PLBkISg6QfSX9HL6us70IBs9slFciFFa4W) - Pycon 2016 Portland Oregon, [Slide](https://storage.googleapis.com/amy-jo/talks/tf-workshop.pdf) & [Code](https://github.com/amygdala/tensorflow-workshop) by Julia Ferraioli, Amy Unruh, Eli Bixby\n* [Large Scale Deep Learning with TensorFlow](https://youtu.be/XYwIDn00PAo) - Spark Summit 2016 Keynote by Jeff Dean\n* [Tensorflow and deep learning - without at PhD](https://www.youtube.com/watch?v=vq2nnJ4g6N0) -  by Martin G\u00f6rner\n* [Tensorflow and deep learning - without at PhD, Part 2 (Google Cloud Next '17)](https://www.youtube.com/watch?v=fTUwdXUFfI8) -  by Martin G\u00f6rner\n* [Image recognition in Go using TensorFlow](https://youtu.be/P8MZ1Z2LHrw) -  by Alex Pliutau\n\n\n\n<a name=\"papers\" />\n\n## Papers\n\n* [TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems](http://download.tensorflow.org/paper/whitepaper2015.pdf) - This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google\n* [TensorFlow Estimators: Managing Simplicity vs. Flexibility in High-Level Machine Learning Frameworks](https://arxiv.org/pdf/1708.02637.pdf)\n* [TF.Learn: TensorFlow's High-level Module for Distributed Machine Learning](https://arxiv.org/abs/1612.04251)\n* [Comparative Study of Deep Learning Software Frameworks](http://arxiv.org/abs/1511.06435) - The study is performed on several types of deep learning architectures and we evaluate the performance of the above frameworks when employed on a single machine for both (multi-threaded) CPU and GPU (Nvidia Titan X) settings\n* [Distributed TensorFlow with MPI](http://arxiv.org/abs/1603.02339) - In this paper, we extend recently proposed Google TensorFlow for execution on large scale clusters using Message Passing Interface (MPI)\n* [Globally Normalized Transition-Based Neural Networks](http://arxiv.org/abs/1603.06042) - This paper describes the models behind [SyntaxNet](https://github.com/tensorflow/models/tree/master/syntaxnet).\n* [TensorFlow: A system for large-scale machine learning](https://arxiv.org/abs/1605.08695) - This paper describes the TensorFlow dataflow model in contrast to existing systems and demonstrate the compelling performance\n* [TensorLayer: A Versatile Library for Efficient Deep Learning Development](https://arxiv.org/abs/1707.08551) - This paper describes a versatile Python library that aims at helping researchers and engineers efficiently develop deep learning systems. (Winner of The Best Open Source Software Award of ACM MM 2017)\n\n<a name=\"blogs\" />\n\n## Official announcements\n\n* [TensorFlow: smarter machine learning, for everyone](https://googleblog.blogspot.com/2015/11/tensorflow-smarter-machine-learning-for.html) - An introduction to TensorFlow\n* [Announcing SyntaxNet: The World\u2019s Most Accurate Parser Goes Open Source](http://googleresearch.blogspot.com/2016/05/announcing-syntaxnet-worlds-most.html) - Release of SyntaxNet, \"an open-source neural network framework implemented in TensorFlow that provides a foundation for Natural Language Understanding systems.\n\n## Blog posts\n* [Official Tensorflow Blog](http://blog.tensorflow.org/)\n* [Why TensorFlow will change the Game for AI](https://archive.fo/o9asj)\n* [TensorFlow for Poets](http://petewarden.com/2016/02/28/tensorflow-for-poets) - Goes over the implementation of TensorFlow\n* [Introduction to Scikit Flow - Simplified Interface to TensorFlow](http://terrytangyuan.github.io/2016/03/14/scikit-flow-intro/) - Key Features Illustrated\n* [Building Machine Learning Estimator in TensorFlow](http://terrytangyuan.github.io/2016/07/08/understand-and-build-tensorflow-estimator/) - Understanding the Internals of TensorFlow Learn Estimators\n* [TensorFlow - Not Just For Deep Learning](http://terrytangyuan.github.io/2016/08/06/tensorflow-not-just-deep-learning/)\n* [The indico Machine Learning Team's take on TensorFlow](https://indico.io/blog/indico-tensorflow)\n* [The Good, Bad, & Ugly of TensorFlow](https://indico.io/blog/the-good-bad-ugly-of-tensorflow/) - A survey of six months rapid evolution (+ tips/hacks and code to fix the ugly stuff), Dan Kuster at Indico, May 9, 2016\n* [Fizz Buzz in TensorFlow](http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/) - A joke by Joel Grus\n* [RNNs In TensorFlow, A Practical Guide And Undocumented Features](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/) - Step-by-step guide with full code examples on GitHub.\n* [Using TensorBoard to Visualize Image Classification Retraining in TensorFlow](http://maxmelnick.com/2016/07/04/visualizing-tensorflow-retrain.html)\n* [TFRecords Guide](http://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/21/tfrecords-guide/) semantic segmentation and handling the TFRecord file format.\n* [TensorFlow Android Guide](https://blog.mindorks.com/android-tensorflow-machine-learning-example-ff0e9b2654cc) - Android TensorFlow Machine Learning Example.\n* [TensorFlow Optimizations on Modern Intel\u00ae Architecture](https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture) - Introduces TensorFlow optimizations on Intel\u00ae Xeon\u00ae and Intel\u00ae Xeon Phi\u2122 processor-based platforms based on an Intel/Google collaboration.\n* [Coca-Cola's Image Recognition App](https://developers.googleblog.com/2017/09/how-machine-learning-with-tensorflow.html) Coca-Cola's product code image recognizing neural network with user input feedback loop.\n* [How Does The TensorFlow Work](https://www.letslearnai.com/2018/02/02/how-does-the-machine-learning-library-tensorflow-work.html) How Does The Machine Learning Library TensorFlow Work?\n\n\n<a name=\"community\" />\n\n## Community\n\n* [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow)\n* [@TensorFlow on Twitter](https://twitter.com/tensorflow)\n* [Reddit](https://www.reddit.com/r/tensorflow)\n* [Mailing List](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss)\n\n\n<a name=\"books\" />\n\n## Books\n\n* [Machine Learning with TensorFlow](http://tensorflowbook.com) by Nishant Shukla, computer vision researcher at UCLA and author of Haskell Data Analysis Cookbook. This book makes the math-heavy topic of ML approachable and practicle to a newcomer. \n* [First Contact with TensorFlow](http://www.jorditorres.org/first-contact-with-tensorflow/) by Jordi Torres, professor at UPC Barcelona Tech and a research manager and senior advisor at Barcelona Supercomputing Center\n* [Deep Learning with Python](https://machinelearningmastery.com/deep-learning-with-python/) - Develop Deep Learning Models on Theano and TensorFlow Using Keras by Jason Brownlee\n* [TensorFlow for Machine Intelligence](https://bleedingedgepress.com/tensor-flow-for-machine-intelligence/) - Complete guide to use TensorFlow from the basics of graph computing, to deep learning models to using it in production environments - Bleeding Edge Press\n* [Getting Started with TensorFlow](https://www.packtpub.com/big-data-and-business-intelligence/getting-started-tensorflow) - Get up and running with the latest numerical computing library by Google and dive deeper into your data, by Giancarlo Zaccone\n* [Hands-On Machine Learning with Scikit-Learn and TensorFlow](http://shop.oreilly.com/product/0636920052289.do) \u2013 by Aur\u00e9lien Geron, former lead of the YouTube video classification team. Covers ML fundamentals, training and deploying deep nets across multiple servers and GPUs using TensorFlow, the latest CNN, RNN and Autoencoder architectures, and Reinforcement Learning (Deep Q).\n* [Building Machine Learning Projects with Tensorflow](https://www.packtpub.com/big-data-and-business-intelligence/building-machine-learning-projects-tensorflow) \u2013 by Rodolfo Bonnin. This book covers various projects in TensorFlow that expose what can be done with TensorFlow in different scenarios. The book provides projects on training models, machine learning, deep learning, and working with various neural networks. Each project is an engaging and insightful exercise that will teach you how to use TensorFlow and show you how layers of data can be explored by working with Tensors.\n* [Deep Learning using TensorLayer](http://www.broadview.com.cn/book/5059) - by Hao Dong et al. This book covers both deep learning and the implmentation by using TensorFlow and TensorLayer.\n\n\n\n<a name=\"contributions\" />\n\n## Contributions\n\nYour contributions are always welcome!\n\nIf you want to contribute to this list (please do), send me a pull request or contact me [@jtoy](https://twitter.com/jtoy)\nAlso, if you notice that any of the above listed repositories should be deprecated, due to any of the following reasons:\n\n* Repository's owner explicitly say that \"this library is not maintained\".\n* Not committed for long time (2~3 years).\n\nMore info on the [guidelines](https://github.com/jtoy/awesome-tensorflow/blob/master/contributing.md)\n\n\n<a name=\"credits\" />\n\n## Credits\n\n* Some of the python libraries were cut-and-pasted from [vinta](https://github.com/vinta/awesome-python)\n* The few go reference I found where pulled from [this page](https://code.google.com/p/go-wiki/wiki/Projects#Machine_Learning)\n\n"
 },
 {
  "repo": "nlintz/TensorFlow-Tutorials",
  "language": "Jupyter Notebook",
  "readme_contents": "# TensorFlow-Tutorials\n[![Build Status](https://travis-ci.org/nlintz/TensorFlow-Tutorials.svg?branch=master)](https://travis-ci.org/nlintz/TensorFlow-Tutorials)\n[![Codacy Badge](https://api.codacy.com/project/badge/grade/2d3ed69cdbec4249ab5c2f7e4286bb8f)](https://www.codacy.com/app/hunkim/TensorFlow-Tutorials)\n\nIntroduction to deep learning based on Google's TensorFlow framework. These tutorials are direct ports of\nNewmu's [Theano Tutorials](https://github.com/Newmu/Theano-Tutorials).\n\n***Topics***\n* [Simple Multiplication](00_multiply.py)\n* [Linear Regression](01_linear_regression.py)\n* [Logistic Regression](02_logistic_regression.py)\n* [Feedforward Neural Network (Multilayer Perceptron)](03_net.py)\n* [Deep Feedforward Neural Network (Multilayer Perceptron with 2 Hidden Layers O.o)](04_modern_net.py)\n* [Convolutional Neural Network](05_convolutional_net.py)\n* [Denoising Autoencoder](06_autoencoder.py)\n* [Recurrent Neural Network (LSTM)](07_lstm.py)\n* [Word2vec](08_word2vec.py)\n* [TensorBoard](09_tensorboard.py)\n* [Save and restore net](10_save_restore_net.py)\n* [Generative Adversarial Network](11_gan.py)\n\n***Dependencies***\n* TensorFlow 1.0 alpha\n* Numpy\n* matplotlib\n"
 },
 {
  "repo": "pkmital/tensorflow_tutorials",
  "language": "Jupyter Notebook",
  "readme_contents": "# UPDATE (July 12, 2016)\n\nNew **free MOOC course** covering all of this material in much more depth, as well as much more including combined variational autoencoders + generative adversarial networks, visualizing gradients, deep dream, style net, and recurrent networks: **https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow-i/info**\n\n# TensorFlow Tutorials\n\nYou can find python source code under the `python` directory, and associated notebooks under `notebooks`.\n\n| | Source code | Description |\n| --- | --- | --- |\n|1| **[basics.py](python/01_basics.py)** | Setup with tensorflow and graph computation.|\n|2| **[linear_regression.py](python/02_linear_regression.py)** | Performing regression with a single factor and bias. |\n|3| **[polynomial_regression.py](python/03_polynomial_regression.py)** | Performing regression using polynomial factors.|\n|4| **[logistic_regression.py](python/04_logistic_regression.py)** | Performing logistic regression using a single layer neural network.|\n|5| **[basic_convnet.py](python/05_basic_convnet.py)** | Building a deep convolutional neural network.|\n|6| **[modern_convnet.py](python/06_modern_convnet.py)** | Building a deep convolutional neural network with batch normalization and leaky rectifiers.|\n|7| **[autoencoder.py](python/07_autoencoder.py)** | Building a deep autoencoder with tied weights.|\n|8| **[denoising_autoencoder.py](python/08_denoising_autoencoder.py)** | Building a deep denoising autoencoder which corrupts the input.|\n|9| **[convolutional_autoencoder.py](python/09_convolutional_autoencoder.py)** | Building a deep convolutional autoencoder.|\n|10| **[residual_network.py](python/10_residual_network.py)** | Building a deep residual network.|\n|11| **[variational_autoencoder.py](python/11_variational_autoencoder.py)** | Building an autoencoder with a variational encoding.|\n\n# Installation Guides\n\n* [TensorFlow Installation](https://github.com/tensorflow/tensorflow)\n* [OS specific setup](https://github.com/tensorflow/tensorFlow/blob/master/tensorflow/g3doc/get_started/os_setup.md)\n* [Installation on EC2 GPU Instances](http://eatcodeplay.com/installing-gpu-enabled-tensorflow-with-python-3-4-in-ec2/)\n\nFor Ubuntu users using python3.4+ w/ CUDA 7.5 and cuDNN 7.0, you can find compiled wheels under the `wheels` directory.  Use `pip3 install tensorflow-0.8.0rc0-py3-none-any.whl` to install, e.g. and be sure to add: `export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64\"\n` to your `.bashrc`.  Note, this still requires you to install CUDA 7.5 and cuDNN 7.0 under `/usr/local/cuda`.\n\n# Resources\n\n* [Official Tensorflow Tutorials](https://www.tensorflow.org/versions/r0.7/tutorials/index.html)\n* [Tensorflow API](https://www.tensorflow.org/versions/r0.7/api_docs/python/index.html)\n* [Tensorflow Google Groups](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss)\n* [More Tutorials](https://github.com/nlintz/TensorFlow-Tutorials)\n\n# Author\n\nParag K. Mital, Jan. 2016.\n\nhttp://pkmital.com\n\n# License\n\nSee LICENSE.md\n"
 },
 {
  "repo": "tomgenoni/cssdig",
  "language": "Python",
  "readme_contents": "# CSS Dig\n\n**Note: There is now a [CSS Dig Chrome Extension](https://chrome.google.com/webstore/detail/css-dig/lpnhmlhomomelfkcjnkcacofhmggjmco) that duplicates most of this functionality and is much easier to use.**\n\nInspired by Nicolle Sullivan\u2019s advice to \u201cgrep your styles\u201d, CSS Dig is is a Python script that runs locally to  unearth properties and values from almost any website \u2014 from both linked CSS files as well as any styles found in the head \u2014 to help you analyze, refactor, standardize and maintain your CSS.\n\nSee [CSS Dig: It\u2019s Time to Refactor](http://www.atomeye.com/css-dig.html) for more context.\n\nSome sample reports:\n\n<ul>\n  <li><a href=\"http://www.atomeye.com/dig-reports/nytimes.html\">Report for http://nytimes.com</a></li>\n  <li><a href=\"http://www.atomeye.com/dig-reports/nytimes-world.html\">Report for http://nytimes.com/pages/world/index.html</a></li>\n  <li><a href=\"http://www.atomeye.com/dig-reports/msn.html\">Report for http://msn.com</a></li>\n  <li><a href=\"http://www.atomeye.com/dig-reports/aol.html\">Report for http://aol.com</a></li>\n  <li><a href=\"http://www.atomeye.com/dig-reports/apple.html\">Report for http://apple.com</a></li>\n</ul>\n\n\n## What it Does\n\n1. CSS Dig will look for `<link>` tags with `rel=\"stylesheet\"` at the URL you provide as well as any `<style>` blocks on the page.\n2. It reads in and combines that CSS and then finds all unique properties, using those to find all the declarations.\n3. From that it creates the counts and groups them for viewing and inspecting.\n\n## Requirements\n\n\nYou'll need at least Python 2.7. Run:\n\n    python -V\n\nin the command line if you're unsure. You should see something like:\n\n    Python 2.7.2\n\nPython modules you\u2019ll likely need to install.\n\n- [BeautifulSoup 4](http://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n\nIf you've never used Python before probably the easiest way to get modules installed is to use Easy Install:\n\n- [Easy Install](https://pypi.python.org/pypi/setuptools/0.7.2#installation-instructions)\n\nAfter that completes you can run:\n\n    easy_install beautifulsoup4\n\nor\n\n    sudo easy_install beautifulsoup4\n\n\n## Installation and Usage\n\nClone or download this repository to your computer. Open a terminal window and navigate to this folder. Run the following:\n\n    python cssdig.py http://atomeye.com\n\n...replacing http://atomeye.com with any URL you'd like to run a report on.\n\nIf it runs successfully you should see the following:\n\n    Attempting to reach URL...\n    Finding CSS at URL...\n    Building report...\n    Report complete.\n\nYou'll find your report inside a 'report' folder that will be created in the same directory. That 'report' directory is self-contained so you copy it, archive it, etc.\n\nIf you get errors about 'No module found...' please install any missing modules.\n\n## Limitations\n\n- It currently only works with http:// URLs that return HTML. It won't yet work against files on your computer or individual CSS files (like http://domain.com/style.css).\n- There are a number of regular expressions that work with the Combined CSS in the third panel. This was done so that it would be readable but would leave any original errors in place. It's not perfect but seems to work well enough. If you see any strangeness let me know.\n- In the script I've excluded some URLs from font providers so it won't be mixed in with the data. You can edit this list as needed.\n\n## Please Contribute\n\nI am neither a Python programmer nor a regex master so there are likely improvements to be made. I'd also love to see this built into a proper web app so it would be dead simple to use.\n"
 },
 {
  "repo": "heynemann/pyccuracy",
  "language": "Python",
  "readme_contents": "h1. PYCCURACY IS LOOKING FOR A NEW MAINTAINER!\n\nIf you wish to be the new maintainer leave a message in an issue.\n\nh1. Pyccuracy\n\n\"Pyccuracy\":https://github.com/heynemann/pyccuracy/wiki is a \"Behaviour-Driven-Development-style\":http://behaviour-driven.org tool written in Python that aims to make it easier to write automated acceptance tests. It improves the readability of those tests by using a structured natural language - and a simple mechanism to extend this language - so that both developers and customers can collaborate and understand what the tests do.\n\nh2. Help\n\nPlease check \"our documentation\":https://github.com/heynemann/pyccuracy/wiki. For quick usage help use the *pyccuracy_help* command line tool.\n\nh2. Mailing list\n\nJoin the \"Pyccuracy mailing list at Google Groups\":http://groups.google.com/group/pyccuracy to discuss and get support from the community and team.\n\nh2. Get in touch with the team\n\nIf you have further questions, please contact the team:\n\n  * \"Bernardo Heynemann\":http://github.com/heynemann (\"@heynemann\":http://twitter.com/heynemann)\n  * \"Claudio Figueiredo\":http://github.com/jcfigueiredo (\"@jcfigueiredo\":http://twitter.com/jcfigueiredo)\n  * \"Gabriel Falc\u00e3o\":http://github.com/gabrielfalcao (\"@gabrielfalcao\":http://twitter.com/gabrielfalcao)\n  * \"Guilherme Chapiewski\":http://github.com/guilhermechapiewski (\"@gchapiewski\":http://twitter.com/gchapiewski)\n"
 },
 {
  "repo": "tmm1/gdb.rb",
  "language": "Python",
  "readme_contents": "gdb7 hooks for MRI\n  (c) 2009 Aman Gupta (tmm1)\n\n=== Requirements\n\n  gdb.rb currently requires x86_64 linux, and MRI/REE 1.8.x or 1.9.2.\n\n=== Usage\n\n  $ ps aux | grep deploy.rb\n  13074  0.0  0.7  90164 31720 ?        R    Sep23   2:21 /usr/bin/ruby /usr/bin/god -c deploy.rb\n\n  $ sudo gdb.rb 13074\n  GNU gdb (GDB) 7.0\n  Reading symbols from /usr/bin/ruby...done.\n  Attaching to program: /usr/bin/ruby, process 13074\n  0x00007fa8b9cb3c93 in select () from /lib/libc.so.6\n\n  (gdb) ruby eval 1+2\n  2\n\n  (gdb) ruby eval Thread.list.count\n  17\n\n  (gdb) ruby threads list\n  0x1589000 main      thread  THREAD_STOPPED   WAIT_JOIN(0x19ef400)           4417 bytes\n  0x19ef400           thread  THREAD_STOPPED   WAIT_TIME(57.10)               6267 bytes\n  0x19e3400           thread  THREAD_STOPPED   WAIT_FD(5)                    10405 bytes\n  0x19e0000           thread  THREAD_STOPPED   WAIT_NONE                     14237 bytes\n  0x19e0400           thread  THREAD_STOPPED   WAIT_NONE                     14237 bytes\n  0x19e0800           thread  THREAD_STOPPED   WAIT_NONE                     14237 bytes\n  0x19e0c00           thread  THREAD_STOPPED   WAIT_NONE                     14237 bytes\n  0x19de000           thread  THREAD_STOPPED   WAIT_NONE                     14237 bytes\n  0x19de400           thread  THREAD_STOPPED   WAIT_NONE                     14237 bytes\n  0x19de800           thread  THREAD_STOPPED   WAIT_NONE                     14237 bytes\n  0x19dec00           thread  THREAD_STOPPED   WAIT_NONE                     14237 bytes\n  0x19dc000           thread  THREAD_STOPPED   WAIT_NONE                     14237 bytes\n  0x19dc400           thread  THREAD_STOPPED   WAIT_NONE                     14237 bytes\n  0x19dc800           thread  THREAD_STOPPED   WAIT_NONE                     14237 bytes\n  0x19dcc00           thread  THREAD_STOPPED   WAIT_NONE                     14237 bytes\n  0x2266800           thread  THREAD_STOPPED   WAIT_NONE                     14237 bytes\n  0x1d63000      curr thread  THREAD_RUNNABLE  WAIT_NONE\n\n  (gdb) ruby threads\n  0x1589000 main      thread  THREAD_STOPPED   WAIT_JOIN(0x19ef400)           4417 bytes\n         node_call      join in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god.rb:626\n         node_call      start in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god.rb:633\n         node_call      at_exit in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god.rb:666\n         node_fcall     (unknown) in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/cli/run.rb:87\n\n  0x19ef400           thread  THREAD_STOPPED   WAIT_TIME(45.25)               6267 bytes\n         node_fcall     sleep in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god.rb:622\n         node_fcall     start in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god.rb:621\n         node_fcall     loop in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god.rb:621\n         node_call      start in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god.rb:620\n         node_call      initialize in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god.rb:620\n         node_call      new in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god.rb:620\n         node_call      start in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god.rb:633\n         node_call      at_exit in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god.rb:666\n         node_fcall     (unknown) in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/cli/run.rb:87\n\n  0x19e3400           thread  THREAD_STOPPED   WAIT_FD(5)                    10405 bytes\n         node_call      accept in /usr/lib/ruby/1.8/drb/unix.rb:98\n         node_call      accept in /usr/lib/ruby/1.8/drb/drb.rb:1581\n         node_vcall     main_loop in /usr/lib/ruby/1.8/drb/drb.rb:1430\n         node_call      run in /usr/lib/ruby/1.8/drb/drb.rb:1427\n         node_call      start in /usr/lib/ruby/1.8/drb/drb.rb:1427\n         node_vcall     run in /usr/lib/ruby/1.8/drb/drb.rb:1347\n         node_call      initialize in /usr/lib/ruby/1.8/drb/drb.rb:1627\n\n  (gdb) ruby threads trace\n  -- Tracing thread context switches.\n  0x19ef400           thread  THREAD_RUNNABLE  WAIT_NONE                      6267 bytes\n  0x1d63000           thread  THREAD_RUNNABLE  WAIT_NONE                      8623 bytes\n  0x19e3400           thread  THREAD_RUNNABLE  WAIT_NONE                     10405 bytes\n  0x3001c00           thread  THREAD_RUNNABLE  WAIT_NONE                      9507 bytes\n  0x19e3400           thread  THREAD_RUNNABLE  WAIT_NONE                      9507 bytes\n  0x3001c00           thread  THREAD_RUNNABLE  WAIT_NONE                     15059 bytes\n  0x1d63000           thread  THREAD_RUNNABLE  WAIT_NONE                      8623 bytes\n  0x3001c00           thread  THREAD_RUNNABLE  WAIT_NONE                     15059 bytes\n\n  (gdb) ruby trace 25\n  -- Tracing the next 25 ruby method calls.\n  handle_events in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:66\n  watching_pid? in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:58\n  [] in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:59\n  default in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:59\n  handle_events in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:66\n  watching_pid? in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:58\n  [] in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:59\n  default in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:59\n  handle_events in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:66\n  watching_pid? in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:58\n  [] in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:59\n  default in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:59\n  handle_events in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:66\n  handle_events in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:66\n  handle_events in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:66\n  handle_events in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:66\n  handle_events in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:66\n  handle_events in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:66\n  handle_events in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:66\n  handle_events in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:66\n  handle_events in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:66\n  watching_pid? in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:58\n  [] in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:59\n  default in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:59\n  handle_events in /usr/lib/ruby/gems/1.8/gems/god-0.7.13/bin/../lib/god/event_handler.rb:66\n\n  (gdb) ruby objects\n\n    HEAPS            8\n    SLOTS      1686252\n    LIVE        893327 (52.98%)\n    FREE        792925 (47.02%)\n\n    bignum           2 (0.00%)\n    file            51 (0.01%)\n    float           77 (0.01%)\n    module         781 (0.09%)\n    varmap        1354 (0.15%)\n    scope         1641 (0.18%)\n    match         1676 (0.19%)\n    iclass        1744 (0.20%)\n    regexp        2255 (0.25%)\n    data          3539 (0.40%)\n    class         3680 (0.41%)\n    hash          6196 (0.69%)\n    object        8785 (0.98%)\n    array        13850 (1.55%)\n    string      105350 (11.79%)\n    node        742346 (83.10%)\n\n  (gdb) ruby objects classes\n         1 YAML::Syck::Resolver\n         1 YAML::Syck::Resolver\n         1 SystemStackError\n         1 Object\n         1 Object\n         1 NoMemoryError\n         1 fatal\n         1 Object\n         1 Gem::SourceIndex\n         1 Gem::GemPathSearcher\n         1 Gem::ConfigFile\n         1 God::Logger\n         1 God::SimpleLogger\n         1 God::CLI::Run\n         1 DRb::DRbServer::InvokeMethod\n         1 DRb::DRbServer\n         1 DRb::DRbIdConv\n         1 God::Registry\n         1 OptionParser::Switch::OptionalArgument\n         1 OptionParser\n         1 God::Socket\n         1 God::Contacts::Email\n         2 Date::Infinity\n         2 DRb::DRbMessage\n         2 DRb::DRbBadScheme\n         2 DRb::DRbUNIXSocket\n         3 OptionParser::List\n         6 IOError\n         6 Errno::ESRCH\n         6 OptionParser::Switch::RequiredArgument\n         6 Net::InternetMessageIO\n         7 God::System::SlashProcPoller\n         7 God::System::Process\n         7 God::DriverOperation\n         7 Net::SMTP\n         8 StandardError\n        10 OptionParser::Switch::NoArgument\n        13 God::Conditions::Flapping\n        13 God::Conditions::ProcessExits\n        13 God::Behaviors::CleanPidFile\n        13 God::Process\n        13 God::Watch\n        13 God::Driver\n        13 God::DriverEventQueue\n        19 God::DriverEvent\n        38 Process::Status\n        39 God::Conditions::ProcessRunning\n        50 Range\n        63 Gem::Specification\n        65 God::Metric\n        81 Gem::Dependency\n       174 Gem::Version::Part\n       209 Gem::Requirement\n       219 Gem::Version\n\n  (gdb) ruby objects strings \n         70 u'bin'\n         73 u' INFO'\n         74 u'--main'\n         74 u'I'\n         75 u'..'\n         78 u'ruby'\n         80 u'::'\n         92 u'1.3.2'\n        100 u'README.rdoc'\n        102 u' '\n        108 u'Rakefile'\n        110 u'README'\n        114 u'\\r\\n'\n        127 u'>='\n        140 u'lib'\n        158 u'0'\n        294 u'\\n'\n        619 u''\n\n      30503 unique strings\n    3187435 bytes\n\n  (gdb) ruby objects nodes \n      8156 NODE_EVSTR\n      8966 NODE_COLON2\n     10020 NODE_DVAR\n     11493 NODE_AND\n     13157 NODE_FCALL\n     14493 NODE_VCALL\n     15148 NODE_ARGS\n     15180 NODE_CONST\n     15445 NODE_SCOPE\n     16582 NODE_IF\n     21594 NODE_LASGN\n     22084 NODE_METHOD\n     26633 NODE_STR\n     37819 NODE_LIT\n     64325 NODE_LVAR\n     64470 NODE_NEWLINE\n     69436 NODE_BLOCK\n     85825 NODE_CALL\n    126739 NODE_ARRAY\n\n\n=== TODO\n\n  `ruby where` for the current stack trace\n  `ruby print` to inspect ruby variables\n  `ruby breakpoint` to breakpoint on ruby methods\n  `ruby irb` for a simple interactive shell\n\n\n=== Credits\n\n  Tom Tromey for his work on gdb python support (gdb-eval.patch, gdb-breakpoints.patch) and help\n  tracking down some memory leaks (gdb-leak.patch)\n\n  Joe Damato for writing parts of ruby-gdb.py and various gdb hacks to make gdb.rb possible.\n\n\n=== License\n\n  Available under the Ruby License.\n\n"
 },
 {
  "repo": "binaryage/firelogger.py",
  "language": "Python",
  "readme_contents": "FirePython\n==========\n\nFirePython is a sexy Python logger console integrated into Firebug.\nVisit `firepython.binaryage.com`_.\n\n.. _firepython.binaryage.com: http://firepython.binaryage.com\n.. vim:filetype=rst\n"
 },
 {
  "repo": "fbzhong/sublime-jslint",
  "language": "Python",
  "readme_contents": "JSLint support for Sublime Text 2 by using jslint4java\n======================================================\n\n[Sublime Text 2](http://www.sublimetext.com/2) is a sophisticated text editor for code, HTML and prose. You'll love its slick user interface and extraordinary features.\n\n[JSLint4Java](http://code.google.com/p/jslint4java/) is a Java wrapper around the fabulous tool by Douglas Crockford, [JSLint](http://jslint.com). It provides a simple interface for detecting potential problems in JavaScript code.\n\nThis project is a plugin to add JSLint support for Sublime Text 2.\n\nFeatures\n--------\n\n* JSLint: Run JSLint (Ctrl+J), or run JSLint on save\n* JSLint: Show JSLint results\n* Highlight error line by click in the result view\n* Cross-platform: supports Windows, Linux and Mac OS X\n\nRequirements\n------------\n\nJava - also ensure that it has been added to PATH\n\nInstallation\n------------\n\n* Using [Package Control](http://wbond.net/sublime_packages/package_control):\n    * Install Package: sublime-jslint\n* Download and extract to Sublime Text 2 Packages folder\n    * Windows: %APPDATA%\\Sublime Text 2\\Packages\n    * Mac OS X: ~/Library/Application\\ Support/Sublime\\ Text\\ 2/Packages/\n    * Linux: ~/.config/sublime-text-2/Packages\n\nHow to use?\n-----------\n\nOpen the Command Palette (Windows and Linux: Ctrl+Shift+P, OSX: Command+Shift+P), then search for:\n\n* JSLint: Run JSLint (Ctrl+J)\n* JSLint: Show JSLint Result\n\nOpen up a .js file and hit Ctrl+J to run JSLint. An new output panel will appear giving you the JSLint results:\n\nScreenshots\n-----------\n\n![](https://github.com/fbzhong/sublime-jslint/raw/master/images/screenshot.png)\n\nSettings\n--------\n\nSettings can be opened via the Command Palette, or via the Preferences/Package Settings/JSLint/Settings \u2013 User menu entry.\n\n```javascript\n{\n    //Uses system installed jslint.js (node.js based), instead of bundled JSLint jar\n    \"use_node_jslint\": false,\n\n    //Path to the jslint.js\n    //Leave blank to use default JSLint path\n    \"node_jslint_path\": \"\",\n\n    //Options passed to jslint.js\n    \"node_jslint_options\": \"\",\n\n    //Path to the JSLint jar.\n    //Leave blank to use bundled jar.\n    \"jslint_jar\": \"\",\n\n    //Options passed to JSLint.\n    \"jslint_options\": \"\",\n\n    //Errors and RegEx to be ignored\n    \"ignore_errors\":\n    [\n        //\"Expected an identifier and instead saw 'undefined' \\(a reserved word\\)\"\n    ],\n\n    //Run JSLint on save.\n    \"run_on_save\": false,\n\n    //Debug flag.\n    \"debug\": false\n}\n```\n\nAll available jslint_options can be found [here](https://github.com/fbzhong/sublime-jslint/wiki/Available-jslint4java-options).\n\nLicense\n-------\n\nsublime-jslint is released under the New  BSD License, which may be found [here](https://github.com/fbzhong/sublime-jslint/blob/master/LICENSE.md)."
 },
 {
  "repo": "epio/mantrid",
  "language": "Python",
  "readme_contents": "Mantrid\n=======\n\nMantrid is the HTTP load balancer used for [Epio](https://www.ep.io). It is designed with high availability and simplicity in mind: it is configured at runtime with JSON over HTTP and can temporarily hold open connections while backend servers restart. It monitors bandwidth and connection statistics and is ideal for serving large numbers of hostnames.\n\nIt trades some raw speed for flexibility, but is still designed to be fast. Its aim is to have latency of no more than 10ms, and have no more than a 10% reduction in throughput.\n\nCompatibility\n-------------\n\nMantrid is designed to work with Python 2.6 or 2.7, and requires a Python implementation that supports greenlets (so either CPython or PyPy 1.7 and up).\n\nQuick start\n-----------\n\nInstall Mantrid:\n\n    $ sudo python setup.py install\n\nLaunch Mantrid with the default settings (listening on port 80, management on 8042):\n\n    $ sudo mantrid\n\nAdd a host:\n\n    $ mantrid-client set localhost static false type=test\n\nThen visit http://localhost/ to see the test page.\n\n\nConfiguration\n-------------\n\nMantrid is partially configured using a small configuration file (`/etc/mantrid/mantrid.conf`) which sets up basic things like ports to listen on. The hostnames and load balancing rules are configured at runtime using a REST API, and persisted to a state file on disk so they survive restarts.\n\nA command-line interface, `mantrid-client`, ships with Mantrid to make simple interactions with the API easier.\n\nContributing\n------------\n\nMantrid is released under a BSD (3-clause) license. Contributions are very welcome - come and chat with us in #epio on freenode! If you discover a security issue, please email <security@ep.io>.\n\n"
 },
 {
  "repo": "evannuil/aws-snapshot-tool",
  "language": "Python",
  "readme_contents": "aws-snapshot-tool\n=================\naws-snapshot-tool is a python script to make it easy to *roll snapshot of your EBS volumes*. \n\nSimply add a tag to each volume you want snapshots of, configure and install a cronjob for aws-snapshot-tool and you are off. It will even handle rolling snapshots on a day, week and year so that you can setup the retention policy to suit.\n\nFeatures:\n- *Python based*: Leverages boto and is easy to configure and install as a crontab\n- *Simple tag system*: Just add a tag to each of your EBS volumes you want snapshots of\n- *Configure retention policy*: Configure how many days, weeks, and month snapshots you want to retain\n- *SNS Notifications*: aws-snapshot-tool works with Amazon SNS our of the box, so you can be notified of snapshots\n\nUsage\n==========\n1. Install and configure Python and Boto (See: https://github.com/boto/boto)\n2. Create a SNS topic in AWS and copy the ARN into the config file\n3. Subscribe with a email address to the SNS topic\n4. Create a snapshot user in IAM and put the key and secret in the config file\n5. Create a security policy for this user (see the iam.policy.sample)\n6. Copy config.sample to config.py\n7. Decide how many versions of the snapshots you want for day/week/month and change this in config.py\n8. Change the Region and Endpoint for AWS in the config.py file\n9. Optionally specify a proxy if you need to, otherwise set it to '' in the config.py\n10. Give every Volume for which you want snapshots a Tag with a Key and a Value and put these in the config file. Default: \"MakeSnapshot\" and the value \"True\"\n11. Install the script in the cron: \n\n\t\t# chmod +x makesnapshots.py\n\t\t# crontab -e\n\t\t30 1 * * 1-5 /opt/aws-snapshot-tool/makesnapshots.py day\n\t\t30 2 * * 6 /opt/aws-snapshot-tool/makesnapshots.py week\n\t\t30 3 1 * * /opt/aws-snapshot-tool/makesnapshots.py month\n\nAdditional Notes\n=========\nThe user that executes the script needs the following policies: see iam.policy.sample\n"
 },
 {
  "repo": "nlloyd/SubliminalCollaborator",
  "language": "Python",
  "readme_contents": "SubliminalCollaborator\n======================\n\nSublime Text 2 Plugin for remote pair programming.\n\n## NEWS\n\n2014-01-15 : MASSIVE REFACTOR... and IRC ssl support\n\nAfter leaving this project for a while to work on other things I came to the conclusion that it was necessary to seriously refactor this plugin before moving forward on any new feature work, including Sublime Text 3 compatibility.\n\nAfter undoing much hackery I am personally satisfied that this is a more readily extensible project, instead of an embarrassing mess of callbacks I had trouble tracing :-)\n\nAlso I threw in IRC SSL support (SSL support on the peer-to-peer sharing session level still pending).\n\n2012-09-15 : Beta Release!\n\nThe feature set target for **beta** has been achieved!  You can now, once you are setup, do the following:\n\n* Connect to an IRC channel and find other SubliminalCollaborator clients to...\n    * Share a view over a direct connection (local/company network only, *NAT traversal is a planned feature*)\n    * Partner can see the file, including syntax coloring as was configured by the host at the time of send\n    * Host and Partner can highlight multiple sections of the file and see what the other is highlighting in real-time\n    * Partner's viewpoint follows that of the Host when the Host moves their viewpoint\n    * Host edits and cut, copy, and paste events are sent to the Partner\n    * Automatic resync if the Host and Partner view become out of sync\n    * Swap on demand sharing roles (initiated by either Host or Partner)\n\nAlso new, Google Group space for this project!  Since github doesn't offer much by way of a dialog with users and contributors we have [this Google Group](https://groups.google.com/forum/?fromgroups#!forum/subliminalcollaborator) setup for future use.\n\nHave a feature idea that you want to discuss? [Bring it up here](https://groups.google.com/forum/?fromgroups#!forum/subliminalcollaborator)\nHave a question about a feature? [Bring it up here](https://groups.google.com/forum/?fromgroups#!forum/subliminalcollaborator)\nHave a bug you want to report? [Bring it up here](https://groups.google.com/forum/?fromgroups#!forum/subliminalcollaborator)\nWant to hear about the latest that is happening on this project? [Read about it here!](https://groups.google.com/forum/?fromgroups#!forum/subliminalcollaborator)\n\n\n## Roadmap\n\n[Planned Milestones](https://github.com/nlloyd/SubliminalCollaborator/issues/milestones)\n[Issues](https://github.com/nlloyd/SubliminalCollaborator/issues?labels=&milestone=&page=1&state=open)\n\n## Getting Started\n\n### Setup and Configuration\n\n1. git clone into your Packages directory or install using [Sublime Package Control](http://wbond.net/sublime_packages/package_control).\n1. Start Sublime Text 2\n1. To generate a basic User/Accounts.sublime-settings file, select menu **Preferences > Package Settings > SubliminalCollaborator > Settings - User**\n1. Uncomment the following and fill in with the details for your chosen IRC server (can have multiple under \"irc\")... then save.\n\n```javascript\n// \"irc\": [\n//     {\n//         \"host\": \"irc.somewhere.com\",\n//         \"port\": 6667,\n//         \"useSSL\": false,\n//         \"username\": \"\",\n//         \"password\": \"\",\n//         \"channel\": \"subliminalcollaboration\"\n//     }\n// ],\n```\n\n#### Install/Uninstall Cut, Copy, Paste Proxy\n\nIn order to share cut, copy, and paste events in a session some special setup is required...\n\n1. Select the menu **Preferences > Package Settings > SubliminalCollaborator > Install Edit Menu Proxy Commands** or in the command palette select `Collaborate: Install Edit Menu Proxy Commands`.\n\nTo undo this action, choose `Uninstall Edit Menu Proxy Commands` in the same places.\n\n### Starting a Session\n\n1. From the command palette: `Collaborate: Connect to Chat Server`\n1. Select the representative chat config string of your choice (protocol|host:port is the format)\n1. Once connected... from the command palette: `Collaborate: Start New Session`\n1. Choose the chat connection to use, then the username from the list of the known confirmed SubliminalCollaborator clients available through this chat\n    * At this point a dialog between clients is initiated where the two peers attempt to connect directly using the available IP addresses of the peer hosting the session... this may take a while... command/ctrl + ~ to see what is actually going on or just follow along with the updates in the status bar.\n1. Choose a view to share from the presented list of open views.\n\n\n### Interacting in a Session\n\nNow that you are in a session you can do the following as the host:\n\n- Edit the shared view in almost any way (typical edits such as entering and deleting text as well as cut, copy, and paste).\n- Scroll the view and the peer will see what you see.\n- Any selections are specially highlighted on the peer's view.\n- If the view gets out-of-sync for any reason then an automatic resync will occur with minimal interruption (you as the host won't see anything except a brief message in the status bar).\n- At any time from the command palette choose Collaborate: Swap Roles with Peer to request a role change and, if your peer accepts, then they will become the host and you the watching peer.\n\nIf you are not the host you can:\n\n- See what the host sees, and while they are not moving the view you may freely scroll independently.\n- Highlight regions of interest and those regions will be specially highlighted in the host's view.\n- See highlighted regions of the host's view.\n- Request through the command palette to swap roles with the host via the Collaborate: Swap Roles with Peer command.\n\n## Troubleshooting\n\n#### Partner view stutters periodically\n\nThis is a known issue with the automatic resync mechanism.  Basically chunks of the view are sent very quickly to be updated on the partner side, and the view attempts to follow the location of these edits.  Work is ongoing to find a solution to this.\n\n#### Cut, Copy, and Paste no longer working\n\nSomething bad happened with the plugin, you haven't installed the command proxy as instructed below, or you uninstalled the plugin without uninstalling the command proxy.  For most cases you can simply copy the file, if it exists, from **~/.subliminal_collaborator/Main.sublime-menu.backup** into **path/to/SublimeText/Packages/Default**, renaming it and overwriting **Main.sublime-menu**.\n\n\nShow your support by donating!\n\n<a href='http://www.pledgie.com/campaigns/17989'><img alt='Click here to lend your support to: SubliminalCollaborator and make a donation at www.pledgie.com !' src='http://www.pledgie.com/campaigns/17989.png?skin_name=chrome' border='0' /></a>\n\n\n## License\n\nAll of SubliminalCollaborator is licensed under the MIT license.\n\n  Copyright (c) 2012 Nick Lloyd\n\n  Permission is hereby granted, free of charge, to any person obtaining a copy\n  of this software and associated documentation files (the \"Software\"), to deal\n  in the Software without restriction, including without limitation the rights\n  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n  copies of the Software, and to permit persons to whom the Software is\n  furnished to do so, subject to the following conditions:\n\n  The above copyright notice and this permission notice shall be included in\n  all copies or substantial portions of the Software.\n\n  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n  THE SOFTWARE.\n  \n"
 },
 {
  "repo": "rayokota/generator-angular-flask",
  "language": "Python",
  "readme_contents": "# The Angular-Flask generator \n\nA [Yeoman](http://yeoman.io) generator for [AngularJS](http://angularjs.org) and [Flask](http://flask.pocoo.org).\n\nFlask is a Python-based micro-framework.  For AngularJS integration with other micro-frameworks, see https://github.com/rayokota/MicroFrameworkRosettaStone.\n\n## Installation\n\nInstall [Git](http://git-scm.com), [node.js](http://nodejs.org), and [Python 2.7](http://www.python.org/).  The development mode also requires [SQLite](http://www.sqlite.org).\n\nInstall Yeoman:\n\n    npm install -g yo\n\nInstall the Angular-Flask generator:\n\n    npm install -g generator-angular-flask\n\nThe above prerequisites can be installed to a VM using the [Angular-Flask provisioner](https://github.com/rayokota/provision-angular-flask).\n\t\n## Creating a Flask service\n\nIn a new directory, generate the service:\n\n    yo angular-flask\n\nInstall a virtual environment in new `flask` directory using `install.sh` (or `install.bat` for Windows):\n\n\t./install.sh\n\t\nRun the service:\n\n    flask/bin/python run.py\n\nYour service will run at [http://localhost:5000](http://localhost:5000).\n\n\n## Creating a persistent entity\n\nGenerate the entity:\n\n    yo angular-flask:entity [myentity]\n\nYou will be asked to specify attributes for the entity, where each attribute has the following:\n\n- a name\n- a type (String, Integer, Float, Boolean, Date, Enum)\n- for a String attribute, an optional minimum and maximum length\n- for a numeric attribute, an optional minimum and maximum value\n- for a Date attribute, an optional constraint to either past values or future values\n- for an Enum attribute, a list of enumerated values\n- whether the attribute is required\n\nFiles that are regenerated will appear as conflicts.  Allow the generator to overwrite these files as long as no custom changes have been made.\n\nCreate the database as described in [this blog](http://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-iv-database).\n\n    flask/bin/python db_create.py\n\nRun the service:\n\n    flask/bin/python run.py\n    \nA client-side AngularJS application will now be available by running\n\n\tgrunt server\n\t\nThe Grunt server will run at [http://localhost:9000](http://localhost:9000).  It will proxy REST requests to the Flask service running at [http://localhost:5000](http://localhost:5000).\n\nAt this point you should be able to navigate to a page to manage your persistent entities.  \n\nThe Grunt server supports hot reloading of client-side HTML/CSS/Javascript file changes.\n\n"
 },
 {
  "repo": "samuel/kokki",
  "language": "Python",
  "readme_contents": "\nOverview\n========\n\nKokki is a system configuration management framework styled after Chef. It can\nbe used to build a full configuration system, but it also includes a basic\ncommand line interface for simple uses.\n\nDocumentation\n-------------\n\nhttp://github.com/samuel/kokki/wiki\n\nMailing List\n------------\n\nhttps://convore.com/kokki/\n"
 },
 {
  "repo": "tspurway/hustle",
  "language": "Python",
  "readme_contents": "![Hustle](doc/_static/hustle.png)\n\nA column oriented, embarrassingly distributed, relational event database.\n\nFeatures\n--------\n\n* column oriented - super fast queries\n* events - write only semantics\n* distributed insert - designed for petabyte scale distributed datasets with massive write loads\n* compressed - bitmap indexes, lz4, and prefix trie compression\n* relational - join gigantic data sets\n* partitioned - smart shards\n* embarrassingly distributed ([based on Disco](http://discoproject.org/))\n* embarrassingly fast ([uses LMDB](http://symas.com/mdb/))\n* NoSQL - Python DSL\n* bulk append only semantics\n* highly available, horizontally scalable\n* REPL/CLI query interface\n\nExample Query\n-------------\n\n```\nselect(impressions.ad_id, impressions.date, h_sum(pix.amount), h_count(),\n       where=((impressions.date < '2014-01-13') & (impressions.ad_id == 30010),\n               pix.date < '2014-01-13'),\n       join=(impressions.site_id, pix.site_id),\n       order_by=impressions.date)\n```\n\n\nInstallation\n------------\n\nAfter cloning this repo, here are some considerations:\n\n* you will need Python 2.7 or higher - note that it *probably* won't work on 2.6 (has to do with pickling lambdas...)\n* you need to install Disco 0.5 and its dependencies - get that working first\n* you need to install Hustle and its 'deps' thusly:\n\n```\ncd hustle\nsudo ./bootstrap.sh\n```\n\nPlease refer to the [Installation Guide](http://tspurway.github.io/hustle/start/install.html) for more details\n\nDocumentation\n-------------\n\n[Hustle User Guide](http://tspurway.github.io/hustle/)\n\n[Hustle Mailing List](http://groups.google.com/group/hustle-users)\n\nCredits\n-------\n\nSpecial thanks to following open-source projects:\n\n* [EWAHBoolArray](https://github.com/lemire/EWAHBoolArray)\n* [disco](http://discoproject.org/)\n* [liblmdb](http://symas.com/mdb/)\n* [lz4](https://code.google.com/p/lz4/)\n* [ultrajson](https://github.com/esnme/ultrajson)\n\n[![Build Status](https://travis-ci.org/tspurway/hustle.svg?branch=master)](https://travis-ci.org/tspurway/hustle)\n"
 },
 {
  "repo": "jamesgolick/timeline_fu",
  "language": "Ruby",
  "readme_contents": "= TimelineFu\n\nEasily build timelines, much like GitHub's news feed.\n\n== Usage\n\nTimelineFu requires you to have a TimelineEvent model. \nThe simplest way is to use the generator.\n\n  $ script/generate timeline_fu && rake db:migrate\n        exists  db/migrate\n        create  db/migrate/20090333222034_create_timeline_events.rb\n        create  app/models/timeline_event.rb\n  # Migration blabber...\n\nNext step is to determine what generates an event in your various models.\n\n  class Post < ActiveRecord::Base\n    #...\n    belongs_to :author, :class_name => 'Person'\n    fires :new_post, :on    => :create,\n                     :actor => :author\n  end\n\nYou can add 'fires' statements to as many models as you want on as many models\nas you want. \n\nThey are hooked for you after standard ActiveRecord events. In\nthe previous example, it's an after_create on Posts. \n\n=== Parameters for #fires\n\nYou can supply a few parameters to fires, two of them are mandatory.\n- the first param is a custom name for the event type. It'll be your way of figuring out what events your reading back from the timeline_events table later.\n  - :new_post in the example\n\nThe rest all fit neatly in an options hash.\n\n- :on => [ActiveRecord event] \n  - mandatory. You use it to specify whether you want the event created after a create, update or destroy. You can also supply an array of events, e.g. [:create, :update].\n- :actor is your way of specifying who took this action.\n  - In the example, post.author is going to be this person.\n- :subject is automatically set to self, which is good most of the time.  You can however override it if you need to, using :subject.\n- :secondary_subject can let you specify something else that's related to the event. A comment to a blog post would be a good example.\n- :if => symbol or proc/lambda lets you put conditions on when a TimelineEvent is created. It's passed right to the after_xxx ActiveRecord event hook, so it's has the same behavior.\n\nHere's another example:\n\n  class Comment < ActiveRecord::Base\n    #...\n    belongs_to :commenter, :class_name => 'Person'\n    belongs_to :post\n    fires :new_comment, :on                 => :create,\n                        :actor              => :commenter,\n                        #implicit :subject  => self,\n                        :secondary_subject  => 'post',\n                        :if => lambda { |comment| comment.commenter != comment.post.author }\n  end\n\n=== TimelineEvent instantiation\n\nThe ActiveRecord event hook will automatically instantiate a \nTimelineEvent instance for you.\nIt will receive the following parameters in #create!\n\n- event_type \n  - \"new_comment\" in the comment example\n- actor \n  - the commenter\n- subject\n  - the comment instance\n- secondary_subject\n  - the post instance\n\nThe generated model stores most of its info as polymorphic relationships.\n\n  class TimelineEvent < ActiveRecord::Base\n    belongs_to :actor,              :polymorphic => true\n    belongs_to :subject,            :polymorphic => true\n    belongs_to :secondary_subject,  :polymorphic => true\n  end\n\n== How you actually get your timeline\n\nTo get your timeline you'll probably have to create your own finder SQL or scopes \n(if your situation is extremely simple). \n\nTimelineFu is not currently providing anything to generate your timeline because \ndifferent situations will have wildly different requirements. Like access control \nissues and actually just what crazy stuff you're cramming in that timeline.\n\nWe're not saying it can't be done, just that we haven't done it yet. \nContributions are welcome :-)\n\n== Get it\n\n  # Gemfile\n  gem \"timeline_fu\"\n\n== License\n\nCopyright (c) 2008 James Golick, released under the MIT license\n"
 },
 {
  "repo": "bigbinary/admin_data",
  "language": "Ruby",
  "readme_contents": "# admin_data\n\n## Rails 3.0.x\n\nIf you are using Rails 3.0.x then use\n\n    gem 'admin_data', '= 1.1.14'\n\n## Rails 3.1.x\n\nIf you are using Rails 3.1.x then use\n\n    gem 'admin_data', '= 1.2.1'\n\nAlso add following lines to config/application.rb just below the line that says <tt>config.assets.enabled = true</tt> .\n\n    config.assets.precompile += ['admin_data.css', 'admin_data.js']\n\nBefore deploying the code to production execute\n\n    bundle exec rake assets:precompile\n\n## Live Demo\n\nLive demo is available at http://admin-data-demo.heroku.com/admin_data (read only version)\n\n## Docs\n\nDocumentation is available at https://github.com/bigbinary/admin_data/wiki\n\n## Tests\n\nTo execute tests for this gem <tt>cd test/rails_root</tt> and read the instructions mentioned at README.md there.\n\n## License\n\nReleased under [MIT](http://github.com/jquery/jquery/blob/master/MIT-LICENSE.txt) License\n"
 },
 {
  "repo": "techiferous/tabulous",
  "language": "Ruby",
  "readme_contents": "# Tabulous\n\n[![Code Climate](https://codeclimate.com/github/techiferous/tabulous.png)](https://codeclimate.com/github/techiferous/tabulous)\n\nTabulous provides an easy way to add tabs to your Rails application.\n\n## Future\n\nFuture support for tabulous is uncertain, so you may not want to start a new project with it.\nSee [FUTURE](FUTURE.md) for details.\n\n## Requirements\n\nTabulous officially supports Rails 5.0, 4.2, 4.1, 4.0 and MRI Ruby 2.3, 2.2, 2.1 and 2.0.\n\nTabulous should also work with Rails 3.2, 3.1, 3.0 and MRI Ruby 1.9 but automated tests to verify this have been removed.\n\nTabulous definitely does not work with Ruby 1.8.\n\n## Getting Started\n\n* [OVERVIEW](OVERVIEW.md) -- if you're new to tabulous, start here\n* [CHANGES FROM VERSION ONE](CHANGES_FROM_VERSION_ONE.md) -- if you've been using version one of tabulous, read this\n* [REFERENCE](REFERENCE.md) -- complete documentation on the usage of tabulous\n* [CONTRIBUTING](CONTRIBUTING.md) -- if you want to help out\n"
 },
 {
  "repo": "pelle/oauth",
  "language": "Ruby",
  "readme_contents": "= Ruby OAuth\n\n== What\n\nThis is a RubyGem for implementing both OAuth clients and servers in Ruby applications.\n\nSee the OAuth specs http://oauth.net/core/1.0/\n\n== Installing\n\n  sudo gem install oauth\n\nThe source code is now hosted on the OAuth GitHub Project http://github.com/oauth/oauth-ruby\n\n== The basics\n\nThis is a ruby library which is intended to be used in creating Ruby Consumer and Service Provider applications. It is NOT a Rails plugin, but could easily be used for the foundation for such a Rails plugin.\n\nAs a matter of fact it has been pulled out from an OAuth Rails Plugin http://code.google.com/p/oauth-plugin/ which now requires this GEM.\n\n== Demonstration of usage\n\nCreate a new consumer instance by passing it a configuration hash:\n\n  @consumer = OAuth::Consumer.new(\"key\",\"secret\", :site => \"https://agree2\")\n\nStart the process by requesting a token\n\n  @request_token = @consumer.get_request_token\n  session[:request_token] = @request_token\n  redirect_to @request_token.authorize_url\n\nWhen user returns create an access_token\n\n  @access_token = @request_token.get_access_token\n  @photos = @access_token.get('/photos.xml')\n\nNow that you have an access token, you can use Typhoeus to interact with the OAuth provider if you choose.\n\n  oauth_params = {:consumer => oauth_consumer, :token => access_token}\n  hydra = Typhoeus::Hydra.new\n  req = Typhoeus::Request.new(uri, options) \n  oauth_helper = OAuth::Client::Helper.new(req, oauth_params.merge(:request_uri => uri))\n  req.headers.merge!({\"Authorization\" => oauth_helper.header}) # Signs the request\n  hydra.queue(req)\n  hydra.run\n  @response = req.response\n\n\n== More Information\n\n* RDoc: http://rdoc.info/projects/oauth/oauth-ruby/\n* Mailing List/Google Group: http://groups.google.com/group/oauth-ruby\n\n== How to submit patches\n\nThe source code is now hosted on the OAuth GitHub Project http://github.com/oauth/oauth-ruby\n\nTo submit a patch, please fork the oauth project and create a patch with tests. Once you're happy with it send a pull request and post a message to the google group.\n\n== License\n\nThis code is free to use under the terms of the MIT license. \n\n== Contact\n\nOAuth Ruby has been created and maintained by a large number of talented individuals. \nThe current maintainer is Aaron Quint (quirkey).\n\nComments are welcome. Send an email to via the OAuth Ruby mailing list http://groups.google.com/group/oauth-ruby"
 },
 {
  "repo": "ezmobius/chef-deploy",
  "language": "Ruby",
  "readme_contents": "== chef-deploy\n\nA gem that provides resources and providers for deploying ruby web apps from chef recipes\n\n\nUses the same directory layout as capistrano and steals the git remote cached deploy strategy \nfrom cap and adapts it to work without cap and under chef. Not all options are required but they \nare all shown here as an example.\n\n require 'chef-deploy'\n\n deploy \"/data/myrackapp\" do\n   repo \"git://github.com/engineyard/rack-app.git\"\n   branch \"HEAD\"\n   user \"ez\"\n   role \"app_master\"\n   enable_submodules true\n   migrate true\n   migration_command \"rake db:migrate\"\n   environment \"production\"\n   shallow_clone true\n   revision '0xbeadbeef'\n   restart_command \"touch tmp/restart.txt\" # \"monit restart all -g foo\", etc.\n   action :deploy # or :rollback\n end\n\n\nChef-deploy is backwards compatible with capistrano in the fact that it uses the same exact filesystem layout.\nYou can deploy with cap on top of a chef-deploy system and vice versa.\n\nChef-deploy hooks:\n\nIf you create a APP_ROOT/deploy directory in your app you can place named hook files in there that will be triggered\nat the appropriate times during the deploy. the hooks are defined as follows:\n\nAPP_ROOT/\n  deploy/\n    before_migrate.rb\n    before_symlink.rb\n    before_restart.rb\n    after_restart.rb\n\nThese scripts will be instance_eval'd in the context of the chef-deploy resource. This means that you will have certain commands and variables available to you in these hooks. For example:\n\n  run \"echo 'release_path: #{release_path}' >> #{shared_path}/logs.log\"\n  run \"echo 'current_path: #{current_path}' >> #{shared_path}/logs.log\"\n  run \"echo 'shared_path: #{shared_path}' >> #{shared_path}/logs.log\"\n  sudo \"echo 'sudo works' >> /root/sudo.log\"\n\n\nYou have access to a run command and a sudo command. Both of these will run shell commands, run will run as your normal unix user that the app is deployed as and sudo will run as root for when you need more permissions.\n\nYou will have variables like in capistrano:\n\n  release_path: this is the full path to the current release:  /data/appname/releases/12345678\n  shared_path: this is the path to the shared dir: /data/appname/shared\n  current_path: this is the path to the currently symlinked release:  /data/appname/current\n  node:  node is the full chef node object, this will have all of the JSON collected by ohai as well as any custom json you passed into your client run. THis way you can get at *any* data you have available to any of your chef recipes.\n\nUsing subversion:\n\nIn your deploy block, simply add: scm 'subversion' (as well as svn_username and svn_password, if needed). Obviously, git-specific options like enable_submodules can be removed as they're not applicable."
 },
 {
  "repo": "meldium/breach-mitigation-rails",
  "language": "Ruby",
  "readme_contents": "# breach-mitigation-rails\n\nMakes Rails 3 and 4 applications less susceptible to the BREACH /\nCRIME attacks. See [breachattack.com](http://breachattack.com/) for\ndetails.\n\n## How it works\n\nThis gem implements two of the suggestion mitigation strategies from\nthe paper:\n\n*Masking Secrets*: The Rails CSRF token is 'masked' by encrypting it\nwith a 32-byte one-time pad, and the pad and encrypted token are\nreturned to the browser, instead of the \"real\" CSRF token. This only\nprotects the CSRF token from an attacker; it does not protect other\ndata on your pages (see the paper for details on this).\n\n*Length Hiding*: The BreachMitigation::LengthHiding middleware\nappends an HTML comment up to 2k in length to the end of all HTML\ndocuments served by your app. As noted in the paper, this does not\nprevent plaintext recovery, but it can slow the attack and it's\nrelatively inexpensive to implement. Unlike the CSRF token masking,\nlength hiding protects the entire page body from recovery.\n\n## Warning!\n\nBREACH and CRIME are **complicated and wide-ranging attacks**, and this\ngem offers only partial protection for Rails applications. If you're\nconcerned about the security of your web app, you should review the\nBREACH paper and look for other, application-specific things you can\ndo to prevent or mitigate this class of attacks.\n\n## Installation\n\nAdd this line to your Rails Gemfile:\n\n    gem 'breach-mitigation-rails'\n\nAnd then execute:\n\n    $ bundle\n\nThe length-hiding can be disabled by doing:\n\n    Rails.application.config.exclude_breach_length_hiding = true\n\nFor most Rails apps, that should be enough, but read on for the gory\ndetails...\n\n## Gotchas\n\n* The length-hiding middleware adds random text (in the form of an HTML\n  comment) to every page you serve. This can break HTTP caching / ETags for\n  public pages since they are no longer identical on each request.\n* The length-hiding middleware adds up to 2k of text to each page you\n  serve, which means more bandwidth consumed and potentially slower\n  performance.\n* If you have overridden the verified_request? method in your\n  application (likely in ApplicationController) you may need to update\n  it to be compatible with the secret masking code. See\n  `lib/breach_mitigation/railtie.rb` for an example.\n\n## Contributing\n\nPull requests are welcome, either to enhance the existing mitigation\nstrategies or to add new ways to mitigate against the attack.\n"
 },
 {
  "repo": "wvanbergen/state_machine-audit_trail",
  "language": "Ruby",
  "readme_contents": "= StateMachine audit trail {<img src=\"https://travis-ci.org/wvanbergen/state_machine-audit_trail.png\" />}[https://travis-ci.org/wvanbergen/state_machine-audit_trail]\n\n== Superceded\nThis gem has been superceded by {state_machines-audit_trail}[https://github.com/state-machines/state_machines-audit_trail].  For more information, see {the wiki on converting to the new gem.}[https://github.com/state-machines/state_machines-audit_trail/wiki/Converting-from-former-state_machine-audit_trail-to-state_machines-audit_trail]. \n\n== Deprecated\nThis gem is deprecated and no longer maintained due to unmaintained, outdated, and conflicting depdencies.\n\nThis plugin for the state machine gem (see https://github.com/pluginaweek/state_machine) adds support for keeping an audit trail for any state machine. Having an audit trail gives you a complete history of the state changes in your model. This history allows you to investigate incidents or perform analytics, like: \"How long does it take on average to go from state a to state b?\", or \"What percentage of cases goes from state a to b via state c?\"\n\n== ORM support\n\nNote: while the state_machine gem integrates with multiple ORMs, this plugin is currently limited to the following ORM backends:\n\n* ActiveRecord\n* Mongoid\n\nIt should be easy to add new backends by looking at the implementation of the current backends. Pull requests are welcome!\n\n== Usage\n\nFirst, make the gem available by adding it to your <tt>Gemfile</tt>, and run <tt>bundle install</tt>:\n\n  gem 'state_machine-audit_trail'\n\nCreate a model/table that holds the audit trail. The table needs to have a foreign key to the original object, an \"event\" field, a \"from\" state field, a \"to\" state field, and a \"created_at\" timestamp that stores the timestamp of the transition. This gem comes with a Rails 3 generator to create a model and a migration like that.\n\n  rails generate state_machine:audit_trail <model> <state_attribute>\n\n  (For Rails 2, use rails generate state_machine_audit_trail <model> <state_attribute>  [note the underscore instead of the colon])\n\nFor a model called \"Model\", and a state attribute \"state\", this will generate the ModelStateTransition model and an accompanying migration.\n\nNext, tell your state machine you want to store an audit trail:\n\n  class Model < ActiveRecord::Base\n    state_machine :state, :initial => :start do\n      store_audit_trail\n      ...\n\nIf your audit trail model does not use the default naming scheme, provide it using the <tt>:to</tt> option:\n\n  class Model < ActiveRecord::Base\n    state_machine :state, :initial => :start do\n      store_audit_trail :to => 'ModelAuditTrail'\n      ...\n\nThat's it! The plugin will register an <tt>after_transition</tt> callback that is used to log all transitions. It will also log the initial state if there is one.\n\nIf you would like to store additional messages in the audit trail, you can do so with the following:\n    store_audit_trail :context_to_log => :state_message # Will grab the results of the state_message method on the model and store it in a field called state_message on the audit trail model\nor\n    store_audit_trail :context_to_log => [:field1, :field2] # Will grab the results of the field1 and field2 methods on the model and store them in fields called field1 and field2 on the audit trail model\n\n=== Store virtual attributes\n\nSometimes it can be useful to store dynamically computed information.\n\nIn these situations it's just a matter of defining a new column on table <tt>DeploymentStateTransitions</tt> and configure <tt>context_to_log</tt>.\n\ni.e.\n\n  class Model < ActiveRecord::Base\n    state_machine :state, :initial => :start do\n      store_audit_trail :context_to_log => :version\n      ...\n\n    def version\n      # Dynamically computed field e.g., based on other models\n      ...\n\n  class AddVersionToDeploymentStateTransitions < ActiveRecord::Migration\n    def change\n      add_column :deployment_state_transitions, :version, :string\n      ...\n\n\n== About\n\nThis plugin is written by Jesse Storimer and Willem van Bergen for Shopify. Mongoid support was contributed by Siddharth (https://github.com/svs). It is released under the MIT license (see LICENSE).\n"
 },
 {
  "repo": "anthonyshort/stitch-css",
  "language": "Ruby",
  "readme_contents": "# Stitch - CSS Pattern Framework for Compass\n\nPatterns are chunks of styles that we use on every project. These chunks of styles generally perform a particular function, such as clearing floats. Having to write these styles each time is annoying and the function of these chunks of code in our stylesheets is obscure. \n\nBy breaking these patterns into reusable classes and mixins we can:\n\n* Make our stylesheets free from clutter\n* Give our styles more meaning\n* Reduce the size of our stylesheet\n* Only have to write them once\n* Are updatable when newer methods are discovered by updating the framework\n\n## As Compass Extension\n\n\tsudo gem install stitch\n\n\t\nAdd it to your Compass config.rb\n\n\trequire 'stitch'\n\nImport it into your Sass\n\n\t@import 'stitch';\n\nOr import it in parts\n\n\t@import 'stitch/reset';\n\t@import 'stitch/patterns';\n\n## With Bower\n\n```\nbower install stitch-css\n```\n\nThen include it into your projects:\n\n```scss\n@import 'components/stitch-css/stylesheets/stitch';\n```\n\n## Patterns\n\n[See all of the patterns available](https://github.com/anthonyshort/stitch-css/tree/master/stylesheets/stitch/patterns)\n\nBy using @import 'stitch/patterns'; you will have access to all of the pattern mixins. The patterns are all mixins with a couple of extra classes used for extending.\n\n## Reset\n\nStitch includes a unique CSS reset. It resets everything back to it's raw text form so that it's easy to build from.\n\nTo manually import the Stitch reset.\n\n\t@import 'stitch/reset';"
 },
 {
  "repo": "bpot/poseidon",
  "language": "Ruby",
  "readme_contents": "# Unmaintained\n\nThis project is currently unmaintained. There are a handful of other options for interacting with Kafka from Ruby:\n\n  * A pure ruby client, [ruby-kafka](https://github.com/zendesk/ruby-kafka), which is 0.9 compatible and support consumer groups.\n  * A REST proxy, [Kafka Rest](https://github.com/confluentinc/kafka-rest).\n  * For JRuby there is [jruby-kafka](https://github.com/joekiller/jruby-kafka) which wraps the Java consumer.\n\n# Poseidon [![Build Status](https://travis-ci.org/bpot/poseidon.png?branch=master)](https://travis-ci.org/bpot/poseidon) [![Code Climate](https://codeclimate.com/github/bpot/poseidon.png)](https://codeclimate.com/github/bpot/poseidon)\n\nPoseidon is a Kafka client. Poseidon only supports the 0.8 API and above.\n\n**Until 1.0.0 this should be considered ALPHA software and not neccessarily production ready.**\n\n## Usage\n\n### API Documentation\n\n* [Latest release](http://rubydoc.info/gems/poseidon)\n* [Github master](http://rubydoc.info/github/bpot/poseidon)\n\n### Installing a Kafka broker locally\n\nFollow the [instructions](http://kafka.apache.org/documentation.html#quickstart) on the Kafka wiki to build Kafka 0.8 and get a test broker up and running.\n\n### Sending messages to Kafka\n\n```ruby\nrequire 'poseidon'\n\nproducer = Poseidon::Producer.new([\"localhost:9092\"], \"my_test_producer\")\n\nmessages = []\nmessages << Poseidon::MessageToSend.new(\"topic1\", \"value1\")\nmessages << Poseidon::MessageToSend.new(\"topic2\", \"value2\")\nproducer.send_messages(messages)\n```\n\nMore detailed [Poseidon::Producer](http://rubydoc.info/github/bpot/poseidon/Poseidon/Producer) documentation.\n\n### Fetching messages from Kafka\n\n```ruby\nrequire 'poseidon'\n\nconsumer = Poseidon::PartitionConsumer.new(\"my_test_consumer\", \"localhost\", 9092,\n                                            \"topic1\", 0, :earliest_offset)\n\nloop do\n  messages = consumer.fetch\n  messages.each do |m|\n    puts m.value\n  end\nend\n```\n\nMore detailed [Poseidon::PartitionConsumer](http://rubydoc.info/github/bpot/poseidon/Poseidon/PartitionConsumer) documentation.\n\n### Using snappy compression\n\nTo use snappy compression in your producers or consumers, install the [snappy](http://rubygems.org/gems/snappy) gem or simply add `gem 'snappy'` to your project's Gemfile.\n\n## Semantic Versioning\n\nThis gem follows [SemVer](http://semver.org). In particular, the public API should not be considered stable and anything may change without warning until Version 1.0.0.  Additionally, for the purposes of the versioning the public API is everything documented in the [public API docs](http://rubydoc.info/github/bpot/poseidon).\n\n## Requirements\n\n* Ruby 1.9.3 or higher (1.9.2 and below not supported!!!)\n* Kafka 0.8 or higher\n\n## Integration Tests\n\nIn order to run integration tests you must specify a `KAFKA_PATH` environment variable which points to a built Kafka installation.  To build Kafka locally follow the [instructions](http://kafka.apache.org/documentation.html#quickstart) provided by the project.\n\n    # cd ~/src/poseidon/\n    # bundle\n    # KAFKA_PATH=~/src/kafka bundle exec rake spec:all # run all unit and integration specs\n\nThe poseidon test suite will take care of spinning up and down the broker(s) needed for the integration tests.\n"
 },
 {
  "repo": "alindeman/zonebie",
  "language": "Ruby",
  "readme_contents": "# Zonebie\n\n[![Build Status](https://secure.travis-ci.org/alindeman/zonebie.svg)](http://travis-ci.org/alindeman/zonebie)\n\nZonebie prevents bugs in code that deals with timezones by randomly assigning a\nzone on every run.\n\nIf Zonebie helps trigger a timezone-related bug, you can temporarily assign the\n`ZONEBIE_TZ` environment variable to make your tests deterministic while you\ndebug (more information below).\n\n## Requirements\n\n* MRI (2.0.x, 2.1.x, 2.2.x, 2.3.x, 2.4.x)\n* JRuby (1.7)\n* Rubinius (3.x)\n\n***\n\nAnd **either** of these gems which adds timezone support to Ruby:\n\n* `activesupport` >= 3.0 (Rails 3.0, 3.1, 3.2, 4.0, 4.1, 4.2)\n* `tzinfo` >= 1.2\n\n## Installation\n\nIf using Bundler (recommended), add to Gemfile:\n\n````ruby\ngem 'zonebie'\n````\n\n## Usage with Rails & Active Support\n\nActive Support allows setting a global timezone that will be used for many date\nand time calculations throughout the application.\n\nZonebie can set this to a random timezone at the beginning of test runs.\nSpecifically for Active Support, it sets `Time.zone`.\n\n### Test::Unit & Minitest\n\nAdd to `test/test_helper.rb`:\n\n```ruby\nZonebie.set_random_timezone\n```\n\n### RSpec\n\nAdd to `spec/spec_helper.rb`:\n\n```ruby\nrequire \"zonebie/rspec\"\n```\n\n### Cucumber\n\nAdd a file `features/support/zonebie.rb` with the following contents:\n\n```ruby\nZonebie.set_random_timezone\n```\n\n## Usage with TZInfo\n\nZonebie can use the `tzinfo` gem, allowing it to work outside of Active Support\n(Rails).\n\nHowever, `Zonebie.set_random_timezone` does not work outside of Active Support\nbecause there is not a concept of a global timezone setting. If you simply need\na random timezone for some other part of your tests, Zonebie can help.\n\n```ruby\nzone = TZInfo::Timezone.get(Zonebie.random_timezone)\nputs zone.now\n\n# Also works in Rails/Active Support\nzone = ActiveSupport::TimeZone[Zonebie.random_timezone]\nputs zone.now\n```\n\n## Reproducing Bugs\n\nWhen `Zonebie.set_random_timezone` is called, Zonebie assigns a timezone and\nprints a message to STDOUT:\n\n```\n[Zonebie] Setting timezone: ZONEBIE_TZ=\"Eastern Time (US & Canada)\"\n```\n\nIf you would rather that Zonebie not print out this information during your\ntests, put Zonebie in quiet mode before calling `set_random_timezone`:\n\n```ruby\nZonebie.quiet = true\n```\n\nTo rerun tests with a specific timezone (e.g., to reproduce a bug that only\nseems present in one zone), set the `ZONEBIE_TZ` environment variable:\n\n```ruby\n# Assuming tests run with simply `rake`\nZONEBIE_TZ=\"Eastern Time (US & Canada)\" rake\n```\n\n## Contributing\n\n1. Fork it\n2. Create your feature branch (`git checkout -b my-new-feature`)\n3. Commit your changes (`git commit -am 'Added some feature'`)\n4. Push to the branch (`git push origin my-new-feature`)\n5. Create new Pull Request\n"
 },
 {
  "repo": "bigbinary/admin_data",
  "language": "Ruby",
  "readme_contents": "# admin_data\n\n## Rails 3.0.x\n\nIf you are using Rails 3.0.x then use\n\n    gem 'admin_data', '= 1.1.14'\n\n## Rails 3.1.x\n\nIf you are using Rails 3.1.x then use\n\n    gem 'admin_data', '= 1.2.1'\n\nAlso add following lines to config/application.rb just below the line that says <tt>config.assets.enabled = true</tt> .\n\n    config.assets.precompile += ['admin_data.css', 'admin_data.js']\n\nBefore deploying the code to production execute\n\n    bundle exec rake assets:precompile\n\n## Live Demo\n\nLive demo is available at http://admin-data-demo.heroku.com/admin_data (read only version)\n\n## Docs\n\nDocumentation is available at https://github.com/bigbinary/admin_data/wiki\n\n## Tests\n\nTo execute tests for this gem <tt>cd test/rails_root</tt> and read the instructions mentioned at README.md there.\n\n## License\n\nReleased under [MIT](http://github.com/jquery/jquery/blob/master/MIT-LICENSE.txt) License\n"
 },
 {
  "repo": "hashrocket/hr-til",
  "language": "Ruby",
  "readme_contents": "# HR-TIL\n\n![til](https://raw.githubusercontent.com/hashrocket/hr-til/master/app/assets/images/banner.png)\n\n> TIL is an open-source project by the team at\n> [Hashrocket](https://hashrocket.com/) that catalogues the sharing &\n> accumulation of knowledge as it happens day-to-day. Posts have a 200-word\n> limit, and posting is open to any Rocketeer as well as select friends of the\n> team. We hope you enjoy learning along with us.\n\nThis site was open-sourced as a window into our development process, as well as\nto allow people to experiment with the site on their own and contribute to the\nproject.\n\nToday I Learned was ported to Elixir/Phoenix in 2017 ([source\ncode](https://github.com/hashrocket/tilex)), and the Rails application is no\nlonger in production or actively maintained by Hashrocket. Thanks to all of our\n[contributors](https://github.com/hashrocket/hr-til/graphs/contributors)!\n\n### Install\n\nIf you are creating your own version of the site,\n[fork](https://help.github.com/articles/fork-a-repo/) the repository.\n\nThen, follow these setup steps:\n\n```sh\n$ git clone https://github.com/hashrocket/hr-til\n$ cd hr-til\n$ gem install bundler\n$ bundle install\n$ cp config/application.yml{.example,}\n$ rake db:create db:migrate db:seed\n$ rails s\n```\n\nIn development, `db:seed` will load sample data for channels, developers, and\nposts. Omit this command to opt-out of this step, or create your own sample\ndata in `db/seeds/development.rb`.\n\nAuthentication is managed by Omniauth and Google. See the\n[omniauth-google-oauth2\nREADME](https://github.com/zquestz/omniauth-google-oauth2/blob/master/README.md)\nand [Google Oauth 2\ndocs](https://developers.google.com/identity/protocols/OAuth2WebServer) for\nsetup instructions. To allow users from a domain, multiple domains, or a\nspecific email to log in, set those configurations in your environmental\nvariables:\n\n```yml\n# config/application.yml\n\npermitted_domains: 'hashrocket.com|hshrckt.com'\npermitted_emails: 'friend@whitelist.com'\n```\nEnsure you have set the google client id and google client secret via Oauth instructions.\n\nOnce set, visit '/admin' and log in with a permitted email address or domain.\n\n### Testing\n\nRun all tests with:\n\n```\n$ rake\n```\n\nOr, run all the test in parallel with [flatware](https://github.com/briandunn/flatware):\n\n```\n$ flatware fan rake db:test:prepare\n$ flatware rspec && flatware cucumber\n```\n\n### Dependencies\n\n- The gem `selenium-webdriver` depends on the Firefox browser.\n- The gems `flatware-rspec` and `flatware-cucumber` require ZeroMQ. Learn more\n[here](https://github.com/briandunn/flatware).\n\n### Environmental Variables\n\n`basic_auth_credentials` both toggles and defines basic authentication:\n\n```yml\n# config/application.yml\n\nbasic_auth_credentials: username:password\n```\n\n`slack_post_endpoint` allows the app to post to [Slack](https://slack.com/):\n\n```yml\n# config/application.yml\n\nslack_post_endpoint: /services/some/hashes\n```\n\n### Contributing\n\n1. [Fork](https://help.github.com/articles/fork-a-repo/) it\n2. Create your feature branch (`git checkout -b my-new-feature`)\n3. Commit your changes (`git commit -am 'Add some feature'`)\n4. Push to the branch (`git push origin my-new-feature`)\n5. Create new Pull Request\n\nBug reports and pull requests are welcome on GitHub at\nhttps://github.com/hashrocket/hr-til. This project is intended to be a safe,\nwelcoming space for collaboration, and contributors are expected to adhere to\nthe [Contributor Covenant](http://contributor-covenant.org) code of conduct.\n\n### Usage\n\nWe love seeing forks of Today I Learned in production! Please consult our\n[usage guide](/USAGE.md) for guidelines on appropriate styling and attribution.\n\n### License\n\nTIL is released under the [MIT License](http://www.opensource.org/licenses/MIT).\n\n---\n\n### About\n\n[![Hashrocket logo](https://hashrocket.com/hashrocket_logo.svg)](https://hashrocket.com)\n\nTIL is supported by the team at [Hashrocket, a\nmultidisciplinary design and development consultancy](https://hashrocket.com). If you'd like to [work with\nus](https://hashrocket.com/contact-us/hire-us) or [join our\nteam](https://hashrocket.com/contact-us/jobs), don't hesitate to get in touch.\n"
 },
 {
  "repo": "nesquena/dante",
  "language": "Ruby",
  "readme_contents": "# Dante\n\nTurn any ruby into a daemon.\n\n## Description\n\nDante is the simplest possible thing that will work to turn arbitrary ruby code into an executable that\ncan be started via command line or start/stop a daemon, and will store a pid file for you.\n\nIf you need to create a ruby executable and you want standard daemon start/stop with pid files\nand no hassle, this gem will be a great way to get started.\n\n## Installation\n\nAdd to your Gemfile:\n\n```ruby\n# Gemfile\n\ngem \"dante\"\n```\n\nor to your gemspec:\n\n```ruby\n# mygem.gemspec\n\nGem::Specification.new do |s|\n  s.add_dependency \"dante\"\nend\n```\n\n## Usage\n\nDante is meant to be used from any \"bin\" executable. For instance, to create a binary for a web server, create a file in `bin/myapp`:\n\n```ruby\n#!/usr/bin/env ruby\n\nrequire File.expand_path(\"../../myapp.rb\", __FILE__)\n\nDante.run('myapp') do |opts|\n  # opts: host, pid_path, port, daemonize, user, group\n  Thin::Server.start('0.0.0.0', opts[:port]) do\n    use Rack::CommonLogger\n    use Rack::ShowExceptions\n    run MyApp\n  end\nend\n```\n\nBe sure to properly make your bin executable:\n\n```\nchmod +x bin/myapp\n```\n\n### CLI\n\nThis gives your binary several useful things for free:\n\n```\n./bin/myapp\n```\n\nwill start the app undaemonized in the terminal, handling trapping and stopping the process.\n\n```\n./bin/myapp -l /var/log/myapp.log\n```\n\nwill start the app undaemonized in the terminal and redirect all stdout and stderr to the specified logfile.\n\n```\n./bin/myapp -p 8080 -d -P /var/run/myapp.pid -l /var/log/myapp.log\n```\n\nwill daemonize and start the process, storing the pid in the specified pid file.\nAll stdout and stderr will be redirected to the specified logfile. If no logfile is specified in daemon mode then all \nstdout and stderr will be directed to /var/log/<myapp name>.log.\n\n```\n./bin/myapp -k -P /var/run/myapp.pid\n```\n\nwill stop all daemonized processes for the specified pid file.\n\n```\n./bin/myapp --help\n```\n\nWill return a useful help banner message explaining the simple usage.\n\n### Advanced\n\nIn many cases, you will need to add custom flags/options or a custom description to your executable. You can do this\neasily by using `Dante::Runner` more explicitly:\n\n```ruby\n#!/usr/bin/env ruby\n\nrequire File.expand_path(\"../../myapp.rb\", __FILE__)\n\n# Set default port to 8080\nrunner = Dante::Runner.new('myapp', :port => 8080)\n# Sets the description in 'help'\nrunner.description = \"This is myapp\"\n# Setup custom 'test' option flag\nrunner.with_options do |opts|\n  opts.on(\"-t\", \"--test TEST\", String, \"Test this thing\") do |test|\n    options[:test] = test\n  end\nend\n# Create validation hook for options\nrunner.verify_options_hook = lambda { |opts|\n  raise Exception.new(\"Must supply test parameter\") if opts[:test].nil?\n}\n# Parse command-line options and execute the process\nrunner.execute do |opts|\n  # opts: host, pid_path, port, daemonize, user, group\n  Thin::Server.start('0.0.0.0', opts[:port]) do\n    puts opts[:test] # Referencing my custom option\n    use Rack::CommonLogger\n    use Rack::ShowExceptions\n    run MyApp\n  end\nend\n```\n\nNow you would be able to do:\n\n```\n./bin/myapp -t custom\n```\n\nand the `opts` would contain the `:test` option for use in your script. In addition, help will now contain\nyour customized description in the banner.\n\nYou can also use dante programmatically to start, stop and restart arbitrary code:\n\n```ruby\n# daemon start\nDante::Runner.new('gitdocs').execute(:daemonize => true, :pid_path => @pid, :log_path => @log_path) { something! }\n# daemon stop\nDante::Runner.new('gitdocs').execute(:kill => true, :pid_path => @pid)\n# daemon restart\nDante::Runner.new('gitdocs').execute(:daemonize => true, :restart => true, :pid_path => @pid) { something! }\n```\n\nso you can use dante as part of a more complex CLI executable.\n\n## God\n\nDante can be used well in conjunction with the excellent God process manager. Simply, use Dante to daemonize a process\nand then you can easily use God to monitor:\n\n```ruby\n# /etc/god/myapp.rb\n\nGod.watch do |w|\n  w.name            = \"myapp\"\n  w.interval        = 30.seconds\n  w.start           = \"ruby /path/to/myapp/bin/myapp -d\"\n  w.stop            = \"ruby /path/to/myapp/bin/myapp -k\"\n  w.start_grace     = 15.seconds\n  w.restart_grace   = 15.seconds\n  w.pid_file        = \"/var/run/myapp.pid\"\n\n  w.behavior(:clean_pid_file)\n\n  w.start_if do |start|\n    start.condition(:process_running) do |c|\n      c.interval = 5.seconds\n      c.running = false\n    end\n  end\nend\n```\n\nand that's all. Of course now you can also easily daemonize as well as start/stop the process on the command line as well.\n\n## Copyright\n\nCopyright \u00a9 2011 Nathan Esquenazi. See [LICENSE](https://github.com/bazaarlabs/dante/blob/master/LICENSE) for details.\n"
 },
 {
  "repo": "leppert/remotipart",
  "language": "Ruby",
  "readme_contents": "= THIS REPOSITORY IS NO LONGER BEING MAINTAINED\n\n<b>Remotipart has a new home and is now being maintained at https://github.com/JangoSteve/remotipart.</b>\n\n== Remotipart\n\nRemotipart is a Ruby on Rails gem enabling remote multipart forms (AJAX style file uploads) with jQuery.\nThis gem augments the native Rails jQuery remote form function enabling asynchronous file uploads with little to no modification to your application.\n\n{View Homepage and Demos}[http://opensource.alfajango.com/remotipart/]\n\n== Copyright\n\nCopyright (c) 2011 Greg Leppert, Steve Schwartz. See LICENSE for details.\n"
 },
 {
  "repo": "leshill/resque_spec",
  "language": "Ruby",
  "readme_contents": "ResqueSpec\n==========\n\n[![Build\nStatus](https://travis-ci.org/leshill/resque_spec.png)](https://travis-ci.org/leshill/resque_spec)\n\nA test double of Resque for RSpec and Cucumber. The code was originally based\non\n[http://github.com/justinweiss/resque\\_unit](http://github.com/justinweiss/resque_unit).\n\nResqueSpec will also fire Resque hooks if you are using them. See below.\n\nInstall\n-------\n\nUpdate your Gemfile to include `resque_spec` only in the *test* group (Not\nusing `bundler`? Do the necessary thing for your app's gem management and use\n`bundler`. `resque_spec` monkey patches `resque` it should only be used with\nyour tests!)\n\n```ruby\ngroup :test do\n  gem 'resque_spec'\nend\n```\n\nCucumber\n--------\n\nBy default, the above will add the `ResqueSpec` module and make it available in\nCucumber. If you want the `with_resque` and `without_resque` helpers, manually\nrequire the `resque_spec/cucumber` module:\n\n```ruby\nrequire 'resque_spec/cucumber'\n```\nThis can be done in `features/support/env.rb` or in a specific support file\nsuch as `features/support/resque.rb`.\n\nWhat is ResqueSpec?\n===================\n\nResqueSpec implements the *stable API* for Resque 1.19+ (which is `enqueue`,\n`enqueue_to`,  `dequeue`, `peek`, `reserve`, `size`, the Resque hooks, and\nbecause of the way `resque_scheduler` works `Job.create` and `Job.destroy`).\n\nIt does not have a test double for Redis, so this may lead to some interesting and\npuzzling behaviour if you use some of the popular Resque plugins (such as\n`resque_lock`).\n\nResque with Specs\n=================\n\nGiven this scenario\n\n    Given a person\n    When I recalculate\n    Then the person has calculate queued\n\nAnd I write this spec using the `resque_spec` matcher\n\n```ruby\ndescribe \"#recalculate\" do\n  before do\n    ResqueSpec.reset!\n  end\n\n  it \"adds person.calculate to the Person queue\" do\n    person.recalculate\n    expect(Person).to have_queued(person.id, :calculate)\n  end\nend\n```\n\nAnd I see that the `have_queued` assertion is asserting that the `Person` queue has a job with arguments `person.id` and `:calculate`\n\nAnd I take note of the `before` block that is calling `reset!` for every spec\n\nAnd I might use the `in` statement to specify the queue:\n```ruby\ndescribe \"#recalculate\" do\n  before do\n    ResqueSpec.reset!\n  end\n\n  it \"adds person.calculate to the Person queue\" do\n    person.recalculate\n    expect(Person).to have_queued(person.id, :calculate).in(:people)\n  end\nend\n```\n\nAnd I might write this as a Cucumber step\n\n```ruby\nThen /the (\\w?) has (\\w?) queued/ do |thing, method|\n  thing_obj = instance_variable_get(\"@#{thing}\")\n  expect(thing_obj.class).to have_queued(thing_obj.id, method.to_sym)\nend\n```\n\nThen I write some code to make it pass:\n\n```ruby\nclass Person\n  @queue = :people\n\n  def recalculate\n    Resque.enqueue(Person, id, :calculate)\n  end\nend\n```\n\nYou can check the size of the queue in your specs too.\n\n```ruby\ndescribe \"#recalculate\" do\n  before do\n    ResqueSpec.reset!\n  end\n\n  it \"adds an entry to the Person queue\" do\n    person.recalculate\n    expect(Person).to have_queue_size_of(1)\n  end\nend\n```\n\nTurning off ResqueSpec and calling directly to Resque\n-----------------------------------------------------\n\nOccasionally, you want to run your specs directly against Resque instead of\nResqueSpec. For one at a time use, pass a block to the `without_resque_spec`\nhelper:\n\n```ruby\ndescribe \"#recalculate\" do\n  it \"recalculates the persons score\" do\n    without_resque_spec do\n      person.recalculate\n    end\n    ... assert recalculation after job done\n  end\nend\n```\n\nOr you can manage when ResqueSpec is disabled by flipping the\n`ResqueSpec.disable_ext` flag:\n\n```ruby\n# disable ResqueSpec\nResqueSpec.disable_ext = true\n```\n\nYou will most likely (but not always, see the Resque docs) need to ensure that\nyou have `redis` running.\n\nResqueMailer with Specs\n=======================\n\nTo use with [ResqueMailer](https://github.com/zapnap/resque_mailer) you should\nhave an initializer that does *not* exclude the `test` (or `cucumber`)\nenvironment. Your initializer will probably end up looking like:\n\n```ruby\n# config/initializers/resque_mailer.rb\nResque::Mailer.excluded_environments = []\n```\n\nIf you have a mailer like this:\n\n```ruby\nclass ExampleMailer < ActionMailer::Base\n  include Resque::Mailer\n\n  def welcome_email(user_id)\n  end\nend\n```\n\nYou can write a spec like this:\n\n```ruby\ndescribe \"#welcome_email\" do\n  before do\n    ResqueSpec.reset!\n    Examplemailer.welcome_email(user.id).deliver\n  end\n\n  subject { described_class }\n  it { should have_queue_size_of(1) }\n  it { should have_queued(:welcome_email, [user.id]) }\nend\n```\n\nresque-scheduler with Specs\n==========================\n\nUpdate the Gemfile to enable the `resque-schedular` matchers:\n\n```ruby\ngroup :test do\n  gem 'resque_spec', require: 'resque_spec/scheduler'\nend\n```\n\nGiven this scenario\n\n    Given a person\n    When I schedule a recalculate\n    Then the person has calculate scheduled\n\nAnd I write this spec using the `resque_spec` matcher\n\n```ruby\ndescribe \"#recalculate\" do\n  before do\n    ResqueSpec.reset!\n  end\n\n  it \"adds person.calculate to the Person queue\" do\n    person.recalculate\n    expect(Person).to have_scheduled(person.id, :calculate)\n  end\nend\n```\n\nAnd I might use the `at` statement to specify the time:\n\n```ruby\ndescribe \"#recalculate\" do\n  before do\n    ResqueSpec.reset!\n  end\n\n  it \"adds person.calculate to the Person queue\" do\n    person.recalculate\n\n    # Is it scheduled to be executed at 2010-02-14 06:00:00 ?\n    expect(Person).to have_scheduled(person.id, :calculate).at(Time.mktime(2010,2,14,6,0,0))\n  end\nend\n```\n\nAnd I might use the `in` statement to specify time interval (in seconds):\n\n```ruby\ndescribe \"#recalculate\" do\n  before do\n    ResqueSpec.reset!\n  end\n\n  it \"adds person.calculate to the Person queue\" do\n    person.recalculate\n\n    # Is it scheduled to be executed in 5 minutes?\n    expect(Person).to have_scheduled(person.id, :calculate).in(5 * 60)\n  end\nend\n```\n\nYou can also check the size of the schedule:\n\n```ruby\ndescribe \"#recalculate\" do\n  before do\n    ResqueSpec.reset!\n  end\n\n  it \"adds person.calculate to the Person queue\" do\n    person.recalculate\n\n    expect(Person).to have_schedule_size_of(1)\n  end\nend\n```\n\n(And I take note of the `before` block that is calling `reset!` for every spec)\n\nYou can explicitly specify the queue when using enqueue_at_with_queue and\nenqueue_in_with_queue:\n\n```ruby\ndescribe \"#recalculate_in_future\" do\n  before do\n    ResqueSpec.reset!\n  end\n\n  it \"adds person.calculate to the :future queue\" do\n    person.recalculate_in_future\n\n    Person.should have_schedule_size_of(1).queue(:future)\n  end\nend\n```\n\nAnd I might write this as a Cucumber step\n\n```ruby\nThen /the (\\w?) has (\\w?) scheduled/ do |thing, method|\n  thing_obj = instance_variable_get(\"@#{thing}\")\n  expect(thing_obj.class).to have_scheduled(thing_obj.id, method.to_sym)\nend\n```\n\nThen I write some code to make it pass:\n\n```ruby\nclass Person\n  @queue = :people\n\n  def recalculate\n    Resque.enqueue_at(Time.now + 3600, Person, id, :calculate)\n  end\n\n  def recalculate_in_future\n    Resque.enqueue_at_with_queue(:future, Time.now + 3600, Person, id, :calculate)\n  end\nend\n```\n\nPerforming Jobs in Specs\n========================\n\nNormally, ResqueSpec does not perform queued jobs within tests. You may want to\nmake assertions based on the result of your jobs. ResqueSpec can process jobs\nimmediately as they are queued or under your control.\n\nPerforming jobs immediately\n---------------------------\n\nTo perform jobs immediately, you can pass a block to the `with_resque` helper:\n\nGiven this scenario\n\n    Given a game\n    When I score\n    Then the game has a score\n\nI might write this as a Cucumber step\n\n```ruby\nWhen /I score/ do\n  with_resque do\n    visit game_path\n    click_link 'Score!'\n  end\nend\n```\n\nOr I write this spec using the `with_resque` helper\n\n```ruby\ndescribe \"#score!\" do\n  before do\n    ResqueSpec.reset!\n  end\n\n  it \"increases the score\" do\n    with_resque do\n      game.score!\n    end\n    expect(game.score).to == 10\n  end\nend\n```\n\nYou can turn this behavior on by setting `ResqueSpec.inline = true`.\n\nPerforming jobs at your discretion\n----------------------------------\n\nYou can perform the first job on a queue at a time, or perform all the jobs on\na queue.  Use `ResqueSpec#perform_next(queue_name)` or\n`ResqueSpec#perform_all(queue_name)`\n\nGiven this scenario:\n\n    Given a game\n    When I score\n    And the score queue runs\n    Then the game has a score\n\nI might write this as a Cucumber step\n\n```ruby\nWhen /the (\\w+) queue runs/ do |queue_name|\n  ResqueSpec.perform_all(queue_name)\nend\n```\n\nHooks\n=====\n\nResque provides hooks at different points of the queueing lifecylce.\nResqueSpec fires these hooks when appropriate.\n\nThe before and after `enqueue` hooks are always called when you use\n`Resque#enqueue`. If your `before_enqueue` hook returns `false`, the job will\nnot be queued and `after_enqueue` will not be called.\n\nThe `perform` hooks: before, around, after, and on failure are fired by\nResqueSpec if you are using the `with_resque` helper or set `ResqueSpec.inline = true`.\n\nImportant! If you are using resque-scheduler, `Resque#enqueue_at/enqueue_in`\ndoes not fire the after enqueue hook (the job has not been queued yet!), but\nwill fire the `perform` hooks if you are using `inline` mode.\n\nNote on Patches/Pull Requests\n=============================\n\n* Fork the project.\n* Make your feature addition or bug fix.\n* Add tests for it. This is important so I don't break it in a\n  future version unintentionally.\n* Commit, do not mess with rakefile, version, or history.\n  (if you want to have your own version, that is fine but bump version in a commit by itself I can ignore when I pull)\n* Send me a pull request. Bonus points for topic branches.\n\nAuthor\n======\n\nI made `resque_spec` because **resque** is awesome and should be easy to spec.\nFollow me on [Github](https://github.com/leshill) and\n[Twitter](https://twitter.com/leshill).\n\nContributors\n============\n\n* Kenneth Kalmer                (@kennethkalmer)  : rspec dependency fix\n* Brian Cardarella              (@bcardarella)    : fix mutation bug\n* Joshua Davey                  (@joshdavey)      : with\\_resque helper\n* Lar Van Der Jagt              (@supaspoida)     : with\\_resque helper\n* Evan Sagge                    (@evansagge)      : Hook in via Job.create, have\\_queued.in\n* Jon Larkowski                 (@l4rk)           : inline perform\n* James Conroy-Finn             (@jcf)            : spec fix\n* Dennis Walters                (@ess)            : enqueue\\_in support\n*                               (@RipTheJacker)   : remove\\_delayed support\n* Kurt Werle                    (@kwerle)         : explicit require spec for v020\n*                               (@dwilkie)        : initial before\\_enqueue support\n* Marcin Balinski               (@marcinb)        : have\\_schedule\\_size\\_of matcher, schedule matcher at, in\n*                               (@alexeits)       : fix matcher in bug with RSpec 2.8.0\n*                               (@ToadJamb)       : encode/decode of Resque job arguments\n* Mateusz Konikowski            (@mkonikowski)    : support for anything matcher\n* Mathieu Ravaux                (@mathieuravaux)  : without\\_resque\\_spec support\n* Arjan van der Gaag            (@avdgaag)        : peek support\n*                               (@dtsiknis)       : Updated removed\\_delayed\n* Li Ellis Gallardo             (@lellisga)       : fix inline/disable\\_ext bug\n* Jeff Deville                  (@jeffdeville)    : Resque.size\n* Frank Wambutt                 (@opinel)         : Fix DST problem in `have_scheduled`\n* Luke Melia                    (@lukemelia)      : Add `times` chained matcher\n* Pablo Fernandez               (@heelhook)       : Add `have_queue_size_of_at_least` and `have_schedule_size_of_at_least` matchers\n*                               (@k1w1)           : Add support for enqueue\\_at\\_with\\_queue/enqueue\\_in\\_with\\_queue\n* Oz\u00e9ias Sant'ana               (@ozeias)         : Update specs to RSpec 2.10\n* Yuya Kitajima                 (@yuyak)          : Add ResqueMailer examples to README\n* Andr\u00e9s Bravo                  (@andresbravog)   : Replace `rspec` dependency with explicit dependencies\n* Ben Woosley                   (@Empact)         : Loosen rubygems version constraint\n* Jeff Dickey                   (@dickeyxxx)      : Remove 2.0 warnings, added Travis\n* Earle Clubb                   (@eclubb)         : `be_queued` matcher\n* Erkki Eilonen                 (@erkki)          : RSpec 3 support\n* Gavin Heavyside               (@gavinheavyside) : RSpec three warnings\n* Pavel Khrulev                 (@PaulSchuher)    : Resque 2 and RSpec 3 support\n* Ilya Katz                     (@ilyakatz)       : Cleanup README.md for RSpec 3\n*                               (@addbrick)       : Compare times as integers in `have_scheduled` matcher\n* Serious Haircut               (@serioushaircut) : Fix ArgumentListMatcher to make it work with any\\_args\n* Harry Lascelles               (@hlascelles)     : Fix error when resque-spec is disabled\n\nCopyright\n=========\n\nCopyright (c) 2010-2015 Les Hill. See LICENSE for details.\n"
 },
 {
  "repo": "berkshelf/ridley",
  "language": "Ruby",
  "readme_contents": "# Ridley\n[![Gem Version](https://badge.fury.io/rb/ridley.svg)](http://badge.fury.io/rb/ridley)\n[![Build Status](https://secure.travis-ci.org/berkshelf/ridley.svg?branch=master)](http://travis-ci.org/berkshelf/ridley)\n[![Dependency Status](https://gemnasium.com/berkshelf/ridley.svg?travis)](https://gemnasium.com/berkshelf/ridley)\n[![Code Climate](https://codeclimate.com/github/berkshelf/ridley.svg)](https://codeclimate.com/github/berkshelf/ridley)\n\nA reliable Chef API client with a clean syntax\n\nNotice\n------\n\nThis is the HTTP Client API for Berkshelf.  It is supported for that purpose, but for all external purposes its use\nis deprecated.  Chef users should use the `Chef::ServerAPI` class in the `chef` gem.\n\nThe old documentation is still available at [README.old.md](README.old.md)\n"
 },
 {
  "repo": "marianoguerra/rst2html5",
  "language": "HTML",
  "readme_contents": "rst2html5 tools - RestructuredText to HTML5 + bootstrap css\n===========================================================\n\nWe all love rst and the ability to generate any format, but the rst2html tool generates really basic html and css.\n\nThis tool will generate newer, nicer, more readable markup and provide ways to modify the output with extensions like nice css\nthanks to twitter's bootstrap css or online presentations with deck.js\n\nget it\n------\n\nvia pip::\n\n        pip install rst2html5-tools\n\nlocally::\n\n        git clone https://github.com/marianoguerra/rst2html5.git\n        cd rst2html5\n        git submodule init\n        git submodule update\n\n        sudo python setup.py install\n\nuse it\n------\n\nto generate a basic html document::\n\n        rst2html5 examples/slides.rst > clean.html\n\nto generate a set of slides using deck.js::\n\n        rst2html5 --deck-js --pretty-print-code --embed-content examples/slides.rst > deck.html\n\nto generate a set of slides using reveal.js::\n\n        rst2html5 --jquery --reveal-js --pretty-print-code examples/slides.rst > reveal.html\n\nto generate a set of slides using impress.js::\n\n    rst2html5 --stylesheet-path=html5css3/thirdparty/impressjs/css/impress-demo.css --impress-js examples/impress.rst > output/impress.html\n\nto generate a page using bootstrap::\n\n        rst2html5 --bootstrap-css --pretty-print-code --jquery --embed-content examples/slides.rst > bootstrap.html\n\nto higlight code with pygments::\n\n    rst2html5 --pygments examples/codeblock.rst > code.html\n\nnote that you will have to add the stylesheet for the code to actually highlight, this just does the code parsing and html transformation.\n\nto embed images inside the html file to have a single .html file to distribute\nadd the --embed-images option.\n\npost processors support optional parameters, they are passed with a command\nline option with the same name as the post processor appending \"-opts\" at the\nend, for example to change the revealjs theme you can do::\n\n        rst2html5 --jquery --reveal-js --reveal-js-opts theme=serif examples/slides.rst > reveal.html\n\nyou can also pass the base path to the theme css file::\n\n        rst2html5 --jquery --reveal-js --reveal-js-opts theme=serif,themepath=~/mytheme examples/slides.rst > reveal.html\n\nit will look at the theme at ~/mytheme/serif.css\n\noptions are passed as a comma separated list of key value pairs separated with\nan equal sign, values are parsed as json, if parsing fails they are passed as\nstrings, for example here is an example of options::\n\n    --some-processor-opts theme=serif,count=4,verbose=true,foo=null\n\nif a key is passed more than once that parameter is passed to the processor as a list of values, note that if only one value is passed it's passed as it is, the convenience function as_list is provided to handle this case if you want to always receive a list.\n\nto add custom js files to the resulting file you can use the --add-js post processor like this::\n\n    rst2html5 slides.rst --add-js --add-js-opts path=foo.js,path=bar.js\n\nthat command will add foo.js and bar.js as scripts in the resulting html file.\n\nPretty Print Code Notes\n.......................\n\nenable it::\n\n    --pretty-print-code\n\nadd language specific lexers::\n\n    --pretty-print-code-opts langs=clj:erlang\n\nNote: you have to pass both options when passing opts to prettify like this::\n\n    --pretty-print-code --pretty-print-code-opts langs=clj:erlang\n\nthat is, the name of the languages separated by colons, available lexers at the\nmoment of this writing are:\n\n* apollo\n* basic\n* clj\n* css\n* dart\n* erlang\n* go\n* hs\n* lisp\n* llvm\n* lua\n* matlab\n* ml\n* mumps\n* n\n* pascal\n* proto\n* rd\n* r\n* scala\n* sql\n* tcl\n* tex\n* vb\n* vhdl\n* wiki\n* xq\n* yaml\n\nyou can see the available lexers under html5css3/thirdparty/prettify/lang-\\*.js\n\nRevealJs Notes\n..............\n\nto print pass --reveal-js-opts printpdf=true, for example::\n\n    rst2html5 --jquery --reveal-js --reveal-js-opts printpdf=true examples/slides.rst > reveal-print.html\n\nthis can be used to open with chrome or chromium and print as pdf as described here: https://github.com/hakimel/reveal.js#pdf-export\n\n\nMath Support\n------------\nUse the ``math`` role and directive to include inline math and block-level equations into your document::\n\n    When :math`a \\ne 0`, there are two solutions to :math:`ax^2 + bx + c = 0`\n    and they are\n\n    .. math::\n\n       x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}\n\nBoth of these support a basic subset of LaTeX_ syntax.\n\nBy default, MathJax_ is used for displaying math. You can choose a different output format using the ``--math-output`` command line option:\n\n* ``--math-output mathjax`` uses MathJax (the default)\n* ``--math-output html`` will use plain HTML + CSS\n* ``--math-output mathml`` will use MathML_\n* ``--math-output latex`` outputs raw LaTeX\n\nIf you use MathJax, you can use the ``--mathjax-url`` and ``-mathjax-config`` command line options to configure a custom MathJax JavaScript URL and to provide a file with a custom MathJax configuration, respectively.\n\nIf you use HTML + CSS output, you can use the ``--math-css`` command line option to configure a custom math stylesheet.\n\nNote that the old MathJax postprocessor (activated using ``--mathjax``) has been deprecated.\n\n.. _LaTeX: https://www.latex-project.org\n.. _MathJax: https://www.mathjax.org\n.. _MathML: https://en.wikipedia.org/wiki/MathML\n\n\nsee it\n------\n\nyou can see the examples from the above commands here:\n\n* http://marianoguerra.github.com/rst2html5/output/clean.html\n* http://marianoguerra.github.com/rst2html5/output/reveal.html\n* http://marianoguerra.github.com/rst2html5/output/deck.html\n* http://marianoguerra.github.com/rst2html5/output/impress.html\n* http://marianoguerra.github.com/rst2html5/output/bootstrap.html\n\nexample of video directive\n\n* http://marianoguerra.github.com/rst2html5/output/videos.html\n\n\ntest it\n-------\nWe use `tox <https://tox.readthedocs.org>`_ to run our test suite. After installing *tox* you can execute the tests by running ``tox`` in the project's root directory.\n\nThe test cases can be found in ``html5css3/tests.py``.\n\n\nwant to contribute ?\n--------------------\n\nclone and send us a pull request! ::\n\n    git clone https://github.com/marianoguerra/rst2html5.git\n    cd rst2html5\n    git submodule update --init\n    python setup.py develop\n\nnote to self to release\n-----------------------\n\n* update version on setup.py\n\n::\n\n    python setup.py sdist upload\n"
 },
 {
  "repo": "propublica/landline",
  "language": "HTML",
  "readme_contents": "                                                                       \n    _/                                  _/  _/  _/                     \n   _/          _/_/_/  _/_/_/      _/_/_/  _/      _/_/_/      _/_/    \n  _/        _/    _/  _/    _/  _/    _/  _/  _/  _/    _/  _/_/_/_/   \n _/        _/    _/  _/    _/  _/    _/  _/  _/  _/    _/  _/          \n_/_/_/_/    _/_/_/  _/    _/    _/_/_/  _/  _/  _/    _/    _/_/_/     \n                                                                       \nLandline is a JavaScript library that creates SVG maps from GeoJSON. \nIt comes with Stateline, which makes creating responsive U.S. state and county maps easy.\n\nDocumentation: http://propublica.github.io/landline/\nIssues or questions: https://github.com/propublica/landline/issues\n\nJan. 13, 2015: Al Jazeera America has created a jQuery wrapper for Landline/Stateline. Check it out: https://github.com/ajam/topline\n"
 },
 {
  "repo": "lmaccherone/sb-admin-2-meteor",
  "language": "HTML",
  "readme_contents": "\n# SB Admin 2 Meteor\n\n## Live demo\n\nhttp://sb-admin-2.meteor.com\n\n## Credits\n\nThis template site was forked from the Start Bootstrap SB Admin 2 template and adapted for Meteor. For more information\non the original see [Startbootstrap.com](http://startbootstrap.com/template-overviews/sb-admin-2/).\n\n## Features:\n\n  * CoffeeScript, Jade, and LESS (only slightly modified from the one provided by startbootstrap.com). The original is\n    raw JavaScript and HTML.\n  * Uses Iron Router (iron:router) for client-side routing.\n  * Left side navigation is dynamically generated and routing is automatically created from a simple specificaiton\n    in routes.coffee. No `<ul>` and `<li>` tags. Just list the menu items in JSON. Second level menu is created by\n    specifying the parent of menu item specification. Simple.\n  * Redirects and missing routes supported.\n  * Only dependency is roughly 70 lines of JavaScript and 60 lines of CSS for metisMenu jQuery plugin. All of this code\n    is included in this repository under plugins and loads correctly by Meteor.\n  * Only other dependencies are a few meteor packages. Run `meteor list` to duplicate.\n  * Wired into Meteor's account system for user creation, login, and logout.\n  * Two themes: a) light (like SB Admin 2), and b) dark (similar to SB Admin 1). However, both themes make better use\n    of LESS functionality (functions, variables, etc.) than the startbootstrap source.\n\n## Not done (yet?)\n\n  * Login for GitHub as an example for other login services.\n  * Storing other information in the user account.\n  * Tooltips. I think it's some missing CSS. Maybe the Bootstrap 3 package for Meteor is missing something? I would\n    appreciate a pull request to fix this. It might just be the .popover classes?\n  * Need to templatize the topbar navigation.\n  * Could use another theme or two.\n  * Would be nice to have a theme switcher.\n  * Charts. My main effort after this is to add my own visualizations so I may never support the Morris and Flot charts\n    that the original SB Admin 2 demonstrated.\n  * Only supports two levels of multi-menu. For my own purposes, I only need two so I doubt I'll ever upgrade this.\n    Three levels hard-coded should be pretty easy. N-levels would require some sort of template recursion.\n\n## This below copyright and license are as provided by the original repository\n\nCopyright 2013-2014 [Iron Summit Media Strategies](http://www.ironsummitmedia.com/)\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the\nLicense. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific\nlanguage governing permissions and limitations under the License.\n"
 },
 {
  "repo": "rikitanone/drunken-parrot-flat-ui",
  "language": "HTML",
  "readme_contents": "Drunken Parrot Flat UI Free\n=======\n\nDrunken Parrot Flat UI Free is licensed under a Creative Commons Attribution 3.0 Unported (CC BY 3.0)  (http://creativecommons.org/licenses/by/3.0/) and MIT License - http://opensource.org/licenses/mit-license.html.\n\n## Links:\n\n+ [Hoarrd.com](https://hoarrd.com/drunken-parrot-flat-ui-kit/)\n+ [Components & Style Guide: Free Version](http://hoarrd.github.io/drunken-parrot-flat-ui/)"
 },
 {
  "repo": "joeferraro/MavensMate-Desktop",
  "language": "HTML",
  "readme_contents": "## Update: August 7, 2017\n\n[MavensMate is no longer available for download and is not being actively developed or supported](http://mavensmate.com/). We recommend using Salesforce's [official Visual Studio Code plugin](https://marketplace.visualstudio.com/items?itemName=salesforce.salesforcedx-vscode) for building Salesforce applications.\n"
 },
 {
  "repo": "osis/simple_fb_html5_sidebar",
  "language": "HTML",
  "readme_contents": "HTML5 Facebook Style Sliding Menu Using Twitter Bootstrap Collapse\n=======================\n\nEver since Facebook, and other \u201csuper\u201d apps started to implement a side menu that slides out for their main navigation, this pattern has pretty much turned into the standard for any application that has a lot of content and a complex navigation stack. Organizing information with a sliding menu is becoming increasingly common, as numerous success stories of companies taking a \u201cmobile first\u201d approach has encouraged many to publish as much content on our mobile devices as possible.\n\nThis all there is to making a simple sliding menu in HTML5. You can check out a working example, with stub content, here. Hopefully, hardware acceleration will continue to improve in mobile browsers, across all platforms, to the point where performance differences using this UI pattern are negligible.\n\nMore information about this demo can be found [here]\n(http://tech.xtremelabs.com/html5-facebook-style-sliding-menu-using-twitter-bootstrap-collapse/).\n"
 },
 {
  "repo": "matthutchinson/paging_keys_js",
  "language": "HTML",
  "readme_contents": "h1. Paging Keys\n\nh2. About\n\nKeyboard short cuts for paging through listings one item at at time (and across entire pages).  Inspired by the navigation at \"FFFFOUND!\":http://ffffound.com/\n\nThe example operates on \"micro-formatted\":http://microformats.org/ html ( \"hAtom\":http://microformats.org/wiki/hatom ).  The script uses either the \"prototype\":http://www.prototypejs.org/ or \"jquery\":http://jquery.com/ js libraries and \"hotkey.js\":http://la.ma.la/blog/diary_200511041713.htm for key event hooks. Take a look at \"this video explanation\":http://www.37signals.com/svn/posts/1409-heres-a-demo-of-some-thoughtful-ui-on-ffffoundcom from \"Ryan Singer\":http://twitter.com/rjs of \"37 Signals\":http://www.37signals.com\n\nh2. \"Demo\":http://matthutchinson.github.com/sandbox/paging_keys/examples/prototype/example.html\n\nHoly explanations Batman!, there is a \"working (prototype) demo here\":http://matthutchinson.github.com/sandbox/paging_keys/examples/prototype/example.html or you can try the \"demo with jQuery\":http://matthutchinson.github.com/sandbox/paging_keys/examples/jquery/example.html\n\nh2. Latest Changes\n\n* Fixed floating navigation positioning in \"jquery\":http://jquery.com/ version (wrong selector doh!)\n* Added \"jquery\":http://jquery.com/ support thanks to Peter O'Toole (\"peteot\":http://github.com/peteot)\n* Fixed floating navigation links, (next/prev were linked the wrong way round)\n* Abstracted out library specific javascript functions (e.g. selector etc.)\n* Added some jquery unit testing, and jquery examples\n\nh2. Caveats\n\nOngoing issues with this script include;\n\n* needs more testing on different browsers\n* needs more testing coverage in general\n* there are some places where CSS id selectors would suit better over class selectors\n* so far manually tested on the following mac browsers; \n** Safari 3/4, FireFox 3/3.5, Opera 9+, Camino 1.6.9\n\nh2. Setup / Using\n\nh3. Requirements\n\nWhat do you need?\n\n* \"prototype\":http://www.prototypejs.org/ (>= 1.6.0.3) OR \"jquery\":http://jquery.com/ (>= 1.3.2)\n* a working keyboard and browser\n* for the example - a sense of \"1970's Batman humor\":http://adamwest.tripod.com/b-lectur.htm\n\nh3. Using\n\nTo use on your own website, simply;\n\n* markup your entry titles with the following CSS hierarchy; .hentry h2 a.entry-title (the example uses the \"hAtom\":http://microformats.org/wiki/hatom style)\n* include the prototype OR jquery library (e.g. from \"ajax.googleapis.com\":http://code.google.com/apis/ajax/)\n* include the appropriate paging_keys.js javascript somewhere on your page\n* add pagination html to each page as shown in the example (in \"Rails\":http://rubyonrails.org/, if you're using \"will_paginate\":http://wiki.github.com/mislav/will_paginate, you'll get this for free)\n* optionally add the paging-nav element somewhere on each page, and include the CSS for it\n* thats it! \n\nMake your browser height small enough to engage vertical scrolling, then use j/k to move up/down through listing (and across the pagination).  You can also use h/l to move between entire pages. So;\n\n* j next item (down)\n* k prev item (up)\n* h next page \n* l next page\n\nh3. Configuration\n\nThe config variable in pagingKeys can be used to customize the class to suit your own HTML/CSS selectors.  See the comments alongside each attribute for more info. \n\nh3. Minimized\n\nThe \"YUI compressor\":http://developer.yahoo.com/yui/compressor/ has been used successfully to minimise paging_keys.js (a minimised version is NOT included in the release).  All unit tests included run on a non-compressed script.\n\n\nh2. Example Configuration Explanation \n\nFour \"micro-formatted\":http://microformats.org/ ( \"hAtom\":http://microformats.org/wiki/hatom ) example pages with pagination links (in the \"will_paginate\":http://wiki.github.com/mislav/will_paginate style) The example uses the #bottom anchor link on the last post of each page, so paging backwards will start at the bottom and work its way up through the page.  Each example page has an optional paging navigation key at the top right corner (when javascript is available).  In the example the \"prototype\":http://www.prototypejs.org/ OR \"jquery\":http://jquery.com/ library is loaded from \"ajax.googleapis.com\":http://code.google.com/apis/ajax/ (this is a good idea).\n\nHave a look at the example html documents and the tests for more information.\n               \nh2. Why use the 'HJKL' keys?\n    \nEveryone has their own preferences/ideas on which keys work best. I chose what seems to be the accepted default right now and the comments below enforce this point somewhat.\n\n\"...because j and k are on the home row. Most keyboards have a little nubbin on the j so you can find it by touch. There is also some precedent for the pair. j/k move the cursor down/up in vi for example...\"\n\n\"...Google reader / Google Mail use the same keys...\"\n\n\"...Earlier computers had no need for arrow keys, so all the basic vi commands use keys you\u2019d find on a very limited keyboard...\"\n\n\"...But they\u2019re also very nice since it\u2019s where the right hand sits when you\u2019re a touch typist. j and k take almost no effort to type...\"\n\n\"...page up/down and cursor keys are not always available on every keyboard, or can be hard to find on laptops or shortened keyboards...\"\n\nh2. Credits\n\nWho's who?\n\n* Authored by \"Matthew Hutchinson\":http://matthewhutchinson.net\n* Some jQuery work by \"Peter O'Toole\":http://github.com/peteot\n* Inspired by \"FFFFOUND!\":http://ffffound.com/ keyboard naviagtion\n* Explained by \"Ryan Singer, 37 Signals\":http://www.37signals.com/svn/posts/1409-heres-a-demo-of-some-thoughtful-ui-on-ffffoundcom\n* Includes \"hotkey.js\":http://la.ma.la/blog/diary_200511041713.htm\n* Wise words followed from \"7 Rules of Unobtrusive Javascript\":http://icant.co.uk/articles/seven-rules-of-unobtrusive-javascript/\n\nh2. Get out clause\n\nRight now this script is provided without warranty, or support from the author.\n\nh2. Creative Commons License\n\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/2.0/uk/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"http://i.creativecommons.org/l/by/2.0/uk/80x15.png\" /></a>\n\n<span xmlns:dc=\"http://purl.org/dc/elements/1.1/\" property=\"dc:title\">Paging Keys</span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"http://github.com/matthutchinson/paging_keys_js\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">Matthew Hutchinson</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/2.0/uk/\">Creative Commons Attribution 2.0 UK: England &amp; Wales License</a>."
 },
 {
  "repo": "craigfrancis/dev-security",
  "language": "HTML",
  "readme_contents": "\n# Dev Security\n\nHow a \"security\" tab might work, using an [interactive demo](https://craigfrancis.github.io/dev-security/).\n\n- Chrome (issue [445359](https://crbug.com/445359))\n- Firefox (suggestion [6896989](https://ffdevtools.uservoice.com/forums/246087-firefox-developer-tools-ideas/suggestions/6896989-security-tab-to-show-tls-ssl-csp-sri-etc-feat))\n- Safari (bug report [19355985](https://bugreport.apple.com/))\n- Internet Explorer (suggestion [6901573](https://wpdev.uservoice.com/forums/257854-internet-explorer-platform/suggestions/6901573-security-tab-to-show-tls-ssl-csp-sri-etc-feat))\n\n[![Screenshot](https://raw.githubusercontent.com/craigfrancis/dev-security/master/resources/CSP.jpg)](https://craigfrancis.github.io/dev-security/)\n\nInspiration from Joel, and Chris, on the Google Chrome [security-dev](https://groups.google.com/a/chromium.org/d/msg/security-dev/yifaG5bDr8Q/lHgsAGs-kEUJ) mailing list.\n\nLucas also has some [notes](https://crbug.com/420813)."
 },
 {
  "repo": "nealrs/readsure",
  "language": "HTML",
  "readme_contents": "#Readsure\n\nYou should have to pass a quiz, that proves you actually _read_ an article before you can comment / share it.\n\n**[Try it](https://nealrs.github.io/readsure)**\n\nThis is a joke, but also a demo of how you could implement that.\n\n![animated gif demo](https://raw.githubusercontent.com/nealrs/readsure/gh-pages/demo.gif)\n"
 },
 {
  "repo": "egoist/data-tip.css",
  "language": "HTML",
  "readme_contents": "**Notice:** [hint.css](http://kushagragour.in/lab/hint/) has been much better since I complained about it months ago, so try out its new features instead of this one!\n\n<img src=\"http://r5.loli.io/aiYVJb.png\" align=\"right\" width=\"100\">\n\n# data-tip.css\n\nWow, such tooltip, with pure css!\n\n## Install\n\nCDN: https://npmcdn.com/data-tip@latest\n\n```bash\nbower install data-tip\nnpm install data-tip\n\n# additionally for Stylus lovers\n# you can import data-tip.styl directly\n@import '/path/to/data-tip'\n```\n\n## Usage\n\nSimply write like this in your HTML:\n\n```html\n<button class=\"data-tip-bottom\" data-tip=\"Tips To Show\">\n  My Custom Button\n</button>\n```\n\nPosition your tip:\n\n```html\ndata-tip-top\ndata-tip-bottom\ndata-tip-left\ndata-tip-right\n```\n\nColorful your tip:\n\n```html\ndata-tip-success\ndata-tip-warning\ndata-tip-danger\ndata-tip-info\n```\n\nAnti-animation:\n\n```html\ndata-tip-no-animation\n```\n\nRounded border:\n\n```html\ndata-tip-rounded\n```\n\nFast mode:\n\n```html\ndata-tip-fast\n```\n\nBox with shadow:\n\n```html\ndata-tip-shadow\n```\n\nAlways visible:\n\n```html\ndata-tip-visible\n```\n\n## Development\n\nUpdate `data-tip.styl` to change styles\n\n|command|description|\n|---|---|\n|npm install|install dependencies for dev|\n|npm run build|build html and css files|\n|npm run dev|build and watch file changes|\n\n## Browser Support\n\nCurrently it works on IE 8+ and most modern browsers. It uses `autoprefixer` so just modify `gulpfile.babel.js` to suit your need.\n\n## License\n\nMIT.\n"
 },
 {
  "repo": "Nemo64/meteor-bootstrap",
  "language": "HTML",
  "readme_contents": "#*Meteor Compatibility* \n`nemo64:bootstrap` does not work with Meteor 1.2+. [`huttonr/bootstrap3`](https://github.com/huttonr/bootstrap3) is a another similar package that works also with 1.2.\n\nBootstrap for Meteor\n====================\n\nThis package integrates bootstrap into meteor and lets you configure what parts you need.\n\nHow to install\n--------------\n\n1. execute `meteor add nemo64:bootstrap less`\n2. create an empty `custom.bootstrap.json` file somewhere in your project. (`/client/lib/custom.bootstrap.json` for example)\n3. start meteor and then edit the file you just created (see [custom.bootstrap.json](#custombootstrapjson)).\n4. (optional) edit `custom.bootstrap.import.less` which now appeared next to the json file\n\n`custom.bootstrap.json`\n---------------------\nThis file is to configure which bootstrap parts you need in your project. Set those you like to `true`!\nIf the file is empty, it will be filled for with the following content:\n\n```JSON\n{\n  \"modules\" : {\n    \"normalize\"            : true,\n    \"print\"                : false,\n    \"glyphicons\"           : false,\n\n    \"scaffolding\"          : false,\n    \"type\"                 : false,\n    \"code\"                 : false,\n    \"grid\"                 : false,\n    \"tables\"               : false,\n    \"forms\"                : false,\n    \"buttons\"              : false,\n\n    \"component-animations\" : false,\n    \"dropdowns\"            : false,\n    \"button-groups\"        : false,\n    \"input-groups\"         : false,\n    \"navs\"                 : false,\n    \"navbar\"               : false,\n    \"breadcrumbs\"          : false,\n    \"pagination\"           : false,\n    \"pager\"                : false,\n    \"labels\"               : false,\n\n    \"badges\"               : false,\n    \"jumbotron\"            : false,\n    \"thumbnails\"           : false,\n    \"alerts\"               : false,\n    \"progress-bars\"        : false,\n    \"media\"                : false,\n    \"list-group\"           : false,\n    \"panels\"               : false,\n    \"responsive-embed\"     : false,\n    \"wells\"                : false,\n    \"close\"                : false,\n\n    \"modals\"               : false,\n    \"tooltip\"              : false,\n    \"popovers\"             : false,\n    \"carousel\"             : false,\n\n    \"affix\"                : false,\n    \"alert\"                : false,\n    \"button\"               : false,\n    \"collapse\"             : false,\n    \"scrollspy\"            : false,\n    \"tab\"                  : false,\n    \"transition\"           : false,\n\n    \"utilities\"            : false,\n    \"responsive-utilities\" : false\n  }\n}\n```\n\nUpgrading\n---------\n\nAfter upgrading using `meteor update`, you may get an error stating:\n\n```\nWhile building the application:\n   client/lib/custom.bootstrap.less:1629:18: Less compiler error: ...\n```\n\nThis is due to the upstream Bootstrap library introducing LESS variables that your project is not aware of. The quickest fix is to execute the following script from within your project:\n\n```\ncurl https://raw.githubusercontent.com/Nemo64/meteor-bootstrap/master/upgrade/upgrade-3.3.4-3.3.5.sh | /bin/bash\n```\n\nAlternately, you can manually apply this [patch](https://github.com/Nemo64/meteor-bootstrap/blob/master/upgrade/3.3.5-upgrade.patch) to `custom.bootstrap.less`: \n\n\nContribution\n-------\n\nContributions are always welcome. I'm also searching for collaborators becuase I'm currently not actively deveolping with meteor. If you area interested, write me at git@marco.zone\n\nLicense\n-------\n\nThis package is licensed with the MIT license.\nAlso, look at the [Bootstrap license](https://github.com/twbs/bootstrap/blob/v3.2.0/LICENSE).\n\nOrigin\n------\n\nThis package is based on and inspired by the [bootstrap3-less](https://github.com/simison/bootstrap3-less) package. I created a new repository because it takes a completely different approach now which is also incompatible.\n"
 },
 {
  "repo": "beakerbrowser/explore",
  "language": "HTML",
  "readme_contents": "# Explore the peer-to-peer Web\n\nA curated list of peer-to-peer websites and apps.\n\n## Add your site\n\n[File an issue](../../issues) or [open a Pull Request](../../pulls) with your site. Be sure to include:\n\n - Which category\n - An image (if you have one)\n - The URL\n - A title\n - And a description\n"
 },
 {
  "repo": "StarterSquad/ngseed",
  "language": "HTML",
  "readme_contents": "ngseed\n======\n\nAngularJS/RequireJS seed project. [Documentation](https://github.com/StarterSquad/ngseed/wiki)\n\nInstallation\n------------\n\n    # Get NPM dependencies:\n    npm install\n\n    # Install global NPM dependencies:\n    npm -g install bower\n    npm -g install gulp\n    npm -g install karma\n\n    # Also to be able to run tests from CLI\n    # without browser window popping consider\n    # to install PhantomJS:\n    # http://phantomjs.org/download.html\n\n\n[Gulp](http://gulpjs.com/) flows\n----------\n\nTo make development faster and more automated there are several Gulp tasks available:\n\n* `gulp`\n\n  Builds project into `build` directory. Under the hood compiles and compresses Sass/CSS, compiles scripts\n  with ([RequireJs](http://requirejs.org/)) and [uglifies](http://lisperator.net/uglifyjs/) it.\n\n* `gulp bump-version`\n\n  Works with gitflow releases.\n  E.g. when you\u2019re on `release/0.4.4` branch it will update cache beaters to follow version.\n\n* `gulp karma`\n\n  Starts Karma server watching scripts updates.\n\n* `gulp karma-ci`\n\n  Runs tests against the build (which should be run first) and quits, is good to use in CI scenarios.\n\n* `gulp protractor`\n\n  Runs E2E tests against source files.\n\n* `gulp protractor-ci`\n\n  Runs E2E tests against the build.\n\n* `gulp sass`\n\n  Compiles Sass project, feeds output to [Autoprefixer](https://github.com/ai/autoprefixer) and minifies it via\n  [CSSO](https://github.com/css/csso).\n\n* `gulp watch`\n\n  Listens to changes to stylesheets and scripts and reloads browser page during development.\n\nCode style\n----------\n\nTo make code prone to minification [ng-annotate](https://github.com/olov/ng-annotate) module is used.\n\nVendor update\n-------------\n\n* `bower install`\n\n  To update all the dependencies to the latest compatible versions.\n\n## Tests\n\nTests use Jasmine for assertions.\n\nYou can write tests in both Coffee and JS\n(see `/source/js/modules/home/home-ctrl.spec.js` and `/source/js/modules/home/home-ctrl.spec.coffee`).\n\n### E2E Tests\n\n[Protractor](https://github.com/angular/protractor) is used to provide way to do E2E tests. To install go to `client`\ndirectory and run:\n\n    npm install -g protractor\n\n    // This installs Selenium standalone\n    // server and Chrome driver:\n    webdriver-manager update\n\n    // Start the server with:\n    gulp webdriver\n\n    // To test source:\n    gulp protractor\n\nCheck `p.conf` and `p-compiled.conf` for Protractor settings.\n\nCheckout [Protractor docs](https://github.com/angular/protractor/blob/master/docs/) for more information.\n\n## Future Releases\n\nYou can checkout planned new features on the [Trello Board](https://trello.com/b/XXevXg3l/ngseed).\nAlso feel free to create feature requests on github issues.\n"
 },
 {
  "repo": "MicrosoftLearning/Bootstrap-edX",
  "language": "HTML",
  "readme_contents": "# Bootstrap-edX\nThis project contains the sample files used for the Bootstrap edX course.\n"
 },
 {
  "repo": "dmfrancisco/activo",
  "language": "HTML",
  "readme_contents": "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">\n<html>\n\n<head>\n  <meta http-equiv=\"content-type\" content=\"text/html; charset=ISO-8859-1\">\n\n  <!-- This file uses a javascript library to convert the 'readme.textile' file to html -->\n\n  <title> </title>\n\n  <!-- Some CSS -->\n  <style>\n    body { padding-left:25px; font-family:Helvetica,Arial,sans-serif; font-size:10pt; }\n    a    { color: black; }\n    p    { margin-top: 7px; margin-bottom: 7px; }\n    h1   { padding-top:25px; padding-bottom:10px; }\n    h2   { padding-top:25px; text-decoration: underline; }\n    li   { list-style: none; }\n    #preview img { -webkit-box-shadow:0 1px 3px rgba(0, 0, 0, 0.4); -moz-box-shadow:0 1px 3px rgba(0, 0, 0, 0.4);\n                   box-shadow:0 1px 3px rgba(0, 0, 0, 0.4); margin-top: 10px; margin-bottom: 10px; border: none; }\n    #preview { width:840px; height:100%; }\n  </style>\n\n  <script src=\"javascripts/textile.js\"></script>\n  <script language=\"javascript\">\n    var filename = \"README.textile\"\n\n    function doConvert(html) {\n      document.getElementById('preview').innerHTML = convert(html);\n    }\n\n    var http = createRequestObject();\n\n    function createRequestObject() {\n      if (navigator.appName == \"Microsoft Internet Explorer\")\n        return new ActiveXObject(\"Microsoft.XMLHTTP\");\n      else\n        return new XMLHttpRequest();\n    }\n\n    function getNewContent() {\n      http.open('get', filename);\n      http.onreadystatechange = updateNewContent;\n      http.send(null);\n      return false;\n    }\n\n    function updateNewContent() {\n      if (http.readyState == 4)\n        doConvert(http.responseText);\n    }\n\n    getNewContent();\n    updateNewContent();\n  </script>\n</head>\n\n<body>\n  <h1><img src=\"http://dmfrancisco.github.com/activo/img/logo.png\" /> is a theme for <em>Web-app-theme</em>, <em>Formtastic</em> and <em>Attrtastic</em> *</h1>\n  <div id=\"preview\"></div>\n</body>\n\n</html>\n"
 },
 {
  "repo": "treeder/go-polymer",
  "language": "HTML",
  "readme_contents": "Go and Polymer - Friends Forever\n=============\n\nRelated blog post: [Go and Polymer so Happy Together](https://medium.com/iron-io-blog/go-and-polymer-so-happy-together-ba15f24b8de3)\n\nThis is an example application taken from the [official Polymer tutorial](https://www.polymer-project.org/docs/start/tutorial/intro.html),\nbut modified to use Go to serve the json rather than a static json file. This can be used as a basis point for your\nprojects where you want to use Go and Polymer.\n\nThis is ready to use standalone or on Google App Engine.\n\n## Live Demo\n\nhttp://go-polymer.appspot.com/\n\n## Go Get\n\n```\ngo get github.com/treeder/go-polymer\ncd $GOPATH/src/github.com/treeder/go-polymer\n```\n\n## Running locally\n\n```\ngo build && ./go-polymer\n```\n\nThen check http://localhost:8080\n\n## Running on App Engine\n\nSince this is ready to use on App Engine, you'll want to use the `goapp` tool.\n\n```\ngoapp serve\n```\n\n## Deploying on App Engine\n\nChange the application name in app.yaml to your application name (from Google Developer Console), then:\n\n```\ngoapp deploy\n```\n"
 },
 {
  "repo": "mavenecommerce/mbootstrap",
  "language": "HTML",
  "readme_contents": "<a href=\"http://mavenecommerce.com/\">![mavenecommerce](http://www.mavenecommerce.com/wp-content/themes/maven/images/logo.png)</a>\n\n# Magento-Bootstrap responsive theme\n\nMagento-Bootstrap theme it is package fully based on Twitter Bootstrap 3 framework.\n\n## Assets\n\n* Bootstrap (SASS latest official version)\n* HTML5 support\n* Microdata support (schema.org)\n* Composer installer\n* Gulp js/css builder\n\n### Requirements\n\n1. Composer  \u2014 Open https://getcomposer.org/doc/00-intro.md and install Composer to your system\n2. Node.js   \u2014 Open http://nodejs.org and install latest Node.js version\n3. Gulp      \u2014 Open http://gulpjs.com/ and install latest Gulp version\n\nYou can check if all of this installed by commands:\n```\n$ composer -V && node -v && gulp -v\nComposer version 1.0-dev (d1a9cfbd634d3b9e3350a77189de9c6b20737111) 2015-07-02 09:36:31\nv0.12.3\n[16:35:34] CLI version 3.9.0\n[16:35:34] Local version 3.9.0\n```\n\n## Project_root/ folders hierarchy example:\n```\n$ tree -L 2\n.\n\u251c\u2500\u2500 composer.json       -> composer.json file\n\u251c\u2500\u2500 composer.lock\n\u251c\u2500\u2500 magento             -> Magento CMS folder\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 LICENSE.html\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gulpconfig.js   -> ../vendor/mavenecommerce/mbootstrap/gulpconfig.js\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gulpfile.js     -> ../vendor/mavenecommerce/mbootstrap/gulpfile.js\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node_modules    -> ../node_modules\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 package.json    -> ../vendor/mavenecommerce/mbootstrap/package.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 var\n\u251c\u2500\u2500 node_modules        -> node_modules folder\n\u2514\u2500\u2500 vendor              -> installed composer modules\n```\n\n## How to install\n### Step 1. Install theme as Composer module\n\nCreate (or edit) `composer.json` file in your project folder, near `magento/` folder. You need add next lines to your composer.json:\n```\n\"repositories\": {\n    \"firegento\": {\n        \"type\": \"composer\",\n        \"url\": \"http://packages.firegento.com\"\n    },\n    \"mbootstrap\": {\n        \"type\": \"vcs\",\n        \"url\": \"git@github.com:mavenecommerce/mbootstrap.git\"\n    }\n},\n\"require\": {\n    \"magento-hackathon/magento-composer-installer\": \"*\",\n    \"mavenecommerce/mbootstrap\": \"dev-master\"\n},\n\"extra\": {\n    \"magento-root-dir\": \"magento\"\n}\n```\n\nYou need add `repository`, `require` and path to your magento folder `extra.magento-root-dir`.\n\nOr do it via commad line interface:\n```\n$ composer config repositories.firegento composer http://packages.firegento.com\n$ composer config repositories.mbootstrap vcs git@github.com:mavenecommerce/mbootstrap.git\n$ composer require magento-hackathon/magento-composer-installer:*\n$ composer require mavenecommerce/mbootstrap:dev-master\n```\n\nSo you will get something like that:\n```\n{\n    \"name\": \"mavenecommerce/mbootstrap-example\",\n    \"description\": \"Mavenecommerce MBootstrap Theme Example\",\n    \"minimum-stability\": \"stable\",\n    \"license\": \"proprietary\",\n    \"authors\": [\n        {\n            \"name\": \"Oleksii Filippovych\",\n            \"email\": \"a.filippovich@mavenecommerce.com\",\n            \"role\": \"Developer\"\n        }\n    ],\n    \"repositories\": {\n        \"firegento\": {\n            \"type\": \"composer\",\n            \"url\": \"http://packages.firegento.com\"\n        },\n        \"mbootstrap\": {\n            \"type\": \"vcs\",\n            \"url\": \"git@github.com:mavenecommerce/mbootstrap.git\"\n        }\n    },\n    \"require\": {\n        \"magento-hackathon/magento-composer-installer\": \"*\",\n        \"mavenecommerce/mbootstrap\": \"dev-master\"\n    },\n    \"extra\": {\n        \"magento-root-dir\": \"magento\"\n    }\n}\n\n```\n\nIf you don't use command line and just edit `composer.json` than run `$ composer install` (if you run it in first time) or `$ composer update --no-plugins` for update your dependies.\n\n### Step 2.1 Prepare Gulp build environment\n\nGo to your project folder and create `project_folder/node_modules/` folder. You get:\n```\n.\n\u251c\u2500\u2500 composer.json\n\u251c\u2500\u2500 composer.lock\n\u251c\u2500\u2500 magento         \u2014 Magento folder\n\u251c\u2500\u2500 node_modules    - npm node_modules/ folder\n\u2514\u2500\u2500 vendor          - Composer modules\n```\n\nThen open `magento/` folder and create symlink for `node_modules/` folder by command\n\n`$ ln -s ../node_modules node_modules`\n\n*If you are Windows user sorry but i don`t know how to help you with that :(*\n\nIt is need for Gulp builder.\nAfter that just run `$ npm install` in your `magento/` folder to install Gulp dependies. All Gulp dependies should be instaled to `project_folder/node_modules/` folder, example:\n```\n.\n\u251c\u2500\u2500 composer.json\n\u251c\u2500\u2500 composer.lock\n\u251c\u2500\u2500 magento\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gulpconfig.js -> ../vendor/mavenecommerce/mbootstrap/gulpconfig.js\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gulpfile.js -> ../vendor/mavenecommerce/mbootstrap/gulpfile.js\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node_modules -> ../node_modules\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 package.json -> ../vendor/mavenecommerce/mbootstrap/package.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 node_modules\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 del\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gulp\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gulp-autoprefixer\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gulp-bless\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gulp-cache\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gulp-concat\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gulp-imagemin\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gulp-load-plugins\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gulp-minify-css\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gulp-rename\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gulp-sass\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gulp-sourcemaps\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gulp-uglify\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 vinyl-paths\n\u2514\u2500\u2500 vendor\n```\n\nnote: *if you know how to make it easier share with us please :)*\n\n### Step 2.2 Build MBootstrap theme resources (JS/CSS)\n\nGo to `project_folder/magento/` and run `$ gulp`:\n```\n$ gulp\n[15:28:37] Using gulpfile /path_to_project_folder/vendor/mavenecommerce/mbootstrap/gulpfile.js\n[15:28:37] Starting 'build-base-scripts'...\n[15:28:38] Finished 'build-base-scripts' after 348 ms\n[15:28:38] Starting 'build-base'...\n[15:28:38] Finished 'build-base' after 17 \u03bcs\n[15:28:38] Starting 'build-mbootstrap-scripts'...\n[15:28:38] Finished 'build-mbootstrap-scripts' after 13 ms\n[15:28:38] Starting 'build-mbootstrap-styles-clean'...\n[15:28:38] Finished 'build-mbootstrap-styles-clean' after 1.36 ms\n[15:28:38] Starting 'build-mbootstrap-styles'...\n[15:28:39] Starting 'build-mbootstrap-images'...\n[15:28:39] Finished 'build-mbootstrap-images' after 759 ms\n[15:28:47] Finished 'build-mbootstrap-styles' after 9.31 s\n[15:28:47] Starting 'build-mbootstrap-styles-ie9'...\n[15:28:47] Finished 'build-mbootstrap-styles-ie9' after 129 ms\n[15:28:47] Starting 'build-mbootstrap'...\n[15:28:47] Finished 'build-mbootstrap' after 4.4 \u03bcs\n[15:28:47] Starting 'build'...\n[15:28:47] Finished 'build' after 2.82 \u03bcs\n[15:28:47] Starting 'default'...\n[15:28:47] Finished 'default' after 2.59 \u03bcs\n```\n\nThis bulder will create JS/CSS theme files:\n* JS: builder will generate scripts into `magento/js/build/` folder\n* CSS: builder will generate css-files into `magento/skin/frontend/mbootstrap/default/build/css/` folder\n\n### Step 3. Update Magento configuration\n\nGo to `Admin Panel -> System -> Configuration -> Developer` and open **Template Settings** Tab.\n\nSet **Allow Symlinks** to **Yes** and Save.\n\nGo to `Admin Panel -> System -> Configuration -> Design` and open **Package** Tab.\n\nSet **Current Package Name** with **mbootstrap** value and Save. Open **Theme** Tab and check **Default** option value \u2014 it should be empty or **default** value and Save.\n\nPS: rebuild cache if enable via `Admin Panel -> System -> Cache Management`. **Select All** and select *Refresh* or *Disable* action.\n"
 },
 {
  "repo": "fdisotto/SlimBlog",
  "language": "HTML",
  "readme_contents": "[SlimBlog](https://slimblog.fdisotto.com/)\n=====\n\n[![Join the chat at https://gitter.im/fdisotto/SlimBlog](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/fdisotto/SlimBlog?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nDemo\n---\n[Demo](https://slimblog.fdisotto.com/)\n\nDependencies\n---\n* [Slim Framework](http://slimframework.com)\n* [Eloquent](http://laravel.com/docs/eloquent)\n* [Twig](http://twig.sensiolabs.org)\n* [Bootstrap](http://getbootstrap.com)\n\nInstall\n---\n* Run 'php composer.phar install' in root directory\n* Run every *.sql files located in src/SQL and src/SQL/updates\n* Rename the config/database.config.php.install file to config/database.config.php and edit it with your database settings\n* Login with username:password\n* Enjoy\n\nFeatures\n---\n* Create new post with live markdown editor\n  * [Github flavored markdown](https://help.github.com/articles/github-flavored-markdown)\n* Edit/Delete posts and manage users\n* Manage settings\n* Template system\n* Multilanguage support\n\nToDo List\n---\n* Error manager\n* Different user level (Super admin, writers, other)\n* ~~Template system~~\n* Installation more user friendly\n* ~~I18n support~~\n\nScreenshots\n---\n![Home](http://i.imgur.com/W2KOdC5.png)\n![Live editor](http://i.imgur.com/V9ryFTk.png)\n![Post list](http://i.imgur.com/WVLQJKG.png)\n\nAuthor\n---\n**Fabio Di Sotto**\n* [Website](http://fabiodisotto.it)\n* [Github](https://github.com/fdisotto)\n* [Twitter](https://twitter.com/fdisotto)\n* [Facebook](https://facebook.com/fdisotto)\n\nContributors\n---\n**Andrew Smith**\n* [Github](https://github.com/silentworks)\n* [Website](http://silentworks.co.uk/)\n\n**mnlg**\n* [Github](https://github.com/mnlg)\n"
 },
 {
  "repo": "lashford/modern-web-template",
  "language": "HTML",
  "readme_contents": "Modern Web Template\n===========\n\n**AngularJS - Scala - Play - Guice - PlayReactiveMongo**\n\nA full application stack for a Modern Web application, lets review the components:\n\n* **AngularJS** - client side javascript framework for creating complex MVC applications in Javascript,\nfronted with Twitter bootstrap CSS framework, because well, im not a web designer.\n  * Take a look at what the google cool kids are upto here : [AngularJS](http://angularjs.org/)\n\n* **Bootstrap** - Bootstrap components written in pure AngularJS\n  *  [http://angular-ui.github.io/bootstrap/](http://angular-ui.github.io/bootstrap/)\n\n* **CoffeeScript** - CoffeeScript is an attempt to expose the good parts of JavaScript in a simple way.\n  *  [http://coffeescript.org/](http://coffeescript.org/)\n\n* **PlayFramework** - currently using 2.3.9 with the scala API\n  *  [PlayFramework Docs](http://www.playframework.com/documentation/2.3.9/Home)\n\n* **Guice** integration for Dependency injection,\n  * Special thanks to the typesafehub team for their activator : [Play-Guice](http://www.typesafe.com/activator/template/play-guice)\n\n* **PlayReactiveMongo** gives interaction with MongoDB providing a non-blocking driver as well as some useful additions for handling JSON.\n  * Check out their GitHub: [Play-ReactiveMongo](https://github.com/ReactiveMongo/Play-ReactiveMongo)\n\n\n\nGetting Started\n----------\n\nYour development environment will require:\n*  SBT / Play see [here]() for installation instructions.\n*  MongoDB see [here]() for installation instructions.\n\nOnce the prerequisites have been installed, you will be able to execute the following from a terminal.\n\n```\n../modern-web-template >  sbt run\n```\n\nThis should fetch all the dependencies and start a Web Server listening on *localhost:9000*\n\n```\n[info] Loading project definition from ../modern-web-template/project\n[info] Set current project to modern-web-template\n[info] Updating modern-web-template...\n...\n[info] Done updating.\n\n--- (Running the application from SBT, auto-reloading is enabled) ---\n\n[info] play - Listening for HTTP on /0:0:0:0:0:0:0:0:9000\n\n(Server started, use Ctrl+D to stop and go back to the console...)\n\n```\n\nNote: This will create a MongoDB Collection for you automatically, a freebie from the Driver!"
 },
 {
  "repo": "gastonmorixe/MessengerNative",
  "language": "HTML",
  "readme_contents": "![Facebook Messenger](https://cdn.rawgit.com/imton/MessengerNative/1dba4bb2b7b5e200ddcd58f7ee28db59fe2c8fc9/render/logo_github.png \"Facebook Messenger Native!\")\nMessenger Native\n================\n\nA native chromeless & frameless window wrapper around the new www.messenger.com\n \n![ScreenShot](https://cdn.rawgit.com/imton/MessengerNative/4d745f6d5f359f3c0455e1615c5caba9e57aedff/render/screenshot.png \"Screenshot!\")\n\n## How to Build it\n\n````bash\n$ npm install && grunt nodewebkit\n````\n\n--\n\n\n### HELP NEEDED! \n\nPULL REQUESTS + TESTING + FEEDBACK + ISSUES WELCOME!  :)\n"
 },
 {
  "repo": "aliceatlas/hexagen",
  "language": "C++",
  "readme_contents": "Hexagen\n=======\n\nTrue coroutines for Swift, and several familiar concurrency structures built on top of them.\n\nFeatures\n--------\n\n* Very little boilerplate for most use cases. (Largely made possible by Swift's type inference.)\n\n* Simple unidirectional and bidirectional generator functions, as in Python, C#, ECMAscript 6, etc.:\n\n  ```swift\n  let counter = { (n: Int) in Gen<Int> { yield in\n      for i in 0..<n {\n          yield(i)\n      }\n  }}\n\n  for i in counter(5) {\n      println(i)\n  }\n  ```\n\n* Interruptible Grand Central Dispatch task API allowing you to write asynchronous code in straightforward blocking style. When a task is waiting on some event (a timer firing, I/O availability or completion, etc.), instead of blocking the thread, it will suspend itself so its dispatch queue can continue processing tasks. When the event arrives, a block to resume the task is added to its dispatch queue.\n\n    * Included abstractions that know how to seamlessly suspend and resume tasks as needed:\n\n        * Channel: Supports a style of communication between tasks largely inspired by Go's channels and Goroutines.\n        * Promise: Allows any number of tasks to await a potentially pending result and awakens all of them when one becomes available.\n            * Timer: A Promise<Void> that is marked as fulfilled at a specified time.\n            * Feed: A lazily constructed open-ended series of Promises of a given type wrapped in Optional: after receiving a value from a Promise obtained from a Feed, you can get its successor and await the next value, and repeat until nil is returned, indicating that the feed has ended and won't contain any further values. (Feed implements Sequence so you can iterate over it with a for loop, this is often the most straightforward way to use it.)\n                * AsyncGen: A Task subclass with additional generator-like behavior \u2014 the body function receives a \"post\" function which is used somewhat like yield, but doesn't actually suspend the task; instead it sends values to an internal Feed, which other tasks can subscribe to by iterating over the task object.\n\n* 97% elegant! Very minimal abomination content, you should almost never have to encounter it.\n\n\nWarnings\n--------\n\n* Hexagen is in early development and pretty experimental to begin with, don't count on the API not changing drastically.\n\n* Your coroutines should always exit by returning \u2014 you can leave them hanging but you will leak memory. With Swift's lack of exceptions and use of ARC instead of garbage collection, I don't currently see a way to unilaterally tell a coroutine to terminate but still clean up after itself.\n\n* The task API needs to account for Cocoa APIs with thread-local behavior in order to make them work coherently and currently doesn't. This may be tricky in the cases of components that don't expose their thread-local variables in any directly manipulable form. Expect it to interact badly with autorelease pools.\n\n* More generally, this approach has turned out to be very surprisingly low on complications so far, but the whole thing is still a sketchy self-indulgent hack that violates some basic assumptions that almost all existing Objective-C and Swift code can expect to safely make. It's hard to say what potential interactions I might be overlooking, particularly given that the Swift toolchain is still closed-source. For now, I strongly discourage using this in production code unless you are very, very silly and reasonably confident that you are already going to hell.\n\n* If you are under the age of 180\u00ba or find this framework offensive, please don't look at it.\n\nNotes\n-----\n\n* Hexagen is written for Swift 1.2, first available in Xcode 6.3.\n\n* Currently Promises as implemented here are fulfill-only, i.e. there isn't a separate path for errors to take, like there tends to be in other languages' implementations of Promises. This is meant to mirror Swift's overall approach to error handling: to the extent that you need to write Promises that can express error conditions, you should encode that in your own types.\n\nIdeas/Todo\n----------\n\n* Library components\n    * Select\n    * Timeouts\n    * Read-only and write-only views of channels (and promises?)\n    * Elegant task-aware I/O API\n    * Subscribe to Cocoa events, notifications, key-value observing, etc. via Promises/Feeds\n    * Bridges to and from Hexagen features for existing widely-used Swift/Objective-C concurrency libraries/frameworks/approaches\n    * Task-local storage API\n* Internals\n    * Adapt to use Boost.Context directly once the upcoming version with execution\\_context is released and Xcode is shipping with usable support for thread\\_local in clang/libc++?\n* Project quality\n    * Unit tests\n    * Benchmarks\n    * More examples, better organized examples\n\n### Extra Credit ###\n\n* Side project: implement an alternative framework based on stackless generators (like e.g. Python's built-in yield: a function can only yield from itself to the function that called or most recently reentered it, because it's implemented more like an ordinary function call, getting its own frame on top of the current stack while it's running rather than having a separate stack to switch to).\n* Implement exception handling in pure Swift using Hexagen. *(Completing this successfully is worth negative points, and I will grudgingly respect you but never fully trust you.)*\n\nColophon\n--------\n\nHexagen is released under an MIT license (see LICENSE.md), so you can pretty freely incorporate and redistribute it wherever. The internal context-switching primitive is a thin wrapper around [Boost.Coroutine](http://www.boost.org/libs/coroutine/). Boost is free software under a permissive MIT-style [license](http://www.boost.org/users/license.html) and the parts of it used by Hexagen are already included in this repository.\n\nMy name is [Alice Atlas](https://github.com/aliceatlas) and I wrote the rest of this, I did it on purpose and I'm not sorry dad"
 },
 {
  "repo": "ldiqual/tesseract-ios-lib",
  "language": "C++",
  "readme_contents": "Tesseract Lib for iOS\n=====================\n\n\nAbout\n-----\n\nThis project contains only the [leptonica](http://www.leptonica.com/download.html) and [tesseract-ocr](http://code.google.com/p/tesseract-ocr/) libraries compiled for iOS.\n\nThere is no support for armv6, so it won't work with iPhone 1st Gen and iPhone 3G.\n\n\nUsage\n-----\n\nYou might want to use this [tesseract-ios](https://github.com/ldiqual/tesseract-ios) to include Tesseract in your iOS project. For a more advanced usage, you can use the raw library with regular C++ code.\n\nDon't forget to rename your implementation classes with `.mm` instead of `.m` as it uses C++ code.\n\n\nCode Sample\n-----------\n\nFollow [this blog post](http://tinsuke.wordpress.com/2011/11/01/how-to-compile-and-use-tesseract-3-01-on-ios-sdk-5/) for more informations.\n\n\n\n"
 },
 {
  "repo": "mangecoeur/ipython-desktop",
  "language": "C++",
  "readme_contents": "# PROJECT ON HOLD - Waiting for Jupyter ascending\n\nThe IPython project is undergoing heavy development at the moment as it is split into the Jupyter environment for interactive computing with IPython as just one of many supported kernels. As such, developing a desktop wrapped around the project means tracking a rapidly moving target. Therefore, I've decided to wait until things settle down a bit - likely the split will be more or less stabilised for version 4.0 which is planned for release later this year, at which point development of IPython desktop can start up again.\n\n# IPython Notebook Desktop\n\nThis is a proof of concept desktop interface for the IPython Notebook.\n\n## What's new\nThe latest revision improves ipython configuration and process handling. It will now try to automatically figure out the location of your ipython install and the url where the server is available when launched.\n\n## Concept\nIt's well established that IPython is awesome.\n\nMost IPython users end up using local installs of the IPython notebook in their browser. However this is somewhat clunky, mixing the browser interface and the notebook interface and generally requiring a trip to the command line to get the server running.\n\nThe IPython Notebook Desktop wraps the webapp in a more friendly interface, powered by node-webkit. You can configure a notebook to run to power the interface (optionally have it run on startup).\n\nWhat this does **NOT** do is provide you with an IPython installation. This is deliberate, since people have different needs and tastes with regards to their Python installs. Some people want to use the Python bundled with their operating system, others use Python distributions like Canopy or Anaconda. With IPython Desktop the Python distribution and the interface are separate, but you must configure IPython desktop to use your IPython installation.\n\nThe IPython Notebook Desktop doesn't aim to make it easier to install a scientific python environment, but should be easy enough to get by itself. It could eventually be a candidate for bundling with existing packages or with IPython itself.\n\n\n## Pretty pictures\nIPython embedded\n\n![Screenshot1](https://raw.githubusercontent.com/mangecoeur/ipython-desktop/master/assets/Screenshot1.png \"Screenshot1\")\n\nStart Screen:\n\n![Screenshot2](https://raw.githubusercontent.com/mangecoeur/ipython-desktop/master/assets/Screenshot2.png \"Start screen\")\n\nConfig Screen:\n\n![Screenshot3](https://raw.githubusercontent.com/mangecoeur/ipython-desktop/master/assets/Screenshot3.png \"Screenshot3\")\n\n\n## Get it!\n[App bundle for Mac](https://github.com/mangecoeur/ipython-desktop/raw/master/apps/ipython-desktop.zip)\n\nYou also need to have IPython installed. My personal recommendation is the Anaconda python distribution if you are mainly doing science and engineering work.\n\n**Coming sometime - binary bundles for each platform. Contributions welcome**\n\n## Configuration\n\nIPython desktop can either launch the IPython notebook server for you or connect to an existing URL.\n\nTo launch a server you must specify the location of your IPython executable, by default this is pre-filled (using the output of the command `which ipython`). You can optionally specify a Profile to use (which will be used with --profile=...)\n\n\n**IMPORTANT** - you must supply the full path to your IPython install otherwise it will fail to launch the ipython server\n\n**WARNING:** ipython-desktop is by no means idiot proof at the moment. If you don't configure it correctly the page will simply fail to load without explanation. This should improve in future versions.\n\n\n### URL only\nIf you set the \"remote\" option in the config you can simply type in the URL of your IPython server **including `http://` at the front!** handy if you just want a nicer interface for a remote system or just for testing.\n\n\n## Building ipython-desktop\nIn theory, the following steps should work (on Mac):\n\nRequirements\n- Xcode developer tools installed\n- `node` (nodejs) with `npm`, if you use Homebrew (and you should) just do `brew install node`.\n- `grunt` and `grunt-cli` (`npm install -g grunt grunt-cli` normally you will have to use sudo)\n\nSet up the project\nIn the terminal, `cd` into the source folder. Run `npm install`, `grunt nodewebkit`, `grunt install` to set up the dependencies for ipython desktop.\n\nFINALLY you should be able to run `grunt run` and see you shiny new ipython-desktop app, ready to configure.\n\n## Known Issues\n\n- IMPORTANT - you must configure ipython desktop with the FULL PATH of your ipython executable\n- Certain combinations of starting/stopping ipython servers and opening/closing windows might leave orphaned IPython processes (especially if you force quit the app)\n\n## TODO\n- Bundles for all OSes\n- Add fault tolerance e.g. for missing or misconfigured Ipython\n- More user friendly configuration of ipython\n- better integration with ipython notebooks - start/stop events, clean shutdown\n- Integration with Native menus!\n- Try to find the current iPython install using \"which iPython\" -> Done!\n- Try to auto-config profiles using \"ipython profile locate\"\n- Get url/port of running ipython using json from profile folder\n\n## Similar Work\n- Canopy: Enthought provide their Canopy desktop interface with IPython notebook integration. However this ties you into the EPD distribution. The IPython Notebook Desktop aims to be a lighter, more versatile solution\n- [IPython notebook](https://github.com/liyanage/ipython-notebook) Works in a similar vein, though is Mac only. It also differs in aim, since it bundles the essentials for scientific python computing. My aim with this project is to allow the interface to work with different Python installs, making it possible to use different python version and different virtual environments.\n- [IPyApp](https://github.com/ptone/IPyApp) Another project that uses node-webkit to wrap IPython notebook, but embeds the full python executable environment in the app.\n\n\n\n## Credits\nIPython desktop is powered by Node Webkit and makes use of the angular-desktop-app template. Icon is [IPython faenza](http://gnome-look.org/content/show.php?content=162145)\n\n## LICENCE\nThis software is currently under LGPL licence.\n"
 },
 {
  "repo": "Slashed/daemon.node",
  "language": "C++",
  "readme_contents": "Daemon Addon for Node.js\n\nTo build this module, type:\n> node-waf configure build\n\nFor more examples, read here: http://slashed.posterous.com/writing-daemons-in-javascript-with-nodejs-0\n"
 },
 {
  "repo": "patriciogonzalezvivo/KinectCoreVision",
  "language": "C++",
  "readme_contents": "<iframe class=\"roundPhoto\" src=\"http://player.vimeo.com/video/20904879?autoplay=1\" width=\"575\" height=\"359\" frameborder=\"0\" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>\n\n**KinectCoreVision** is a refurnished version of the well know track application [CommunityCoreVision](http://ccv.nuigroup.com/) to use a KinectSensor.\n\nThis version of CCV was original designed for a light and portable setup of [Communitas multiTouch](http://www.patriciogonzalezvivo.com/2010/communitas/) Table for IEATA\u00b4s 9th International Conference (more info and videos about it). But it was lot of potencial.\n\n# Binaries\n\n**MacOSX** \n\n* [Snow Leopard](https://dl.dropbox.com/u/335522/web/apps/KinectCoreVision-10.6.zip)\n* [Lion](https://dl.dropbox.com/u/335522/web/apps/KinectCoreVision.zip) \n\nMacports is required to be installed. Please check http://www.macports.org/ for its own dependencies and installation procedure. It requires XCode and would need to be installed on your Mac. Then: \n\n```sudo port install libtool```\n\n```sudo port install libusb-devel +universal```\n\n**Linux**\n\n* [Ubuntu 64bit](https://dl.dropbox.com/u/335522/web/apps/KinectCoreVision-linux64.zip)\n\n```sudo apt-get install libusb-1.0-0-dev```\n\n**Windows**\n\n* [Windows XP](https://dl.dropbox.com/u/335522/web/apps/KinectCoreVision-Win.zip)\n\n\n\n\n"
 },
 {
  "repo": "highpower/xiva",
  "language": "C++",
  "readme_contents": null
 },
 {
  "repo": "shmuelyr/CaptainHook",
  "language": "C++",
  "readme_contents": "# CaptainHook (version 2.1 beta)\n\n**this one isnt release, plz download from [here](https://github.com/shmuelyr/CaptainHook/releases)**\n\nCaptainHook is a hook framework for x86/x64 arch, it's based on [capstone](https://github.com/aquynh/capstone) disassembler engine.\nCaptainHook is easy to use, and very friendly.\nThe hook engine is much like MS Detours, so why to choose it?\n* it supports x64 (Detours x64 is commerical - $10,000~).\n* CaptainHook will know where to locate your hook in real time, the engine analyzes the code, and finds if small API redirection (Wow64 hook on kernelbase for example or some minimal JMP table redirection[particularly common in packed/protected code]) was occurred, and locate your hook in safe area.\n* in the next release, CaptainHook will contain an engine for jmp/conditional jmp repair - if your hook corrupts sensitive code.\n\n## whats new?\n* Disable/Eneble hook at runtime, without remove them.\n* some fix in the design of the code and the class.\n\ncode example:\n```c++\n\n#include \"CaptainHook.h\"\n#pragma comment(lib, \"CaptainHook.lib\")\n\nvoid (__fastcall *CH_OriginalFunction_1)(...) = OriginalFunction_1;\nvoid (__stdcall  *CH_OriginalFunction_2)(...) = OriginalFunction_2;\nvoid (__fastcall *CH_OriginalFunction_3)(...) = OriginalFunction_3;\nvoid (__fastcall *CH_OriginalFunction_4)(...) = OriginalFunction_4;\n\nint main() {\n\n\tunsigned int uiHookId1, uiHookId2;\n    CaptainHook *pChook = new CaptainHook();\n    if (!pChook) return 0;\n    \n\tpChook->AddInlineHook(&(void *&)CH_OriginalFunction_1, HookedOriginalFunction_1);\n\tpChook->AddPageGuardHook(&(void *&)CH_OriginalFunction_2, HookedOriginalFunction_2);\n    \n\tpChook->AddInlineHook(&(void *&)CH_OriginalFunction_3, HookedOriginalFunction_3, &uiHookId1);\n\tpChook->AddPageGuardHook(&(void *&)CH_OriginalFunction_4, HookedOriginalFunction_4, &uiHookId2);\n\t/*\n\t:\n\t*/\n\tpChook->DisableHook(uiHookId1);\n\t/*\n\t:\n\t*/\n\tpChook->EnableHook(uiHookId1);\n\t/*\n\t:\n\t*/\n\tpChook->~CaptainHook(); // this function clean all the hook(s).\n    return 0;\n}\n```\nCaptainHook.h for include is just [this](https://github.com/shmuelyr/CaptainHook/blob/master/CaptainHook/CaptainHook_for_include.h) file\n\n### how to build?\n```shell\n> git clone --recursive https://github.com/shmuelyr/CaptainHook.git\n> cd CaptainHook\n> \"C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\Common7\\Tools\\vsvars32.bat\"\n> msbuild.exe\n```\n\n### in the next version:\n* IAT hooking, for dll function.\n* hooking with hardware breakpoint.\n\n### Full example:\n```c++\n\n#include <Windows.h>\n#include \"CaptainHook.h\"\n\n#pragma comment(lib, \"CaptainHook.lib\")\n\nint(__stdcall *CH_OriginalMessageBoxA)\n\t(HWND hWnd, LPCSTR lpText, LPCSTR lpCaption, UINT uType) = MessageBoxA;\nint __stdcall MyMessageBoxA(HWND hWnd, LPCSTR lpText, LPCSTR lpCaption, UINT uType) {\n\n\treturn CH_OriginalMessageBoxA(hWnd, \"HOOK\", \"HOOK\", uType);\n}\n\nint WinMain(HINSTANCE hIns, HINSTANCE hPrev, LPSTR lpCmdLine, int cCmdShow) {\n\t\n\tunsigned int uiHookId;\n\tCaptainHook *pChook = new CaptainHook();\n\tpChook->AddPageGuardHook(&(void *&)CH_OriginalMessageBoxA, MyMessageBoxA, &uiHookId);\n\tMessageBoxA(NULL, \"test\", \"test\", MB_OK);\n\tpChook->DisableHook(uiHookId);\n\tMessageBoxA(NULL, \"test\", \"test\", MB_OK);\n\tpChook->EnableHook(uiHookId);\n\tMessageBoxA(NULL, \"test\", \"test\", MB_OK);\n\treturn 0;\n}\n```\n\n\nHappy Hooking!\n"
 },
 {
  "repo": "kylemcdonald/Makerbot",
  "language": "C++",
  "readme_contents": "This repository contains a collection of experiments and projects developed while in residence at Makerbot Industries.\n\nHere are some directions for getting a development environment running for KinectToStl:\ngit clone https://github.com/openframeworks/openFrameworks.git\ncd openFrameworks/addons\ngit clone https://github.com/kylemcdonald/ofxCv.git\ncd ofxCv\n./update-projects.sh\ncd ..\ngit clone https://github.com/kylemcdonald/ofxControlPanel.git\ngit clone https://github.com/ofTheo/ofxKinect.git\nsvn checkout http://julapy.googlecode.com/svn/trunk/openframeworks/ofxDelaunay ofxDelaunay\ncurl https://ruicode.googlecode.com/files/ofxSTL%20001.zip > ofxStl.zip\nunzip ofxStl.zip\nrm ofxStl.zip\n\nKinectToStl requires:\nopenFrameworks https://github.com/openframeworks/openFrameworks/\nofxDelaunay https://code.google.com/p/julapy/source/browse/trunk/openframeworks/ofxDelaunay/\nofxSTL https://code.google.com/p/ruicode/downloads/list\nofxKinect https://github.com/ofTheo/ofxKinect/tree/experimental\nofxControlPanel https://github.com/ofTheo/ofxControlPanel/\nofxReplicatorG https://github.com/cibomahto/ofxReplicatorG\nofxCv https://github.com/kylemcdonald/ofxCv\n"
 },
 {
  "repo": "KoffeinFlummi/AGM",
  "language": "C++",
  "readme_contents": "<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/KoffeinFlummi/AGM/master/.devfiles/Assets/Logo/agm_logo_black_transparent.png\" height=\"150px\" /><br />\n  <a href=\"https://github.com/KoffeinFlummi/AGM/releases\">\n    <img src=\"http://img.shields.io/badge/release-0.95.3-green.svg?style=flat\"\n         alt=\"AGM version\" />\n  </a>\n  <a href=\"https://github.com/KoffeinFlummi/AGM/releases/download/v0.95.3/AGM_v0.95.3.zip\">\n    <img src=\"http://img.shields.io/badge/download-21.9_MB-blue.svg?style=flat\"\n         alt=\"AGM download\" />\n  </a>\n  <a href=\"https://github.com/KoffeinFlummi/AGM/issues\">\n    <img src=\"http://img.shields.io/github/issues/KoffeinFlummi/AGM.svg?style=flat\"\n         alt=\"AGM issues\" />\n  </a>\n  <a href=\"https://github.com/KoffeinFlummi/AGM/blob/master/LICENSE\">\n    <img src=\"http://img.shields.io/badge/license-GPLv2-red.svg?style=flat\"\n         alt=\"AGM license\" />\n  </a>\n</p>\n<p align=\"center\"><sup><strong>Requires the latest version of <a href=\"http://www.armaholic.com/page.php?id=18767\">CBA A3</a> | <a href=\"http://makearmanotwar.com/entry/7jnWM53S2e\">Make Arma Not War</a> | <a href=\"http://forums.bistudio.com/showthread.php?178253-Authentic-Gameplay-Modification\">BIF thread</a></strong></sup></p>\n\n<p align=\"center\"><img src=\"http://makearmanotwar.com/assets/img/badges/Total-Modification.png\" height=\"150\" /></p>\n\n### &#x26A0; THIS MOD IS DISCONTINUED. PLEASE MOVE TO [ACE3](https://github.com/acemod/ACE3)\n\n**AGM** is a modular authenticism/realism mod for Arma 3, partly based on Taosenai's work with [TMR](https://github.com/Taosenai/tmr).\n\nThis mod is entirely **open-source**, and everyone is free to propose changes or maintain their own, customized version as long as they make their changes open to the public in accordance with the GNU General Public License (for more information check the license file attached to this project).\n\nThe mod is **built modularly** \u2014 almost any PBO can be removed, thus a team can maintain its own tailored version of AGM, which excludes a select number of components that they don't like, or which conflict with other add-ons. Components themselves, like e.g. the medical system, also include various customization options, allowing mission designers to tweak the overall experience.\n\nMore information on the purpose of the different components of AGM and which ones they rely on can be found [here](https://github.com/KoffeinFlummi/AGM/wiki#features).\n\n### Features\n*   Improved medical system\n*   Logistics system including cargo transport and vehicle maintenance\n*   Explosives system including different trigger types\n*   Captivity system\n*   Realistic names for vehicles and weapons\n*   Realistic ballistics including wind and humidity\n*   Backblast simulation\n*   Weapon resting and bipod deployment\n*   A fire control system for armored vehicles and helicopters<br />\n    ***and more...***\n\nFor a full list of current features, check [the official wiki](https://github.com/KoffeinFlummi/AGM/wiki).\n\n#### Guides & How-Tos\nIf you installed AGM, but have trouble understanding how it all works, or where to start \u2014 see [Getting Started](https://github.com/KoffeinFlummi/AGM/wiki/Getting-Started).\n\n#### Contributing\nIf you want to help put with the ongoing development, you can do so by looking for possible bugs or by contributing new features. To contribute something to AGM, simply fork this repository and submit your pull requests for review by other collaborators. In the process, feel free to add yourself to the author array of any .pbo you will be editing and the `AUTHORS.txt` file.\n\nTo report a bug, propose a feature, or suggest a change to the existing one \u2014 please, use our [Issue Tracker](https://github.com/KoffeinFlummi/AGM/issues).\n\nFor detailed information on how to make your weapons or vehicles compatible with AGM \u2014 read [this article](https://github.com/KoffeinFlummi/AGM/wiki/For-Addon-Makers).\n\n#### Testing & Building\nIf you want to help us test the latest development changes, feel free to download our master branch, using either git \u2014 if familiar with the process \u2014 or directly, by following [this link](https://github.com/KoffeinFlummi/AGM/archive/master.zip).\n\nTo binarize the addon for testing you can use the `binarizer.exe` in the main folder or, if you have [Python 3](https://www.python.org/) installed, use the python script for some more options. Alternatively you can manually binarize every PBO using Addon Builder or any other binarizer. All of those methods require the Arma 3 Tools to be installed, which can be found on Steam.\n\nIn order for the addons to be correctly binarized, you need to exclude the following file types from binarization (the binarizer script does this for you):\n```\n*.pac;*.paa;*.sqf;*.sqs;*.bikb;*.fsm;*.wss;*.ogg;*.wav;*.fxy;*.csv;*.html;*.lip;*.txt;*.wrp;*.bisurf;*.xml;*.hqf;\n```\n\n---\n<p align=\"center\"><a href=\"https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=HPAXPTVCNLDZS\"><img src=\"https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif\" style=\"max-width:100%;\"></a></p>\n"
 },
 {
  "repo": "kellabyte/rewind",
  "language": "C++",
  "readme_contents": "# Rewind\nRewind is a command log library. Similar to a WAL (write-ahead-log).\n\nCommand logging provides durability of database operations. Command logging keeps a record of every transaction. If the server fails for any reason, the database can restore the last snapshot and \"replay\" the log entries to recover the database.\n\n# Differences between a Command Log and a Write-Ahead-Log\nThe difference between a `command log` and `write-ahead-log` is that a `command log` records the invocations, whereas a `WAL` records the consequences of the transactions.\n\n# Goals\nThe goal for Rewind is to implement an optional logical command log for LMDB. LMDB is a very fast storage engine that performs very fast for many types of workloads but there are a few workloads that could potentially benefit from having a commit log in place. It's my long standing theory that a command log or WAL can help LMDB catch up to RocksDB in benchmarks RocksDB excels against LMDB without all the LSM complexity and LSM drawbacks. Rewind is an attempt to implement my ideas without adding a bunch of internal complexity to LMDB while enabling most LMDB consumers to use Rewind with very little or no code change.\n\n# Log entry format\nTODO.\n\n# Prerequisites\nIf you're on macOS don't forget to install the CLI developer tools.\n```\nxcode-select --install\n```\n\n# Compiling\n```\nmake\n```\n\n# Running tests\n```\nmake test\n\n===============================================================================\ntest cases: 1 | 1 passed\nassertions: - none -\n```\n\n# Running benchmarks\n```\nmake benchmark\n```"
 },
 {
  "repo": "naibaf7/libdnn",
  "language": "C++",
  "readme_contents": "# Greentea LibDNN\nGreentea LibDNN - a universal convolution implementation supporting CUDA and OpenCL\n"
 },
 {
  "repo": "vincentriemer/yoga-dom",
  "language": "C++",
  "readme_contents": "# Yoga DOM\n\nCustom bindings of Yoga Layout compiled to WebAssembly.\n"
 },
 {
  "repo": "mana/manaserv",
  "language": "C++",
  "readme_contents": "Setting up a Mana server\n========================\n\nCOMPILATION\n\nBefore trying to compile, make sure all the dependencies are installed. For\neach dependency the Ubuntu package name is listed as well as the website.\n\n * libsigc++ 2.0 (libsigc++-2.0-dev)  - http://libsigc.sourceforge.net/\n * libxml2       (libxml2-dev)        - http://xmlsoft.org/\n * Lua           (liblua5.1-0-dev)    - http://lua.org/\n * PhysFS        (libphysfs-dev)      - http://icculus.org/physfs/\n * SQLite 3      (libsqlite3-dev)     - http://sqlite.org/\n * zlib          (zlib1g-dev)         - http://zlib.net/\n\nOptional dependencies:\n\n * MySQL         (libmysqlclient-dev) - http://dev.mysql.com/\n   (replaces the SQLite 3 depency)\n\n\n1) cmake .\n2) make\n\nThe compilation should produce two binaries:\n\n* manaserv-account - The account + chat server\n* manaserv-game - The game server\n\n\nSERVER DATA\n\nThe client and the server share a big part of the data. See the example/\nsubfolder for an example world data.\n\n\nCONFIGURATION\n\nThe server loads its configuration from manaserv.xml, which it tries to find in\nthe directory where you're running the server from. An example file is located\nat docs/manaserv.xml.example.\n\nDefault option values:\n\n    accountServerAddress    localhost\n    accountServerPort       9601\n    gameServerAddress       localhost\n    gameServerPort          9604\n\n    worldDataPath           example\n\n\nRUNNING\n\n1. Run manaserv-account on one single computer. It will open three consecutive\nUDP ports, starting from the one given by the configuration option\n\"accountServerPort\". This first port is the one you should advertise to your\nusers. The configuration option \"accountServerAddress\" should contain the\npublic address the server runs on, as it will be sent to the users as the\naddress of the chat server, which happens to be the account server for now.\n\n2. Run manaserv-game on multiple computers. Each game server will open one UDP\nport given by the configuration option \"gameServerPort\". It will also connect\nto the account server given by the configuration options \"accountServerAddress\"\nand \"accountServerPort\". The configuration option \"gameServerAddress\" should\ncontain the public address of the computer the server runs on, as it will be\nsent to the users. The file data/maps.xml contains the maps the server will\nload and register on the account server; split it across your multiple game\nservers, in order to balance the load.\n\nAccess to port \"accountServerPort + 1\" of the account server can be restricted\nto connections from game servers only. Users do not need to access it.\n\n\nINITIAL DATABASE SETUP\n\nTo initally setup the database run the following command:\n\n\tcat src/sql/sqlite/createTables.sql | sqlite3 mana.db\n\nThis will generate a database called mana.db according to the needs of the server.\n\nFor making a player admin do:\n\n\tsqlite3 mana.db \"UPDATE mana_accounts SET level=255 WHERE username='MyAccount';\"\n\n"
 },
 {
  "repo": "ROCmSoftwarePlatform/hipCaffe",
  "language": "C++",
  "readme_contents": "# hipCaffe: the HIP Port of Caffe #\n\n\n## Introduction ##\n\nThis repository hosts the HIP port of [Caffe](https://github.com/BVLC/caffe) (or hipCaffe, for short). For details on HIP, please refer [here](https://github.com/GPUOpen-ProfessionalCompute-Tools/HIP). This HIP-ported framework is able to target both AMD ROCm and Nvidia CUDA devices from the same source code. Hardware-specific optimized library calls are also supported within this codebase.\n\n## Prerequisites ##\n\n### Hardware Requirements ###\n\n* For ROCm hardware requirements, see [here](https://github.com/RadeonOpenCompute/ROCm/blob/master/README.md#supported-cpus)\n\n### Software and Driver Requirements ###\n\n* For ROCm software requirements, see [here](https://github.com/RadeonOpenCompute/ROCm/blob/master/README.md#the-latest-rocm-platform---rocm-15)\n\n## Installation ##\n\n### AMD ROCm Installation ###\n\nFor further background information on ROCm, refer [here](https://github.com/RadeonOpenCompute/ROCm/blob/master/README.md)\n\nInstall ROCm Debian packages:\n  \n      PKG_REPO=\"http://repo.radeon.com/rocm/apt/debian/\"\n      \n      wget -qO - $PKG_REPO/rocm.gpg.key | sudo apt-key add -\n      \n      sudo sh -c \"echo deb [arch=amd64] $PKG_REPO xenial main > /etc/apt/sources.list.d/rocm.list\"\n     \n      sudo apt-get update\n      \n      sudo apt-get install rocm-dkms rocm-utils rocm-opencl rocm-opencl-dev rocm-profiler cxlactivitylogger\n\nNext, update your paths and reboot: \n\n      echo 'export PATH=/opt/rocm/bin:$PATH' >> $HOME/.bashrc\n      \n      echo 'export LD_LIBRARY_PATH=/opt/rocm/lib:$LD_LIBRARY_PATH' >> $HOME/.bashrc\n\n      source $HOME/.bashrc\n      \n      sudo reboot\n\nThen, verify the installation. Double-check your kernel and the installed kernel modules:\n\n      uname -r\n      \n      lsmod | grep kfd\n\nIn addition, check that you can run the simple HSA vector_copy sample application:\n\n      pushd /opt/rocm/hsa/sample\n        \n      make\n       \n      ./vector_copy\n      \n      popd\n\n### Pre-requisites Installation ###\n\nInstall Caffe dependencies:\n\n    sudo apt-get install \\\n    \tpkg-config \\\n    \tprotobuf-compiler \\\n    \tlibprotobuf-dev \\\n    \tlibleveldb-dev \\\n    \tlibsnappy-dev \\\n    \tlibhdf5-serial-dev \\\n    \tlibatlas-base-dev \\\n    \tlibboost-all-dev \\\n    \tlibgflags-dev \\\n    \tlibgoogle-glog-dev \\\n    \tliblmdb-dev \\\n    \tpython-numpy python-scipy python3-dev python-yaml python-pip \\\n    \tlibopencv-dev \\\n    \tlibfftw3-dev \\\n    \tlibelf-dev\n\t\nInstall some misc development dependencies:\n\n    sudo apt-get install git wget\n\nInstall the necessary ROCm compute libraries:\n\n    sudo apt-get install rocm-libs miopen-hip miopengemm\n\n      \n### hipCaffe Build Steps ###\n\nClone hipCaffe (1.7.1 update: choosing the hip implementation):\n\n    git clone -b hip https://github.com/ROCmSoftwarePlatform/hipCaffe.git\n\n    cd hipCaffe\n\nYou may need to modify the Makefile.config file for your own installation.  Then, build it:\n\n    cp ./Makefile.config.example ./Makefile.config\n    \n    make \n\nTo improve build time, consider invoking parallel make with the \"-j$(nproc)\" flag.\n\n\n## Unit Testing ##\n\nRun the following commands to perform unit testing of different components of Caffe.\n\n    make test\n    \n    ./build/test/test_all.testbin\n\n## Example Workloads ##\n\n### MNIST training ###\n\nSteps:\n\n       ./data/mnist/get_mnist.sh\n\n       ./examples/mnist/create_mnist.sh\n       \n       ./examples/mnist/train_lenet.sh\n\n### CIFAR-10 training ###\n\nSteps:\n\n       ./data/cifar10/get_cifar10.sh\n       \n       ./examples/cifar10/create_cifar10.sh\n       \n       ./build/tools/caffe train --solver=examples/cifar10/cifar10_quick_solver.prototxt\n\n### CaffeNet inference ###\n\nSteps:\n\n       ./data/ilsvrc12/get_ilsvrc_aux.sh\n\n       ./scripts/download_model_binary.py models/bvlc_reference_caffenet\n\n       ./build/examples/cpp_classification/classification.bin \\\n            models/bvlc_reference_caffenet/deploy.prototxt \\\n\t    models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel \\\n\t    data/ilsvrc12/imagenet_mean.binaryproto \\\n\t    data/ilsvrc12/synset_words.txt \\\n\t    examples/images/cat.jpg\n\n### Soumith's Convnet benchmarks ###\n\nSteps:\n\n\tgit clone https://github.com/soumith/convnet-benchmarks.git\n\n\tcd convnet-benchmarks/caffe\n\nOPTIONAL:  reduce the batch sizes to avoid running out of memory for GoogleNet and VGG.  For example, these configs work on Fiji:\n\tsed -i 's|input_dim: 128|input_dim: 8|1' imagenet_winners/googlenet.prototxt\n\n\texport CAFFE_ROOT=/path/to/your/caffe/installation\n\n\tsed -i 's#./caffe/build/tools/caffe#$CAFFE_ROOT/build/tools/caffe#' ./run_imagenet.sh\n\n\t./run_imagenet.sh\n\n\n## Known Issues\n\n### Temp workaround for multi-GPU data transfer error\n\nSometimes when training with multiple GPUs, we hit this type of error signature:  \n```\n*** SIGSEGV (@0x0) received by PID 57122 (TID 0x7fd841500b80) from PID 0; stack trace: ***\n    @     0x7fd8409a1390 (unknown)\n    @     0x7fd8400a71f7 (unknown)\n    @     0x7fd840515263 (unknown)\n    @     0x7fd81f5ef907 UnpinnedCopyEngine::CopyHostToDevice()\n    @     0x7fd81f5d3bb9 HSACopy::syncCopyExt()\n    @     0x7fd81f5d28bc Kalmar::HSAQueue::copy_ext()\n    @     0x7fd8410dba5b ihipStream_t::locked_copySync()\n    @     0x7fd8411030bf hipMemcpy\n    @           0x6cfd43 caffe::caffe_gpu_rng_uniform()\n    @           0x5a32ba caffe::DropoutLayer<>::Forward_gpu()\n    @           0x430bbf caffe::Layer<>::Forward()\n    @           0x6fefe7 caffe::Net<>::ForwardFromTo()\n    @           0x6feeff caffe::Net<>::Forward()\n    @           0x801e8c caffe::Solver<>::Step()\n    @           0x8015c3 caffe::Solver<>::Solve()\n    @           0x71a277 caffe::P2PSync<>::Run()\n    @           0x42dcbc train()\n```\n\nSee this [comment](https://github.com/ROCmSoftwarePlatform/hipCaffe/issues/11#issuecomment-318518802).\n\nIn short, here's the temporary workaround:  \n```\nexport HCC_UNPINNED_COPY_MODE=2\n```\n\nPlease note that we have a long-term solution -- using a new RNG lib -- that we'll be pushing out soon.  \n\n\n## Tutorials\n\n* [hipCaffe Quickstart Guide](http://rocm-documentation.readthedocs.io/en/latest/Tutorial/hipCaffe%20.html#hipcaffe)\n"
 },
 {
  "repo": "google/crunchy",
  "language": "C++",
  "readme_contents": "# CrunchyCrypt - Safe and Simple Cryptography\n\nCrunchyCrypt is an opensource library offering safe and easy-to-use cryptography\nAPIs with a built-in key-versioning protocol.\n\n## Table of Contents\n\n- [About CrunchyCrypt](#about)\n- [Codemap](#codemap)\n- [Compatibility guarantees](#compatibility)\n- [License](#license)\n\nContact us at crunchy-discuss@googlegroups.com\n([link](https://groups.google.com/forum/#!forum/crunchy-discuss))\n\n<a name=\"about\"></a>\n## About CrunchyCrypt\n\nCrunchyCrypt is an open-source collection of cryptography APIs, safe and\neasy-to-use wrappings of lower-level crypto libraries such as boringssl.\nAlthough CrunchyCrypt is intended to primarily be a curated collection of\nmodern cryptography, CrunchyCrypt is designed to be extendable to both\nbleeding-edge and legacy cryptography.\n\nCrunchyCrypt has a built-in key versioning protocol, where cryptographic\npayloads (signatures and ciphertexts) are (optionally) prefixed with a few\nbytes of key versioning information. This allows project owners to gracefully\nrotate new crunchy keys while maintaining backwards compatibility with old keys,\neven while switching the underlying algorithm.\n\nSafety and ease-of-use are primary features of CrunchyCrypt, which is good for\nmost, but not all, use cases. For example, user-specificed nonces is not\nsomething we intend to support. As a consequence, CrunchyCrypt is not meant to\nbe a comprehensive replacement for openssl/boringssl.\n\n<a name=\"codemap\"></a>\n## Codemap\n\nCrunchyCrypt supports crypto and key management in C++. CrunchyCrypt supports\ncrypto in Java via JNI bindings.\n\nCrunchyCrypt supports the following primitives in C++:\n\n* [`AEAD Encryption`](crunchy/crunchy_crypter.h) Authenticated Encryption with\n  Associated Data, aka symmetric encryption\n  <br /> We support AES-GCM and AES-EAX at 128 and 256 bits of security.\n* [`MACs`](crunchy/crunchy_macer.h) Message authentication code, aka symmetric\n  authentication\n  <br /> We support HMAC-SHA256 with 16-byte tags and a 32-byte key.\n* [`Digital Signatures`](crunchy/crunchy_signer.h), aka asymmetric\n  authentication\n  <br /> We support P256-ECDSA, Ed25519, and RSA-PKCS1 using SHA256 and a\n  2048-bit modulus.\n* [`Hybrid Encryption`](crunchy/crunchy_hybrid_crypter.h), aka asymmetric\n  encryption\n  <br /> We support ECIES using HKDF and AEADs in various combinations,\n  including versions using P256 and curve25519.\n\nCrunchyCrypt supports [`key management`](crunchy/key_management/) in C++.\nCrunchyCrypt's built-in key-versioning protocol allows for graceful rotation of\nkeys. [`KeysetManager`](crunchy/key_management/key_manager.h) is used to create,\nrotate, and delete keys.  Serialization of unencrypted key material is in a\nseparate [`keyset_serialization`](crunchy/key_management/keyset_serialization.h)\nbuild target.\n\n[`Java APIs`](crunchy/java/src/main/java/com/google/security/crunchy/) for the\nabove are implemented via [`JNI\nbindings`](crunchy/java/src/main/com/google/security/crunchy/jni).\n\nSome internal APIs may be eventually made user-facing as we gain more experience\nas to how they might be used. For example,\n[`AdvancedKeysetManager`](crunchy/key_management/) and [`our subtle crypto\nAPIs`](crunchy/internal/algs) might be made non-internal if they're deemed\nuseful.\n\n<a name=\"compatibility\"></a>\n## Compatibility guarantees\n\nWe do not offer any ABI compatibility. We will strive to not break API\ncompatibility. If we plan to break API compability, we will provide a migration\npath.\n\nWe ask that you:\n\n* Don't open the crunchy namespace.\n* Don't forward-declare crunchy types.\n* Don't depend on internal details, namespaces or files that contain the word\n  \"internal\".\n\n<a name=\"license\"></a>\n## License\n\nCrunchyCrypt is licensed under the terms of the Apache License, Version 2.0. See\n[LICENSE](LICENSE) for more information.\n"
 },
 {
  "repo": "j-cube/multiverse",
  "language": "C++",
  "readme_contents": "# Multiverse\n\n`Multiverse` is the next-generation data storage back-end for the widely used [Alembic](https://github.com/alembic/alembic) file format.\n\n> Authors: Marco Pantaleoni, Aghiles Kheffache, Bo Zhou, Paolo Berto Durante.\n\n\nThe _\"Multiverse\"_ name is also used for the proprietary commercial product [Multiverse | Studio](http://multi-verse.io). `Multiverse | Studio` is a fast, efficient and easy to use solution for CGI/VFX studios, whose main purposes are to scale complexity in look-development, render procedurally, inter-op between DCC Apps like [Maya](https://autodesk.com/products/maya) and [Katana](https://foundry.com/products/katana). `Multiverse | Studio` offers full integration for the Multiverse back-end and as of June 2017 is distributed by [Foundry](https://foundry.com).\n\n> NOTE: This repository is about the Multiverse _back-end_, not the end-user product Multiverse | Studio.\n\n## Repository organization\n\nThe original upstream Alembic code resides in the [master](https://github.com/j-cube/multiverse/tree/master) branch.\n\nIn terms of version numbering, `Multiverse` stays in-sync with the Alembic releases: \n\nthe `Multiverse` code for the `1.7.1` release, resides in its own [1.7.1/multiverse](https://github.com/j-cube/multiverse/tree/1.7.1/multiverse) branch and is the default landing branch for the repository when browsing on GitHub.\n\nTo use `Multiverse`, just checkout its branch:\n\n```\n$ git clone https://github.com/j-cube/multiverse.git\n...\n\n$ git checkout 1.7.1/multiverse\n```\n\n\n## Multiverse dependencies\n\nIn addition to the original Alembic dependencies, `Multiverse` needs the following additional dependencies:\n\n* [libgit2 v0.23.4+](https://github.com/libgit2/libgit2/archive/v0.23.4.tar.gz)\n* [msgpack 1.0.1+](https://github.com/msgpack/msgpack-c/releases/download/cpp-1.0.1/msgpack-1.0.1.tar.gz)\n* [boost 1.48.0+](http://sourceforge.net/projects/boost/files/boost/1.48.0/boost_1_48_0.tar.bz2/download) compiled also with `chrono`, `filesystem`, `system` (in addition to the original `program_options`, `thread` and `python`)\n\n### \"Classic\" Git and Milliways\n\n`Multiverse` source contains [Milliways](https://github.com/j-cube/milliways) \"The storage at the back-end of the Multiverse\". Milliways is a high-performance on-disk tree-based key-value store, used in Multiverse as an optional pluggable back-end to `libgit2`. `Milliways` was created to improve performance over the \"classic\" Multiverse Git backend by introducing a single file store database. As of release 1.7.1 we default to Milliways and a single .abc file is created, making the Git repository non visible to the user. CLassic Git repository can always be created optionall. \n\n\n## Research\n\n`Multiverse` and `Milliways` are research projects by J CUBE Inc. Multiverse was published at SIGGRAPH Asia 2015 in Kobe, Japan. More information on [J CUBE Research](http://j-cube.jp/research).\n\n## Multiverse license\n\n`Multiverse` modifications and additions are:\n\n```\nMultiverse - a next generation storage back-end for Alembic\n\nCopyright 2015\u20142017 J CUBE Inc. Tokyo, Japan.     \n                                                                     \nLicensed under the Apache License, Version 2.0 (the \"License\");         \nyou may not use this file except in compliance with the License.        \nYou may obtain a copy of the License at                                 \n                                                                        \n    http://www.apache.org/licenses/LICENSE-2.0                          \n                                                                        \nUnless required by applicable law or agreed to in writing, software     \ndistributed under the License is distributed on an \"AS IS\" BASIS,       \nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and     \nlimitations under the License.                             \n\n```\n\n```\nAuthors:\n\n Marco Pantaleoni\n Aghiles Kheffache\n Bo Zhou\n Paolo Berto Durante\n```\n\n```\nContact Informations:\n\n    J CUBE Inc.                                                          \n    6F Azabu Green Terrace                                                   \n    3-20-1 Minami-Azabu, Minato-ku, Tokyo, Japan                                 \n    info@-jcube.jp                                                           \n    http://j-cube.jp\n```\n\nSee the [LICENSE-multiverse.txt](LICENSE-multiverse.txt) file for more details.\n\nThe rest of Alembic remains covered by the original [LICENSE.txt](LICENSE.txt) file.\n\nSee also the original [README.txt](README.txt) part of Alembic.\n"
 },
 {
  "repo": "ArbiterLib/Arbiter",
  "language": "C++",
  "readme_contents": "# Arbiter [![Build Status](https://travis-ci.org/ArbiterLib/Arbiter.svg?branch=master)](https://travis-ci.org/ArbiterLib/Arbiter)\n\nArbiter is a cross-platform C library<sup>1</sup> which implements the baseline functionality that should be expected of any dependency manager or package manager, without being coupled to any particular use case, so that many more specific tools can be built on top.\n\nIn other words, **Arbiter does not prescribe any one user experience**\u2014it just tries to solve those backend concerns which are common to all dependency managers.\n\n_<sup>1</sup> Note that Arbiter is actually implemented in C++14, but currently only exposes a plain C API to minimize surface area and maximize interoperability._\n\n## Motivation\n\nArbiter\u2019s main contributors originally built [Carthage](https://github.com/Carthage/Carthage), a dependency manager for iOS and macOS frameworks. Carthage introduced some novel ideas which aren\u2019t commonly found in other dependency managers, like a focus on [decentralization](#lazy-decentralized-dependency-resolution). Unfortunately, Carthage is fairly coupled to Apple\u2019s development tools and process.\n\nArbiter was conceived, in part, to **generalize the best of Carthage\u2019s features** for use by _any_ dependency manager, on a variety of platforms.\n\nThere are also some baseline features that we believe any dependency manager should support in order to be taken seriously, including:\n\n* **Dependency \u201cfreezing.\u201d** Different tools have different names for this concept, but the basic idea is the same: once dependencies have been resolved, _all projects and the versions selected for each_ should be saved to disk so that anyone else collaborating on the parent project can reproduce the dependency checkouts exactly.\n* **Automatic conflict resolution.** If two projects in the dependency graph specify mutually exclusive requirements for a shared dependency, the dependency resolver should still be able to back up (e.g., to different versions of the two projects) and try another configuration that might result in success.\n* **Safe uninstallation.** The tool should understand when uninstalling one package would break another, and surface this information to the user.\n\nSince there are many dependency managers and package managers which do not meet all of the above criteria, a generic library like Arbiter could be used to fill in the gaps.\n\n## Functionality\n\nSome major features of Arbiter include:\n\n### Compliance with Semantic Versioning\n\n[Semantic Versioning](http://semver.org), or SemVer, is a specification for what software version numbers _mean_, and how they should be used to convey compatibility (and the lack thereof).\n\nArbiter implements SemVer and incorporates it into its [dependency resolution algorithm](#lazy-decentralized-dependency-resolution), so that complex versioning and compatibility logic does not have to be reinvented from scratch for each new tool.\n\n### Lazy, decentralized dependency resolution\n\nMost package managers require a centralized server which has knowledge of all packages and versions in the system.\n\nHowever, Arbiter resolves individual dependencies _on demand_, allowing them to be loaded from anywhere\u2014even different places for different versions! This doesn\u2019t preclude using a centralized server, but means that it is not a requirement.\n\n### Parallelizable dependency installation\n\nArbiter does not itself determine what \u201cinstalling\u201d a package means, but can provide information to the package manager about the installation process.\n\nSpecifically, Arbiter understands when one package must be installed before another, and conversely when certain packages have no implicit relationship to each other. The package manager can use this information to download and install multiple packages concurrently, potentially reducing wait times for the end user.\n\n### \u2026 and more to come\n\nFor a full list of planned features, check out our [backlog](https://github.com/ArbiterLib/Arbiter/issues?q=is%3Aopen+is%3Aissue+label%3Aenhancement+sort%3Acreated-desc). If you\u2019d be interested in making any of these a reality, please consider [contributing](CONTRIBUTING.md)!\n\n## Documentation\n\nThe Arbiter API is extensively documented in header comments, from which we periodically generate [Doxygen pages](http://arbiterlib.github.io/Arbiter/). For the public C API, look at headers under [`include/arbiter/`](include/arbiter/) in the [file list of the documentation](http://arbiterlib.github.io/Arbiter/files.html).\n\n## Examples\n\nThis repository contains not-production-strength [examples](examples/) for demonstrating how the Arbiter API can be used to build different functionality.\n\nTo compile all included examples, run `make examples`.\n\nFor more information about individual examples, see the README in each folder. Of course, there are almost certainly other possible uses that we assuredly haven\u2019t thought of or implemented, so this shouldn\u2019t be taken as an exhaustive showcase!\n\n## Bindings\n\nBecause the functionality of Arbiter is exposed in a C interface, it\u2019s easy to build bindings into other languages. Currently, Arbiter already has [Swift bindings](bindings/swift/), with more planned!\n\nTo compile all included bindings, run `make bindings`.\n\nIf you\u2019d like to implement your own bindings, please let us know about them [in a GitHub issue](https://github.com/ArbiterLib/Arbiter/issues/new), and we can include a link here in the README.\n\n## License\n\nArbiter is released under the [MIT license](LICENSE.md).\n\nI am providing code in this repository to you under an open source license. Because this is my personal repository, the license you receive to my code is from me and not from my employer (Facebook).\n"
 },
 {
  "repo": "laurentlb/Ctrl-Alt-Test",
  "language": "C++",
  "readme_contents": "# Ctrl-Alt-Test\n\n[ctrl-alt-test.fr](http://ctrl-alt-test.fr)\n\nThis repository contains source code of our demoscene productions.\nThese productions are tiny executable files, that display real-time\nanimations with music. This is made using procedural generation and\ncompression tools.\n\n\nThe links below provide additional information (including video captures):\n\n* [B-Incubation](http://www.ctrl-alt-test.fr/?page_id=94) (64kb)\n* [D-Four](http://www.ctrl-alt-test.fr/?page_id=315) (4kb)\n* [E-Departure](http://www.ctrl-alt-test.fr/?page_id=197) (64kb)\n* [F-Felix's Workshop](http://www.ctrl-alt-test.fr/?page_id=373) (64kb)\n"
 },
 {
  "repo": "memo/ofxKinect-demos",
  "language": "C++",
  "readme_contents": "/*\n A bunch of demos for using Kinect with openFrameworks.\n Copyright (c) 2010 Memo Akten http://www.memo.tv\n \n Using ofxKinect by Theo Watson, Dan Wilcox and co.\n github.com/\u200bofTheo/\u200bofxKinect\n\n based on libfreenect by Hector Martin \"marcan\" \n git.marcansoft.com/\u200b?p=libfreenect.git\n\n\n This code is licensed to you under the terms of the GNU GPL, version 2 or version 3;\n see:\n gnu.org/\u200blicenses/\u200bold-licenses/\u200bgpl-2.0.txt\n gnu.org/\u200blicenses/\u200bgpl-3.0.txt\n\n More info on the opensource kinect drivers at github.com/\u200bOpenKinect/\u200bopenkinect\n */\n"
 },
 {
  "repo": "lluisgomez/text_extraction",
  "language": "C++",
  "readme_contents": "text_extraction\n===============\n\nThis code is the implementation of the method proposed in the paper \u201cMulti-script text extraction from natural scenes\u201d (Gomez &amp; Karatzas), International Conference on Document Analysis and Recognition, ICDAR2013.\n\nThis code should reproduce the same quantitative results published on the paper for the KAIST dataset (for the task of text segmentation at pixel level). If you plan to compare this method with your's in other datasets please drop us a line ({lgomez,dimos}@cvc.uab.es). Thanks!\n\n\nIncludes the following third party code:\n\n  - fast_clustering.cpp Copyright (c) 2011 Daniel M\u00fcllner, under the BSD license. http://math.stanford.edu/~muellner/fastcluster.html\n  - mser.cpp Copyright (c) 2011 Idiap Research Institute, under the GPL license. http://www.idiap.ch/~cdubout/\n  - binomial coefficient approximations are due to Rafael Grompone von Gioi. http://www.ipol.im/pub/art/2012/gjmr-lsd/\n"
 },
 {
  "repo": "macmade/FaceDetect",
  "language": "C++",
  "readme_contents": "FaceDetect\n==========\n\n[![Build Status](https://img.shields.io/travis/macmade/FaceDetect.svg?branch=master&style=flat)](https://travis-ci.org/macmade/FaceDetect)\n[![Issues](http://img.shields.io/github/issues/macmade/FaceDetect.svg?style=flat)](https://github.com/macmade/FaceDetect/issues)\n![Status](https://img.shields.io/badge/status-inactive-lightgray.svg?style=flat)\n![License](https://img.shields.io/badge/license-boost-brightgreen.svg?style=flat)\n[![Contact](https://img.shields.io/badge/contact-@macmade-blue.svg?style=flat)](https://twitter.com/macmade)  \n[![Donate-Patreon](https://img.shields.io/badge/donate-patreon-yellow.svg?style=flat)](https://patreon.com/macmade)\n[![Donate-Gratipay](https://img.shields.io/badge/donate-gratipay-yellow.svg?style=flat)](https://www.gratipay.com/macmade)\n[![Donate-Paypal](https://img.shields.io/badge/donate-paypal-yellow.svg?style=flat)](https://paypal.me/xslabs)\n\nFace detection for iPhone using OpenCV\n--------------------------------------\n\n### About\n\nThis project is an example of using [OpenCV][1] on the iPhone.\n\nIt provides a simple application that let you take a picture (or select one from your photo library), and that detects the face(s) on the picture.\n\n### Building\n\nThe application does not compile (yet) for the iPhone simulator.\nYou need to build it for your iOS device in order to test it.\n\n### OpenCV port\n\nFaceDetect uses the [OpenCV port][2] provided by Macmade (myself), and which is also [available on GitHub][3].\n\nProject Status\n--------------\n\nThis project is no longer maintained.  \nIt might not build on latest iOS versions, and might not be compatible with latest OpenCV versions.\n\nLicense\n-------\n\nFaceDetect is released under the terms of the [Boost][4] Software License - Version 1.0.\n\n[1]: http://opencv.willowgarage.com/                    \"OpenCV\"\n[2]: http://www.eosgarden.com/en/opensource/opencv-ios/ \"OpenCV-iOS\"\n[3]: https://github.com/macmade/OpenCV-iOS              \"GitHub\"\n[4]: http://www.boost.org/LICENSE_1_0.txt               \"BOOST\"\n\nRepository Infos\n----------------\n\n    Owner:\t\t\tJean-David Gadina - XS-Labs\n    Web:\t\t\twww.xs-labs.com\n    Blog:\t\t\twww.noxeos.com\n    Twitter:\t\t@macmade\n    GitHub:\t\t\tgithub.com/macmade\n    LinkedIn:\t\tch.linkedin.com/in/macmade/\n    StackOverflow:\tstackoverflow.com/users/182676/macmade\n"
 },
 {
  "repo": "clayallsopp/routable-android",
  "language": "Java",
  "readme_contents": "# Routable\n\nRoutable is an in-app native URL router, for Android. Also available for [iOS](https://github.com/usepropeller/routable-ios).\n\n## Usage\n\nSet up your app's router and URLs:\n\n```java\nimport com.usepropeller.routable.Router;\n\npublic class PropellerApplication extends Application {\n    @Override\n    public void onCreate() {\n        super.onCreate();\n        \n        // Set the global context\n        Router.sharedRouter().setContext(getApplicationContext());\n        // Symbol-esque params are passed as intent extras to the activities\n        Router.sharedRouter().map(\"users/:id\", UserActivity.class);\n        Router.sharedRouter().map(\"users/new/:name/:zip\", NewUserActivity.class);\n    }\n}\n```\n\nIn your `Activity` classes, add support for the URL params:\n\n```java\nimport com.usepropeller.routable.Router;\n\npublic class UserActivity extends Activity {\n    @Override\n    public void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n\n        Bundle intentExtras = getIntent().getExtras();\n        // Note this extra, and how it corresponds to the \":id\" above\n        String userId = intentExtras.get(\"id\");\n    }\n}\n\npublic class NewUserActivity extends Activity {\n    @Override\n    public void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n\n        Bundle intentExtras = getIntent().getExtras();\n        // Corresponds to the \":name\" above\n        String name = intentExtras.get(\"name\");\n        // Corresponds to the \":zip\" above\n        String zip = intentExtras.get(\"zip\");\n    }\n}\n```\n\n*Anywhere* else in your app, open some URLs:\n\n```java\n// starts a new UserActivity\nRouter.sharedRouter().open(\"users/16\");\n// starts a new NewUserActivity\nRouter.sharedRouter().open(\"users/new/Clay/94303\");\n```\n\n## Installation\n\nRoutable is currently an Android library project (so no Maven).\n\nIf you're in a hurry, you can just copy-paste the [Router.java](https://github.com/usepropeller/routable-android/blob/master/src/com/usepropeller/routable/Router.java) file.\n\nOr if you're being a little more proactive, you should import the Routable project (this entire git repo) into Eclipse and [reference it](http://developer.android.com/tools/projects/projects-eclipse.html#ReferencingLibraryProject) in your own project. \n\n## Features\n\n### Routable Functions\n\nYou can call arbitrary blocks of code with Routable:\n\n```java\nRouter.sharedRouter().map(\"logout\", new Router.RouterCallback() {\n    public void run(Router.RouteContext context) {\n        User.logout();\n    }\n});\n\n// Somewhere else\nRouter.sharedRouter().open(\"logout\");\n```\n\n### Open External URLs\n\nSometimes you want to open a URL outside of your app, like a YouTube URL or open a web URL in the browser. You can use Routable to do that:\n\n```java\nRouter.sharedRouter().openExternal(\"http://www.youtube.com/watch?v=oHg5SJYRHA0\")\n```\n\n### Multiple Routers\n\nIf you need to use multiple routers, simply create new instances of `Router`:\n\n```java\nRouter adminRouter = new Router();\n\nRouter userRouter = new Router();\n```\n\n## Contact\n\nClay Allsopp ([http://clayallsopp.com](http://clayallsopp.com))\n\n- [http://twitter.com/clayallsopp](http://twitter.com/clayallsopp)\n- [clay@usepropeller.com](clay@usepropeller.com)\n\n## License\n\nRoutable for Android is available under the MIT license. See the LICENSE file for more info.\n"
 },
 {
  "repo": "jclouds/legacy-jclouds",
  "language": "Java",
  "readme_contents": "jclouds moved to Apache in 2013 and archived this repository.  Please visit our\nnew [JIRA issue tracker](https://issues.apache.org/jira/projects/JCLOUDS),\n[GitHub repository](https://github.com/jclouds/jclouds), and\n[web page](https://jclouds.apache.org/).\n"
 },
 {
  "repo": "StanKocken/EfficientAdapter",
  "language": "Java",
  "readme_contents": "# Efficient Adapter for Android\n\nCreate a new adapter for a [RecyclerView](https://developer.android.com/jetpack/androidx/releases/recyclerview) or [ViewPager](https://developer.android.com/reference/kotlin/androidx/viewpager/widget/ViewPager) is now much easier.\n\n## Overview\n\nCreate a list of elements into a [RecyclerView](https://developer.android.com/jetpack/androidx/releases/recyclerview) or [ViewPager](https://developer.android.com/reference/kotlin/androidx/viewpager/widget/ViewPager) is not that easy for a beginner, and repetitive for others. The goal of this library is to simplify that for you.\n\n## How does it work?\n\nCreate a class ViewHolder (`BookViewHolder` for example). The method `updateView` will be call with the object, when an update of your view is require:\n\n    public class BookViewHolder extends EfficientViewHolder<Book> {\n        public BookViewHolder(View itemView) {  super(itemView); }\n\n        @Override\n        protected void updateView(Context context, Book object) {\n            TextView textView = findViewByIdEfficient(R.id.title_textview);\n            textView.setText(object.getTitle());\n\t\t\t// or just\n\t\t\tsetText(R.id.title_textview, object.getTitle());\n        }\n    }\n\nGive this ViewHolder class to the constructor of the adapter (SimpleAdapter) of your [RecyclerView](https://developer.android.com/jetpack/androidx/releases/recyclerview), with the resource id of your item view and the list of objects:\n\n    EfficientRecyclerAdapter<Plane> adapter = new EfficientRecyclerAdapter<Plane>(R.layout.item_book, BookViewHolder.class, listOfBooks);\n    recyclerView.setAdapter(adapter);\n\nAnd that's it!\n\nIt's also working with a [ViewPager](https://developer.android.com/reference/kotlin/androidx/viewpager/widget/ViewPager):\n\n    EfficientPagerAdapter<Plane> adapter = new EfficientPagerAdapter<Plane>(R.layout.item_book, BookViewHolder.class, listOfBooks);\n    viewPager.setAdapter(adapter);\n\n## Other features\n\n### Heterogenous list\nFor a list of different kind of objects, layout, viewholder\u2026\n\n    EfficientRecyclerAdapter adapter = new EfficientRecyclerAdapter(generateListObjects()) {\n        @Override\n        public int getItemViewType(int position) {\n            if (get(position) instanceof Plane) {\n                return VIEW_TYPE_PLANE;\n            } else {\n                return VIEW_TYPE_BOOK;\n            }\n        }\n\n        @Override\n        public Class<? extends EfficientViewHolder> getViewHolderClass(int viewType) {\n            switch (viewType) {\n                case VIEW_TYPE_BOOK:\n                    return BookViewHolder.class;\n                case VIEW_TYPE_PLANE:\n                    return PlaneViewHolder.class;\n                default:\n                    return null;\n            }\n        }\n\n        @Override\n        public int getLayoutResId(int viewType) {\n            switch (viewType) {\n                case VIEW_TYPE_BOOK:\n                    return R.layout.item_book;\n                case VIEW_TYPE_PLANE:\n                    return R.layout.item_plane;\n                default:\n                    return 0;\n            }\n        }\n    };\n\n### Efficient findViewById()\n\nBecause `findViewById(int)` is CPU-consuming (this is why we use the ViewHolder pattern), this library use a `findViewByIdEfficient(int id)` into the ViewHolder class.\n\nYou can use it like a `findViewById(int id)` but the view return will be cached to be returned into the next call.\n\nYour view id should be unique into your view hierarchy, but sometimes is not that easy (with an include for example). It's now easier to find a subview by specify the parent of this subview with `findViewByIdEfficient(int parentId, int id)` to say \"the view with this id into the parent with this id\".\n\n\n### Let the element be clickable\n\nYour ViewHolder class can override the method `isClickable()` to tell is this element is clickable or not.\n\nBy default, the view is clickable if you have a listener on your adapter.\n\n## Proguard\n\nThis library includes the proguard configuration file.\nIf you want to add it manually:\n\n    -keepclassmembers public class * extends com.skocken.efficientadapter.lib.viewholder.EfficientViewHolder {\n        public <init>(...);\n    }\n\n## Gradle\n\n    dependencies {\n        compile 'com.skocken:efficientadapter:2.4.0'\n    }\n\n## Android Support library\n\nIf you are still using the deprecated Android Support Library (instead of AndroidX), please use the dependency 2.3.X instead.\n\n## License\n\n* [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0.html)\n\n## Contributing\n\nPlease fork this repository and contribute back using\n[pull requests](https://github.com/StanKocken/EfficientAdapter/pulls).\n\nAny contributions, large or small, major features, bug fixes, additional\nlanguage translations, unit/integration tests are welcomed and appreciated\nbut will be thoroughly reviewed and discussed.\n"
 },
 {
  "repo": "florent37/Carpaccio",
  "language": "Java",
  "readme_contents": "# Carpaccio\n\n[![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-Carpaccio-brightgreen.svg?style=flat)](http://android-arsenal.com/details/1/2211)\n[![Join the chat at https://gitter.im/florent37/Carpaccio](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/florent37/Carpaccio?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n\n<a href=\"https://goo.gl/WXW8Dc\">\n  <img alt=\"Android app on Google Play\" src=\"https://developer.android.com/images/brand/en_app_rgb_wo_45.png\" />\n</a>\n\n\nDeveloped to facilitate integration on Android ( Designers can thanks me :D )\n\n![logo](https://raw.githubusercontent.com/florent37/Carpaccio/master/screenshot/carpaccio_small.png)\n\n**Data Mapping & Smarter Views framework for android**\n\nWith Carpaccio, your views became smarter, instead of calling functions on views, now your views can call functions !\nYou no longer need to extend a view to set a custom behavior\n\nCarpaccio also come with a beautiful mapping engine !\n\n# Usage\n\n```xml\n<com.github.florent37.carpaccio.Carpaccio\n        android:id=\"@+id/carpaccio\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"\n        app:register=\"\n            com.github.florent37.carpaccio.controllers.ImageViewController;\n            com.github.florent37.carpaccio.controllers.TextViewController\n        \">\n\n        <ImageView\n               android:layout_width=\"match_parent\"\n               android:layout_height=\"150dp\"\n               android:scaleType=\"centerCrop\"\n               android:tag=\"url(http://i.imgur.com/DSjXNox.jpg)\" />\n\n        <TextView\n            android:layout_width=\"match_parent\"\n            android:layout_height=\"wrap_content\"\n            android:textSize=\"20sp\"\n            android:tag=\"\n                font(Roboto-Thin.ttf);\n                setText($user.name);\"/>\n\n</com.github.florent37.carpaccio.Carpaccio>\n```\n\n![url](https://raw.githubusercontent.com/florent37/Carpaccio/master/screenshot/sample_crop.png)\n\n# Download\n\n<a href='https://ko-fi.com/A160LCC' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://az743702.vo.msecnd.net/cdn/kofi1.png?v=0' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a>\n\nAdd into your **build.gradle**\n\n[![Download](https://api.bintray.com/packages/florent37/maven/Carpaccio/images/download.svg)](https://bintray.com/florent37/maven/Carpaccio/_latestVersion)\n\n```groovy\ncompile ('com.github.florent37:carpaccio:(lastest version)@aar'){\n    transitive=true\n}\n```\n\n------------\n\n# DataBinding\n\n```xml\n<TextView\n       android:tag=\"\n            setText($user)\n       \"/>\n\n<ImageView\n       android:tag=\"\n           url($user.getImageUrl())\n       \" />\n\n<ImageView\n       android:tag=\"\n            url($user.imageUrl)\n       \"/>\n\n<TextView\n       android:text=\"$user.name\"/>\n```\n\nIn your activity / fragment :\n\n```java\nCarpaccio carpaccio = (Carpaccio)findViewById(\"R.id.carpaccio\");\ncarpaccio.mapObject(\"user\",new User(\"florent\", \"www.\"));\n```\n\n![set_text](https://raw.githubusercontent.com/florent37/Carpaccio/master/screenshot/set_text.png)\n\n------------\n\n## RecyclerView Mapping\n\nYou dreamed it, Carpaccio did it ! You can now bind a List with a RecyclerView !\n\n**WORKS WITH ANDROID STUDIO PREVIEW !!!**, don't hesitate to refresh your preview\n![url](https://raw.githubusercontent.com/florent37/Carpaccio/master/screenshot/refresh.png)\n\n![recycler](https://raw.githubusercontent.com/florent37/Carpaccio/master/screenshot/recycler_small.png)\n![recycler_preview](https://raw.githubusercontent.com/florent37/Carpaccio/master/screenshot/recycler_preview_small.png)\n\n\nR.layout.activity_main_recyclerview_mapping\n```xml\n<com.github.florent37.carpaccio.Carpaccio\n        android:id=\"@+id/carpaccio\"\n        app:register=\"\n            com.github.florent37.carpaccio.controllers.RecyclerViewController;\n            com.github.florent37.carpaccio.controllers.ImageViewController;\n            com.github.florent37.carpaccio.controllers.TextViewController;\n        \">\n\n        <android.support.v7.widget.RecyclerView\n                    android:layout_width=\"match_parent\"\n                    android:layout_height=\"match_parent\"\n\n                    android:tag=\"\n                        addHeader(header,R.layout.header_user);\n                        adapter(user,R.layout.cell_user)\n                    \"\n                    />\n\n</com.github.florent37.carpaccio.Carpaccio>\n```\n\nR.layout.cell_user\n```xml\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"100dp\"\n    android:gravity=\"center_vertical\"\n    android:orientation=\"horizontal\">\n\n        <ImageView\n            android:layout_width=\"100dp\"\n            android:layout_height=\"match_parent\"\n            android:layout_marginRight=\"20dp\"\n            android:tag=\"\n                enablePreview();\n                previewUrl(http://lorempixel.com/400/400/);\n                url($user.image);\n                \" />\n\n        <TextView\n            android:layout_width=\"match_parent\"\n            android:layout_height=\"wrap_content\"\n            android:tag=\"\n                setText($user.name);\n                setFont(Roboto-Black.ttf);\"\n            />\n\n</LinearLayout>\n```\n\nFinally, in your activiy/fragment you just have to indicate the List to map !\n\n```java\nsetContentView(R.layout.activity_main_recyclerview_mapping);\nCarpaccio carpaccio = (Carpaccio)findViewById(\"R.id.carpaccio\");\ncarpaccio.mapList(\"user\", this.users);\ncarpaccio.mapObject(\"header\", this.headerObject);\n```\n\n------------\n\n# ViewControllers\n\nCarpaccio provide some awesome ViewControllers, you can use them directly into your project.\n[Read the Wiki to have a list of all provided viewControllers](https://github.com/florent37/Carpaccio/wiki)\n\n------------\n\nTextViewController can set a custom font (from assets/fonts/) to a TextView and provide text binding\n\n**WORKS WITH ANDROID STUDIO PREVIEW !!!**\n\n![font](https://raw.githubusercontent.com/florent37/Carpaccio/master/screenshot/custom_ttf_small.png)\n\n```xml\n<com.github.florent37.carpaccio.Carpaccio\n        app:register=\"\n            com.github.florent37.carpaccio.controllers.TextViewController;\n        \">\n\n        <TextView\n             android:tag=\"\n                 font(Pacifico.ttf);\n                 android:text=\"$user.getName()\"\n             \"/>\n</com.github.florent37.carpaccio.Carpaccio>\n```\n\n------------\n\n## ImageViewController\n\n```xml\n<com.github.florent37.carpaccio.Carpaccio\n        app:register=\"\n            com.github.florent37.carpaccio.controllers.ImageViewController;\n        \">\n\n        <ImageView\n             android:tag=\"\n                 url(http://i.imgur.com/DvpvklR.png)\n             \" />\n</com.github.florent37.carpaccio.Carpaccio>\n```\n\n![url](https://raw.githubusercontent.com/florent37/Carpaccio/master/screenshot/url_small.png)\n\n**WORKS WITH ANDROID STUDIO PREVIEW !!!**, don't hesitate to refresh your preview\n![url](https://raw.githubusercontent.com/florent37/Carpaccio/master/screenshot/refresh.png)\n\nPreview an url image\n\n```xml\n<ImageView\n     android:tag=\"\n        enablePreview();\n        url(http://i.imgur.com/DvpvklR.png);\n     \" />\n```\n\n![url](https://raw.githubusercontent.com/florent37/Carpaccio/master/screenshot/preview_image_url_small.png)\n\nAnd some awesome customisations\n\n\n![circle](https://raw.githubusercontent.com/florent37/Carpaccio/master/screenshot/circle_small_2.png)\n![blur](https://raw.githubusercontent.com/florent37/Carpaccio/master/screenshot/blur_small.png)\n![greyscale](https://raw.githubusercontent.com/florent37/Carpaccio/master/screenshot/greyscale_small.png)\n\n```xml\n<ImageView\n      android:tag=\"\n\n           circle();\n           blur(); OR willBlur();\n           greyScale(); OR willGreyScale();\n\n           url(http://i.imgur.com/DvpvklR.png);\n      \" />\n```\n\n[![Video](http://share.gifyoutube.com/mGz9OZ.gif)](https://youtu.be/A0eyvpNh5wM)\n[![Video](http://share.gifyoutube.com/vpMYjp.gif)](https://youtu.be/4b84gswKGkA)\n\n\n```xml\n<ImageView\n      android:tag=\"\n           animateMaterial(6000);\n           kenburns();\n           url(http://i.imgur.com/DvpvklR.png);\n      \" />\n```\n\n# Community\n\nLooking for contributors, feel free to fork !\n\nTell me if you're using my library in your application, I'll share it in this README\n\n# Dependencies\n\n* [Picasso][picasso] (from Square)\n* [KenBurnsView][kenburnsview] (from flavioarfaria)\n* [Android-Observablescrollview][android-observablescrollview] (from ksoichiro)\n\n# Credits\n\nAuthor: Florent Champigny\nwww.florentchampigny.com/\n\n\n<a href=\"https://goo.gl/WXW8Dc\">\n  <img alt=\"Android app on Google Play\" src=\"https://developer.android.com/images/brand/en_app_rgb_wo_45.png\" />\n</a>\n\n<a href=\"https://plus.google.com/+florentchampigny\">\n  <img alt=\"Follow me on Google+\"\n       src=\"https://raw.githubusercontent.com/florent37/DaVinci/master/mobile/src/main/res/drawable-hdpi/gplus.png\" />\n</a>\n<a href=\"https://twitter.com/florent_champ\">\n  <img alt=\"Follow me on Twitter\"\n       src=\"https://raw.githubusercontent.com/florent37/DaVinci/master/mobile/src/main/res/drawable-hdpi/twitter.png\" />\n</a>\n<a href=\"https://www.linkedin.com/profile/view?id=297860624\">\n  <img alt=\"Follow me on LinkedIn\"\n       src=\"https://raw.githubusercontent.com/florent37/DaVinci/master/mobile/src/main/res/drawable-hdpi/linkedin.png\" />\n</a>\n\n\nLicense\n--------\n\n    Copyright 2015 florent37, Inc.\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n\n[picasso]: https://github.com/square/picasso\n[kenburnsview]: https://github.com/flavioarfaria/KenBurnsView\n[android-observablescrollview]: https://github.com/ksoichiro/Android-ObservableScrollView\n"
 },
 {
  "repo": "chiuki/friendspell",
  "language": "Java",
  "readme_contents": "# Friend Spell\nParty icebreaker game based on the Google Nearby API\n\nhttps://play.google.com/store/apps/details?id=com.sqisland.friendspell\n\n## Credentials\n\n1. Add your `com.google.android.nearby.messages.API_KEY` in `AndroidManifest.xml`: https://developers.google.com/nearby/messages/android/get-started\n2. Add your `app/google-services.json` file:\nhttps://developers.google.com/identity/sign-in/android/start-integrating\n\n## Technologies used in the project\n\n* [Google Nearby](https://developers.google.com/nearby/messages/android/get-started)\n* [Google Plus](https://developers.google.com/identity/sign-in/android/start-integrating)\n* [Dagger 2](http://google.github.io/dagger)\n* [Espresso 2](https://code.google.com/p/android-test-kit/wiki/EspressoSetupInstructions)\n* [Mockito](http://mockito.org)\n* [VectorDrawableCompat](https://developer.android.com/reference/android/support/graphics/drawable/VectorDrawableCompat.html)\n* [Noto emoji](https://github.com/googlei18n/noto-emoji)\n* [Picasso](http://square.github.io/picasso)\n* [GSON](https://github.com/google/gson)\n* [Cupboard](https://bitbucket.org/qbusict/cupboard)\n* [Timber](https://github.com/JakeWharton/timber)\n* [Butterknife](http://jakewharton.github.io/butterknife)\n"
 },
 {
  "repo": "applidium/HeaderListView",
  "language": "Java",
  "readme_contents": "# DEPRECATED\n\nHeaderListView is deprecated. No new development will be taking place.\n\n# Quickstart\n\n1. Import the HeaderListView module in your Android Studio project.\n2. Replace your `ListView` with `HeaderListView`\n3. Implement a subclass of `SectionAdapter`\n4. Set it to your `HeaderListView` with `setAdapter(SectionAdapter adapter)`\n\n# HeaderListView\n\n`HeaderListView` is a list view with sections and with a cool iOS-like \"sticky\" section headers. Notice that `HeaderListView` is not a subclass of Android's `ListView` but uses composition. Hence, you will need to call `getListView()` to access the underlying `ListView`. \n"
 },
 {
  "repo": "androidessence/PinchZoomTextView",
  "language": "Java",
  "readme_contents": "PinchZoomTextView Library\n=============\n\nThis library allows you to have a TextView that will grow/shrink the font size using gestures from the user.\n\nUsage\n-----\n\nTo have access to the library, add the dependency to your build.gradle:\n\n```java\n\tcompile 'com.androidessence:pinchzoomtextview:1.0.1'\n```\n\nDeveloper Notes\n---------------\n\nBy default, the zoom feature is always enabled. If for any reason you want to disable it, simply call the `PinchZoomTextView#setZoomEnabled(boolean enabled)` method.\n\nSample\n-----\n\nTo see the library in action, here is a sample of the text size changing:\n\n<img src='sample.gif' width='400' height='640' />\n\nCredits & Contact\n-----------------\n\nPinchZoomTextView was created by:\n\n- [Adam McNeilly](https://github.com/AdamMc331)\n\nWith special thanks to [Eric Cugota](https://github.com/tryadelion) for helping me get this into Bintray and make it available for you.\n\nAnd it's released under [Android Essence blog](http://androidessence.com/).\n\nLicense\n-------\n\nPinchZoomTextView is available under the [MIT License](https://opensource.org/licenses/MIT). You are free to do with it what you want. If you submit a pull request, please add your name to the credits section. :)\n"
 },
 {
  "repo": "dessalines/flowchat",
  "language": "Java",
  "readme_contents": "[FlowChat](http://flow-chat.com) &mdash; An open-source, self-hostable reddit alternative featuring communities and live-updating threaded conversations.\n==========\n![](http://img.shields.io/version/0.3.1.png?color=green)\n[![Build Status](https://travis-ci.org/dessalines/flowchat.svg?branch=master)](https://travis-ci.org/dessalines/flowchat)\n\n<!---\n\tFlowChat: An open-source, self-hostable reddit alternative featuring communities and live-updating threaded conversations.\n\n\tHey /r/blank, leftist programmer here. I've made a reddit alternative called Flowchat, featuring communities, and live updating threaded conversations. Self-hostable, open-source, explicitly anti-racist.\n\n\tDescription from the [github](https://github.com/dessalines/flowchat):\n\n> [FlowChat](http://flow-chat.com) is an open-source, self-hostable reddit alternative. It has communities, hashtags, **live-updating** threaded conversations, and voting.\n\n> Flowchat tries to solve the problem of having a fluid, free-feeling group chat, while allowing for side conversations so that every comment isn't at the top level, and doesn't disrupt the flow. \n\n> Multiple conversations can take place **at once**, without interrupting the flow of the chatroom.\n\n> Check out the default community, [vanilla](http://flow-chat.com/#/community/1), or create your own.\n\n> It features:\n- Private or public discussions and communities.\n- Sorting by recentness, hotness, or popularity.\n- Antiracist policies including a global slur filter (No racism will be allowed on the main Flowchat instance).\n- Image and video focused, with auto-zoom.\n- Moderation including blocking users, appointing moderators, or deleting comments.\n- NSFW filtering.\n- Stickied posts.\n- Hashtags.\n\n\tI'd also like to invite the moderators of this sub to take over the equivalent leftist community on it. Message my reddit user if your community's already been created and I'll gladly do this. \n-->\n\n[FlowChat](http://flow-chat.com) is an open-source, self-hostable reddit alternative. It has communities, hashtags, **live-updating** threaded conversations, and voting.\n\nFlowchat tries to solve the problem of having a fluid, free-feeling group chat, while allowing for side conversations so that every comment isn't at the top level, and doesn't disrupt the flow. \n\nMultiple conversations can take place **at once**, without interrupting the flow of the chatroom.\n\nCheck out the default community, [vanilla](http://flow-chat.com/#/community/1), or create your own.\n\nIt features:\n- Private or public discussions and communities.\n- Sorting by recentness, hotness, or popularity.\n- Antiracist policies including a global slur filter (No racism will be allowed on the main Flowchat instance).\n- Image and video focused, with auto-zoom.\n- Moderation including blocking users, appointing moderators, or deleting comments.\n- NSFW filtering.\n- Stickied posts.\n- Hashtags.\n\nTech used:\n- [Java Spark](https://github.com/perwendel/spark), [Bootstrap v4](https://github.com/twbs/bootstrap), [Angular.io](https://github.com/angular/angular), [Angular-cli](https://github.com/angular/angular-cli), [ngx-bootstrap](http://valor-software.com/ngx-bootstrap/), [ActiveJDBC](http://javalite.io/activejdbc), [Liquibase](http://www.liquibase.org/), [Postgres](https://www.postgresql.org/), [Markdown-it](https://github.com/markdown-it/markdown-it), [angular2-toaster](https://github.com/Stabzs/Angular2-Toaster)\n\nCheck out a sample discussion [here](http://flow-chat.com/#/discussion/13).\n\nJoin the community: [flowchat](https://www.reddit.com/r/flowchat/)\n\n[Change log](https://github.com/dessalines/flowchat/issues?q=is%3Aissue+is%3Aclosed)\n\n![screen1](https://i.imgur.com/hDeDamH.png)\n\n![screen2](https://i.imgur.com/yTKwfhd.png)\n\n\n\n---\n\n## Installation \n\n*If you want to self-host or develop flowchat.*\n\n### Docker\n\n#### Requirements\n\n- Docker\n- docker-compose\n\n#### Start the docker instance\n\n```sh\ngit clone https://github.com/dessalines/flowchat\ncd flowchat\n// edit ARG ENDPOINT_NAME=http://localhost:4567 in ./Dockerfile to your hostname\ndocker-compose up\n```\n\nGoto to http://localhost:4567\n\n### Local development\n\n#### Requirements\n- Java 8 + Maven\n- Node + npm/yarn, [nvm](https://github.com/creationix/nvm) is the preferred installation method.\n- angular-cli: `npm i -g @angular/cli`\n- Postgres 9.3 or higher\n\n#### Download Flowchat\n`git clone https://github.com/dessalines/flowchat`\n\n#### Setup a postgres database\n\n[Here](https://www.digitalocean.com/community/tutorials/how-to-install-and-use-postgresql-on-ubuntu-16-04) are some instructions to get your postgres DB up and running.\n\n```sh\npsql -c \"create user flowchat with password 'asdf' superuser\"\npsql -c 'create database flowchat with owner flowchat;'\n```\n\n#### Edit your pom.xml file to point to your database\n```sh\ncd flowchat\nvim service/flowchat.properties\n```\n\nEdit it to point to your own database:\n```\n<!--The Database location and login, here's a sample-->\njdbc.url=jdbc\\:postgresql\\://127.0.0.1/flowchat\njdbc.username=flowchat\njdbc.password=asdf\nsorting_created_weight=86400\nsorting_number_of_votes_weight=0.001\nsorting_avg_rank_weight=0.01\nreddit_client_id=\nreddit_client_secret=\nreddit_username=\nreddit_password=\n```\n#### Install flowchat\n\nfor local testing: \n\n`./install_dev.sh` and goto http://localhost:4567/\n\nfor front end angular development, do:\n\n```\ncd ui\nng serve\n```\n\nand goto http://localhost:4200\n\nfor a production environment, edit `ui/config/environment.prod.ts` to point to your hostname, then run:\n\n`./install_prod.sh`\n\nYou can redirect ports in linux to route from port 80 to this port:\n\n`sudo iptables -t nat -I PREROUTING -p tcp --dport 80 -j REDIRECT --to-ports 4567`\n\n---\n\n## Bugs and feature requests\n\nHave a bug or a feature request? If your issue isn't [already listed](https://github.com/dessalines/flowchat/issues/), then open a [new issue here](https://github.com/dessalines/flowchat/issues/new).\n"
 },
 {
  "repo": "Kenber/DoubleStickyHeadersList",
  "language": "Java",
  "readme_contents": "# Overview\nAn Android library for double level section headers that stick to the top of list. Android widget especially for displaying items with multilevel classification.\n\n![Sample GIF](samplePicture/sample.gif)\n\n# Usage\n#### 1. Include the library in your project.  \n\nTo include with Gradle:\n  \nAdd the code below to your module-level `build.gradle`:\n  \n```groovy\ndependencies {\n   compile 'com.kenber.doublestickyheaderslist:library:1.0.0'\n}\n```\n\n#### 2. Replace your `ListView` with `com.kenber.view.DoubleStickyHeaderListView` in the `layout.xml` file.\n\n```xml\n<com.kenber.view.DoubleStickyHeaderListView\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"wrap_content\" />\n```\n\n#### 3. Make the `adapter` for your `DoubleStickyHeaderListView` implements `DoubleStickHeadersListAdapter` interface. You need to override the method `getHeaderLevel(int position)` to tell the `DoubleStickyHeaderListView` which header should be sticky as level 0 or level 1, or not sticky as level 2.\n\n```java\npublic class ListAdapter extends ArrayAdapter<ListItem> implements DoubleStickHeadersListAdapter {\n\n    ...\n        \n    @Override\n    public int getHeaderLevel(int position) {\n        return getItem(position).level;\n    }\n}\n```\nFor now, everything is done. An example for this library can be found in `example` folder.\nIf you have any advice for this project, please contact me - kenberspeng@gmail.com  \n\nThanks\n=======\n\n[beworker](https://github.com/beworker)\n\nI got the inspiration from his project [pinned-section-listview](https://github.com/beworker/pinned-section-listview)\n\nLicense\n=======\n\n    Copyright 2016 Kenber\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n"
 },
 {
  "repo": "timroes/SwipeToDismissUndoList",
  "language": "Java",
  "readme_contents": "SwipeToDismissUndoList\n=======================\n\n_**This library is deprecated in favor of [EnhancedListView](https://github.com/timroes/EnhancedListView). I won't fix any bugs on this library anymore.**_\n\nThe SwipeToDismissUndoList is a library to add swipe to dismiss functionality to\na `ListView` and undo deletions again. The lib is based on \n[Jake Wharton's SwipeToDismissNOA](https://github.com/JakeWharton/SwipeToDismissNOA)\nthat is based on [Roman Nurik's SwipeToDismiss sample](https://gist.github.com/romannurik/2980593).\n\nThe code is licensed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0).\n\nYou can view a demonstration of this lib in the [Demonstration App](https://play.google.com/store/apps/details?id=de.timroes.swipetodismiss.demo).\nThe source code of this demonstration app can be found in the [SwipteToDismissUndoDemo](https://github.com/timroes/SwipeToDismissUndoDemo) project.\n\nChanges\n-------\n\n*2013-05-28* Introduced `setSwipeDirection` and improved detection of swipes.\n\n*2013-05-13* Use `AbsListView` instead of `ListView` to make this library also available for GridViews\n\n**Changes required:** You need to change the parameter in the `onDismiss` method to `AbsListView` (see below).\n\n*2013-03-22* Added minimum SDK version to manifest\n\n*2013-02-24* Properly discard undo items (BUG FIX)\n\nUsage\n-----\n\n[Add this project](http://developer.android.com/tools/projects/projects-cmdline.html#ReferencingLibraryProject)\nas an Android library to your project. The project should work from API level 3 upwards. If you find\nthat this isn't true (might be, that I missed some newer methods), please inform me about that :)\n\nTo use the list create a regular `ListView` (e.g. via a `ListActivity`) and wrap\nit up in the `SwipeDismissList` of this lib:\n\n```java\nListView listView = // ... (findById or getListView)\nSwipeDismissList.OnDismissCallback callback = // .. see below\nUndoMode mode = // .. see below\nSwipeDismissList swipeList = new SwipeDismissList(listView, callback, mode);\n```\n\nYou would normally want to do that in your `onCreate` method.\n\n## OnDismissCallback\n\nThe second parameter to the constructor of `SwipeDismissList` is an `OnDismissCallback`.\nYou must implement that, to handle the deletion of elements:\n\n```java\nSwipeDismissList.OnDismissCallback callback = new SwipeDismissList.OnDismissCallback() {\n\tpublic SwipeDismissList.Undoable onDismiss(ListView listView, int position) {\n\t\t// Delete the item from your adapter (sample code):\n\t\tfinal String itemToDelete = mAdapter.get(position);\n\t\tmAdapter.remove(itemToDelete);\n\t\treturn null;\n\t}\n}\n```\n\nIf you return `null` from the `onDismiss` method, your deletion won't be undoable.\nTo make your deletion undoable, you must return a valid `Undoable` (implementing \nat least its `undo` method), that restores the element again:\n\n```java\nSwipeDismissList.OnDismissCallback callback = new SwipeDismissList.OnDismissCallback() {\n\tpublic SwipeDismissList.Undoable onDismiss(AbsListView listView, final int position) {\n\t\t// Delete the item from your adapter (sample code):\n\t\tfinal String itemToDelete = mAdapter.get(position);\n\t\tmAdapter.remove(itemToDelete);\n\t\treturn new SwipeDismissList.Undoable() {\n\t\t\tpublic void undo() {\n\t\t\t\t// Return the item at its previous position again\n\t\t\t\tmAdapter.insert(itemToDelete, position);\n\t\t\t}\n\t\t};\n\t}\n}\n```\n\nYou can override `getTitle` in the `Undoable` to provide an individual title for \nthe item, that has been deleted. That title will be shown beside the undo button\nin the popup. If you don't override that method (or it returns `null`) the default\ndeletion message will be shown in the popup. You can change this message with\n`SwipeDismissList.setUndoString(String)`.\n\nYou can return `null` from the `onDismiss` method in general to disable undo on the \nlist or just on special items, you don't want (or cannot) undo.\n\n### Notification about final delete\n\nIf you want to get notified when the user doesn't have a chance to make the undo anymore,\nmeaning the popup dialog vanished (see below), just override the `Undoable.discard()`\nmethod in your `Undoable`. This will be called as soon as the popup dialog vanishes.\nYou can e.g. really delete items from the database in that callback, that was just hidden\nbeforehand.\n\nIf you implement the discard method you need to call `SwipeDismissList.discardUndo()` in\nthe `onStop` method of your Activity. (See also [Leaky Popup](#leaky-popup) below). Otherwise\nsome items might not receive the `discard` call, when e.g. the device is rotated.\n\n### Complete onDismissCallback example\n\nAn (pseudo) example of a complete `OnDismissCallback`:\n\n```java\nSwipeDismissList.OnDismissCallback callback = new SwipeDismissList.OnDismissCallback() {\n\t// Gets called whenever the user deletes an item.\n\tpublic SwipeDismissList.Undoable onDismiss(AbsListView listView, final int position) {\n\t\t// Get your item from the adapter (mAdapter being an adapter for MyItem objects)\n\t\tfinal MyItem deletedItem = mAdapter.getItem(position);\n\t\t// Delete item from adapter\n\t\tmAdapter.remove(deletedItem);\n\t\t// Return an Undoable implementing every method\n\t\treturn new SwipeDismissList.Undoable() {\n\n\t\t\t// Method is called when user undoes this deletion\n\t\t\tpublic void undo() {\n\t\t\t\t// Reinsert item to list\n\t\t\t\tmAdapter.insert(deletedItem, position)\n\t\t\t}\n\t\t\t\n\t\t\t// Return an undo message for that item\n\t\t\tpublic String getTitle() {\n\t\t\t\treturn deletedItem.toString() + \" deleted\";\n\t\t\t}\n\n\t\t\t// Called when user cannot undo the action anymore\n\t\t\tpublic void discard() {\n\t\t\t\t// Use this place to e.g. delete the item from database\n\t\t\t\tfinallyDeleteFromSomeStorage(deletedItem);\n\t\t\t}\n\t\t};\n\t}\n};\n```\n\n## setAutoHideDelay\n\nThe undo popup will be hidden automatically after some time, after the user has\ntouched the screen after the deletion. The delay until it will hide is by default\n5 seconds. You can change that value with the `setAutoHideDelay(int)` method,\nthat takes a new delay in milliseconds. \n\n## setSwipeDirection\n\nBy default the user can swipe an item left or right to delete it. With this method\nyou can limit it to the left or right side. See the Javadoc of `setSwipeDirection`\nand `SwipeDismissList.SwipeDirection` for further information.\n\n## UndoMode\n\nThe undo list can handle multiple undos in three different ways. You define the way\nwith the third constructor parameter. If you don't pass in an argument, the default\nmode will be `SwipeDismissList.UndoMode.SINGLE_UNDO`.\n\n### SwipeDismissList.UndoMode.SINGLE_UNDO\n\nOnly the last deletion can be undone. As soon as the user deletes another item\nfrom the list, this will be undoable, but the previous won't be anymore.\n\n### SwipeDismissList.UndoMode.MULTI_UNDO\n\nThis mode is a multilevel undo. When the user deletes an item, while there is\nstill the undo popup shown for a previous deleted one, both items will be saved\nfor undo. Pressing now undo will undo the last deletion. Pressing undo after that\nthe deletion that has done before, and so on ...\n\nAs soon as the undo popup vanishes all stored undos will be discarded.\n\nBy default the undo popup shows a message with the amount of deleted items in this\nmode. You can change this message with the `setUndoMultipleString(String)`.\nThis message can contain one placeholder (`%d`) that will be filled with the\namount of stored undos. \n\nIf you pass `null` to that method, the undo popup\nwill always show the individual message (returned by `Undoable.getTitle()`) for\nthe last deletion (the one that will be undone, by a click on undo). If there is\nno individual message for an `Undoable` the default message (`setUndoString(String)`)\nwill be shown instead.\n\n### SwipeDismissList.UndoMode.COLLAPSE_UNDO\n\nThis mode collapsed multiple undos into one. When the user deletes an item, while\nan undo popup is already shown, the new undo is stored. From now on the user sees\nan \"Undo all\" instead of \"Undo\" button. A click on that will undo all stored deletions\nat once.\n\nIf the popup vanished (due to the auto hide delay passed) all stored undos will be\ndiscarded.\n\nYou can again use `setUndoMultipleString(String)` to set an individual message.\nAlso passig `null` is possible, but doesn't make too much sense in that case, since\nthe user will only see the last undo messgae, but all deletions will be undone.\n\n## Customizing and Internationalization\n\nIf you want to customize the look and feel, just modify the resources as you like.\n\nYou can internationalize the \"Undo\" and \"Undo all\" string, in your own resources\nby adding a string for the name \"undo\" and one for \"undoall\". This library only provides\nthe english translation.\n\nYou can pause the dismiss behavior of the list for some time by using the `setEnabled(boolean)`\nmethod on the `SwipeDismissList`.\n\nBugs\n----\n\nFor bugs and feature requests please use the GitHub issue tracker. I haven't limited\nthe library to any android api versions, but it might be, that it doesn't work on \nvery old versions, so please feel free to tell me via an issue, I will then try to fix it.\n\n### Leaky Popup\n\nWhen the popup is shown and the device is rotate, it causes an exception to be thrown.\nThis is nothing really bad, but you can (and should) prevent that exception by calling \n`SwipeDismissList.discardUndo()` in the `onStop` method of your activity.\n\nIf you have implemented the `discard` method (see [above](#notification-about-final-delete)) you \n*MUST* call this method.\n\n\nContact\n-------\n\nFor other questions or help, you can find contact data on [my page](http://www.timroes.de)\nor you will find me often in #android-dev on irc.freenode.net.\n"
 },
 {
  "repo": "tyczj/MapNavigator",
  "language": "Java",
  "readme_contents": "MapNavigator\n============\n\nEasy to use library to get and display driving directions on Google Maps v2 in Android. This library gives you directions\nand displays the route on the map.\n\nOnly works with Google Maps v2\n\n![ScreenShot](MapNavigator/Screenshot_2013-10-07-21-32-56_framed.png)\n\nUsage\n=====\n\n    GoogleMap map = getMap();\n    Navigator nav = new Navigator(map,start,end);\n    nav.findDirections(true, false);\n    \nPath displayed callbacks\n========================\n\nSometimes you may need to know when the path is finished being displayed so you can show all the different route segments\nin a listview or something.\n\nSimple just implement the onPathSetListener\n\n    nav.setOnPathSetListener(this);\n    \nmake sure your fragment or activity implements the interface OnPathSetListener then implement onPathSetListenerand you will get \nthe callback when the path is ready\n\nImportant Notices\n=================\n\nAs per Google's TOS using this library to supply turn-by-turn directions is strictly prohibited\n\n    Google Maps/Google Earth APIs Terms of Service\n \n    Last updated: May 27, 2009\n    ...\n    10. License Restrictions. Except as expressly permitted under the Terms, or unless you\n    have received prior written authorization from Google (or, as applicable, from the\n    provider of particular Content), Google's licenses above are subject to your adherence\n    to all of the restrictions below. Except as explicitly permitted in Section 7 or the\n    Maps APIs Documentation, you must not (nor may you permit anyone else to):\n    ...\n    10.9 use the Service or Content with any products, systems, or applications for or in\n    connection with:\n \n    (a) real time navigation or route guidance, including but not limited to turn-by-turn\n    route guidance that is synchronized to the position of a user's sensor-enabled device;\n \n    and may be disabled for certain apps (somehow, at least on Android)... FromGeocode\n    scraping in .NET conversation:\n    This is not allowed by the API terms of use. You should not scrape Google Maps to\n    generate geocodes. We will block services that do automated queries of our servers.\n \n    Bret Taylor\n    Product Manager, Google Maps\n    \nYou also only get 2,500 requests/day, more for business customers\n\n    The Directions API has the following limits in place:\n\n    2,500 directions requests per day.\n    When the mode of transportation is driving, walking, or cycling, each directions search counts as a single request.\n    Searching for transit directions counts as 4 requests.\n    Individual requests for driving, walking, or cycling directions may contain up to 8 intermediate waypoints in the request. Waypoints can not be specified for transit requests.\n    Google Maps API for Business customers have higher limits:\n\n    100,000 directions requests per day.\n    23 waypoints allowed in each request. Waypoints are not available for transit directions.\n    \n"
 },
 {
  "repo": "uPhyca/GAlette",
  "language": "Java",
  "readme_contents": "## DEPRECATED\nThe plugin only works for com.android.tools.build:gradle:1.x\n\n[![wercker status](https://app.wercker.com/status/da37f33da36130c5cda62f25542f0640/s \"wercker status\")](https://app.wercker.com/project/bykey/da37f33da36130c5cda62f25542f0640)\n\n\n# GAlette\n\nAnnotation-triggered tracking along with Google Analytics for Android.\n\n\n```java\n@SendEvent(category = \"HelloWorld\", action = \"sayHello\", label=\"%1$s\")\nString sayHello (String name) {\n  return format(\"Hello, %s.\", name);\n}\n```\n\n# Before you begin\n\nBefore using GAlette, make sure you have done following instructions described in https://developers.google.com/analytics/devguides/collection/android/v4/\n- Before you begin\n- Updating AndroidManifest.xml\n- Initialize Trackers\n- Create a configuration XML file\n\n\n# Getting Started\n\n\nAdd the GAlette plugin to your `buildscript`'s `dependencies` section and apply the plugin:\n```groovy\nbuildscript {\n    repositories {\n        ...\n        mavenCentral()\n    }\n    dependencies {\n        ...\n        classpath 'com.uphyca.galette:galette-plugin:0.9.16'\n    }\n}\n\n...\napply plugin: 'com.uphyca.galette'\n```\n\n\nImplements TrackerProvider to your Application class\n```java\npublic class MyApplication extends Application implements TrackerProvider {\n\n    private Tracker mTracker;\n\n    @Override\n    public void onCreate() {\n        super.onCreate();\n        GoogleAnalytics ga = GoogleAnalytics.getInstance(this);\n        mTracker = ga.newTracker(R.xml.your_tracker_resource);\n    }\n\n    @Override\n    public Tracker getByName(String trackerName) {\n        return mTracker;\n    }\n}\n```\n\nDeclare your application in AndroidManifest.xml.\n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n          package=\"com.uphyca.example.galette\">\n  ...\n    \n  <application\n    android:name=\".MyApplication\" ...>\n\n    ...\n\n  </application>\n</manifest>\n```\n\n## Send Screen View\n\nAnnotate @SendScreenView to your methods to track appView\n\n```java\npublic class MainActivity extends Activity {\n\n    @Override\n    @SendScreenView(screenName = \"main\")\n    protected void onCreate(Bundle savedInstanceState) {\n        ...\n    }\n}\n```\n\n## Measuring Events\n\nAnnotate @SendEvent to your methods to track event\n\n```java\npublic class MainActivity extends Activity {\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        ...\n        \n        findViewById(R.id.button).setOnClickListener(new View.OnClickListener() {\n            @Override\n            @SendEvent(category = \"button\", action = \"click\")\n            public void onClick(View v) {\n                onButtonClicked();\n            }\n        });\n    }\n\n}\n```\n\n## Advanced Usage\n\n### String Templates\n\nUse string template to apply method parameters\n\n```java\npublic class MainActivity extends Activity {\n\n    private int mClickCount;\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        ...\n\n        findViewById(R.id.button).setOnClickListener(new View.OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                onButtonClicked(++mClickCount);\n            }\n        });\n    }\n\n    @SendEvent(category = \"button\", action = \"click\", label = \"times\", value = \"%1$d\")\n    private void onButtonClicked(int count) {\n        ...\n    }\n}\n```\n\n### Customizing field values\n\nUse FieldBuilder to build each field value\n\n```java\npublic class MainActivity extends Activity {\n\n    @Override\n    @SendScreenView(screenName = \"foo\", screenNameBuilder = BundleValueFieldBuilder.class)\n    protected void onCreate(Bundle savedInstanceState) {\n        ...\n    }\n\n    public static class BundleValueFieldBuilder implements FieldBuilder<String> {\n        @Override\n        public String build(Fields fields, String fieldValue, Object declaredObject, Method method, Object[] arguments) {\n            // annotation's screenName value paths to fieldValue, in this case 'foo'\n            return ((Bundle) arguments[0]).getString(fieldValue);\n        }\n    }\n}\n```\n\n\n### Interceptor\n\nUse HitInterceptor to send arbitrary values\n\n```java\npublic class MyApplication extends Application implements TrackerProvider, HitInterceptor.Provider {\n\n    private Tracker mTracker;\n\n    private HitInterceptor hitInterceptor = new HitInterceptor() {\n        @Override\n        public void onEvent(EventFacade eventFacade) {\n            eventFacade.setCustomDimension(1, Build.MODEL);\n        }\n\n        @Override\n        public void onScreenView(ScreenViewFacade screenViewFacade) {\n            screenViewFacade.setCustomDimension(1, Build.MODEL);\n        }\n    };\n\n    @Override\n    public void onCreate() {\n        super.onCreate();\n        GoogleAnalytics ga = GoogleAnalytics.getInstance(this);\n        ga.setLocalDispatchPeriod(1);\n        mTracker = ga.newTracker(\"SET-YOUR-TRACKING-ID\");\n    }\n\n    @Override\n    public Tracker getByName(String trackerName) {\n        return mTracker;\n    }\n\n    @Override\n    public HitInterceptor getHitInterceptor(String trackerName) {\n        return hitInterceptor;\n    }\n}\n```\n\n\n### Supported types\n\nFollowing types or these subclasses are supported.\n\n- Application\n- Activity\n- Service\n- Fragment (android.app or android.support.v4.app)\n- View\n- Context\n\nor implements ContextProvider to orbitary classes.\n\n```Java\npublic class MyClass implements ContextProvider {\n\n    private Context mContext;\n    \n    public MyClass(Context context) {\n        mContext = context;\n    }\n    \n    @Override\n    public Context get() {\n        return mContext;\n    }\n    \n    @SendScreenView(screenName = \"my-class\")\n    void foo() {\n    }\n}\n```\n\n\n## Specify Google Play Services version\n\n```\nconfigurations.all {\n    resolutionStrategy {\n        force \"com.google.android.gms:play-services-analytics:${playservicesVersion}\"\n    }\n}\n```\n\nSee more details [ResolutionStrategy - Gradle DSL Version 2.8](https://docs.gradle.org/current/dsl/org.gradle.api.artifacts.ResolutionStrategy.html)\n\n\n## Proguard\n\n```\n-keepclassmembernames class * {\n    @com.uphyca.galette.SendScreenView *;\n    @com.uphyca.galette.SendEvent *;\n}\n\n-keepclassmembers class * implements com.uphyca.galette.FieldBuilder {\n   <init>();\n}\n```\n\n\n# License\n\n    Copyright 2014 uPhyca, Inc.\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n"
 },
 {
  "repo": "mallethugo/RealParallaxAndroid",
  "language": "Java",
  "readme_contents": "![RealParallaxAndroid](https://github.com/mallethugo/RealParallaxAndroid/blob/master/realparallaxandroid.png)\n\n===\n\nRealParallaxAndroid is a View Pager with a Real Parallax Android Effect\n\n[![Sample Screenshot](https://img.shields.io/badge/Android%20Arsenal-RealParallaxAndroid-green.svg?style=true)](https://android-arsenal.com/details/1/2918) <a href=\"http://www.methodscount.com/?lib=com.github.hmallet%3Arealparallaxandroid%3A1.0.2\"><img src=\"https://img.shields.io/badge/Methods count-core: 39 | deps: 14253-e91e63.svg\"></img></a>\n\n![RealParallaxAndroid](https://github.com/mallethugo/RealParallaxAndroid/blob/master/Demo.gif)\n\n# Usage\n\n```xml\n<com.github.hmallet.realparallaxandroid.RealHorizontalScrollView\nandroid:id=\"@+id/main_horizontal_scrollview\"\nandroid:layout_width=\"match_parent\"\nandroid:layout_height=\"match_parent\"\napp:src=\"@mipmap/background\"/>\n\n<com.github.hmallet.realparallaxandroid.RealViewPager\nandroid:id=\"@+id/main_view_pager\"\nandroid:layout_width=\"match_parent\"\nandroid:layout_height=\"match_parent\"\napp:parallaxVelocity=\"0.2\"/>\n```\n\n## Custom Attributes for RealHorizontalScrollView\n\n- `app:src`\n\n## Custom Attributes for RealViewPager\n\n- `parallaxVelocity` float to manage parallax effect\n\n\n## Gradle\n\nAt your project `build.gradle` file:\n\n```groovy\ndependencies {\n    compile 'com.github.hmallet:realparallaxandroid:1.0.2'\n}\n```\n\nDone!\n\n#Author\nHugo Mallet:\n- hello@mallethugo.com\n- [@mallethugo](https://twitter.com/mallethugo)\n- [Linkedin](https://www.linkedin.com/in/hugomallet)\n\n# License\n\nThe MIT License (MIT)\n\nCopyright (c) 2015 Hugo Mallet\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
 },
 {
  "repo": "konifar/android-strings-search-plugin",
  "language": "Java",
  "readme_contents": "Android Strings Search Plugin\n=============================================\n\nThis plugin makes it easy to search text in strings resources.\n\n![capture](https://github.com/konifar/android-strings-search-plugin/raw/master/art/demo.gif)\n\n#Installation\n\n##Manually\n\n1. Download the [android-strings-search-plugin.jar](https://github.com/konifar/android-strings-search-plugin/raw/master/android-strings-search-plugin.jar)\n\nOn MAC:\n\n2. `Preference > Plugins > Install plugin from disk...` Select android-strings-search-plugin.jar above.\n\nOn Linux or Windows:\n\n2. `File > Settings... > IDE Settings > Plugins > Install plugin from disk...` Select android-strings-search-plugin.jar above.\n\n3. Restart IntelliJ/Android Studio to activate plugin.\n\n##Install IntelliJ Plugin Repositories\n\nOn MAC:\n\nGo `Preference > Plugins > Browse repositories`, then search `strings search`.\n\nOn Linux or Windows:\n\nGo `File > Settings... > IDE Settings > Plugins > Browse repositories`, then search `strings search`.\n\n![capture](https://github.com/konifar/android-strings-search-plugin/raw/master/art/browse_repository.png)\n\n\n#License\n\n```\nCopyright 2016 Yusuke Konishi\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n"
 },
 {
  "repo": "JPMoresmau/eclipsefp",
  "language": "Java",
  "readme_contents": "# EclipseFP\n\nEclipseFP is a set of plugins for Eclipse that add support for the Haskell programming language.\n\nEclipseFP is currently NOT maintained anymore. Feel free to fork and take up maintainership!\n\nThis file is at the root of the project, and below it you should see all the plugins that make up EclipseFP. Most of current plugins that are part of the EclipseFP installation, some are just test plugins that contains, er, tests on other classes.\n\n* [docs](https://github.com/JPMoresmau/eclipsefp/tree/master/docs) contains some documentation, mainly the release notes\n* [net.sf.eclipsefp.haskell](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell) contains cheatsheets\n* [net.sf.eclipsefp.haskell.browser](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.browser) the low level classes for scion-browser interaction\n* [net.sf.eclipsefp.haskell.buildwrapper](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.buildwrapper) the low level classes for buildwrapper interaction\n* [net.sf.eclipsefp.haskell.compat](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.compat) ensure compatiblity between several version of Eclipse\n* [net.sf.eclipsefp.haskell.core](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.core) low level GHC and Cabal support\n* [net.sf.eclipsefp.haskell.core.test](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.core.test) tests for core plugin\n* [net.sf.eclipsefp.haskell.debug.core](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.debug.core) integration with Eclipse Run/Debug infrastructure\n* [net.sf.eclipsefp.haskell.debug.ui](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.debug.ui) integration with Eclipse Run/Debug UI\n* [net.sf.eclipsefp.haskell.debug.ui.test](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.debug.ui.test) test for debug ui\n* [net.sf.eclipsefp.haskell.doc.user](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.doc.user) user documentation\n* [net.sf.eclipsefp.haskell-feature](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell-feature) EclipseFP feature\n* [net.sf.eclipsefp.haskell.ghccompiler](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.ghccompiler) specific GHC support\n* [net.sf.eclipsefp.haskell.ghccompiler.test](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.ghccompiler.test) tests for specific GHC support\n* [net.sf.eclipsefp.haskell.hlint](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.hlint) HLint integration\n* [net.sf.eclipsefp.haskell.hlint.test](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.hlint.test) HLint integration test\n* [net.sf.eclipsefp.haskell.hugs](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.hugs) Hugs support, compiles but not maintained\n* [net.sf.eclipsefp.haskell.profiler](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.profiler) profiling support\n* [net.sf.eclipsefp.haskell.style](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.style) Stylish-Haskell integration\n* [net.sf.eclipsefp.haskell.style.test](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.style.test) Tests for Stylish Haskell integration\n* [net.sf.eclipsefp.haskell.test](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.test) General test\n* [net.sf.eclipsefp.haskell.ui](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.ui) the biggie, all the UI components\n* [net.sf.eclipsefp.haskell.ui.test](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.ui.test) tests for UI\n* [net.sf.eclipsefp.haskell-update-site](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell-update-site) update site, not used really since the update site is on sourceforge\n* [net.sf.eclipsefp.haskell.util](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.util) utilities methods\n* [net.sf.eclipsefp.haskell.util.test](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.util.test) tests for utilities\n* [net.sf.eclipsefp.haskell.visual](https://github.com/JPMoresmau/eclipsefp/tree/master/net.sf.eclipsefp.haskell.visual) dabbling with visual programming, not working\n"
 },
 {
  "repo": "bulletnoid/StaggeredGridView",
  "language": "Java",
  "readme_contents": "StaggeredGridView\n=================\n\n## A sweeter StaggeredGridView\n\nThis widget is based on [maurycyw/StaggeredGridView][1], which is a modification of Android's experimental StaggeredGridView: [com.android.ex.widget.StaggeredGridView][2]\n\nThis widget has fixed some of the major bugs and has some new features that you may be interested.\n\n![](snapshot/snap.png)\n\n## Suggestion\n\nIt is very helpful for you to use this widget in your app and understand the restriction in it if you had knowledge of how to create Android custom views;\nhttp://developer.android.com/guide/topics/ui/custom-components.html has good information about that;\n\n## Features\n\n* Stability and high performance\n\n  This widget fix some bugs of [maurycyw/StaggeredGridView][1]. Such as when fling the view, the scroll sometimes slow down and speed up later.\n\n  Notice that the image loading also has a contribution to the performance. I use [square/Picasso][3], it provides the best performance I've ever seen.\n* Header and Footer View and an Adapter to wrap all child views, just like android.widget.ListView\n\n  Header and footer views can cross columns, but the widget currently only support no more than one header and no more than one footer.\n* Load more when get to the bottom\n\n  You may find the footer view useful here.\n* Work with PullToRefresh\n\n  A compatible part enable this widget to be pulled to refresh.\n\n## Restriction\n\n* You have to determine the dimension of each child view in the widget before the parent the child.measure()\n\n  This is because the after the child is first time added to the parent widget, its size should not be changed, otherwise it may cause gird misalign as you may have seen in [maurycyw/StaggeredGridView][1].\n  \n  eg. You want to display pictures in the widget, and the pictures are loaded from network.\n  The height of each grid is decided by the size of the picture in them.\n  Say, if you set your picture container to WRAP_CONTENT, the size of the pic container may change when the picture is loaded, and then the size of the grid change.\n  This can cause gird misalign. \n  Unfortunately, the current methodology has nothing to do to fix this. \n  Instead, you can let this widget know the size of each child before the picture is actually downloaded. \n  You can do this by overwrite the onMeasure() method of the container.\n\n* Load more is lazy\n\n  When load more, the widget only add new items to the old ones, the old ones is not reloaded.\n\n* Screen rotation\n\n  The widget may have problem holding the state when it is destroyed and restored the them in a different screen orientation.\n  Besides, you may want to change the column number when the screen orientation changed, you'd better rebuild the whole content from start.\n\n## Project structure\n\nProject contains StaggeredGridView library, StaggeredGridView demo, modified PullToRefresh library to work with StaggeredGridView.\nIn order to avoid some dependency problems, I add the libs into one project, but it is easy to retrieve the libs.\n* StaggeredGridView lib\n\n  code: src/com.bulletoid.android.widget.StaggeredGridView\n  \n  res: res/stgv_*.xml\n* PullToRefresh lib\n\n  code: src/com.handmark.pulltorefresh.library\n  \n  res: res/ptr_*.xml\n\n## Usage\n\nPlease refer to the Demo of how to use the widget and use it with PullToRefresh.\n\n## Pictures in the demo are from Pinterest\n\nLicense\n--------\n\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n\n[1]: https://github.com/maurycyw/StaggeredGridView\n[2]: http://grepcode.com/file/repository.grepcode.com/java/ext/com.google.android/android/4.3_r2.1/com/android/ex/widget/StaggeredGridView.java?av=f\n[3]: https://github.com/square/picasso\n"
 },
 {
  "repo": "liaohuqiu/SimpleHashSet",
  "language": "Java",
  "readme_contents": "[![Build Status](https://travis-ci.org/liaohuqiu/SimpleHashSet.svg)](https://travis-ci.org/liaohuqiu/SimpleHashSet)\n\n### maven \u4e2d\u592e\u5e93\n\n```xml\n<dependency>\n    <groupId>in.srain.cube</groupId>\n    <artifactId>simple-hashset</artifactId>\n    <type>jar</type>\n    <version>1.0.1</version>\n</dependency>\n```\n\ngradle:\n\n```\ncompile 'in.srain.cube:simple-hashset:1.0.1'\n```\n\n### \u6700\u591a\u8282\u7701 25% \u5185\u5b58\n\n> Java \u5185\u5b58\u5927\u5c0f: http://www.liaohuqiu.net/cn/posts/caculate-object-size-in-java/\n\n> Davlik \u5185\u5b58\u5927\u5c0f: http://www.liaohuqiu.net/posts/android-object-size-dalvik/\n\n`java.util.HashSet` \u4ec5\u4ec5\u662f\u7b80\u5355\u5305\u88c5\u4e86 `java.util.HashMap`, \u5185\u5b58\u5360\u7528\u4e3a O(`n` * `entry_size`).\n\n`java.util.HashMap` \u91c7\u7528\u94fe\u63a5\u6cd5\u89e3\u51b3\u51b2\u7a81, `n` \u662f\u5143\u7d20\u603b\u6570\u548c\u6570\u7ec4\u4e2d\u53ef\u7528\u69fd\u4e4b\u548c\u3002\n\n`entry_size` \u662f `jave.util.HashMap.Entry` \u5bf9\u8c61\u5728\u5185\u5b58\u4e2d\u7684\u5927\u5c0f\u3002\n\n`java.util.HashMap.Entry`\u7684\u5b9a\u4e49\u5982\u4e0b:\n\n```\nstatic class Entry<K,V> implements Map.Entry<K,V> {\n    final K key;\n    V value;\n    Entry<K,V> next;\n    int hash;\n}\n```\n\n\u5728\u4e0d\u540c\u5e73\u53f0\uff0c\u5185\u5b58\u5360\u7528\u4e3a:\n\n*  32bit JVM:  8 + 4 * 4 = 24 bytes\n*  64bit JVM `-UseCompressedOops`: 16 + 8 * 4 = 48 bytes.\n*  64bit JVM `+UseCompressedOops`: 12 + 4 * 4 + 4(padding) = 32 bytes.\n*  Davlik:    12 + 4 * 4 + 4(padding) = 32 bytes.\n\n\u5bf9\u4e8e`HashSet`\u6765\u8bf4\uff0c`V value` \u8fd9\u4e2a\u5b57\u6bb5\u662f\u6ca1\u7528\u7684\uff0c\u5982\u679c\u6211\u4eec\u91c7\u7528\u4ee5\u4e0b\u7684`SimpleHashSetEntry`\u6765\u5b9e\u73b0 `SimpleHashSet`:\n\n```\nprivate static class SimpleHashSetEntry<T> {\n\n    private int mHash;\n    private T mKey;\n    private SimpleHashSetEntry<T> mNext;\n}\n```\n\n\u5360\u7528\u7684\u5185\u5b58\u5927\u5c0f\u4e3a:\n\n*  32bit JVM:  8 + 4 * 3 + (padding) = 24 bytes\n*  64bit JVM `-UseCompressedOops`: 16 + 8 * 3 = 40 bytes. (8 bytes saved, 16.66%)\n*  64bit JVM `+UseCompressedOops`: 12 + 4 * 3 = 24 bytes. (8 bytes saved, 25%)\n*  Davlik:    12 + 4 * 3 = 24 bytes (8 bytes saved, 25%).\n\n\u9664\u4e86\u4e8632\u4f4d\u7684JVM\u4e0a\uff0c\u90fd\u80fd\u8282\u7701\u53ef\u89c2\u7684\u5185\u5b58\u3002\n\n\n### License\n\nApache 2\n"
 },
 {
  "repo": "andyxialm/CutoLoadingView",
  "language": "Java",
  "readme_contents": "#CutoLoadingView\n[![](https://jitpack.io/v/andyxialm/CutoLoadingView.svg)](https://jitpack.io/#andyxialm/CutoLoadingView)\n\nA custom loading view, just like CutoWallpaper.\n\n![](https://github.com/andyxialm/CutoLoadingView/blob/master/art/screenshot.gif?raw=true)\n### Usage\n\n##### Gradle\n###### Step 1. Add the JitPack repository to your build file\nAdd it in your root build.gradle at the end of repositories:\n~~~ xml\nallprojects {\n\t\trepositories {\n\t\t\t...\n\t\t\tmaven { url \"https://jitpack.io\" }\n\t\t}\n\t}\n~~~\n###### Step 2. Add the dependency\n~~~ xml\ndependencies {\n        compile 'com.github.andyxialm:CutoLoadingView:1.0.1'\n}\n~~~\n\t\n##### Edit your layout XML:\n\n~~~ xml\n<cn.refactor.cutoloadingview.CutoLoadingView\n    android:id=\"@+id/cuto_loading_view\"\n    xmlns:cuto=\"http://schemas.android.com/apk/res-auto\"\n    android:layout_width=\"42dp\"\n    android:layout_height=\"42dp\"\n    cuto:animDuration=\"1400\"\n    cuto:circleRadius=\"3dp\"\n    cuto:strokeColor=\"@android:color/white\"\n    cuto:strokeWidth=\"2dp\"/>\n~~~\n\n### License\n\n    Copyright 2016 andy\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License."
 },
 {
  "repo": "rjsvieira/morphos",
  "language": "Java",
  "readme_contents": "# Morphos\n\n![Current Version](https://img.shields.io/badge/Current%20Version-1.0.0-brightgreen.svg)\n[![](https://jitpack.io/v/rjsvieira/morphos.svg)](https://jitpack.io/#rjsvieira/morphos)\n![Minimum SDK](https://img.shields.io/badge/minSdkVersion%20-19-blue.svg)\n\n<img src=\"images/banner.png\">\n\n<h2> They say a pic is worth a 1000 words. Is it true to admit a .gif is worth a 1000 pics? </h2>\n<kbd><img src=\"https://github.com/rjsvieira/morphos/blob/master/images/one.gif\" width=\"49%\" height=\"200px\">  <img src=\"https://github.com/rjsvieira/morphos/blob/master/images/two.gif\" width=\"49%\" height=\"200px\"> </kbd>\n\n<h2>Include in your project</h2>\n\n<h4> In your root/build.gradle</h4>\n\n```groovy\nallprojects {\n  repositories {\n  ...\n  maven { url 'https://jitpack.io' }\n  }\n}  \n```\n\n<h4> In your app/build.gradle</h4>\n\n```groovy\ndependencies {\n  compile 'com.github.rjsvieira:morphos:1.0.0'\n}\n```\n\n\nIntroducing Morphos : an animation wrapper. \nMorphos will take care of your views' animations without you having to write all that boring boilerplate code.\n\n\n<h2>Initialization</h2>\n\nMorphos are easy to interact with. Go ahead and create the following simple Morpho :\n\n```java\nView viewToAnimate = ... ;\nMorpho morph = new Morpho(viewToAnimate)\n  .translate(50,50,0,1500) // will translate the view (x=50,y=50,z=0) in 1500 milliseconds, \n  .rotationXY(45,45,2000); // will rotate the view by 45 degrees on both the X-axis and Y-axis in 2000 milliseconds\n```\n\nYou can then animate it by doing \n\n```java\nmorph.animate(); \n```\n\nWhich will then animate the desired Morphos using the default animation type (SEQUENTIAL).\n\nWhat if I want to reverse the animation? Sure, just do :\n\n```java\nmorph.reverse();\n```\n\n\n\n\n\n\n\n<h2> Configuration </h2>\n\n<h4>Create a Morpho</h4>\n\n```java\nMorpho morphoOne = new Morphos(view);\n```\n\n<h4>Configure the Morphos' animations</h4>\n\nAs of the first release, Morphos supports the 7 most basic and common animations.\nSince Morphos has plenty of combinations for interpolation, animation type, duration, etc, the user is allowed to configure them according to his needs.\nEvery animation configuration method returns the Morphos object itself, thus allowing the user to chain his preferred animations.\n\nNote : If the user does not specify the duration and/or interpolator, the animation will assume a 0 second duration and the default interpolator.\n\n```java\nalpha(double value)\nalpha(double value, int duration)\nalpha(double value, int duration, Interpolator interpolator)\n\nscale(double valueX, double valueY)\nscale(double valueX, double valueY, int duration)\nscale(double valueX, double valueY, int duration, Interpolator interpolator)\n\ntranslationX(AnimationTarget target, float valueX)\ntranslationX(AnimationTarget target, float valueX, int duration)\ntranslationX(AnimationTarget target, float valueX, int duration, Interpolator interpolator)\n\ntranslationY(AnimationTarget target, float valueX)\ntranslationY(AnimationTarget target, float valueX, int duration)\ntranslationY(AnimationTarget target, float valueX, int duration, Interpolator interpolator)\n\ntranslationZ(AnimationTarget target, float valueX)\ntranslationZ(AnimationTarget target, float valueX, int duration)\ntranslationZ(AnimationTarget target, float valueX, int duration, Interpolator interpolator)\n\ntranslation(AnimationTarget target, float valueX, float valueY, float valueZ)\ntranslation(AnimationTarget target, float valueX, float valueY, float valueZ, int duration)\ntranslation(AnimationTarget target, float valueX, float valueY, float valueZ, int duration, Interpolator interpolator)\n\ndimensions(float width, float height)\ndimensions(float width, float height, int duration)\ndimensions(float width, float height, int duration, Interpolator interpolator)\n\nrotationXY(AnimationTarget target, double degreesX, double degreesY)\nrotationXY(AnimationTarget target, double degreesX, double degreesY, int duration)\nrotationXY(AnimationTarget target, double degreesX, double degreesY, int duration, Interpolator interpolator)\n\nrotation(AnimationTarget target, double degrees)\nrotation(AnimationTarget target, double degrees, int duration)\nrotation(AnimationTarget target, double degrees, int duration, Interpolator interpolator)\n```\n\n\nSet a Listener to track the Morphos' animation process\n\n```java\nmorphoOne.setListener(new Animator.AnimatorListener() {\n  @Override\n  public void onAnimationStart(Animator animator) {\n      System.out.println(\"Start\");\n  }\n\n  @Override\n  public void onAnimationEnd(Animator animator) {\n      System.out.println(\"End\");\n  }\n\n  @Override\n  public void onAnimationCancel(Animator animator) {\n\n  }\n\n  @Override\n  public void onAnimationRepeat(Animator animator) {\n\n  }\n})  \n```\n\n\n<h4> Animate() the Morpho</h4>\nThis can be done through one of the several methods created just to make the invocation easier.\nThe animate() method's default values are : \n<br>\nAnimationType : SEQUENTIAL - All animations are executed one sequentially\nDuration : -1 - Since no duration was specified, -1 is assumed thus executing every animation in its given duration. For example : morphoOne.translate(100,0,0,3000).scale(2,2,2000).animate() will execute the translation animation in 3 seconds, followed by an upscale animation of 2 seconds\nInterpolator - The overall interpolator (LinearInterpolator)\n\n\n```java\nanimate()\nanimate(AnimationType type, int duration)\nanimate(AnimationType type, int duration, Interpolator interpolator)\n```\n\n\n\n<h4>Reverse</h4>\n\nAfter executing animate(), the user can rollback the animation by invoking the reverse() method.\nThe reverse method works just like the animate() method, having the same combinations and parameters.\n\n\n```java\nreverse()\nreverse(AnimationType type, int duration)\nreverse(AnimationType type, int duration, Interpolator interpolator)\n```\n\n\n\n<h4>Cancel</h4>\n\nIf the user wishes to cancel the on-going animation by any reason, he can do so by invoking the cancel method : \n\n```java\nmorphoOne.cancel();\n```\n\n\n\n<h4>updateView</h4>\n\nLike the method explicitly indicates, the user can update the view associated with the Morpho. This will clear every animation already configured for the given object. \nupdateView(View v) also invokes reset();\n\n```java\nmorphoOne.updateView(newView);\n```\n\n\n<h4>Reset</h4>\n\nIf by any chance the user wants to reset he Morpho and re-build it from scratch, he can do so by invoking the reset() method\n\n```java\nmorphoOne.reset()\n```\n\n\n<h4>Dispose</h4>\n\nLast but not least, if they user wishes to discard the Morphos object, he can invoke the dispose() method, thus clearing and preparing the Morphos' inner variables for garbage collection.\n\n```java\nmorphoOne.dispose();\n```\n\n"
 },
 {
  "repo": "tuesda/SubmitDemo",
  "language": "Java",
  "readme_contents": "This is library project with a custom view that implements concept of Submit Button (<https://dribbble.com/shots/1426764-Submit-Button?list=likes&offset=3>) made by Colin Garven.  \n\n###Demo###\n\n![](gifs/submitview.gif)\n\n###Usage###\n\n``` xml\n <com.tuesda.submit.SubmitView\n        android:layout_centerInParent=\"true\"\n        android:id=\"@+id/submit\"\n        android:layout_width=\"200dp\"\n        android:layout_height=\"200dp\" />\n```\n\n``` xml\nmSubmit.setOnProgressStart(new SubmitView.OnProgressStart() {\n            @Override\n            public void progressStart() {\n                // do something when progress start\n            }\n        });\n        \n        mSubmit.setOnProgressDone(new SubmitView.OnProgressDone() {\n            @Override\n            public void progressDone() {\n                // do something when progress is done\n            }\n        });\n```\n\n###public interface###\n\n| \u51fd\u6570\u540d |  \u4f5c\u7528|\n|:------|:-----|\n|`setBackColor(int color)`| \u8bbe\u7f6e\u56fe\u6807\u80cc\u666f\u8272\uff0c\u9ed8\u8ba4\u662f\u7eff\u8272(0xff00cd97)\uff0c\u4e0a\u56feDemo\u8bbe\u7f6e\u4e3a\u84dd\u8272(0xff0097cd)|\n|`setText(String str)`|\u8bbe\u7f6e\u6309\u94ae\u540d\u5b57\uff0c\u9ed8\u8ba4\u662f`Submit`|\n|`reset()`|\u5c06\u6309\u94ae\u91cd\u7f6e\u5230\u521d\u59cb\u72b6\u6001|\n|`setProgress(float progress)`|\u8bbe\u7f6e\u6b63\u5728\u6267\u884c\u5de5\u4f5c\u7684\u6267\u884c\u8fdb\u7a0b|\n|`isProgressDone()`| \u6b63\u5728\u6267\u884c\u5de5\u4f5c\u662f\u5426\u5b8c\u6210|\n|`setOnProgressStart(OnProgressStart listener)`|\u8bbe\u7f6eprogress\u5f00\u59cb\u56de\u8c03|\n|`setOnProgressDone(OnProgressDone listener)` | \u8bbe\u7f6eprogress\u5b8c\u6210\u56de\u8c03|"
 },
 {
  "repo": "avenwu/support",
  "language": "Java",
  "readme_contents": "Support\n========\n\nCustom Android support library, include some useful utils and widget.\n\n\tsupport\u5185\u662f\u81ea\u5b9a\u4e49\u7684\u4e00\u4e9b\u4e1c\u897f\uff0csammple\u4e2d\u5305\u542bsupport\u7684\u5b9e\u73b0demo\u6837\u4f8b\uff0c\n\nDownload\n-------\nDownload the latest repo or grab the stable released version via Maven:\n\nClone the master branch:\n\n```\n\tgit clone https://github.com/avenwu/support.git\n```\n\nor Gradle:\n\n```Groovy\n\tcompile 'com.github.avenwu:support:0.1.1'\n```\n\n\nLicense\n=======\n\n    Copyright 2014 Chaobin Wu.\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n\n---\n\n\u5b9e\u6218\u6848\u4f8b\n-------\n\n### \u5361\u7247\u7ffb\u8f6c\u4f18\u5316\n\n![device-2016-01-19-162559.mp4.gif](images/device-2016-01-19-162559.mp4.gif)\n\n### TextView\u7f29\u7565\n\u901a\u8fc7\u63a7\u5236\u6587\u672c\u5c55\u793a\u5b9e\u73b0\u5355\u51fb\u5c55\u5f00\u548c\u6536\u7f29\u6587\u672c\uff0c\u5e76\u5728\u6536\u7f29\u72b6\u6001\u663e\u793a\u63d0\u793a\u56fe\u6807\n\n![TextView](http://7u2jir.com1.z0.glb.clouddn.com/expand_text.gif)\n\n### \u5b57\u4f53\u8bbe\u7f6e\u6c47\u603b\n\u901a\u8fc7\u4e0d\u540c\u65b9\u5f0f\u5b9e\u73b0\u81ea\u5b9a\u4e49\u5b57\u4f53\u7684\u8bbe\u7f6e\uff0c\u4e3b\u8981\u533a\u522b\u5728\u4e8e\u8c03\u7528\u5c42\n\n![\u5b57\u4f53\u8bbe\u7f6e\u6c47\u603b](http://7u2jir.com1.z0.glb.clouddn.com/device-2015-10-08-173228.png)\n\n\n### \u5de7\u7528BitmapShader\n\u901a\u8fc7\u4e3aPaint\u753b\u7b14\u8bbe\u7f6eShader\uff0c\u53ef\u4ee5\u518d\u753b\u5e03\u4e0a\u7ed8\u5236\u8bb8\u591a\u6709\u610f\u601d\u7684\u4e1c\u897f\n\n![BitmapShader](http://7u2jir.com1.z0.glb.clouddn.com/bitmapshader_imageview.gif)\n\n\n### Property\u81ea\u5b9a\u4e49\u5c5e\u6027\u52a8\u753b\n\u57fa\u4e8eProperty\u7684\u81ea\u5b9a\u4e49\u5c5e\u6027\u52a8\u753b\n\n![Property\u52a8\u753b](http://7u2jir.com1.z0.glb.clouddn.com/property_animation.gif)\n\n### \u81ea\u5b9a\u4e49ResideMenu\nresidemenu\u662f\u662f\u4fa7\u6ed1\u83dc\u5355\u7684\u4e00\u79cd\uff0c\u4f46\u662f\u89c6\u89c9\u6548\u679c\u66f4\u7279\u522b\uff0c\u6b64\u6b21\u5b9e\u73b0\u662f\u57fa\u4e8eandroid v4\u4e2dSlidePanelLayout\u6269\u5c55\u800c\u6765\uff0c\u4e3b\u8981\u662f\u4e3a\u4e86\u51cf\u5c11\u975e\u6838\u5fc3\u4ee3\u7801\u7684\u5f00\u53d1\u5de5\u4f5c\u3002\n\n\u8be6\u7ec6\u5b9e\u73b0\u8bf7\u770b:[\u57fa\u4e8eSlidePanelLayout\u5b9e\u73b0ResideMenu](http://avenwu.net/2015/02/24/custom_slide_panel_layout_as_reside_style_on_dribble_and_qq)\n\n\u5bf9\u4e8e\u666e\u901a\u4fa7\u6ed1\u83dc\u5355\u7684\u5b9e\u73b0\u4e5f\u53ef\u4ee5\u53c2\u7167\u6211\u4e4b\u524d\u7684\u4e00\u7247\u6587\u7ae0[\u81ea\u5b9a\u4e49\u4fa7\u6ed1\u83dc\u5355](http://avenwu.net/customlayout/2014/12/16/sliding_menu/)\n\n![\u81ea\u5b9a\u4e49ResideMenu](http://7u2jir.com1.z0.glb.clouddn.com/custom_residemenu.gif)\n\n### RadioGroup\u4effiOS Segmented Control\n\u8fd9\u4e2a\u6ca1\u4ec0\u4e48\u597d\u8bf4\u7684\u7528\u7684\u5b9e\u9645\u4e0a\u662fRadioGroup\uff0c\u4f46\u662f\u52a0\u5f3a\u4e86\u5c01\u88c5\u548c\u914d\u7f6e\uff0c\u6240\u4ee5\u4f7f\u7528\u4f1a\u65b9\u4fbf\u4e00\u4e9b\uff0c\u5426\u5219\u6bcf\u6b21\u9700\u8981\u7c7b\u4f3cUI\u6548\u679c\u90fd\u4ece\u65b0\u5199\u662f\u5f88\u7d2f\u7684\u4e8b\u60c5\u3002\n\n![RadioGroup\u4effiOS Segmented Control ](http://7u2jir.com1.z0.glb.clouddn.com/styled_radiogroup.png)\n\n### \u6d41\u5f0f\u6807\u7b7e\u751f\u6210\u63a7\u4ef6\n\u8fd9\u4e2a\u4e1c\u897f\u8fd8\u662f\u6bd4\u8f83\u6709\u610f\u601d\u7684\uff0c\u770b\u56fe\u8bf4\u8bdd\uff0c\u901a\u8fc7EditText\u548cTextView\u4ee5\u53caViewGroup\u7684\u6709\u673a\u7ed3\u5408\uff0c\u5c31\u53ef\u4ee5\u505a\u51fa\u8fd9\u4e2a\u6548\u679c\u4e0d\u4e00\u822c\u7684\u8f93\u5165\u4ea4\u4e92\u63a7\u4ef6\u3002\n\n\u8be6\u7ec6\u6280\u672f\u5b9e\u73b0\u8bf7\u770b:[\u6d41\u5f0f\u6807\u7b7e\u751f\u6210\u63a7\u4ef6](http://avenwu.net/customlayout/2015/01/18/tag_layout)\n\n![\u6d41\u5f0f\u6807\u7b7e\u751f\u6210\u63a7\u4ef6](http://7u2jir.com1.z0.glb.clouddn.com/tag_input_layout_demo.gif)\n\n### qq\u6d88\u606f\u6c14\u6ce1\u3010\u4e8c\u6b21\u8d1d\u585e\u5c14\u66f2\u7ebf\u591a\u8fb9\u5f62\u3011\n\u8fd9\u4e2a\u5b9e\u9645\u4e0a\u662f\u505a\u4e3a\u5206\u6790QQ\u7ea2\u70b9\u6c14\u6ce1\u7684\u4e00\u90e8\u5206\uff0c\u53ca\u6c14\u6ce1\u62d6\u62fd\u7684\u539f\u7406\u3002\n\n![\u591a\u8fb9\u5f62\u6c14\u6ce1](http://7u2jir.com1.z0.glb.clouddn.com/polygon_bezier.gif)\n\n\n### \u4fa7\u6ed1\u83dc\u5355\n\u7b80\u5355\u7684\u4fa7\u6ed1\u6548\u679c\u5b9e\u73b0\u5e76\u4e0d\u96be\uff0c\u9700\u8981\u5904\u7406\u4e24\u5757view\u5bb9\u5668\u7684\u76f8\u5bf9\u5173\u7cfb\uff0c\u4f46\u662f\u6709\u6548\u679c\u5e76\u4e0d\u4ee3\u8868\u80fd\u4f7f\u7528\uff0c\u53ef\u5b9e\u7528\u7684\u7ec4\u4ef6\u8fd8\u9700\u8981\u8003\u8651\u5f88\u591a\uff0c\u4e0b\u9762\u7684demo\u5b9e\u9645\u4e0a\u4e3b\u8981\u76ee\u7684\u5728\u4e8e\u5904\u7406\u83dc\u5355\u548c\u5185\u5bb9\u533a\u7684\u4f4d\u7f6e\u5173\u7cfb\uff0c\u7528\u7684\u662fscroll\u79fb\u52a8\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408Scroller\uff0c\u6240\u4ee5\u5bfc\u81f4\u53d8\u5316\u7684\u5b9e\u9645\u4e0a\u662f\u83dc\u5355\u5bb9\u5668\u7684\u5185\u5bb9\u800c\u4e0d\u662f\u83dc\u5355,\u6240\u4ee5support\u91cc\u8fd8\u6709\u53e6\u5916\u4e00\u4e2a\u81ea\u5b9a\u4e49\u4fa7\u6ed1\u83dc\u5355\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u66f4\u591a\u6280\u672f\u5b9e\u73b0\u53c2\u8003\u5bf9\u5e94\u7684\u6587\u7ae0:[\u81ea\u5b9a\u4e49\u4fa7\u6ed1\u83dc\u5355](http://avenwu.net/customlayout/2014/12/16/sliding_menu/)\n\n![\u4fa7\u6ed1\u83dc\u5355](http://7u2jir.com1.z0.glb.clouddn.com/drawermenu.gif)\n\n### \u4e0b\u62c9\u5237\u65b0\u5217\u8868\n\u4e0b\u62c9\u5237\u65b0\u7684\u672c\u8d28\u4e0a\u662fLinearLayout\u5d4c\u5957ListView+View\uff08\u5237\u65b0\u7684\u5934\u90e8\u89c6\u56fe\uff09\uff0c\u901a\u8fc7TouchEvent\u7684\u5206\u53d1\u63a7\u5236\uff0c\u52a8\u6001\u6539\u53d8\u89c6\u56fe\u7684Top\u4f4d\u7f6e\u3002\n\n![\u4e0b\u62c9\u5237\u65b0\u5217\u8868](http://7u2jir.com1.z0.glb.clouddn.com/pulltorefresh.gif)\n\n\n### \u5706\u5f62\u6392\u884cView\n\u8fd9\u4e2a\u5b9e\u9645\u4e0a\u6700\u5f00\u662f\u7684\u65f6\u5019\u5df2\u7ecf\u4f5c\u4e3a\u4e00\u4e2a\u5355\u72ec\u7684\u9879\u76ee\u5f00\u53d1\uff0c\u5e76\u4e14\u5df2\u7ecf\u4e0a\u4f20\u503cmaven\uff0c\u6240\u4ee5\u4e5f\u53ef\u7528gradle\u5bfc\u5165\u3002\n\n[IndexImageView\u9879\u76ee\u9996\u9875](http://avenwu.net/IndexImageView/) \n\u8fd9\u4e2a\u9879\u76ee\u6e90\u7075\u611f\u6765\u81ea\u641c\u72d0\u89c6\u5c4f\u5ba2\u6237\u7aef\uff0c\u770b\u597d\u58f0\u97f3\u7684\u65f6\u5019\u770b\u5230\u4eba\u6c14\u9009\u624b\u7684\u5934\u50cf\u662f\u4e00\u4e2a\u5706\u5f62\u5e26\u6392\u884c\u6570\u7684\u89c6\u56fe\uff0c\u611f\u89c9\u6709\u70b9\u610f\u601d\uff0c\u7d22\u6027\u82b1\u4e86\u70b9\u65f6\u95f4\u81ea\u5df1\u5199\u4e86\u4e00\u4e2a\uff0c\u540c\u65f6\u5b66\u4e60\u5982\u4f55\u5c06\u5f00\u6e90\u9879\u76ee\u53d1\u5e03\u5230Maven Central\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u901a\u8fc7gradle\u65b9\u4fbf\u7684\u83b7\u53d6maven\u4f9d\u8d56\u5e93\u3002  \n![Screenshot](https://github.com/avenwu/IndexImageView/raw/master/device-2014-10-21-164818.png)\n\n\n\n\n\n"
 },
 {
  "repo": "cuub/sugared-list-animations",
  "language": "Java",
  "readme_contents": "SugaredListAnimations\n=========\n\n<br />SugaredListAnimations is a library that pretends to add animations to your listview with minor changes to your already existing code.\nCurrently the available animations are:\n\n  - Google Plus alike\n  - Google Now alike\n\nVersion\n-\n0.9 - Yeah, it's still <i>beta</i>\n\nDemo\n-----------\n\n[See demo here]\n\n\nLicense\n-\n\nMIT\n\n*Free Software, Fuck Yeah!*\n\n  [See demo here]: https://github.com/cuub/sugared-list-animations-sample\n  \n\n    \n"
 }
]